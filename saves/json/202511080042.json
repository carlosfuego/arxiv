[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.03092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03092v2",
                "updated": "2025-11-06T18:27:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    27,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching."
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Haggstrom"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Hakan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v2",
                "updated": "2025-11-06T17:09:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    9,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures;Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04489v1",
                "updated": "2025-11-06T16:08:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:08:24Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "title": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear\n  Fusion"
                },
                "summary": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the\nfusion community. EIRENE does not implement domain decomposition, making it\nimpossible to use for simulations where the grid data does not fit on one\ncompute node (see e.g. [2]). This paper presents a domain-decomposed Monte\nCarlo (DDMC) algorithm implemented in a new open source Monte Carlo code,\nEiron. Two parallel algorithms currently used in EIRENE are also implemented in\nEiron, and the three algorithms are compared by running strong scaling tests,\nwith DDMC performing better than the other two algorithms in nearly all cases.\nOn the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids\nthat do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also\nscaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency\nof 45% in a high-collisional (heavier compute load) case, and 26% in a\nlow-collisional (lighter compute load) case. We conclude that implementing this\ndomain decomposition algorithm in EIRENE would improve performance and enable\nsimulations that are currently impossible due to memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the\nfusion community. EIRENE does not implement domain decomposition, making it\nimpossible to use for simulations where the grid data does not fit on one\ncompute node (see e.g. [2]). This paper presents a domain-decomposed Monte\nCarlo (DDMC) algorithm implemented in a new open source Monte Carlo code,\nEiron. Two parallel algorithms currently used in EIRENE are also implemented in\nEiron, and the three algorithms are compared by running strong scaling tests,\nwith DDMC performing better than the other two algorithms in nearly all cases.\nOn the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids\nthat do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also\nscaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency\nof 45% in a high-collisional (heavier compute load) case, and 26% in a\nlow-collisional (lighter compute load) case. We conclude that implementing this\ndomain decomposition algorithm in EIRENE would improve performance and enable\nsimulations that are currently impossible due to memory constraints."
                },
                "authors": [
                    {
                        "name": "Oskar Lappi"
                    },
                    {
                        "name": "Huw Leggate"
                    },
                    {
                        "name": "Yannick Marandet"
                    },
                    {
                        "name": "Jan Åström"
                    },
                    {
                        "name": "Keijo Heljanko"
                    },
                    {
                        "name": "Dmitriy V. Borodin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitriy V. Borodin"
                },
                "author": "Dmitriy V. Borodin",
                "arxiv_comment": "19 pages, 3 figures, submitted to Journal of Computational Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W10 (Primary), 68W15, 65C05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; G.3; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04464v1",
                "updated": "2025-11-06T15:37:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:37:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context"
                },
                "summary": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization."
                },
                "authors": [
                    {
                        "name": "Carnot Braun"
                    },
                    {
                        "name": "Rafael O. Jarczewski"
                    },
                    {
                        "name": "Gabriel U. Talasso"
                    },
                    {
                        "name": "Leandro A. Villas"
                    },
                    {
                        "name": "Allan M. de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Allan M. de Souza"
                },
                "author": "Allan M. de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04421v1",
                "updated": "2025-11-06T14:52:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "title": "Temporal Action Selection for Action Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Action Selection for Action Chunking"
                },
                "summary": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy."
                },
                "authors": [
                    {
                        "name": "Yueyang Weng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Yongjin Mu"
                    },
                    {
                        "name": "Yingcong Zhu"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04406v1",
                "updated": "2025-11-06T14:33:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:33:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation\n  Fine-Tuning"
                },
                "summary": "Data quality and its effective selection are fundamental to improving the\nperformance of machine translation models, serving as cornerstones for\nachieving robust and reliable translation systems. This paper presents a data\nselection methodology specifically designed for fine-tuning machine translation\nsystems, which leverages the synergy between a learner model and a pre-trained\nreference model to enhance overall training effectiveness. By defining a\nlearnability score, our approach systematically evaluates the utility of data\npoints for training, ensuring that only the most relevant and impactful\nexamples contribute to the fine-tuning process. Furthermore, our method employs\na batch selection strategy which considers interdependencies among data points,\noptimizing the efficiency of the training process while maintaining a focus on\ndata relevance. Experiments on English to Persian and several other language\npairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that\nour method can achieve up to a fivefold improvement in data efficiency compared\nto an iid baseline. Experimental results indicate that our approach improves\ncomputational efficiency by 24 when utilizing cached embeddings, as it requires\nfewer training data points. Additionally, it enhances generalization, resulting\nin superior translation performance compared to random selection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality and its effective selection are fundamental to improving the\nperformance of machine translation models, serving as cornerstones for\nachieving robust and reliable translation systems. This paper presents a data\nselection methodology specifically designed for fine-tuning machine translation\nsystems, which leverages the synergy between a learner model and a pre-trained\nreference model to enhance overall training effectiveness. By defining a\nlearnability score, our approach systematically evaluates the utility of data\npoints for training, ensuring that only the most relevant and impactful\nexamples contribute to the fine-tuning process. Furthermore, our method employs\na batch selection strategy which considers interdependencies among data points,\noptimizing the efficiency of the training process while maintaining a focus on\ndata relevance. Experiments on English to Persian and several other language\npairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that\nour method can achieve up to a fivefold improvement in data efficiency compared\nto an iid baseline. Experimental results indicate that our approach improves\ncomputational efficiency by 24 when utilizing cached embeddings, as it requires\nfewer training data points. Additionally, it enhances generalization, resulting\nin superior translation performance compared to random selection method."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Ghanizadeh"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Javad Dousti"
                },
                "author": "Mohammad Javad Dousti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v3",
                "updated": "2025-11-06T13:44:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    44,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04002v1",
                "updated": "2025-11-06T02:55:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T02:55:07Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "title": "Memory- and Latency-Constrained Inference of Large Language Models via\n  Adaptive Split Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and Latency-Constrained Inference of Large Language Models via\n  Adaptive Split Computing"
                },
                "summary": "Large language models (LLMs) have achieved near-human performance across\ndiverse reasoning tasks, yet their deployment on resource-constrained\nInternet-of-Things (IoT) devices remains impractical due to massive parameter\nfootprints and memory-intensive autoregressive decoding. While split computing\noffers a promising solution by partitioning model execution between edge\ndevices and cloud servers, existing approaches fail to address the unique\nchallenges of autoregressive inference, particularly the iterative token\ngeneration process and expanding key-value (KV) cache requirements. This work\nintroduces the first autoregressive-aware split computing framework designed\nexplicitly for LLM deployment on edge devices. Our approach makes three key\ncontributions. First, we develop one-point split compression (OPSC), a\nmixed-precision quantization scheme that prevents out-of-memory failures by\nstrategically partitioning models into front-end and back-end segments with\ndifferent precision levels. Second, we propose a two-stage intermediate\ncompression pipeline that combines threshold splitting (TS) and token-wise\nadaptive bit quantization (TAB-Q) to preserve accuracy-critical activations\nwhile dramatically reducing communication overhead. Third, we formulate a\nunified optimization framework that jointly selects optimal split points,\nquantization settings, and sequence lengths to satisfy strict memory and\nlatency constraints. Extensive evaluations across diverse LLMs and hardware\nplatforms demonstrate superior performance compared to state-of-the-art\nquantization methods, including SmoothQuant, OmniQuant, and Atom. The framework\nachieves a 1.49 inference speedup and significant communication overhead\nreduction while maintaining or improving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved near-human performance across\ndiverse reasoning tasks, yet their deployment on resource-constrained\nInternet-of-Things (IoT) devices remains impractical due to massive parameter\nfootprints and memory-intensive autoregressive decoding. While split computing\noffers a promising solution by partitioning model execution between edge\ndevices and cloud servers, existing approaches fail to address the unique\nchallenges of autoregressive inference, particularly the iterative token\ngeneration process and expanding key-value (KV) cache requirements. This work\nintroduces the first autoregressive-aware split computing framework designed\nexplicitly for LLM deployment on edge devices. Our approach makes three key\ncontributions. First, we develop one-point split compression (OPSC), a\nmixed-precision quantization scheme that prevents out-of-memory failures by\nstrategically partitioning models into front-end and back-end segments with\ndifferent precision levels. Second, we propose a two-stage intermediate\ncompression pipeline that combines threshold splitting (TS) and token-wise\nadaptive bit quantization (TAB-Q) to preserve accuracy-critical activations\nwhile dramatically reducing communication overhead. Third, we formulate a\nunified optimization framework that jointly selects optimal split points,\nquantization settings, and sequence lengths to satisfy strict memory and\nlatency constraints. Extensive evaluations across diverse LLMs and hardware\nplatforms demonstrate superior performance compared to state-of-the-art\nquantization methods, including SmoothQuant, OmniQuant, and Atom. The framework\nachieves a 1.49 inference speedup and significant communication overhead\nreduction while maintaining or improving model accuracy."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Vikas Palakonda"
                    },
                    {
                        "name": "Suhwan Im"
                    },
                    {
                        "name": "Sunghwan Moon"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v2",
                "updated": "2025-11-06T01:18:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    1,
                    18,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03944v1",
                "updated": "2025-11-06T00:42:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T00:42:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era\n  Memory Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era\n  Memory Hierarchies"
                },
                "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy."
                },
                "authors": [
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Chris J. Newburn"
                    },
                    {
                        "name": "Teresa Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jiangpeng Li"
                    },
                    {
                        "name": "Hao Zhong"
                    },
                    {
                        "name": "Wen-Mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Mei Hwu"
                },
                "author": "Wen-Mei Hwu",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v2",
                "updated": "2025-11-06T00:11:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    11,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "20 pages, 9 figures, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03830v1",
                "updated": "2025-11-05T19:53:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T19:53:51Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label\n  LLM-Based Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label\n  LLM-Based Classification"
                },
                "summary": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains."
                },
                "authors": [
                    {
                        "name": "Mikołaj Langner"
                    },
                    {
                        "name": "Jan Eliasz"
                    },
                    {
                        "name": "Ewa Rudnicka"
                    },
                    {
                        "name": "Jan Kocoń"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kocoń"
                },
                "author": "Jan Kocoń",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v2",
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03475v1",
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v2",
                "updated": "2025-11-05T09:18:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    18,
                    48,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically, steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.10x-2.68x\nspeedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving\nsuperior visual fidelity. It significantly outperforms existing methods in\nLPIPS, SSIM, and PSNR, under similar computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically, steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.10x-2.68x\nspeedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving\nsuperior visual fidelity. It significantly outperforms existing methods in\nLPIPS, SSIM, and PSNR, under similar computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache Accepted by\n  NeurIPS 2025",
                "arxiv_journal_ref": "In Proceedings of NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03159v1",
                "updated": "2025-11-05T03:40:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T03:40:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "title": "Joint Optimization of DNN Model Caching and Request Routing in Mobile\n  Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of DNN Model Caching and Request Routing in Mobile\n  Edge Computing"
                },
                "summary": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines."
                },
                "authors": [
                    {
                        "name": "Shuting Qiu"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Siyu Tan"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    },
                    {
                        "name": "Qilin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qilin Fan"
                },
                "author": "Qilin Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02919v1",
                "updated": "2025-11-04T19:02:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T19:02:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "title": "Cache Mechanism for Agent RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Mechanism for Agent RAG Systems"
                },
                "summary": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents."
                },
                "authors": [
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Zhencan Peng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02761v1",
                "updated": "2025-11-04T17:40:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:40:31Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "title": "Non-Contact Manipulation of Induced Magnetic Dipoles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Contact Manipulation of Induced Magnetic Dipoles"
                },
                "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles."
                },
                "authors": [
                    {
                        "name": "Seth Stewart"
                    },
                    {
                        "name": "Joseph Pawelski"
                    },
                    {
                        "name": "Steve Ward"
                    },
                    {
                        "name": "Andrew J. Petruska"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Petruska"
                },
                "author": "Andrew J. Petruska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02749v1",
                "updated": "2025-11-04T17:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "title": "Using Span Queries to Optimize for Cache and Attention Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Span Queries to Optimize for Cache and Attention Locality"
                },
                "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model."
                },
                "authors": [
                    {
                        "name": "Paul Castro"
                    },
                    {
                        "name": "Nick Mitchell"
                    },
                    {
                        "name": "Nathan Ordonez"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    }
                ],
                "author_detail": {
                    "name": "Antoni Viros i Martin"
                },
                "author": "Antoni Viros i Martin",
                "arxiv_comment": "12 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02651v1",
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality."
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valérie Bécaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02647v1",
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks"
                },
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v5",
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02381v1",
                "updated": "2025-11-04T09:03:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:03:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility"
                },
                "summary": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments."
                },
                "authors": [
                    {
                        "name": "R. Agnello"
                    },
                    {
                        "name": "M. Barbisan"
                    },
                    {
                        "name": "R. Pasqualotto"
                    },
                    {
                        "name": "B. Pouradier-Duteil"
                    },
                    {
                        "name": "E. Sartori"
                    },
                    {
                        "name": "A. Tiso"
                    },
                    {
                        "name": "B. Zaniol"
                    }
                ],
                "author_detail": {
                    "name": "B. Zaniol"
                },
                "author": "B. Zaniol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02230v1",
                "updated": "2025-11-04T03:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live"
                },
                "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02132v1",
                "updated": "2025-11-03T23:48:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T23:48:39Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects"
                },
                "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference."
                },
                "authors": [
                    {
                        "name": "Mansi Choudhary"
                    },
                    {
                        "name": "Karthik Sangaiah"
                    },
                    {
                        "name": "Sonali Singh"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Lisa Wu Wills"
                    },
                    {
                        "name": "Ganesh Dasika"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Dasika"
                },
                "author": "Ganesh Dasika",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01815v1",
                "updated": "2025-11-03T18:20:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T18:20:35Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "title": "KV Cache Transform Coding for Compact Storage in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Transform Coding for Compact Storage in LLM Inference"
                },
                "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Łańcucki"
                },
                "author": "Adrian Łańcucki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01633v1",
                "updated": "2025-11-03T14:42:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T14:42:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving"
                },
                "summary": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale."
                },
                "authors": [
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Ziheng Meng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yue Yun"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Xiabao Wu"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Shaonan Ma"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01385v1",
                "updated": "2025-11-03T09:36:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation."
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "arxiv_comment": "Accepted at NeurIPS 2025. Presents a real-domain in-place FFT (rdFFT)\n  operator for memory-efficient fine-tuning of large language models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.1.2; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01266v1",
                "updated": "2025-11-03T06:37:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls"
                },
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience."
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Schechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v3",
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00868v1",
                "updated": "2025-11-02T09:33:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T09:33:12Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "title": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management"
                },
                "summary": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios."
                },
                "authors": [
                    {
                        "name": "Nazmul Takbir"
                    },
                    {
                        "name": "Hamidreza Alikhani"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00819v1",
                "updated": "2025-11-02T06:15:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T06:15:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "title": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies"
                },
                "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Wen Zan"
                    },
                    {
                        "name": "Pingwei Sun"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00745v1",
                "updated": "2025-11-02T00:04:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T00:04:54Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation"
                },
                "summary": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Tian"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Boshuo Wang"
                    },
                    {
                        "name": "Jinshui Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jeannette Ingabire"
                    },
                    {
                        "name": "Samantha Coffler"
                    },
                    {
                        "name": "Guillaume Duret"
                    },
                    {
                        "name": "Quoc-Khanh Pham"
                    },
                    {
                        "name": "Gang Bao"
                    },
                    {
                        "name": "Jacob T. Robinson"
                    },
                    {
                        "name": "Stefan M. Goetz"
                    },
                    {
                        "name": "Angel V. Peterchev"
                    }
                ],
                "author_detail": {
                    "name": "Angel V. Peterchev"
                },
                "author": "Angel V. Peterchev",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v2",
                "updated": "2025-11-01T12:05:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    12,
                    5,
                    18,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00473v1",
                "updated": "2025-11-01T09:53:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction."
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi",
                "arxiv_comment": "40 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "57K20(primary), 16T05, 16W70, 18M75, 20F36, 20F40, 57M05 (secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v3",
                "updated": "2025-11-01T08:49:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    49,
                    20,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v2",
                "updated": "2025-11-01T08:26:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    26,
                    24,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/FastMAS/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v2",
                "updated": "2025-11-01T07:01:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    7,
                    1,
                    0,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Shaolin Lu"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v7",
                "updated": "2025-11-01T04:26:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    4,
                    26,
                    3,
                    5,
                    305,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00321v1",
                "updated": "2025-10-31T23:50:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T23:50:44Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits"
                },
                "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Dowon Kim"
                    },
                    {
                        "name": "MinJae Lee"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "HyuckSung Kwon"
                    },
                    {
                        "name": "Hyeonggyu Jeong"
                    },
                    {
                        "name": "Sang-Soo Park"
                    },
                    {
                        "name": "Minyong Yoon"
                    },
                    {
                        "name": "Si-Dong Roh"
                    },
                    {
                        "name": "Yongsuk Kwon"
                    },
                    {
                        "name": "Jinin So"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v2",
                "updated": "2025-10-31T18:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    19,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v2",
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27171v1",
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models"
                },
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v2",
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v2",
                "updated": "2025-10-31T01:52:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    1,
                    52,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27070v1",
                "updated": "2025-10-31T00:39:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v2",
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26944v1",
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies"
                },
                "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif Çördük"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jörg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Ott"
                },
                "author": "Jörg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00090v1",
                "updated": "2025-10-30T04:57:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation"
                },
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26835v1",
                "updated": "2025-10-29T19:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T19:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads"
                },
                "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xunzhuo Liu"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    },
                    {
                        "name": "Huamin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Chen"
                },
                "author": "Huamin Chen",
                "arxiv_comment": "13 pages including reference, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.04681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04681v1",
                "updated": "2025-11-06T18:59:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    59,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:59:59Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    59,
                    59,
                    3,
                    310,
                    0
                ],
                "title": "Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference\n  from weak lensing and galaxy clustering maps with deep learning. I. Analysis\n  design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference\n  from weak lensing and galaxy clustering maps with deep learning. I. Analysis\n  design"
                },
                "summary": "Data-driven approaches using deep learning are emerging as powerful\ntechniques to extract non-Gaussian information from cosmological large-scale\nstructure. This work presents the first simulation-based inference (SBI)\npipeline that combines weak lensing and galaxy clustering maps in a realistic\nDark Energy Survey Year 3 (DES Y3) configuration and serves as preparation for\na forthcoming analysis of the survey data. We develop a scalable forward model\nbased on the CosmoGridV1 suite of N-body simulations to generate over one\nmillion self-consistent mock realizations of DES Y3 at the map level.\nLeveraging this large dataset, we train deep graph convolutional neural\nnetworks on the full survey footprint in spherical geometry to learn\nlow-dimensional features that approximately maximize mutual information with\ntarget parameters. These learned compressions enable neural density estimation\nof the implicit likelihood via normalizing flows in a ten-dimensional parameter\nspace spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy bias\nparameters, while marginalizing over baryonic, photometric redshift, and shear\nbias nuisances. To ensure robustness, we extensively validate our inference\npipeline using synthetic observations derived from both systematic\ncontaminations in our forward model and independent Buzzard galaxy catalogs.\nOur forecasts yield significant improvements in cosmological parameter\nconstraints, achieving $2-3\\times$ higher figures of merit in the $\\Omega_m -\nS_8$ plane relative to our implementation of baseline two-point statistics and\neffectively breaking parameter degeneracies through probe combination. These\nresults demonstrate the potential of SBI analyses powered by deep learning for\nupcoming Stage-IV wide-field imaging surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven approaches using deep learning are emerging as powerful\ntechniques to extract non-Gaussian information from cosmological large-scale\nstructure. This work presents the first simulation-based inference (SBI)\npipeline that combines weak lensing and galaxy clustering maps in a realistic\nDark Energy Survey Year 3 (DES Y3) configuration and serves as preparation for\na forthcoming analysis of the survey data. We develop a scalable forward model\nbased on the CosmoGridV1 suite of N-body simulations to generate over one\nmillion self-consistent mock realizations of DES Y3 at the map level.\nLeveraging this large dataset, we train deep graph convolutional neural\nnetworks on the full survey footprint in spherical geometry to learn\nlow-dimensional features that approximately maximize mutual information with\ntarget parameters. These learned compressions enable neural density estimation\nof the implicit likelihood via normalizing flows in a ten-dimensional parameter\nspace spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy bias\nparameters, while marginalizing over baryonic, photometric redshift, and shear\nbias nuisances. To ensure robustness, we extensively validate our inference\npipeline using synthetic observations derived from both systematic\ncontaminations in our forward model and independent Buzzard galaxy catalogs.\nOur forecasts yield significant improvements in cosmological parameter\nconstraints, achieving $2-3\\times$ higher figures of merit in the $\\Omega_m -\nS_8$ plane relative to our implementation of baseline two-point statistics and\neffectively breaking parameter degeneracies through probe combination. These\nresults demonstrate the potential of SBI analyses powered by deep learning for\nupcoming Stage-IV wide-field imaging surveys."
                },
                "authors": [
                    {
                        "name": "A. Thomsen"
                    },
                    {
                        "name": "J. Bucko"
                    },
                    {
                        "name": "T. Kacprzak"
                    },
                    {
                        "name": "V. Ajani"
                    },
                    {
                        "name": "J. Fluri"
                    },
                    {
                        "name": "A. Refregier"
                    },
                    {
                        "name": "D. Anbajagane"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "A. Ferté"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "N. Jeffrey"
                    },
                    {
                        "name": "A. Alarcon"
                    },
                    {
                        "name": "A. Amon"
                    },
                    {
                        "name": "K. Bechtol"
                    },
                    {
                        "name": "M. R. Becker"
                    },
                    {
                        "name": "G. M. Bernstein"
                    },
                    {
                        "name": "A. Campos"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "C. Chang"
                    },
                    {
                        "name": "R. Chen"
                    },
                    {
                        "name": "A. Choi"
                    },
                    {
                        "name": "M. Crocce"
                    },
                    {
                        "name": "C. Davis"
                    },
                    {
                        "name": "J. DeRose"
                    },
                    {
                        "name": "S. Dodelson"
                    },
                    {
                        "name": "C. Doux"
                    },
                    {
                        "name": "K. Eckert"
                    },
                    {
                        "name": "J. Elvin-Poole"
                    },
                    {
                        "name": "S. Everett"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "D. Gruen"
                    },
                    {
                        "name": "I. Harrison"
                    },
                    {
                        "name": "K. Herner"
                    },
                    {
                        "name": "E. M. Huff"
                    },
                    {
                        "name": "M. Jarvis"
                    },
                    {
                        "name": "N. Kuropatkin"
                    },
                    {
                        "name": "P. -F. Leget"
                    },
                    {
                        "name": "N. MacCrann"
                    },
                    {
                        "name": "J. McCullough"
                    },
                    {
                        "name": "J. Myles"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Pandey"
                    },
                    {
                        "name": "A. Porredon"
                    },
                    {
                        "name": "J. Prat"
                    },
                    {
                        "name": "M. Raveri"
                    },
                    {
                        "name": "M. Rodriguez-Monroy"
                    },
                    {
                        "name": "R. P. Rollins"
                    },
                    {
                        "name": "A. Roodman"
                    },
                    {
                        "name": "E. S. Rykoff"
                    },
                    {
                        "name": "C. Sánchez"
                    },
                    {
                        "name": "L. F. Secco"
                    },
                    {
                        "name": "E. Sheldon"
                    },
                    {
                        "name": "T. Shin"
                    },
                    {
                        "name": "M. A. Troxel"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "T. N. Varga"
                    },
                    {
                        "name": "N. Weaverdyck"
                    },
                    {
                        "name": "R. H. Wechsler"
                    },
                    {
                        "name": "B. Yanny"
                    },
                    {
                        "name": "B. Yin"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "J. Zuntz"
                    },
                    {
                        "name": "S. Allam"
                    },
                    {
                        "name": "F. Andrade-Oliveira"
                    },
                    {
                        "name": "D. Bacon"
                    },
                    {
                        "name": "J. Blazek"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "R. Camilleri"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "R. Cawthon"
                    },
                    {
                        "name": "L. N. da Costa"
                    },
                    {
                        "name": "M. E. da Silva Pereira"
                    },
                    {
                        "name": "T. M. Davis"
                    },
                    {
                        "name": "J. De Vicente"
                    },
                    {
                        "name": "S. Desai"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "S. R. Hinton"
                    },
                    {
                        "name": "D. L. Hollowood"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "D. J. James"
                    },
                    {
                        "name": "K. Kuehn"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "J. L. Marshall"
                    },
                    {
                        "name": "J. Mena-Fernández"
                    },
                    {
                        "name": "F. Menanteau"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Muir"
                    },
                    {
                        "name": "R. L. C. Ogando"
                    },
                    {
                        "name": "A. A. Plazas Malagón"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Sanchez Cid"
                    },
                    {
                        "name": "I. Sevilla-Noarbe"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "E. Suchyta"
                    },
                    {
                        "name": "M. E. C. Swanson"
                    },
                    {
                        "name": "D. Thomas"
                    },
                    {
                        "name": "C. To"
                    },
                    {
                        "name": "D. L. Tucker"
                    }
                ],
                "author_detail": {
                    "name": "D. L. Tucker"
                },
                "arxiv_affiliation": "DES Collaboration",
                "author": "D. L. Tucker",
                "arxiv_comment": "38 pages, 14 figures, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04670v1",
                "updated": "2025-11-06T18:55:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    55,
                    17,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:55:17Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    55,
                    17,
                    3,
                    310,
                    0
                ],
                "title": "Cambrian-S: Towards Spatial Supersensing in Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cambrian-S: Towards Spatial Supersensing in Video"
                },
                "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience."
                },
                "authors": [
                    {
                        "name": "Shusheng Yang"
                    },
                    {
                        "name": "Jihan Yang"
                    },
                    {
                        "name": "Pinzhi Huang"
                    },
                    {
                        "name": "Ellis Brown"
                    },
                    {
                        "name": "Zihao Yang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "Zihan Zheng"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Muhan Wang"
                    },
                    {
                        "name": "Daohan Lu"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Li Fei-Fei"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Website: https://cambrian-mllm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04668v1",
                "updated": "2025-11-06T18:53:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    53,
                    31,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:53:31Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    53,
                    31,
                    3,
                    310,
                    0
                ],
                "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding"
                },
                "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks."
                },
                "authors": [
                    {
                        "name": "Ellis Brown"
                    },
                    {
                        "name": "Arijit Ray"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ross Girshick"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Project page: https://ellisbrown.github.io/sims-v",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04664v1",
                "updated": "2025-11-06T18:51:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    51,
                    44,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:51:44Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    51,
                    44,
                    3,
                    310,
                    0
                ],
                "title": "SAFe-Copilot: Unified Shared Autonomy Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFe-Copilot: Unified Shared Autonomy Framework"
                },
                "summary": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent."
                },
                "authors": [
                    {
                        "name": "Phat Nguyen"
                    },
                    {
                        "name": "Erfan Aasi"
                    },
                    {
                        "name": "Shiva Sreeram"
                    },
                    {
                        "name": "Guy Rosman"
                    },
                    {
                        "name": "Andrew Silva"
                    },
                    {
                        "name": "Sertac Karaman"
                    },
                    {
                        "name": "Daniela Rus"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Rus"
                },
                "author": "Daniela Rus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04662v1",
                "updated": "2025-11-06T18:50:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    50,
                    8,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:50:08Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    50,
                    8,
                    3,
                    310,
                    0
                ],
                "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks"
                },
                "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Nathaniel Weir"
                    },
                    {
                        "name": "Kaj Bostrom"
                    },
                    {
                        "name": "Sam Bayless"
                    },
                    {
                        "name": "Darion Cassel"
                    },
                    {
                        "name": "Sapana Chaudhary"
                    },
                    {
                        "name": "Benjamin Kiesl-Reiter"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    }
                ],
                "author_detail": {
                    "name": "Huzefa Rangwala"
                },
                "author": "Huzefa Rangwala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04661v1",
                "updated": "2025-11-06T18:48:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    48,
                    39,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:48:39Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    48,
                    39,
                    3,
                    310,
                    0
                ],
                "title": "$\\texttt{unimpeded}$: A Public Grid of Nested Sampling Chains for\n  Cosmological Model Comparison and Tension Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{unimpeded}$: A Public Grid of Nested Sampling Chains for\n  Cosmological Model Comparison and Tension Analysis"
                },
                "summary": "Bayesian inference is central to modern cosmology, yet comprehensive model\ncomparison and tension quantification remain computationally prohibitive for\nmany researchers. To address this, we release $\\texttt{unimpeded}$, a publicly\navailable Python library and data repository providing pre-computed nested\nsampling and MCMC chains. We apply this resource to conduct a systematic\nanalysis across a grid of eight cosmological models, including $\\Lambda$CDM and\nseven extensions, and 39 datasets, including individual probes and their\npairwise combinations. Our model comparison reveals that whilst individual\ndatasets show varied preferences for model extensions, the base $\\Lambda$CDM\nmodel is most frequently preferred in combined analyses, with the general trend\nsuggesting that evidence for new physics is diluted when probes are combined.\nUsing five complementary statistics, we quantify tensions, finding the most\nsignificant to be between DES and Planck (3.57$\\sigma$) and SH0ES and Planck\n(3.27$\\sigma$) within $\\Lambda$CDM. We characterise the $S_8$ tension as\nhigh-dimensional ($d_G=6.62$) and resolvable in extended models, whereas the\nHubble tension is low-dimensional and persists across the model space. Caution\nshould be exercised when combining datasets in tension. The\n$\\texttt{unimpeded}$ data products, hosted on Zenodo, provide a powerful\nresource for reproducible cosmological analysis and underscore the robustness\nof the $\\Lambda$CDM model against the current compendium of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference is central to modern cosmology, yet comprehensive model\ncomparison and tension quantification remain computationally prohibitive for\nmany researchers. To address this, we release $\\texttt{unimpeded}$, a publicly\navailable Python library and data repository providing pre-computed nested\nsampling and MCMC chains. We apply this resource to conduct a systematic\nanalysis across a grid of eight cosmological models, including $\\Lambda$CDM and\nseven extensions, and 39 datasets, including individual probes and their\npairwise combinations. Our model comparison reveals that whilst individual\ndatasets show varied preferences for model extensions, the base $\\Lambda$CDM\nmodel is most frequently preferred in combined analyses, with the general trend\nsuggesting that evidence for new physics is diluted when probes are combined.\nUsing five complementary statistics, we quantify tensions, finding the most\nsignificant to be between DES and Planck (3.57$\\sigma$) and SH0ES and Planck\n(3.27$\\sigma$) within $\\Lambda$CDM. We characterise the $S_8$ tension as\nhigh-dimensional ($d_G=6.62$) and resolvable in extended models, whereas the\nHubble tension is low-dimensional and persists across the model space. Caution\nshould be exercised when combining datasets in tension. The\n$\\texttt{unimpeded}$ data products, hosted on Zenodo, provide a powerful\nresource for reproducible cosmological analysis and underscore the robustness\nof the $\\Lambda$CDM model against the current compendium of data."
                },
                "authors": [
                    {
                        "name": "Dily Duan Yi Ong"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "47 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07325v2",
                "updated": "2025-11-06T18:38:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    38,
                    30,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-09T01:49:29Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    1,
                    49,
                    29,
                    1,
                    252,
                    0
                ],
                "title": "CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement\n  Estimation"
                },
                "summary": "The National Comprehensive Cancer Network (NCCN) provides evidence-based\nguidelines for cancer treatment. Translating complex patient presentations into\nguideline-compliant treatment recommendations is time-intensive, requires\nspecialized expertise, and is prone to error. Advances in large language model\n(LLM) capabilities promise to reduce the time required to generate treatment\nrecommendations and improve accuracy. We present an LLM agent-based approach to\nautomatically generate guideline-concordant treatment trajectories for patients\nwith non-small cell lung cancer (NSCLC). Our contributions are threefold.\nFirst, we construct a novel longitudinal dataset of 121 cases of NSCLC patients\nthat includes clinical encounters, diagnostic results, and medical histories,\neach expertly annotated with the corresponding NCCN guideline trajectories by\nboard-certified oncologists. Second, we demonstrate that existing LLMs possess\ndomain-specific knowledge that enables high-quality proxy benchmark generation\nfor both model development and evaluation, achieving strong correlation\n(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.\nThird, we develop a hybrid approach combining expensive human annotations with\nmodel consistency information to create both the agent framework that predicts\nthe relevant guidelines for a patient, as well as a meta-classifier that\nverifies prediction accuracy with calibrated confidence scores for treatment\nrecommendations (AUROC=0.800), a critical capability for communicating the\naccuracy of outputs, custom-tailoring tradeoffs in performance, and supporting\nregulatory compliance. This work establishes a framework for clinically viable\nLLM-based guideline adherence systems that balance accuracy, interpretability,\nand regulatory requirements while reducing annotation costs, providing a\nscalable pathway toward automated clinical decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The National Comprehensive Cancer Network (NCCN) provides evidence-based\nguidelines for cancer treatment. Translating complex patient presentations into\nguideline-compliant treatment recommendations is time-intensive, requires\nspecialized expertise, and is prone to error. Advances in large language model\n(LLM) capabilities promise to reduce the time required to generate treatment\nrecommendations and improve accuracy. We present an LLM agent-based approach to\nautomatically generate guideline-concordant treatment trajectories for patients\nwith non-small cell lung cancer (NSCLC). Our contributions are threefold.\nFirst, we construct a novel longitudinal dataset of 121 cases of NSCLC patients\nthat includes clinical encounters, diagnostic results, and medical histories,\neach expertly annotated with the corresponding NCCN guideline trajectories by\nboard-certified oncologists. Second, we demonstrate that existing LLMs possess\ndomain-specific knowledge that enables high-quality proxy benchmark generation\nfor both model development and evaluation, achieving strong correlation\n(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.\nThird, we develop a hybrid approach combining expensive human annotations with\nmodel consistency information to create both the agent framework that predicts\nthe relevant guidelines for a patient, as well as a meta-classifier that\nverifies prediction accuracy with calibrated confidence scores for treatment\nrecommendations (AUROC=0.800), a critical capability for communicating the\naccuracy of outputs, custom-tailoring tradeoffs in performance, and supporting\nregulatory compliance. This work establishes a framework for clinically viable\nLLM-based guideline adherence systems that balance accuracy, interpretability,\nand regulatory requirements while reducing annotation costs, providing a\nscalable pathway toward automated clinical decision support."
                },
                "authors": [
                    {
                        "name": "Alyssa Unell"
                    },
                    {
                        "name": "Noel C. F. Codella"
                    },
                    {
                        "name": "Sam Preston"
                    },
                    {
                        "name": "Peniel Argaw"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Zelalem Gero"
                    },
                    {
                        "name": "Cliff Wong"
                    },
                    {
                        "name": "Rajesh Jena"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Amanda K. Hall"
                    },
                    {
                        "name": "Ruican Rachel Zhong"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Mu Wei"
                    },
                    {
                        "name": "Matthew Lungren"
                    },
                    {
                        "name": "Hoifung Poon"
                    }
                ],
                "author_detail": {
                    "name": "Hoifung Poon"
                },
                "author": "Hoifung Poon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04647v1",
                "updated": "2025-11-06T18:38:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    38,
                    24,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:38:24Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    38,
                    24,
                    3,
                    310,
                    0
                ],
                "title": "Optimal Inference Schedules for Masked Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Inference Schedules for Masked Diffusion Models"
                },
                "summary": "A major bottleneck of standard auto-regressive large language models is that\ntheir inference process is inherently sequential, resulting in very long and\ncostly inference times. To circumvent this, practitioners proposed a class of\nlanguage models called diffusion language models, of which the masked diffusion\nmodel (MDM) is the most successful. The MDM is able to sample tokens\nout-of-order and, ostensibly, many tokens at once and in parallel. However,\nthere is very limited rigorous understanding of how much parallel sampling\nthese models can perform without noticeable degradation in their sampling\nperformance. Prior work of Li and Cai obtained some preliminary bounds, but\nthese are not tight for many natural classes of distributions. In this work, we\ngive a new, exact characterization of the expected divergence between the true\ndistribution and the sampled distribution, for any distribution and any\nunmasking schedule for the sampler, showing an elegant connection to the theory\nof univariate function approximation.\n  By leveraging this connection, we then attain a number of novel lower and\nupper bounds for this problem. While the connection to function approximation\nin principle gives the optimal unmasking schedule for any distribution, we show\nthat it is in general impossible to compete with it without strong a priori\nknowledge of the distribution, even in seemingly benign settings. However, we\nalso demonstrate new upper bounds and new sampling schedules in terms of\nwell-studied information-theoretic properties of the base distribution, namely,\nits total correlation and dual total correlation, which show that in some\nnatural settings, one can sample in $O(log n)$ steps without any visible loss\nin performance, where $n$ is the total sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major bottleneck of standard auto-regressive large language models is that\ntheir inference process is inherently sequential, resulting in very long and\ncostly inference times. To circumvent this, practitioners proposed a class of\nlanguage models called diffusion language models, of which the masked diffusion\nmodel (MDM) is the most successful. The MDM is able to sample tokens\nout-of-order and, ostensibly, many tokens at once and in parallel. However,\nthere is very limited rigorous understanding of how much parallel sampling\nthese models can perform without noticeable degradation in their sampling\nperformance. Prior work of Li and Cai obtained some preliminary bounds, but\nthese are not tight for many natural classes of distributions. In this work, we\ngive a new, exact characterization of the expected divergence between the true\ndistribution and the sampled distribution, for any distribution and any\nunmasking schedule for the sampler, showing an elegant connection to the theory\nof univariate function approximation.\n  By leveraging this connection, we then attain a number of novel lower and\nupper bounds for this problem. While the connection to function approximation\nin principle gives the optimal unmasking schedule for any distribution, we show\nthat it is in general impossible to compete with it without strong a priori\nknowledge of the distribution, even in seemingly benign settings. However, we\nalso demonstrate new upper bounds and new sampling schedules in terms of\nwell-studied information-theoretic properties of the base distribution, namely,\nits total correlation and dual total correlation, which show that in some\nnatural settings, one can sample in $O(log n)$ steps without any visible loss\nin performance, where $n$ is the total sequence length."
                },
                "authors": [
                    {
                        "name": "Sitan Chen"
                    },
                    {
                        "name": "Kevin Cong"
                    },
                    {
                        "name": "Jerry Li"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Li"
                },
                "author": "Jerry Li",
                "arxiv_comment": "33 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04646v1",
                "updated": "2025-11-06T18:37:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    37,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:37:18Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    37,
                    18,
                    3,
                    310,
                    0
                ],
                "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for\n  Embodied LLM-Based Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for\n  Embodied LLM-Based Multi-Agent Collaboration"
                },
                "summary": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies."
                },
                "authors": [
                    {
                        "name": "Narjes Nourzad"
                    },
                    {
                        "name": "Hanqing Yang"
                    },
                    {
                        "name": "Shiyu Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04643v1",
                "updated": "2025-11-06T18:35:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    35,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:35:45Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    35,
                    45,
                    3,
                    310,
                    0
                ],
                "title": "When retrieval outperforms generation: Dense evidence retrieval for\n  scalable fake news detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When retrieval outperforms generation: Dense evidence retrieval for\n  scalable fake news detection"
                },
                "summary": "The proliferation of misinformation necessitates robust yet computationally\nefficient fact verification systems. While current state-of-the-art approaches\nleverage Large Language Models (LLMs) for generating explanatory rationales,\nthese methods face significant computational barriers and hallucination risks\nin real-world deployments. We present DeReC (Dense Retrieval Classification), a\nlightweight framework that demonstrates how general-purpose text embeddings can\neffectively replace autoregressive LLM-based approaches in fact verification\ntasks. By combining dense retrieval with specialized classification, our system\nachieves better accuracy while being significantly more efficient. DeReC\noutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%\non RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%\non LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),\nshowcasing its effectiveness across varying dataset sizes. On the RAWFC\ndataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art\nmethod L-Defense (61.20%). Our results demonstrate that carefully engineered\nretrieval-based systems can match or exceed LLM performance in specialized\ntasks while being significantly more practical for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation necessitates robust yet computationally\nefficient fact verification systems. While current state-of-the-art approaches\nleverage Large Language Models (LLMs) for generating explanatory rationales,\nthese methods face significant computational barriers and hallucination risks\nin real-world deployments. We present DeReC (Dense Retrieval Classification), a\nlightweight framework that demonstrates how general-purpose text embeddings can\neffectively replace autoregressive LLM-based approaches in fact verification\ntasks. By combining dense retrieval with specialized classification, our system\nachieves better accuracy while being significantly more efficient. DeReC\noutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%\non RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%\non LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),\nshowcasing its effectiveness across varying dataset sizes. On the RAWFC\ndataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art\nmethod L-Defense (61.20%). Our results demonstrate that carefully engineered\nretrieval-based systems can match or exceed LLM performance in specialized\ntasks while being significantly more practical for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Alamgir Munir Qazi"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Jamal Abdul Nasir"
                    }
                ],
                "author_detail": {
                    "name": "Jamal Abdul Nasir"
                },
                "author": "Jamal Abdul Nasir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04637v1",
                "updated": "2025-11-06T18:32:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    32,
                    19,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:32:19Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    32,
                    19,
                    3,
                    310,
                    0
                ],
                "title": "Advancing Risk Gene Discovery Across the Allele Frequency Spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Risk Gene Discovery Across the Allele Frequency Spectrum"
                },
                "summary": "The discovery of genetic risk factors has transformed human genetics, yet the\npace of new gene identification has slowed despite the exponential expansion of\nsequencing and biobank resources. Current approaches are optimized for the\nextremes of the allele frequency spectrum: rare, high-penetrance variants\nidentified through burden testing, and common, low-effect variants mapped by\ngenome-wide association studies. Between these extremes lies variants of\nintermediate frequency and effect size where statistical power is limited,\npathogenicity is often misclassified, and gene discovery lags behind empirical\nevidence of heritable contribution. This 'missing middle' represents a critical\nblind spot across disease areas, from neurodevelopmental and psychiatric\ndisorders to cancer and aging. In this review, we organize strategies for risk\ngene identification by variant frequency class, highlighting methodological\nstrengths and constraints at each scale. We draw on lessons across fields to\nillustrate how innovations in variant annotation, joint modeling, phenotype\nrefinement, and network-based inference can extend discovery into the\nintermediate range. By framing the frequency spectrum as a unifying axis, we\nprovide a conceptual map of current capabilities, their limitations, and\nemerging directions toward more comprehensive risk gene discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of genetic risk factors has transformed human genetics, yet the\npace of new gene identification has slowed despite the exponential expansion of\nsequencing and biobank resources. Current approaches are optimized for the\nextremes of the allele frequency spectrum: rare, high-penetrance variants\nidentified through burden testing, and common, low-effect variants mapped by\ngenome-wide association studies. Between these extremes lies variants of\nintermediate frequency and effect size where statistical power is limited,\npathogenicity is often misclassified, and gene discovery lags behind empirical\nevidence of heritable contribution. This 'missing middle' represents a critical\nblind spot across disease areas, from neurodevelopmental and psychiatric\ndisorders to cancer and aging. In this review, we organize strategies for risk\ngene identification by variant frequency class, highlighting methodological\nstrengths and constraints at each scale. We draw on lessons across fields to\nillustrate how innovations in variant annotation, joint modeling, phenotype\nrefinement, and network-based inference can extend discovery into the\nintermediate range. By framing the frequency spectrum as a unifying axis, we\nprovide a conceptual map of current capabilities, their limitations, and\nemerging directions toward more comprehensive risk gene discovery."
                },
                "authors": [
                    {
                        "name": "Madison Caballero"
                    },
                    {
                        "name": "Behrang Mahjani"
                    }
                ],
                "author_detail": {
                    "name": "Behrang Mahjani"
                },
                "author": "Behrang Mahjani",
                "arxiv_comment": "Review; 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03092v2",
                "updated": "2025-11-06T18:27:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    27,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching."
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Haggstrom"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Hakan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04628v1",
                "updated": "2025-11-06T18:23:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    23,
                    55,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:23:55Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    23,
                    55,
                    3,
                    310,
                    0
                ],
                "title": "NovisVQ: A Streaming Convolutional Neural Network for No-Reference\n  Opinion-Unaware Frame Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovisVQ: A Streaming Convolutional Neural Network for No-Reference\n  Opinion-Unaware Frame Quality Assessment"
                },
                "summary": "Video quality assessment (VQA) is vital for computer vision tasks, but\nexisting approaches face major limitations: full-reference (FR) metrics require\nclean reference videos, and most no-reference (NR) models depend on training on\ncostly human opinion labels. Moreover, most opinion-unaware NR methods are\nimage-based, ignoring temporal context critical for video object detection. In\nthis work, we present a scalable, streaming-based VQA model that is both\nno-reference and opinion-unaware. Our model leverages synthetic degradations of\nthe DAVIS dataset, training a temporal-aware convolutional architecture to\npredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without\nreferences at inference. We show that our streaming approach outperforms our\nown image-based baseline by generalizing across diverse degradations,\nunderscoring the value of temporal modeling for scalable VQA in real-world\nvision systems. Additionally, we demonstrate that our model achieves higher\ncorrelation with full-reference metrics compared to BRISQUE, a widely-used\nopinion-aware image quality assessment baseline, validating the effectiveness\nof our temporal, opinion-unaware approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video quality assessment (VQA) is vital for computer vision tasks, but\nexisting approaches face major limitations: full-reference (FR) metrics require\nclean reference videos, and most no-reference (NR) models depend on training on\ncostly human opinion labels. Moreover, most opinion-unaware NR methods are\nimage-based, ignoring temporal context critical for video object detection. In\nthis work, we present a scalable, streaming-based VQA model that is both\nno-reference and opinion-unaware. Our model leverages synthetic degradations of\nthe DAVIS dataset, training a temporal-aware convolutional architecture to\npredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without\nreferences at inference. We show that our streaming approach outperforms our\nown image-based baseline by generalizing across diverse degradations,\nunderscoring the value of temporal modeling for scalable VQA in real-world\nvision systems. Additionally, we demonstrate that our model achieves higher\ncorrelation with full-reference metrics compared to BRISQUE, a widely-used\nopinion-aware image quality assessment baseline, validating the effectiveness\nof our temporal, opinion-unaware approach."
                },
                "authors": [
                    {
                        "name": "Kylie Cancilla"
                    },
                    {
                        "name": "Alexander Moore"
                    },
                    {
                        "name": "Amar Saini"
                    },
                    {
                        "name": "Carmen Carrano"
                    }
                ],
                "author_detail": {
                    "name": "Carmen Carrano"
                },
                "author": "Carmen Carrano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08604v2",
                "updated": "2025-11-06T18:15:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    15,
                    30,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-10T14:02:18Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    18,
                    2,
                    253,
                    0
                ],
                "title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information."
                },
                "authors": [
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Mengmeng Du"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Yihang Fu"
                    },
                    {
                        "name": "Erica Stutz"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Siru Liu"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    },
                    {
                        "name": "Hyunghoon Cho"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Qingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingyu Chen"
                },
                "author": "Qingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04619v1",
                "updated": "2025-11-06T18:12:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    12,
                    9,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:12:09Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    12,
                    9,
                    3,
                    310,
                    0
                ],
                "title": "Dynamic causal discovery in Alzheimer's disease through latent\n  pseudotime modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic causal discovery in Alzheimer's disease through latent\n  pseudotime modelling"
                },
                "summary": "The application of causal discovery to diseases like Alzheimer's (AD) is\nlimited by the static graph assumptions of most methods; such models cannot\naccount for an evolving pathophysiology, modulated by a latent disease\npseudotime. We propose to apply an existing latent variable model to real-world\nAD data, inferring a pseudotime that orders patients along a data-driven\ndisease trajectory independent of chronological age, then learning how causal\nrelationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC\n0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge\nsubstantially improved graph accuracy and orientation. Our framework reveals\ndynamic interactions between novel (NfL, GFAP) and established AD markers,\nenabling practical causal discovery despite violated assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of causal discovery to diseases like Alzheimer's (AD) is\nlimited by the static graph assumptions of most methods; such models cannot\naccount for an evolving pathophysiology, modulated by a latent disease\npseudotime. We propose to apply an existing latent variable model to real-world\nAD data, inferring a pseudotime that orders patients along a data-driven\ndisease trajectory independent of chronological age, then learning how causal\nrelationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC\n0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge\nsubstantially improved graph accuracy and orientation. Our framework reveals\ndynamic interactions between novel (NfL, GFAP) and established AD markers,\nenabling practical causal discovery despite violated assumptions."
                },
                "authors": [
                    {
                        "name": "Natalia Glazman"
                    },
                    {
                        "name": "Jyoti Mangal"
                    },
                    {
                        "name": "Pedro Borges"
                    },
                    {
                        "name": "Sebastien Ourselin"
                    },
                    {
                        "name": "M. Jorge Cardoso"
                    }
                ],
                "author_detail": {
                    "name": "M. Jorge Cardoso"
                },
                "author": "M. Jorge Cardoso",
                "arxiv_comment": "Accepted to the NeurIPS 2025 Workshop on CauScien: Uncovering\n  Causality in Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07110v2",
                "updated": "2025-11-06T18:08:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    8,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-03-18T20:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    38,
                    31,
                    1,
                    77,
                    0
                ],
                "title": "DashCLIP: Leveraging multimodal models for generating semantic\n  embeddings for DoorDash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DashCLIP: Leveraging multimodal models for generating semantic\n  embeddings for DoorDash"
                },
                "summary": "Despite the success of vision-language models in various generative tasks,\nobtaining high-quality semantic representations for products and user intents\nis still challenging due to the inability of off-the-shelf models to capture\nnuanced relationships between the entities. In this paper, we introduce a joint\ntraining framework for product and user queries by aligning uni-modal and\nmulti-modal encoders through contrastive learning on image-text data. Our novel\napproach trains a query encoder with an LLM-curated relevance dataset,\neliminating the reliance on engagement history. These embeddings demonstrate\nstrong generalization capabilities and improve performance across applications,\nincluding product categorization and relevance prediction. For personalized ads\nrecommendation, a significant uplift in the click-through rate and conversion\nrate after the deployment further confirms the impact on key business metrics.\nWe believe that the flexibility of our framework makes it a promising solution\ntoward enriching the user experience across the e-commerce landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of vision-language models in various generative tasks,\nobtaining high-quality semantic representations for products and user intents\nis still challenging due to the inability of off-the-shelf models to capture\nnuanced relationships between the entities. In this paper, we introduce a joint\ntraining framework for product and user queries by aligning uni-modal and\nmulti-modal encoders through contrastive learning on image-text data. Our novel\napproach trains a query encoder with an LLM-curated relevance dataset,\neliminating the reliance on engagement history. These embeddings demonstrate\nstrong generalization capabilities and improve performance across applications,\nincluding product categorization and relevance prediction. For personalized ads\nrecommendation, a significant uplift in the click-through rate and conversion\nrate after the deployment further confirms the impact on key business metrics.\nWe believe that the flexibility of our framework makes it a promising solution\ntoward enriching the user experience across the e-commerce landscape."
                },
                "authors": [
                    {
                        "name": "Omkar Gurjar"
                    },
                    {
                        "name": "Kin Sum Liu"
                    },
                    {
                        "name": "Praveen Kolli"
                    },
                    {
                        "name": "Utsaw Kumar"
                    },
                    {
                        "name": "Mandar Rahurkar"
                    }
                ],
                "author_detail": {
                    "name": "Mandar Rahurkar"
                },
                "author": "Mandar Rahurkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04610v1",
                "updated": "2025-11-06T18:01:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    1,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:01:59Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    1,
                    59,
                    3,
                    310,
                    0
                ],
                "title": "Addressing the DESI DR2 Phantom-Crossing Anomaly and Enhanced $H_0$\n  Tension with Reconstructed Scalar-Tensor Gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the DESI DR2 Phantom-Crossing Anomaly and Enhanced $H_0$\n  Tension with Reconstructed Scalar-Tensor Gravity"
                },
                "summary": "Recent cosmological data, including DESI DR2, highlight significant tensions\nwithin the $\\Lambda$CDM paradigm. When analyzed in the context of General\nRelativity (GR), the latest DESI data favor a dynamical dark energy (DDE)\nequation of state, $w(z)$, that crosses the phantom divide line $w=-1$.\nHowever, this framework prefers a lower Hubble constant, $H_0$, than Planck\n2018, thereby worsening the tension with local measurements. This phantom\ncrossing is a key feature that cannot be achieved by minimally coupled scalar\nfields (quintessence) within GR. This suggests the need for a new degree of\nfreedom that can simultaneously: (A) increase the best-fit value of $H_0$ in\nthe context of the DESI DR2 data, and (B) allow the crossing of the $w=-1$ line\nwithin a new theoretical approach. We argue that both of these goals may be\nachieved in the context of Modified Gravity (MG), and in particular,\nScalar-Tensor (ST) theories, where phantom crossing is a natural and viable\nfeature. We demonstrate these facts by analyzing a joint dataset including DESI\nDR2, Pantheon+, CMB, and growth-rate (RSD) data in the context of simple\nparametrizations for the effective gravitational constant, $\\mu_G(z) \\equiv\nG_{eff}/G_N$, and the DDE equation of state, $w(z)$. This MG framework\nsignificantly alleviates the tension, leading to a higher inferred value of\n$H_0 = 70.6 \\pm 1.7 \\, \\text{km s}^{-1} \\text{Mpc}^{-1}$. We also present a\nsystematic, data-driven reconstruction of the required underlying ST Lagrangian\nand provide simple, generic analytical expressions for both the non-minimal\ncoupling $F(\\Phi) = 1+\\xi\\Phi^{2}e^{n\\Phi}$ and the scalar potential $U(\\Phi) =\nU_{0}+ae^{b\\Phi^{2}}$, which well-describe the reconstructed functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent cosmological data, including DESI DR2, highlight significant tensions\nwithin the $\\Lambda$CDM paradigm. When analyzed in the context of General\nRelativity (GR), the latest DESI data favor a dynamical dark energy (DDE)\nequation of state, $w(z)$, that crosses the phantom divide line $w=-1$.\nHowever, this framework prefers a lower Hubble constant, $H_0$, than Planck\n2018, thereby worsening the tension with local measurements. This phantom\ncrossing is a key feature that cannot be achieved by minimally coupled scalar\nfields (quintessence) within GR. This suggests the need for a new degree of\nfreedom that can simultaneously: (A) increase the best-fit value of $H_0$ in\nthe context of the DESI DR2 data, and (B) allow the crossing of the $w=-1$ line\nwithin a new theoretical approach. We argue that both of these goals may be\nachieved in the context of Modified Gravity (MG), and in particular,\nScalar-Tensor (ST) theories, where phantom crossing is a natural and viable\nfeature. We demonstrate these facts by analyzing a joint dataset including DESI\nDR2, Pantheon+, CMB, and growth-rate (RSD) data in the context of simple\nparametrizations for the effective gravitational constant, $\\mu_G(z) \\equiv\nG_{eff}/G_N$, and the DDE equation of state, $w(z)$. This MG framework\nsignificantly alleviates the tension, leading to a higher inferred value of\n$H_0 = 70.6 \\pm 1.7 \\, \\text{km s}^{-1} \\text{Mpc}^{-1}$. We also present a\nsystematic, data-driven reconstruction of the required underlying ST Lagrangian\nand provide simple, generic analytical expressions for both the non-minimal\ncoupling $F(\\Phi) = 1+\\xi\\Phi^{2}e^{n\\Phi}$ and the scalar potential $U(\\Phi) =\nU_{0}+ae^{b\\Phi^{2}}$, which well-describe the reconstructed functions."
                },
                "authors": [
                    {
                        "name": "Dimitrios Efstratiou"
                    },
                    {
                        "name": "Evangelos Achilleas Paraskevas"
                    },
                    {
                        "name": "Leandros Perivolaropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Perivolaropoulos"
                },
                "author": "Leandros Perivolaropoulos",
                "arxiv_comment": "28 pages, 13 Figures. The numerical analysis file used for the\n  construction of the figures may be found at\n  https://github.com/Dimitrios1993/Reconstructing-Scalar-Tensor-Theories\n  (Python and Mathematica v13)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04603v1",
                "updated": "2025-11-06T17:57:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    57,
                    16,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:57:16Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    57,
                    16,
                    3,
                    310,
                    0
                ],
                "title": "Analyzing the topological structure of composite dynamical systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing the topological structure of composite dynamical systems"
                },
                "summary": "This chapter explores dynamical structural equation models (DSEMs) and their\nnonlinear generalizations into sheaves of dynamical systems. It demonstrates\nthese two disciplines on part of the food web in the Bering Sea. The\ntranslation from DSEMs to sheaves passes through a formal construction borrowed\nfrom electronics called a netlist that specifies how data route through a\nsystem. A sheaf can be considered a formal hypothesis about how variables\ninteract, that then specifies how observations can be tested for consistency,\nhow missing data can be inferred, and how uncertainty about the observations\ncan be quantified. Sheaf modeling provides a coherent mathematical framework\nfor studying the interaction of various dynamical subsystems that together\ndetermine a larger system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter explores dynamical structural equation models (DSEMs) and their\nnonlinear generalizations into sheaves of dynamical systems. It demonstrates\nthese two disciplines on part of the food web in the Bering Sea. The\ntranslation from DSEMs to sheaves passes through a formal construction borrowed\nfrom electronics called a netlist that specifies how data route through a\nsystem. A sheaf can be considered a formal hypothesis about how variables\ninteract, that then specifies how observations can be tested for consistency,\nhow missing data can be inferred, and how uncertainty about the observations\ncan be quantified. Sheaf modeling provides a coherent mathematical framework\nfor studying the interaction of various dynamical subsystems that together\ndetermine a larger system."
                },
                "authors": [
                    {
                        "name": "Michael Robinson"
                    },
                    {
                        "name": "Michael L. Szulczewski"
                    },
                    {
                        "name": "James T. Thorson"
                    }
                ],
                "author_detail": {
                    "name": "James T. Thorson"
                },
                "author": "James T. Thorson",
                "arxiv_comment": "56 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18F20, 46M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04601v1",
                "updated": "2025-11-06T17:54:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    54,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:54:12Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    54,
                    12,
                    3,
                    310,
                    0
                ],
                "title": "PixCLIP: Achieving Fine-grained Visual Language Understanding via\n  Any-granularity Pixel-Text Alignment Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PixCLIP: Achieving Fine-grained Visual Language Understanding via\n  Any-granularity Pixel-Text Alignment Learning"
                },
                "summary": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved\nremarkable success in a variety of downstream vison language understanding\ntasks, enhancing its capability for fine-grained image-text alignment remains\nan active research focus. To this end, most existing works adopt the strategy\nof explicitly increasing the granularity of visual information processing,\ne.g., incorporating visual prompts to guide the model focus on specific local\nregions within the image. Meanwhile, researches on Multimodal Large Language\nModels(MLLMs) have demonstrated that training with long and detailed textual\ndescriptions can effectively improve the model's fine-grained vision-language\nalignment. However, the inherent token length limitation of CLIP's text encoder\nfundamentally limits CLIP to process more granular textual information embedded\nin long text sequences. To synergistically leverage the advantages of enhancing\nboth visual and textual content processing granularity, we propose PixCLIP, a\nnovel framework designed to concurrently accommodate visual prompt inputs and\nprocess lengthy textual descriptions. Specifically, we first establish an\nautomated annotation pipeline capable of generating pixel-level localized,\nlong-form textual descriptions for images. Utilizing this pipeline, we\nconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 million\nsamples. Secondly, we replace CLIP's original text encoder with the LLM and\npropose a three-branch pixel-text alignment learning framework, facilitating\nfine-grained alignment between image regions and corresponding textual\ndescriptions at arbitrary granularity. Experiments demonstrate that PixCLIP\nshowcases breakthroughs in pixel-level interaction and handling long-form\ntexts, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved\nremarkable success in a variety of downstream vison language understanding\ntasks, enhancing its capability for fine-grained image-text alignment remains\nan active research focus. To this end, most existing works adopt the strategy\nof explicitly increasing the granularity of visual information processing,\ne.g., incorporating visual prompts to guide the model focus on specific local\nregions within the image. Meanwhile, researches on Multimodal Large Language\nModels(MLLMs) have demonstrated that training with long and detailed textual\ndescriptions can effectively improve the model's fine-grained vision-language\nalignment. However, the inherent token length limitation of CLIP's text encoder\nfundamentally limits CLIP to process more granular textual information embedded\nin long text sequences. To synergistically leverage the advantages of enhancing\nboth visual and textual content processing granularity, we propose PixCLIP, a\nnovel framework designed to concurrently accommodate visual prompt inputs and\nprocess lengthy textual descriptions. Specifically, we first establish an\nautomated annotation pipeline capable of generating pixel-level localized,\nlong-form textual descriptions for images. Utilizing this pipeline, we\nconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 million\nsamples. Secondly, we replace CLIP's original text encoder with the LLM and\npropose a three-branch pixel-text alignment learning framework, facilitating\nfine-grained alignment between image regions and corresponding textual\ndescriptions at arbitrary granularity. Experiments demonstrate that PixCLIP\nshowcases breakthroughs in pixel-level interaction and handling long-form\ntexts, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Caorui Li"
                    },
                    {
                        "name": "Lingxiang Wu"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04599v1",
                "updated": "2025-11-06T17:51:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    51,
                    32,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:51:32Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    51,
                    32,
                    3,
                    310,
                    0
                ],
                "title": "Geometric Decomposition of Statistical Inference through Gradient Flow\n  and Co-Monotonicity Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric Decomposition of Statistical Inference through Gradient Flow\n  and Co-Monotonicity Measures"
                },
                "summary": "Understanding feature-outcome associations in high-dimensional data remains\n  challenging when relationships vary across subpopulations, yet standard\n  methods assuming global associations miss context-dependent patterns,\nreducing\n  statistical power and interpretability. We develop a geometric decomposition\n  framework offering two strategies for partitioning inference problems into\n  regional analyses on data-derived Riemannian graphs. Gradient flow\n  decomposition uses path-monotonicity-validated discrete Morse theory to\n  partition samples into basins where outcomes exhibit monotonic behavior.\n  Co-monotonicity decomposition leverages association structure: vertex-level\n  coefficients measuring directional concordance between outcome and features,\n  or between feature pairs, define embeddings of samples into association\nspace.\n  These embeddings induce Riemannian k-NN graphs on which biclustering\n  identifies co-monotonicity cells (coherent regions) and feature modules. This\n  extends naturally to multi-modal integration across multiple feature sets.\n  Both strategies apply independently or jointly, with Bayesian posterior\n  sampling providing credible intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding feature-outcome associations in high-dimensional data remains\n  challenging when relationships vary across subpopulations, yet standard\n  methods assuming global associations miss context-dependent patterns,\nreducing\n  statistical power and interpretability. We develop a geometric decomposition\n  framework offering two strategies for partitioning inference problems into\n  regional analyses on data-derived Riemannian graphs. Gradient flow\n  decomposition uses path-monotonicity-validated discrete Morse theory to\n  partition samples into basins where outcomes exhibit monotonic behavior.\n  Co-monotonicity decomposition leverages association structure: vertex-level\n  coefficients measuring directional concordance between outcome and features,\n  or between feature pairs, define embeddings of samples into association\nspace.\n  These embeddings induce Riemannian k-NN graphs on which biclustering\n  identifies co-monotonicity cells (coherent regions) and feature modules. This\n  extends naturally to multi-modal integration across multiple feature sets.\n  Both strategies apply independently or jointly, with Bayesian posterior\n  sampling providing credible intervals."
                },
                "authors": [
                    {
                        "name": "Pawel Gajer"
                    },
                    {
                        "name": "Jacques Ravel"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Ravel"
                },
                "author": "Jacques Ravel",
                "arxiv_comment": "48 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08, 62H30, 58E05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04592v1",
                "updated": "2025-11-06T17:49:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    49,
                    25,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:49:25Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    49,
                    25,
                    3,
                    310,
                    0
                ],
                "title": "The Pre-Outburst Properties of the FU Ori Object HBC 722",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pre-Outburst Properties of the FU Ori Object HBC 722"
                },
                "summary": "FU Ori outbursts are thought to play an important role in stellar assembly\nand the evolution of protoplanetary disks. However, the progenitor young\nstellar objects are largely uncharacterized. We obtained a low-resolution\noptical spectrum of HBC 722 before its FU Ori outburst as part of a survey of\nyoung stellar objects in the North America Nebula. The spectrum yields a\nspectral type of M3.3$\\pm$0.4, which when combined with archival photometry\nallows us to measure the stellar and accretion properties of a young star prior\nto its FU Ori outburst. The pre-outburst accretion rate of $7\\times10^{-9}$\nM$_\\odot$ yr$^{-1}$ is high for a protoplanetary disk around an M3-M3.5 star,\nthough about 15,000 times weaker than the accretion rate during the outburst.\nThe pre-outburst variability, inferred from archival B-band photometry, is\nabout a factor 5 with a standard deviation of 0.16 dex and is consistent with\nvariable accretion onto young low-mass stars. The stellar radius is larger than\nthe radius of accreting young stars of similar spectral type by a factor of\ntwo. The extinction to HBC 722 is $\\sim 1.45\\pm0.3$~mag, lower than the\n2.5--3.7~mag extinction values measured during the outburst. The u-band\nphotometry plays an especially important role in constraining the veiling at\nlonger wavelengths and therefore also the extinction and photospheric\nluminosity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FU Ori outbursts are thought to play an important role in stellar assembly\nand the evolution of protoplanetary disks. However, the progenitor young\nstellar objects are largely uncharacterized. We obtained a low-resolution\noptical spectrum of HBC 722 before its FU Ori outburst as part of a survey of\nyoung stellar objects in the North America Nebula. The spectrum yields a\nspectral type of M3.3$\\pm$0.4, which when combined with archival photometry\nallows us to measure the stellar and accretion properties of a young star prior\nto its FU Ori outburst. The pre-outburst accretion rate of $7\\times10^{-9}$\nM$_\\odot$ yr$^{-1}$ is high for a protoplanetary disk around an M3-M3.5 star,\nthough about 15,000 times weaker than the accretion rate during the outburst.\nThe pre-outburst variability, inferred from archival B-band photometry, is\nabout a factor 5 with a standard deviation of 0.16 dex and is consistent with\nvariable accretion onto young low-mass stars. The stellar radius is larger than\nthe radius of accreting young stars of similar spectral type by a factor of\ntwo. The extinction to HBC 722 is $\\sim 1.45\\pm0.3$~mag, lower than the\n2.5--3.7~mag extinction values measured during the outburst. The u-band\nphotometry plays an especially important role in constraining the veiling at\nlonger wavelengths and therefore also the extinction and photospheric\nluminosity."
                },
                "authors": [
                    {
                        "name": "Gregory J. Herczeg"
                    },
                    {
                        "name": "Bo Reipurth"
                    }
                ],
                "author_detail": {
                    "name": "Bo Reipurth"
                },
                "author": "Bo Reipurth",
                "arxiv_comment": "Accepted by ApJ Letters. 10 pages, including 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04589v1",
                "updated": "2025-11-06T17:45:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    45,
                    30,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:45:30Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    45,
                    30,
                    3,
                    310,
                    0
                ],
                "title": "Automatic detection of CMEs using synthetically-trained Mask R-CNN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic detection of CMEs using synthetically-trained Mask R-CNN"
                },
                "summary": "Coronal mass ejections (CMEs) are a major driver of space weather. To assess\nCME geoeffectiveness, among other scientific goals, it is necessary to reliably\nidentify and characterize their morphology and kinematics in coronagraph\nimages. Current methods of CME identification are either subjected to human\nbiases or perform a poor identification due to deficiencies in the automatic\ndetection. In this approach, we have trained the deep convolutional neural\nmodel Mask R-CNN to automatically segment the outer envelope of one or multiple\nCMEs present in a single difference coronagraph image. The empirical training\ndataset is composed of 10^5 synthetic coronagraph images with known pixel-level\nCME segmentation masks. It is obtained by combining quiet coronagraph\nobservations, with synthetic white-light CMEs produced using the GCS geometric\nmodel and ray-tracing technique. We found that our model-based trained Mask\nR-CNN infers segmentation masks that are smooth and topologically connected.\nWhile the inferred masks are not representative of the detailed outer envelope\nof complex CMEs, the neural model can better differentiate a CME from other\nradially moving background/foreground features, segment multiple simultaneous\nCMEs that are close to each other, and work with images from different\ninstruments. This is accomplished without relying on kinematic information,\ni.e. only the included in the single input difference image. We obtain a median\nIoU=0.98 for 1.6*10^4 synthetic validation images, and IoU=0.77 when compared\nwith two independent manual segmentations of 115 observations acquired by the\nCOR2-A, COR2-B and LASCO C2 coronagraphs. The methodology presented in this\nwork can be used with other CME models to produce more realistic synthetic\nbrightness images while preserving desired morphological features, and obtain\nmore robust and/or tailored segmentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronal mass ejections (CMEs) are a major driver of space weather. To assess\nCME geoeffectiveness, among other scientific goals, it is necessary to reliably\nidentify and characterize their morphology and kinematics in coronagraph\nimages. Current methods of CME identification are either subjected to human\nbiases or perform a poor identification due to deficiencies in the automatic\ndetection. In this approach, we have trained the deep convolutional neural\nmodel Mask R-CNN to automatically segment the outer envelope of one or multiple\nCMEs present in a single difference coronagraph image. The empirical training\ndataset is composed of 10^5 synthetic coronagraph images with known pixel-level\nCME segmentation masks. It is obtained by combining quiet coronagraph\nobservations, with synthetic white-light CMEs produced using the GCS geometric\nmodel and ray-tracing technique. We found that our model-based trained Mask\nR-CNN infers segmentation masks that are smooth and topologically connected.\nWhile the inferred masks are not representative of the detailed outer envelope\nof complex CMEs, the neural model can better differentiate a CME from other\nradially moving background/foreground features, segment multiple simultaneous\nCMEs that are close to each other, and work with images from different\ninstruments. This is accomplished without relying on kinematic information,\ni.e. only the included in the single input difference image. We obtain a median\nIoU=0.98 for 1.6*10^4 synthetic validation images, and IoU=0.77 when compared\nwith two independent manual segmentations of 115 observations acquired by the\nCOR2-A, COR2-B and LASCO C2 coronagraphs. The methodology presented in this\nwork can be used with other CME models to produce more realistic synthetic\nbrightness images while preserving desired morphological features, and obtain\nmore robust and/or tailored segmentations."
                },
                "authors": [
                    {
                        "name": "Francisco A. Iglesias"
                    },
                    {
                        "name": "Diego G. Lloveras"
                    },
                    {
                        "name": "Florencia L. Cisterna"
                    },
                    {
                        "name": "Hebe Cremades"
                    },
                    {
                        "name": "Mariano Sanchez Toledo"
                    },
                    {
                        "name": "Fernando M. López"
                    },
                    {
                        "name": "Yasmin Machuca"
                    },
                    {
                        "name": "Franco Manini"
                    },
                    {
                        "name": "Andrés Asensio Ramos"
                    }
                ],
                "author_detail": {
                    "name": "Andrés Asensio Ramos"
                },
                "arxiv_affiliation": "Instituto de Astrofísica de Canarias",
                "author": "Andrés Asensio Ramos",
                "arxiv_comment": "30 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04588v1",
                "updated": "2025-11-06T17:45:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    45,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:45:12Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    45,
                    12,
                    3,
                    310,
                    0
                ],
                "title": "Question the Questions: Auditing Representation in Online Deliberative\n  Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question the Questions: Auditing Representation in Online Deliberative\n  Processes"
                },
                "summary": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations."
                },
                "authors": [
                    {
                        "name": "Soham De"
                    },
                    {
                        "name": "Lodewijk Gelauff"
                    },
                    {
                        "name": "Ashish Goel"
                    },
                    {
                        "name": "Smitha Milli"
                    },
                    {
                        "name": "Ariel Procaccia"
                    },
                    {
                        "name": "Alice Siu"
                    }
                ],
                "author_detail": {
                    "name": "Alice Siu"
                },
                "author": "Alice Siu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27052v2",
                "updated": "2025-11-06T17:44:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    44,
                    55,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-30T23:45:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    23,
                    45,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "VISTA Score: Verification In Sequential Turn-based Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA Score: Verification In Sequential Turn-based Assessment"
                },
                "summary": "Hallucination--defined here as generating statements unsupported or\ncontradicted by available evidence or conversational context--remains a major\nobstacle to deploying conversational AI systems in settings that demand factual\nreliability. Existing metrics either evaluate isolated responses or treat\nunverifiable content as errors, limiting their use for multi-turn dialogue. We\nintroduce VISTA (Verification In Sequential Turn-based Assessment), a framework\nfor evaluating conversational factuality through claim-level verification and\nsequential consistency tracking. VISTA decomposes each assistant turn into\natomic factual claims, verifies them against trusted sources and dialogue\nhistory, and categorizes unverifiable statements (subjective, contradicted,\nlacking evidence, or abstaining). Across eight large language models and four\ndialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA\nsubstantially improves hallucination detection over FACTSCORE and LLM-as-Judge\nbaselines. Human evaluation confirms that VISTA's decomposition improves\nannotator agreement and reveals inconsistencies in existing benchmarks. By\nmodeling factuality as a dynamic property of conversation, VISTA offers a more\ntransparent, human-aligned measure of truthfulness in dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination--defined here as generating statements unsupported or\ncontradicted by available evidence or conversational context--remains a major\nobstacle to deploying conversational AI systems in settings that demand factual\nreliability. Existing metrics either evaluate isolated responses or treat\nunverifiable content as errors, limiting their use for multi-turn dialogue. We\nintroduce VISTA (Verification In Sequential Turn-based Assessment), a framework\nfor evaluating conversational factuality through claim-level verification and\nsequential consistency tracking. VISTA decomposes each assistant turn into\natomic factual claims, verifies them against trusted sources and dialogue\nhistory, and categorizes unverifiable statements (subjective, contradicted,\nlacking evidence, or abstaining). Across eight large language models and four\ndialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA\nsubstantially improves hallucination detection over FACTSCORE and LLM-as-Judge\nbaselines. Human evaluation confirms that VISTA's decomposition improves\nannotator agreement and reveals inconsistencies in existing benchmarks. By\nmodeling factuality as a dynamic property of conversation, VISTA offers a more\ntransparent, human-aligned measure of truthfulness in dialogue systems."
                },
                "authors": [
                    {
                        "name": "Ashley Lewis"
                    },
                    {
                        "name": "Andrew Perrault"
                    },
                    {
                        "name": "Eric Fosler-Lussier"
                    },
                    {
                        "name": "Michael White"
                    }
                ],
                "author_detail": {
                    "name": "Michael White"
                },
                "author": "Michael White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04586v1",
                "updated": "2025-11-06T17:43:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    43,
                    9,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:43:09Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    43,
                    9,
                    3,
                    310,
                    0
                ],
                "title": "Towards extreme event prediction of turbulent flows with quantized local\n  reduced-order models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards extreme event prediction of turbulent flows with quantized local\n  reduced-order models"
                },
                "summary": "This work develops quantized local reduced-order models (ql-ROMs) of the\nturbulent Minimal Flow Unit (MFU) for the analysis and interpretation of\nintermittent dissipative dynamics and extreme events. The ql-ROM combines\ndata-driven clustering of the flow state space with intrusive Galerkin\nprojection on locally defined Proper Orthogonal Decomposition (POD) bases. This\nconstruction enables an accurate and stable low-dimensional representation of\nnonlinear flow dynamics whilst preserving the structure of the governing\nequations. The model is trained on direct numerical simulation data of the MFU.\nWhen deployed, the ql-ROM is numerically stable for long-term integration, and\ncorrectly infers the statistical behavior of the kinetic energy and dissipation\nobserved of the full-order system. A local modal energy-budget formulation is\nemployed to quantify intermodal energy transfer and viscous dissipation within\neach region of the attractor. The analysis reveals that dissipation bursts\ncorrespond to localized energy transfer from streamwise streaks and\ntravelling-wave modes toward highly dissipative vortical structures, consistent\nwith the self-sustaining process of near-wall turbulence. Beyond reduced\nmodeling, the ql-ROM framework provides a pathway for the reduced-space\ncharacterization and potential prediction of extreme events. ql-ROM offer an\ninterpretable and computationally efficient framework for the analysis and\nprediction of extreme events in turbulent flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work develops quantized local reduced-order models (ql-ROMs) of the\nturbulent Minimal Flow Unit (MFU) for the analysis and interpretation of\nintermittent dissipative dynamics and extreme events. The ql-ROM combines\ndata-driven clustering of the flow state space with intrusive Galerkin\nprojection on locally defined Proper Orthogonal Decomposition (POD) bases. This\nconstruction enables an accurate and stable low-dimensional representation of\nnonlinear flow dynamics whilst preserving the structure of the governing\nequations. The model is trained on direct numerical simulation data of the MFU.\nWhen deployed, the ql-ROM is numerically stable for long-term integration, and\ncorrectly infers the statistical behavior of the kinetic energy and dissipation\nobserved of the full-order system. A local modal energy-budget formulation is\nemployed to quantify intermodal energy transfer and viscous dissipation within\neach region of the attractor. The analysis reveals that dissipation bursts\ncorrespond to localized energy transfer from streamwise streaks and\ntravelling-wave modes toward highly dissipative vortical structures, consistent\nwith the self-sustaining process of near-wall turbulence. Beyond reduced\nmodeling, the ql-ROM framework provides a pathway for the reduced-space\ncharacterization and potential prediction of extreme events. ql-ROM offer an\ninterpretable and computationally efficient framework for the analysis and\nprediction of extreme events in turbulent flows."
                },
                "authors": [
                    {
                        "name": "Antonio Colanera"
                    },
                    {
                        "name": "Luca Magri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Magri"
                },
                "author": "Luca Magri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15616v2",
                "updated": "2025-11-06T17:34:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    34,
                    54,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-21T13:41:07Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    41,
                    7,
                    0,
                    202,
                    0
                ],
                "title": "On zeros and algorithms for disordered systems: mean-field spin glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On zeros and algorithms for disordered systems: mean-field spin glasses"
                },
                "summary": "Spin glasses are fundamental probability distributions at the core of\nstatistical physics, the theory of average-case computational complexity, and\nmodern high-dimensional statistical inference. In the mean-field setting, we\ndesign deterministic quasipolynomial-time algorithms for estimating the\npartition function to arbitrarily high accuracy for all inverse temperatures in\nthe second moment regime. In particular, for the Sherrington--Kirkpatrick\nmodel, our algorithms succeed for the entire replica-symmetric phase. To\nachieve this, we study the locations of the zeros of the partition function.\nNotably, our methods are conceptually simple, and apply equally well to the\nspherical case and the case of Ising spins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin glasses are fundamental probability distributions at the core of\nstatistical physics, the theory of average-case computational complexity, and\nmodern high-dimensional statistical inference. In the mean-field setting, we\ndesign deterministic quasipolynomial-time algorithms for estimating the\npartition function to arbitrarily high accuracy for all inverse temperatures in\nthe second moment regime. In particular, for the Sherrington--Kirkpatrick\nmodel, our algorithms succeed for the entire replica-symmetric phase. To\nachieve this, we study the locations of the zeros of the partition function.\nNotably, our methods are conceptually simple, and apply equally well to the\nspherical case and the case of Ising spins."
                },
                "authors": [
                    {
                        "name": "Ferenc Bencs"
                    },
                    {
                        "name": "Brice Huang"
                    },
                    {
                        "name": "Daniel Z. Lee"
                    },
                    {
                        "name": "Kuikui Liu"
                    },
                    {
                        "name": "Guus Regts"
                    }
                ],
                "author_detail": {
                    "name": "Guus Regts"
                },
                "author": "Guus Regts",
                "arxiv_comment": "Compared to the previous version, we establish an improved zero-free\n  result for the second moment regime",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20855v2",
                "updated": "2025-11-06T17:29:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    29,
                    29,
                    3,
                    310,
                    0
                ],
                "published": "2025-08-28T14:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    52,
                    37,
                    3,
                    240,
                    0
                ],
                "title": "Uniform Quasi ML based inference for the panel AR(1) model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Quasi ML based inference for the panel AR(1) model"
                },
                "summary": "This paper proposes new inference methods for panel AR models with arbitrary\ninitial conditions and heteroskedasticity and possibly additional regressors\nthat are robust to the strength of identification. Specifically, we consider\nseveral Maximum Likelihood based methods of constructing tests and confidence\nsets (CSs) and show that (Quasi) LM tests and CSs that use the expected Hessian\nrather than the observed Hessian of the log-likelihood have correct asymptotic\nsize (in a uniform sense). We derive the power envelope of a Fixed Effects\nversion of such a LM test for hypotheses involving the autoregressive parameter\nwhen the average information matrix is estimated by a centered OPG estimator\nand the model is only second-order identified, and show that it coincides with\nthe maximal attainable power curve in the worst case setting. We also study the\nempirical size and power properties of these (Quasi) LM tests and CSs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes new inference methods for panel AR models with arbitrary\ninitial conditions and heteroskedasticity and possibly additional regressors\nthat are robust to the strength of identification. Specifically, we consider\nseveral Maximum Likelihood based methods of constructing tests and confidence\nsets (CSs) and show that (Quasi) LM tests and CSs that use the expected Hessian\nrather than the observed Hessian of the log-likelihood have correct asymptotic\nsize (in a uniform sense). We derive the power envelope of a Fixed Effects\nversion of such a LM test for hypotheses involving the autoregressive parameter\nwhen the average information matrix is estimated by a centered OPG estimator\nand the model is only second-order identified, and show that it coincides with\nthe maximal attainable power curve in the worst case setting. We also study the\nempirical size and power properties of these (Quasi) LM tests and CSs."
                },
                "authors": [
                    {
                        "name": "Hugo Kruiniger"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Kruiniger"
                },
                "author": "Hugo Kruiniger",
                "arxiv_comment": "44 pages; in the second version, I have edited the introduction,\n  corrected some typos, added some previously omitted symbols, improved some\n  notation, and removed some redundant references. The essence of the paper has\n  not changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F03, 62F05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03325v2",
                "updated": "2025-11-06T17:28:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    28,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T09:40:16Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    40,
                    16,
                    2,
                    309,
                    0
                ],
                "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding"
                },
                "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA."
                },
                "authors": [
                    {
                        "name": "Mauro Orazio Drago"
                    },
                    {
                        "name": "Luca Carlini"
                    },
                    {
                        "name": "Pelinsu Celebi Balyemez"
                    },
                    {
                        "name": "Dennis Pierantozzi"
                    },
                    {
                        "name": "Chiara Lena"
                    },
                    {
                        "name": "Cesare Hassan"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Elena De Momi"
                    },
                    {
                        "name": "Sophia Bano"
                    },
                    {
                        "name": "Mobarak I. Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Mobarak I. Hoque"
                },
                "author": "Mobarak I. Hoque",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04570v1",
                "updated": "2025-11-06T17:25:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    25,
                    23,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:25:23Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    25,
                    23,
                    3,
                    310,
                    0
                ],
                "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm"
                },
                "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Yongzhuo Yang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Xinchi Chen"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "36 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04564v1",
                "updated": "2025-11-06T17:20:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    20,
                    2,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:20:02Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    20,
                    2,
                    3,
                    310,
                    0
                ],
                "title": "Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in\n  Scientific AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in\n  Scientific AI"
                },
                "summary": "Physics-informed machine learning (PIML) integrates partial differential\nequations (PDEs) into machine learning models to solve inverse problems, such\nas estimating coefficient functions (e.g., the Hamiltonian function) that\ncharacterize physical systems. This framework enables data-driven understanding\nand prediction of complex physical phenomena. While coefficient functions in\nPIML are typically estimated on the basis of predictive performance, physics as\na discipline does not rely solely on prediction accuracy to evaluate models.\nFor example, Kepler's heliocentric model was favored owing to small\ndiscrepancies in planetary motion, despite its similar predictive accuracy to\nthe geocentric model. This highlights the inherent uncertainties in data-driven\nmodel inference and the scientific importance of selecting physically\nmeaningful solutions. In this paper, we propose a framework to quantify and\nanalyze such uncertainties in the estimation of coefficient functions in PIML.\nWe apply our framework to reduced model of magnetohydrodynamics and our\nframework shows that there are uncertainties, and unique identification is\npossible with geometric constraints. Finally, we confirm that we can estimate\nthe reduced model uniquely by incorporating these constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed machine learning (PIML) integrates partial differential\nequations (PDEs) into machine learning models to solve inverse problems, such\nas estimating coefficient functions (e.g., the Hamiltonian function) that\ncharacterize physical systems. This framework enables data-driven understanding\nand prediction of complex physical phenomena. While coefficient functions in\nPIML are typically estimated on the basis of predictive performance, physics as\na discipline does not rely solely on prediction accuracy to evaluate models.\nFor example, Kepler's heliocentric model was favored owing to small\ndiscrepancies in planetary motion, despite its similar predictive accuracy to\nthe geocentric model. This highlights the inherent uncertainties in data-driven\nmodel inference and the scientific importance of selecting physically\nmeaningful solutions. In this paper, we propose a framework to quantify and\nanalyze such uncertainties in the estimation of coefficient functions in PIML.\nWe apply our framework to reduced model of magnetohydrodynamics and our\nframework shows that there are uncertainties, and unique identification is\npossible with geometric constraints. Finally, we confirm that we can estimate\nthe reduced model uniquely by incorporating these constraints."
                },
                "authors": [
                    {
                        "name": "Yoh-ichi Mototake"
                    },
                    {
                        "name": "Makoto Sasaki"
                    }
                ],
                "author_detail": {
                    "name": "Makoto Sasaki"
                },
                "author": "Makoto Sasaki",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04562v1",
                "updated": "2025-11-06T17:17:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    17,
                    36,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:17:36Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    17,
                    36,
                    3,
                    310,
                    0
                ],
                "title": "Asymptotics for Reinforced Stochastic Processes on Hierarchical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics for Reinforced Stochastic Processes on Hierarchical Networks"
                },
                "summary": "In this paper, we analyze the asymptotic behavior of a system of interacting\nreinforced stochastic processes $({\\bf Z}_n, {\\bf N}_n)_n$ on a directed\nnetwork of $N$ agents. The system is defined by the coupled dynamics ${\\bf\nZ}_{n+1}=(1-r_{n}){\\bf Z}_{n}+r_{n}{\\bf X}_{n+1}$ and ${\\bf\nN}_{n+1}=(1-\\frac{1}{n+1}){\\bf N}_n+\\frac{1}{n+1}{\\bf X}_{n+1}$, where agent\nactions $\\mathbb{P}(X_{n+1,j}=1\\mid{\\cal F}_n)=\\sum_{h} w_{hj}Z_{nh}$ are\ngoverned by a column-normalized adjacency matrix ${\\bf W}$, and $r_n \\sim\ncn^{-\\gamma}$ with $\\gamma \\in (1/2, 1]$. Existing asymptotic theory has\nlargely been restricted to irreducible and diagonalizable ${\\bf W}$. We extend\nthis analysis to the broader and more practical class of reducible and\nnon-diagonalizable matrices ${\\bf W}$ possessing a block upper-triangular form,\nwhich models hierarchical influence. We first establish synchronization,\nproving $({\\bf Z}^\\top_n, {\\bf N}^\\top_n)^\\top \\to Z_\\infty {\\bf 1}$ almost\nsurely, where the distribution of the limit $Z_\\infty$ is shown to be\ndetermined solely by the internal dynamics of the leading subgroup.\nFurthermore, we establish a joint central limit theorem for $({\\bf Z}_n,{\\bf\nN}_n)_n$, revealing how the spectral properties and Jordan block structure of\n${\\bf W}$ govern second-order fluctuations. We demonstrate that the convergence\nrates and the limiting covariance structure exhibit a phase transition\ndependent on $\\gamma$ and the spectral properties of ${\\bf W}$. Crucially, we\nexplicitly characterize how the non-diagonalizability of ${\\bf W}$\nfundamentally alters the asymptotic covariance and introduces new logarithmic\nscaling factors in the critical case ($\\gamma=1$). These results provide a\nprobabilistic foundation for statistical inference on such hierarchical network\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we analyze the asymptotic behavior of a system of interacting\nreinforced stochastic processes $({\\bf Z}_n, {\\bf N}_n)_n$ on a directed\nnetwork of $N$ agents. The system is defined by the coupled dynamics ${\\bf\nZ}_{n+1}=(1-r_{n}){\\bf Z}_{n}+r_{n}{\\bf X}_{n+1}$ and ${\\bf\nN}_{n+1}=(1-\\frac{1}{n+1}){\\bf N}_n+\\frac{1}{n+1}{\\bf X}_{n+1}$, where agent\nactions $\\mathbb{P}(X_{n+1,j}=1\\mid{\\cal F}_n)=\\sum_{h} w_{hj}Z_{nh}$ are\ngoverned by a column-normalized adjacency matrix ${\\bf W}$, and $r_n \\sim\ncn^{-\\gamma}$ with $\\gamma \\in (1/2, 1]$. Existing asymptotic theory has\nlargely been restricted to irreducible and diagonalizable ${\\bf W}$. We extend\nthis analysis to the broader and more practical class of reducible and\nnon-diagonalizable matrices ${\\bf W}$ possessing a block upper-triangular form,\nwhich models hierarchical influence. We first establish synchronization,\nproving $({\\bf Z}^\\top_n, {\\bf N}^\\top_n)^\\top \\to Z_\\infty {\\bf 1}$ almost\nsurely, where the distribution of the limit $Z_\\infty$ is shown to be\ndetermined solely by the internal dynamics of the leading subgroup.\nFurthermore, we establish a joint central limit theorem for $({\\bf Z}_n,{\\bf\nN}_n)_n$, revealing how the spectral properties and Jordan block structure of\n${\\bf W}$ govern second-order fluctuations. We demonstrate that the convergence\nrates and the limiting covariance structure exhibit a phase transition\ndependent on $\\gamma$ and the spectral properties of ${\\bf W}$. Crucially, we\nexplicitly characterize how the non-diagonalizability of ${\\bf W}$\nfundamentally alters the asymptotic covariance and introduces new logarithmic\nscaling factors in the critical case ($\\gamma=1$). These results provide a\nprobabilistic foundation for statistical inference on such hierarchical network\nstructures."
                },
                "authors": [
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Dandan Jiang"
                    },
                    {
                        "name": "Jiang Hu"
                    },
                    {
                        "name": "Zhidong Bai"
                    }
                ],
                "author_detail": {
                    "name": "Zhidong Bai"
                },
                "author": "Zhidong Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v2",
                "updated": "2025-11-06T17:09:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    9,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures;Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04555v1",
                "updated": "2025-11-06T17:07:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    7,
                    49,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:07:49Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    7,
                    49,
                    3,
                    310,
                    0
                ],
                "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic\n  Alignment"
                },
                "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models."
                },
                "authors": [
                    {
                        "name": "Tao Lin"
                    },
                    {
                        "name": "Yilei Zhong"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Jingjing Zhang"
                    },
                    {
                        "name": "Jiting Liu"
                    },
                    {
                        "name": "Yinxinyu Chen"
                    },
                    {
                        "name": "Encheng Gu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Yanwen Zou"
                    },
                    {
                        "name": "Lixing Zou"
                    },
                    {
                        "name": "Zhaoye Zhou"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "arxiv_comment": "Github: https://github.com/MINT-SJTU/Evo-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04552v1",
                "updated": "2025-11-06T17:04:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    4,
                    48,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:04:48Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    4,
                    48,
                    3,
                    310,
                    0
                ],
                "title": "Generative Bayesian Filtering and Parameter Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Bayesian Filtering and Parameter Learning"
                },
                "summary": "Generative Bayesian Filtering (GBF) provides a powerful and flexible\nframework for performing posterior inference in complex nonlinear and\nnon-Gaussian state-space models. Our approach extends Generative Bayesian\nComputation (GBC) to dynamic settings, enabling recursive posterior inference\nusing simulation-based methods powered by deep neural networks. GBF does not\nrequire explicit density evaluations, making it particularly effective when\nobservation or transition distributions are analytically intractable. To\naddress parameter learning, we introduce the Generative-Gibbs sampler, which\nbypasses explicit density evaluation by iteratively sampling each variable from\nits implicit full conditional distribution. Such technique is broadly\napplicable and enables inference in hierarchical Bayesian models with\nintractable densities, including state-space models. We assess the performance\nof the proposed methodologies through both simulated and empirical studies,\nincluding the estimation of $\\alpha$-stable stochastic volatility models. Our\nfindings indicate that GBF significantly outperforms existing likelihood-free\napproaches in accuracy and robustness when dealing with intractable state-space\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Bayesian Filtering (GBF) provides a powerful and flexible\nframework for performing posterior inference in complex nonlinear and\nnon-Gaussian state-space models. Our approach extends Generative Bayesian\nComputation (GBC) to dynamic settings, enabling recursive posterior inference\nusing simulation-based methods powered by deep neural networks. GBF does not\nrequire explicit density evaluations, making it particularly effective when\nobservation or transition distributions are analytically intractable. To\naddress parameter learning, we introduce the Generative-Gibbs sampler, which\nbypasses explicit density evaluation by iteratively sampling each variable from\nits implicit full conditional distribution. Such technique is broadly\napplicable and enables inference in hierarchical Bayesian models with\nintractable densities, including state-space models. We assess the performance\nof the proposed methodologies through both simulated and empirical studies,\nincluding the estimation of $\\alpha$-stable stochastic volatility models. Our\nfindings indicate that GBF significantly outperforms existing likelihood-free\napproaches in accuracy and robustness when dealing with intractable state-space\nmodels."
                },
                "authors": [
                    {
                        "name": "Edoardo Marcelli"
                    },
                    {
                        "name": "Sean O'Hagan"
                    },
                    {
                        "name": "Veronika Rockova"
                    }
                ],
                "author_detail": {
                    "name": "Veronika Rockova"
                },
                "author": "Veronika Rockova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12171v2",
                "updated": "2025-11-06T16:57:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    57,
                    27,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-16T12:05:23Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    5,
                    23,
                    2,
                    197,
                    0
                ],
                "title": "Cosmic Cartography II: completing galaxy catalogs for gravitational-wave\n  cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic Cartography II: completing galaxy catalogs for gravitational-wave\n  cosmology"
                },
                "summary": "The dark siren method exploits the complementarity between gravitational-wave\nbinary coalescence signals and galaxy catalogs originating from the same\nregions of space. However, all galaxy catalogs are incomplete, i.e. they only\ninclude a subset of all galaxies, typically being biased towards the bright end\nof the luminosity distribution. This sub-selection systematically affects the\ndark siren inference of the Hubble constant $H_0$, so a completeness relation\nhas to be introduced that accounts for the missing objects. In the literature\nit is standard to assume that the missing galaxies are uniformly distributed\nacross the sky and that the galaxy magnitude distribution is known. In this\nwork we develop a novel method which improves upon these assumptions and\nreconstructs the underlying true galaxy field, respecting the spatial\ncorrelation of galaxies on large scales. In our method the true magnitude\ndistribution of galaxies is inferred alongside the spatial galaxy distribution.\nOur method results in an improved three-dimensional prior in redshift and sky\nposition for the host galaxy of a GW event, which is expected to make the\nresulting $H_0$ posterior more robust. Building on our previous work, we make a\nnumber of improvements, and validate our method on simulated data based on the\nMillennium simulation. The inference results can be reproduced through our\npublicly available code base light.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dark siren method exploits the complementarity between gravitational-wave\nbinary coalescence signals and galaxy catalogs originating from the same\nregions of space. However, all galaxy catalogs are incomplete, i.e. they only\ninclude a subset of all galaxies, typically being biased towards the bright end\nof the luminosity distribution. This sub-selection systematically affects the\ndark siren inference of the Hubble constant $H_0$, so a completeness relation\nhas to be introduced that accounts for the missing objects. In the literature\nit is standard to assume that the missing galaxies are uniformly distributed\nacross the sky and that the galaxy magnitude distribution is known. In this\nwork we develop a novel method which improves upon these assumptions and\nreconstructs the underlying true galaxy field, respecting the spatial\ncorrelation of galaxies on large scales. In our method the true magnitude\ndistribution of galaxies is inferred alongside the spatial galaxy distribution.\nOur method results in an improved three-dimensional prior in redshift and sky\nposition for the host galaxy of a GW event, which is expected to make the\nresulting $H_0$ posterior more robust. Building on our previous work, we make a\nnumber of improvements, and validate our method on simulated data based on the\nMillennium simulation. The inference results can be reproduced through our\npublicly available code base light."
                },
                "authors": [
                    {
                        "name": "Konstantin Leyde"
                    },
                    {
                        "name": "Tessa Baker"
                    },
                    {
                        "name": "Wolfgang Enzi"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Enzi"
                },
                "author": "Wolfgang Enzi",
                "arxiv_comment": "We refer the busy reader to Fig. 2 for an overview of the method, and\n  to Sec. 4.1 with Figures 7, 8 and 10 for the main results. 45 pages, 20\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04541v1",
                "updated": "2025-11-06T16:54:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    54,
                    54,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:54:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    54,
                    54,
                    3,
                    310,
                    0
                ],
                "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems"
                },
                "summary": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems."
                },
                "authors": [
                    {
                        "name": "Baptiste Bonin"
                    },
                    {
                        "name": "Maxime Heuillet"
                    },
                    {
                        "name": "Audrey Durand"
                    }
                ],
                "author_detail": {
                    "name": "Audrey Durand"
                },
                "author": "Audrey Durand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03179v2",
                "updated": "2025-11-06T16:54:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    54,
                    41,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T04:55:25Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    4,
                    55,
                    25,
                    2,
                    309,
                    0
                ],
                "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent\n  Framework"
                },
                "summary": "The engineering design process often demands expertise from multiple domains,\nleading to complex collaborations and iterative refinements. Traditional\nmethods can be resource-intensive and prone to inefficiencies. To address this,\nwe formalize the engineering design process through a multi-agent AI framework\nthat integrates structured design and review loops. The framework introduces\nspecialized knowledge-driven agents that collaborate to generate and refine\ndesign candidates. As an exemplar, we demonstrate its application to the\naerodynamic optimization of 4-digit NACA airfoils. The framework consists of\nthree key AI agents: a Graph Ontologist, a Design Engineer, and a Systems\nEngineer. The Graph Ontologist employs a Large Language Model (LLM) to\nconstruct two domain-specific knowledge graphs from airfoil design literature.\nThe Systems Engineer, informed by a human manager, formulates technical\nrequirements that guide design generation and evaluation. The Design Engineer\nleverages the design knowledge graph and computational tools to propose\ncandidate airfoils meeting these requirements. The Systems Engineer reviews and\nprovides feedback both qualitative and quantitative using its own knowledge\ngraph, forming an iterative feedback loop until a design is validated by the\nmanager. The final design is then optimized to maximize performance metrics\nsuch as the lift-to-drag ratio. Overall, this work demonstrates how\ncollaborative AI agents equipped with structured knowledge representations can\nenhance efficiency, consistency, and quality in the engineering design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The engineering design process often demands expertise from multiple domains,\nleading to complex collaborations and iterative refinements. Traditional\nmethods can be resource-intensive and prone to inefficiencies. To address this,\nwe formalize the engineering design process through a multi-agent AI framework\nthat integrates structured design and review loops. The framework introduces\nspecialized knowledge-driven agents that collaborate to generate and refine\ndesign candidates. As an exemplar, we demonstrate its application to the\naerodynamic optimization of 4-digit NACA airfoils. The framework consists of\nthree key AI agents: a Graph Ontologist, a Design Engineer, and a Systems\nEngineer. The Graph Ontologist employs a Large Language Model (LLM) to\nconstruct two domain-specific knowledge graphs from airfoil design literature.\nThe Systems Engineer, informed by a human manager, formulates technical\nrequirements that guide design generation and evaluation. The Design Engineer\nleverages the design knowledge graph and computational tools to propose\ncandidate airfoils meeting these requirements. The Systems Engineer reviews and\nprovides feedback both qualitative and quantitative using its own knowledge\ngraph, forming an iterative feedback loop until a design is validated by the\nmanager. The final design is then optimized to maximize performance metrics\nsuch as the lift-to-drag ratio. Overall, this work demonstrates how\ncollaborative AI agents equipped with structured knowledge representations can\nenhance efficiency, consistency, and quality in the engineering design process."
                },
                "authors": [
                    {
                        "name": "Varun Kumar"
                    },
                    {
                        "name": "George Em Karniadakis"
                    }
                ],
                "author_detail": {
                    "name": "George Em Karniadakis"
                },
                "author": "George Em Karniadakis",
                "arxiv_comment": "Revised to fix typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01019v2",
                "updated": "2025-11-06T16:53:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    53,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-02T17:23:58Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    17,
                    23,
                    58,
                    6,
                    306,
                    0
                ],
                "title": "OceanAI: A Conversational Platform for Accurate, Transparent,\n  Near-Real-Time Oceanographic Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OceanAI: A Conversational Platform for Accurate, Transparent,\n  Near-Real-Time Oceanographic Insights"
                },
                "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz."
                },
                "authors": [
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Jayesh Gajbhar"
                    },
                    {
                        "name": "Gregory Dusek"
                    },
                    {
                        "name": "Rob Redmon"
                    },
                    {
                        "name": "Patrick Hogan"
                    },
                    {
                        "name": "Paul Liu"
                    },
                    {
                        "name": "DelWayne Bohnenstiehl"
                    },
                    {
                        "name": "Dongkuan Xu"
                    },
                    {
                        "name": "Ruoying He"
                    }
                ],
                "author_detail": {
                    "name": "Ruoying He"
                },
                "author": "Ruoying He",
                "arxiv_comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04538v1",
                "updated": "2025-11-06T16:52:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    52,
                    27,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:52:27Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    52,
                    27,
                    3,
                    310,
                    0
                ],
                "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities\n  Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities\n  Reporting"
                },
                "summary": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates."
                },
                "authors": [
                    {
                        "name": "Cyril Vallez"
                    },
                    {
                        "name": "Alexander Sternfeld"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Dolamic"
                },
                "author": "Ljiljana Dolamic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04528v1",
                "updated": "2025-11-06T16:43:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    43,
                    37,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:43:37Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    43,
                    37,
                    3,
                    310,
                    0
                ],
                "title": "IntelliProof: An Argumentation Network-based Conversational Helper for\n  Organized Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntelliProof: An Argumentation Network-based Conversational Helper for\n  Organized Reflection"
                },
                "summary": "We present IntelliProof, an interactive system for analyzing argumentative\nessays through LLMs. IntelliProof structures an essay as an argumentation\ngraph, where claims are represented as nodes, supporting evidence is attached\nas node properties, and edges encode supporting or attacking relations. Unlike\nexisting automated essay scoring systems, IntelliProof emphasizes the user\nexperience: each relation is initially classified and scored by an LLM, then\nvisualized for enhanced understanding. The system provides justifications for\nclassifications and produces quantitative measures for essay coherence. It\nenables rapid exploration of argumentative quality while retaining human\noversight. In addition, IntelliProof provides a set of tools for a better\nunderstanding of an argumentative essay and its corresponding graph in natural\nlanguage, bridging the gap between the structural semantics of argumentative\nessays and the user's understanding of a given text. A live demo and the system\nare available here to try: \\textbf{https://intelliproof.vercel.app}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present IntelliProof, an interactive system for analyzing argumentative\nessays through LLMs. IntelliProof structures an essay as an argumentation\ngraph, where claims are represented as nodes, supporting evidence is attached\nas node properties, and edges encode supporting or attacking relations. Unlike\nexisting automated essay scoring systems, IntelliProof emphasizes the user\nexperience: each relation is initially classified and scored by an LLM, then\nvisualized for enhanced understanding. The system provides justifications for\nclassifications and produces quantitative measures for essay coherence. It\nenables rapid exploration of argumentative quality while retaining human\noversight. In addition, IntelliProof provides a set of tools for a better\nunderstanding of an argumentative essay and its corresponding graph in natural\nlanguage, bridging the gap between the structural semantics of argumentative\nessays and the user's understanding of a given text. A live demo and the system\nare available here to try: \\textbf{https://intelliproof.vercel.app}"
                },
                "authors": [
                    {
                        "name": "Kaveh Eskandari Miandoab"
                    },
                    {
                        "name": "Katharine Kowalyshyn"
                    },
                    {
                        "name": "Kabir Pamnani"
                    },
                    {
                        "name": "Anesu Gavhera"
                    },
                    {
                        "name": "Vasanth Sarathy"
                    },
                    {
                        "name": "Matthias Scheutz"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Scheutz"
                },
                "author": "Matthias Scheutz",
                "arxiv_comment": "Accepted for the 40th Annual AAAI Conference on Artificial\n  Intelligence (2026) - Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04508v1",
                "updated": "2025-11-06T16:25:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    25,
                    35,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:25:35Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    25,
                    35,
                    3,
                    310,
                    0
                ],
                "title": "Large Language Models for Cyber Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cyber Security"
                },
                "summary": "This paper studies the integration off Large Language Models into\ncybersecurity tools and protocols. The main issue discussed in this paper is\nhow traditional rule-based and signature based security systems are not enough\nto deal with modern AI powered cyber threats. Cybersecurity industry is\nchanging as threats are becoming more dangerous and adaptive in nature by\nlevering the features provided by AI tools. By integrating LLMs into these\ntools and protocols, make the systems scalable, context-aware and intelligent.\nThus helping it to mitigate these evolving cyber threats. The paper studies the\narchitecture and functioning of LLMs, its integration into Encrypted prompts to\nprevent prompt injection attacks. It also studies the integration of LLMs into\ncybersecurity tools using a four layered architecture. At last, the paper has\ntried to explain various ways of integration LLMs into traditional Intrusion\nDetection System and enhancing its original abilities in various dimensions.\nThe key findings of this paper has been (i)Encrypted Prompt with LLM is an\neffective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber\nsecurity tools are more accurate, scalable and adaptable to new threats as\ncompared to traditional models, (iii) The decoupled model approach for LLM\nintegration into IDS is the best way as it is the most accurate way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the integration off Large Language Models into\ncybersecurity tools and protocols. The main issue discussed in this paper is\nhow traditional rule-based and signature based security systems are not enough\nto deal with modern AI powered cyber threats. Cybersecurity industry is\nchanging as threats are becoming more dangerous and adaptive in nature by\nlevering the features provided by AI tools. By integrating LLMs into these\ntools and protocols, make the systems scalable, context-aware and intelligent.\nThus helping it to mitigate these evolving cyber threats. The paper studies the\narchitecture and functioning of LLMs, its integration into Encrypted prompts to\nprevent prompt injection attacks. It also studies the integration of LLMs into\ncybersecurity tools using a four layered architecture. At last, the paper has\ntried to explain various ways of integration LLMs into traditional Intrusion\nDetection System and enhancing its original abilities in various dimensions.\nThe key findings of this paper has been (i)Encrypted Prompt with LLM is an\neffective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber\nsecurity tools are more accurate, scalable and adaptable to new threats as\ncompared to traditional models, (iii) The decoupled model approach for LLM\nintegration into IDS is the best way as it is the most accurate way."
                },
                "authors": [
                    {
                        "name": "Raunak Somani"
                    },
                    {
                        "name": "Aswani Kumar Cherukuri"
                    }
                ],
                "author_detail": {
                    "name": "Aswani Kumar Cherukuri"
                },
                "author": "Aswani Kumar Cherukuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04506v1",
                "updated": "2025-11-06T16:24:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    24,
                    53,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:24:53Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    24,
                    53,
                    3,
                    310,
                    0
                ],
                "title": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit\n  Uncertainty Markers to Implicit Reasoning Pathways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit\n  Uncertainty Markers to Implicit Reasoning Pathways"
                },
                "summary": "Radiology reports are invaluable for clinical decision-making and hold great\npotential for automated analysis when structured into machine-readable formats.\nThese reports often contain uncertainty, which we categorize into two distinct\ntypes: (i) Explicit uncertainty reflects doubt about the presence or absence of\nfindings, conveyed through hedging phrases. These vary in meaning depending on\nthe context, making rule-based systems insufficient to quantify the level of\nuncertainty for specific findings; (ii) Implicit uncertainty arises when\nradiologists omit parts of their reasoning, recording only key findings or\ndiagnoses. Here, it is often unclear whether omitted findings are truly absent\nor simply unmentioned for brevity. We address these challenges with a two-part\nframework. We quantify explicit uncertainty by creating an expert-validated,\nLLM-based reference ranking of common hedging phrases, and mapping each finding\nto a probability value based on this reference. In addition, we model implicit\nuncertainty through an expansion framework that systematically adds\ncharacteristic sub-findings derived from expert-defined diagnostic pathways for\n14 common diagnoses. Using these methods, we release Lunguage++, an expanded,\nuncertainty-aware version of the Lunguage benchmark of fine-grained structured\nradiology reports. This enriched resource enables uncertainty-aware image\nclassification, faithful diagnostic reasoning, and new investigations into the\nclinical impact of diagnostic uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology reports are invaluable for clinical decision-making and hold great\npotential for automated analysis when structured into machine-readable formats.\nThese reports often contain uncertainty, which we categorize into two distinct\ntypes: (i) Explicit uncertainty reflects doubt about the presence or absence of\nfindings, conveyed through hedging phrases. These vary in meaning depending on\nthe context, making rule-based systems insufficient to quantify the level of\nuncertainty for specific findings; (ii) Implicit uncertainty arises when\nradiologists omit parts of their reasoning, recording only key findings or\ndiagnoses. Here, it is often unclear whether omitted findings are truly absent\nor simply unmentioned for brevity. We address these challenges with a two-part\nframework. We quantify explicit uncertainty by creating an expert-validated,\nLLM-based reference ranking of common hedging phrases, and mapping each finding\nto a probability value based on this reference. In addition, we model implicit\nuncertainty through an expansion framework that systematically adds\ncharacteristic sub-findings derived from expert-defined diagnostic pathways for\n14 common diagnoses. Using these methods, we release Lunguage++, an expanded,\nuncertainty-aware version of the Lunguage benchmark of fine-grained structured\nradiology reports. This enriched resource enables uncertainty-aware image\nclassification, faithful diagnostic reasoning, and new investigations into the\nclinical impact of diagnostic uncertainty."
                },
                "authors": [
                    {
                        "name": "Paloma Rabaey"
                    },
                    {
                        "name": "Jong Hak Moon"
                    },
                    {
                        "name": "Jung-Oh Lee"
                    },
                    {
                        "name": "Min Gwan Kim"
                    },
                    {
                        "name": "Hangyul Yoon"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04502v1",
                "updated": "2025-11-06T16:22:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:22:52Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    52,
                    3,
                    310,
                    0
                ],
                "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific\n  RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github."
                },
                "authors": [
                    {
                        "name": "Joshua Gao"
                    },
                    {
                        "name": "Quoc Huy Pham"
                    },
                    {
                        "name": "Subin Varghese"
                    },
                    {
                        "name": "Silwal Saurav"
                    },
                    {
                        "name": "Vedhus Hoskere"
                    }
                ],
                "author_detail": {
                    "name": "Vedhus Hoskere"
                },
                "author": "Vedhus Hoskere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18597v3",
                "updated": "2025-11-06T16:22:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    17,
                    3,
                    310,
                    0
                ],
                "published": "2025-03-24T11:55:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Testora: Using Natural Language Intent to Detect Behavioral Regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testora: Using Natural Language Intent to Detect Behavioral Regressions"
                },
                "summary": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches."
                },
                "authors": [
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted at IEEE/ACM International Conference on Software Engineering\n  (ICSE) 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04500v1",
                "updated": "2025-11-06T16:21:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    21,
                    27,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:21:27Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    21,
                    27,
                    3,
                    310,
                    0
                ],
                "title": "Large language models replicate and predict human cooperation across\n  experiments in game theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models replicate and predict human cooperation across\n  experiments in game theory"
                },
                "summary": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making."
                },
                "authors": [
                    {
                        "name": "Andrea Cera Palatsi"
                    },
                    {
                        "name": "Samuel Martin-Gutierrez"
                    },
                    {
                        "name": "Ana S. Cardenal"
                    },
                    {
                        "name": "Max Pellert"
                    }
                ],
                "author_detail": {
                    "name": "Max Pellert"
                },
                "author": "Max Pellert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04499v1",
                "updated": "2025-11-06T16:20:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    20,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:20:52Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    20,
                    52,
                    3,
                    310,
                    0
                ],
                "title": "Decoding Emergent Big Five Traits in Large Language Models:\n  Temperature-Dependent Expression and Architectural Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Emergent Big Five Traits in Large Language Models:\n  Temperature-Dependent Expression and Architectural Clustering"
                },
                "summary": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1"
                },
                "authors": [
                    {
                        "name": "Christos-Nikolaos Zacharopoulos"
                    },
                    {
                        "name": "Revekka Kyriakoglou"
                    }
                ],
                "author_detail": {
                    "name": "Revekka Kyriakoglou"
                },
                "author": "Revekka Kyriakoglou",
                "arxiv_comment": "Accepted at IJCNLP-AACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04496v1",
                "updated": "2025-11-06T16:17:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    17,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:17:45Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    17,
                    45,
                    3,
                    310,
                    0
                ],
                "title": "A General Approach for Calibration Weighting under Missing at Random",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Approach for Calibration Weighting under Missing at Random"
                },
                "summary": "We propose a unified class of calibration weighting methods based on weighted\ngeneralized entropy to handle missing at random (MAR) data with improved\nstability and efficiency. The proposed generalized entropy calibration (GEC)\nformulates weight construction as a convex optimization program that unifies\nentropy-based approaches and generalized regression weighting. Double\nrobustness is achieved by augmenting standard covariate balancing with a\ndebiasing constraint tied to the propensity score model and a Neyman-orthogonal\nconstraint that removes first-order sensitivity to nuisance estimation.\nSelection of the weights on the entropy function can lead to the optimal\ncalibration estimator under a correctly specified outcome regression model. The\nproposed GEC weighting ha a nice geometric characterization: the GEC solution\nis the Bregman projection of the initial weights onto a constraint set, which\nyields a generalized Pythagorean identity and a nested decomposition that\nquantifies the incremental distance paid for additional constraints. We also\ndevelop a high-dimensional extension with soft calibration and a projection\ncalibration constraint that preserves doubly robust inference. Two simulation\nstudies are presented to compare the performance of the proposed method with\nthe existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unified class of calibration weighting methods based on weighted\ngeneralized entropy to handle missing at random (MAR) data with improved\nstability and efficiency. The proposed generalized entropy calibration (GEC)\nformulates weight construction as a convex optimization program that unifies\nentropy-based approaches and generalized regression weighting. Double\nrobustness is achieved by augmenting standard covariate balancing with a\ndebiasing constraint tied to the propensity score model and a Neyman-orthogonal\nconstraint that removes first-order sensitivity to nuisance estimation.\nSelection of the weights on the entropy function can lead to the optimal\ncalibration estimator under a correctly specified outcome regression model. The\nproposed GEC weighting ha a nice geometric characterization: the GEC solution\nis the Bregman projection of the initial weights onto a constraint set, which\nyields a generalized Pythagorean identity and a nested decomposition that\nquantifies the incremental distance paid for additional constraints. We also\ndevelop a high-dimensional extension with soft calibration and a projection\ncalibration constraint that preserves doubly robust inference. Two simulation\nstudies are presented to compare the performance of the proposed method with\nthe existing methods."
                },
                "authors": [
                    {
                        "name": "Yonghyun Kwon"
                    },
                    {
                        "name": "Jae Kwang Kim"
                    },
                    {
                        "name": "Yumou Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Yumou Qiu"
                },
                "author": "Yumou Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04495v1",
                "updated": "2025-11-06T16:16:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    16,
                    32,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:16:32Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    16,
                    32,
                    3,
                    310,
                    0
                ],
                "title": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code\n  Generation"
                },
                "summary": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance."
                },
                "authors": [
                    {
                        "name": "Cuong Huynh"
                    },
                    {
                        "name": "Jie Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jie Cao"
                },
                "author": "Jie Cao",
                "arxiv_comment": "Accepted to TSAR 2025 Workshop at EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17737v2",
                "updated": "2025-11-06T16:16:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    16,
                    5,
                    3,
                    310,
                    0
                ],
                "published": "2024-06-25T17:24:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    24,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable\n  Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable\n  Users"
                },
                "summary": "While state-of-the-art large language models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While state-of-the-art large language models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users."
                },
                "authors": [
                    {
                        "name": "Elinor Poole-Dayan"
                    },
                    {
                        "name": "Deb Roy"
                    },
                    {
                        "name": "Jad Kabbara"
                    }
                ],
                "author_detail": {
                    "name": "Jad Kabbara"
                },
                "author": "Jad Kabbara",
                "arxiv_comment": "Paper accepted at AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04491v1",
                "updated": "2025-11-06T16:10:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    10,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:10:03Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    10,
                    3,
                    3,
                    310,
                    0
                ],
                "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within\n  Structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within\n  Structured Tables"
                },
                "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research."
                },
                "authors": [
                    {
                        "name": "Nikhil Abhyankar"
                    },
                    {
                        "name": "Purvi Chaurasia"
                    },
                    {
                        "name": "Sanchit Kabra"
                    },
                    {
                        "name": "Ananya Srivastava"
                    },
                    {
                        "name": "Vivek Gupta"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04486v1",
                "updated": "2025-11-06T16:05:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    28,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:05:28Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    28,
                    3,
                    310,
                    0
                ],
                "title": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed\n  Code Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed\n  Code Edits"
                },
                "summary": "Instructed code editing, where LLMs directly modify a developer's existing\ncode based on a user instruction, is becoming a widely used interaction mode in\nAI coding assistants. However, few benchmarks directly evaluate this capability\nand current datasets often rely on artificial sources. We introduce EDIT-Bench,\na benchmark for evaluating LLM code editing capabilities grounded in real-world\nusage, i.e., user instructions and code contexts collected in the wild.\nEDIT-Bench comprises of 545 problems, multiple natural and programming\nlanguages, and a diverse set of real-world use cases, ranging from resolving\nerrors to adding features. EDIT-Bench introduces context-dependent problems\nthat require the model to understand code context, highlighted code, and cursor\nposition in addition to the user instruction. We evaluate 40 diverse LLMs and\nobserve that EDIT-Bench is a challenging set of problems where only 5 models\nscore over 60%. We find that model performance varies across different\ncategories of user instructions. Further, we find that varying levels of\ncontextual information greatly affect task success rate, with performance\nvarying up to 11%, indicating the importance of evaluating with realistic\ncontext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructed code editing, where LLMs directly modify a developer's existing\ncode based on a user instruction, is becoming a widely used interaction mode in\nAI coding assistants. However, few benchmarks directly evaluate this capability\nand current datasets often rely on artificial sources. We introduce EDIT-Bench,\na benchmark for evaluating LLM code editing capabilities grounded in real-world\nusage, i.e., user instructions and code contexts collected in the wild.\nEDIT-Bench comprises of 545 problems, multiple natural and programming\nlanguages, and a diverse set of real-world use cases, ranging from resolving\nerrors to adding features. EDIT-Bench introduces context-dependent problems\nthat require the model to understand code context, highlighted code, and cursor\nposition in addition to the user instruction. We evaluate 40 diverse LLMs and\nobserve that EDIT-Bench is a challenging set of problems where only 5 models\nscore over 60%. We find that model performance varies across different\ncategories of user instructions. Further, we find that varying levels of\ncontextual information greatly affect task success rate, with performance\nvarying up to 11%, indicating the importance of evaluating with realistic\ncontext."
                },
                "authors": [
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Jenny Liang"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00635v2",
                "updated": "2025-11-06T16:05:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    15,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-01T16:20:16Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    20,
                    16,
                    3,
                    121,
                    0
                ],
                "title": "SOMA: A Novel Sampler for Bayesian Inference from Privatized Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOMA: A Novel Sampler for Bayesian Inference from Privatized Data"
                },
                "summary": "Making valid statistical inferences from privatized data is a key challenge\nin modern analysis. In Bayesian settings, data augmentation MCMC (DAMCMC)\nmethods impute unobserved confidential data given noisy privatized summaries,\nenabling principled uncertainty quantification. However, standard DAMCMC often\nsuffers from slow mixing due to component-wise Metropolis-within-Gibbs updates.\nWe propose the Single-Offer-Multiple-Attempts (SOMA) sampler. This novel\nalgorithm improves acceptance rates by generating a single proposal and\nsimultaneously evaluating its suitability to replace all components. By sharing\nproposals across components, SOMA rejects fewer proposal points. We prove lower\nbounds on SOMA's acceptance probability and establish convergence rates in the\ntwo-component case. Experiments on synthetic and real census data with linear\nregression and other models confirm SOMA's efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making valid statistical inferences from privatized data is a key challenge\nin modern analysis. In Bayesian settings, data augmentation MCMC (DAMCMC)\nmethods impute unobserved confidential data given noisy privatized summaries,\nenabling principled uncertainty quantification. However, standard DAMCMC often\nsuffers from slow mixing due to component-wise Metropolis-within-Gibbs updates.\nWe propose the Single-Offer-Multiple-Attempts (SOMA) sampler. This novel\nalgorithm improves acceptance rates by generating a single proposal and\nsimultaneously evaluating its suitability to replace all components. By sharing\nproposals across components, SOMA rejects fewer proposal points. We prove lower\nbounds on SOMA's acceptance probability and establish convergence rates in the\ntwo-component case. Experiments on synthetic and real census data with linear\nregression and other models confirm SOMA's efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yifei Xiong"
                    },
                    {
                        "name": "Nianqiao Phyllis Ju"
                    }
                ],
                "author_detail": {
                    "name": "Nianqiao Phyllis Ju"
                },
                "author": "Nianqiao Phyllis Ju",
                "arxiv_comment": "34 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04481v1",
                "updated": "2025-11-06T15:59:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    59,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:59:59Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    59,
                    59,
                    3,
                    310,
                    0
                ],
                "title": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy\n  Consumption through Empirical and Theoretical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy\n  Consumption through Empirical and Theoretical Analysis"
                },
                "summary": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful\nagentic systems pushing the boundaries of Large Language Models (LLM). They can\nautonomously interact with the internet at the user's behest, such as\nnavigating websites, filling search masks, and comparing price lists. Though\nweb agent research is thriving, induced sustainability issues remain largely\nunexplored. To highlight the urgency of this issue, we provide an initial\nexploration of the energy and $CO_2$ cost associated with web agents from both\na theoretical -via estimation- and an empirical perspective -by benchmarking.\nOur results show how different philosophies in web agent creation can severely\nimpact the associated expended energy, and that more energy consumed does not\nnecessarily equate to better results. We highlight a lack of transparency\nregarding disclosing model parameters and processes used for some web agents as\na limiting factor when estimating energy consumption. Our work contributes\ntowards a change in thinking of how we evaluate web agents, advocating for\ndedicated metrics measuring energy consumption in benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful\nagentic systems pushing the boundaries of Large Language Models (LLM). They can\nautonomously interact with the internet at the user's behest, such as\nnavigating websites, filling search masks, and comparing price lists. Though\nweb agent research is thriving, induced sustainability issues remain largely\nunexplored. To highlight the urgency of this issue, we provide an initial\nexploration of the energy and $CO_2$ cost associated with web agents from both\na theoretical -via estimation- and an empirical perspective -by benchmarking.\nOur results show how different philosophies in web agent creation can severely\nimpact the associated expended energy, and that more energy consumed does not\nnecessarily equate to better results. We highlight a lack of transparency\nregarding disclosing model parameters and processes used for some web agents as\na limiting factor when estimating energy consumption. Our work contributes\ntowards a change in thinking of how we evaluate web agents, advocating for\ndedicated metrics measuring energy consumption in benchmarks."
                },
                "authors": [
                    {
                        "name": "Lars Krupp"
                    },
                    {
                        "name": "Daniel Geißler"
                    },
                    {
                        "name": "Vishal Banwari"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Jakob Karolus"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Karolus"
                },
                "author": "Jakob Karolus",
                "arxiv_comment": "Accepted by AAAI 2026 AISI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01409v2",
                "updated": "2025-11-06T15:57:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-03T10:00:49Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    0,
                    49,
                    0,
                    307,
                    0
                ],
                "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval\n  and Reasoning over Dynamic Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval\n  and Reasoning over Dynamic Knowledge"
                },
                "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge."
                },
                "authors": [
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Ao Yu"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Jianing Shi"
                    },
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Hejia Geng"
                    },
                    {
                        "name": "Yongting Zhang"
                    },
                    {
                        "name": "Yutao Fan"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Tiancheng He"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhenfei Yin"
                },
                "author": "Zhenfei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04478v1",
                "updated": "2025-11-06T15:57:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    19,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:57:19Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    19,
                    3,
                    310,
                    0
                ],
                "title": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop\n  Refinement of LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop\n  Refinement of LLM Judges"
                },
                "summary": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but\nits effectiveness is often limited by the scarcity of diverse, representative\ndata for refining criteria. We present a tool that integrates synthetic data\ngeneration into the LLM-as-a-judge workflow, empowering users to create\ntailored and challenging test cases with configurable domains, personas,\nlengths, and desired outcomes, including borderline cases. The tool also\nsupports AI-assisted inline editing of existing test cases. To enhance\ntransparency and interpretability, it reveals the prompts and explanations\nbehind each generation. In a user study (N=24), 83% of participants preferred\nthe tool over manually creating or selecting test cases, as it allowed them to\nrapidly generate diverse synthetic data without additional workload. The\ngenerated synthetic data proved as effective as hand-crafted data for both\nrefining evaluation criteria and aligning with human preferences. These\nfindings highlight synthetic data as a promising alternative, particularly in\ncontexts where efficiency and scalability are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but\nits effectiveness is often limited by the scarcity of diverse, representative\ndata for refining criteria. We present a tool that integrates synthetic data\ngeneration into the LLM-as-a-judge workflow, empowering users to create\ntailored and challenging test cases with configurable domains, personas,\nlengths, and desired outcomes, including borderline cases. The tool also\nsupports AI-assisted inline editing of existing test cases. To enhance\ntransparency and interpretability, it reveals the prompts and explanations\nbehind each generation. In a user study (N=24), 83% of participants preferred\nthe tool over manually creating or selecting test cases, as it allowed them to\nrapidly generate diverse synthetic data without additional workload. The\ngenerated synthetic data proved as effective as hand-crafted data for both\nrefining evaluation criteria and aligning with human preferences. These\nfindings highlight synthetic data as a promising alternative, particularly in\ncontexts where efficiency and scalability are critical."
                },
                "authors": [
                    {
                        "name": "Hyo Jin Do"
                    },
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Jasmina Gajcin"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Martín Santillán Cooper"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Werner Geyer"
                    }
                ],
                "author_detail": {
                    "name": "Werner Geyer"
                },
                "author": "Werner Geyer",
                "arxiv_comment": "29 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04477v1",
                "updated": "2025-11-06T15:57:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:57:18Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    18,
                    3,
                    310,
                    0
                ],
                "title": "Enabling Dynamic Sparsity in Quantized LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Dynamic Sparsity in Quantized LLM Inference"
                },
                "summary": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs."
                },
                "authors": [
                    {
                        "name": "Rongxiang Wang"
                    },
                    {
                        "name": "Kangyuan Shu"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12474v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12474v3",
                "updated": "2025-11-06T15:56:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    56,
                    42,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-18T15:52:24Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    15,
                    52,
                    24,
                    6,
                    138,
                    0
                ],
                "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization"
                },
                "summary": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration."
                },
                "authors": [
                    {
                        "name": "Weixiao Zhou"
                    },
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Gengyao Li"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Xinnian Liang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "Accepted to AACL-IJCNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12474v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12474v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04847v2",
                "updated": "2025-11-06T15:46:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    46,
                    58,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-07T22:50:33Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    22,
                    50,
                    33,
                    2,
                    127,
                    0
                ],
                "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards"
                },
                "summary": "Retrieval-augmented generation (RAG) aims to reduce hallucinations by\ngrounding responses in external context, yet large language models (LLMs) still\nfrequently introduce unsupported information or contradictions even when\nprovided with relevant context. This paper presents two complementary efforts\nat Vectara to measure and benchmark LLM faithfulness in RAG. First, we describe\nour original hallucination leaderboard, which has tracked hallucination rates\nfor LLMs since 2023 using our HHEM hallucination detection model. Motivated by\nlimitations observed in current hallucination detection methods, we introduce\nFaithJudge, an LLM-as-a-judge framework that leverages a pool of diverse\nhuman-annotated hallucination examples to substantially improve the automated\nhallucination evaluation of LLMs. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness in\nsummarization, question-answering, and data-to-text generation tasks.\nFaithJudge enables a more reliable benchmarking of LLM hallucinations in RAG\nand supports the development of more trustworthy generative AI systems:\nhttps://github.com/vectara/FaithJudge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) aims to reduce hallucinations by\ngrounding responses in external context, yet large language models (LLMs) still\nfrequently introduce unsupported information or contradictions even when\nprovided with relevant context. This paper presents two complementary efforts\nat Vectara to measure and benchmark LLM faithfulness in RAG. First, we describe\nour original hallucination leaderboard, which has tracked hallucination rates\nfor LLMs since 2023 using our HHEM hallucination detection model. Motivated by\nlimitations observed in current hallucination detection methods, we introduce\nFaithJudge, an LLM-as-a-judge framework that leverages a pool of diverse\nhuman-annotated hallucination examples to substantially improve the automated\nhallucination evaluation of LLMs. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness in\nsummarization, question-answering, and data-to-text generation tasks.\nFaithJudge enables a more reliable benchmarking of LLM hallucinations in RAG\nand supports the development of more trustworthy generative AI systems:\nhttps://github.com/vectara/FaithJudge."
                },
                "authors": [
                    {
                        "name": "Manveer Singh Tamber"
                    },
                    {
                        "name": "Forrest Sheng Bao"
                    },
                    {
                        "name": "Chenyu Xu"
                    },
                    {
                        "name": "Ge Luo"
                    },
                    {
                        "name": "Suleman Kazi"
                    },
                    {
                        "name": "Minseok Bae"
                    },
                    {
                        "name": "Miaoran Li"
                    },
                    {
                        "name": "Ofer Mendelevitch"
                    },
                    {
                        "name": "Renyi Qu"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "EMNLP Industry Track 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04473v1",
                "updated": "2025-11-06T15:45:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    45,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:45:18Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    45,
                    18,
                    3,
                    310,
                    0
                ],
                "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge\n  Graph Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge\n  Graph Augmented LLMs"
                },
                "summary": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it."
                },
                "authors": [
                    {
                        "name": "Alberto Cattaneo"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Daniel Justus"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Justus"
                },
                "author": "Daniel Justus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04466v1",
                "updated": "2025-11-06T15:40:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    40,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:40:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    40,
                    11,
                    3,
                    310,
                    0
                ],
                "title": "Conditional Selective Inference for the Selected Groups in Panel Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Selective Inference for the Selected Groups in Panel Data"
                },
                "summary": "We consider the problem of testing for differences in group-specific slopes\nbetween the selected groups in panel data identified via k-means clustering. In\nthis setting, the classical Wald-type test statistic is problematic because it\nproduces an extremely inflated type I error probability. The underlying reason\nis that the same dataset is used to identify the group structure and construct\nthe test statistic, simultaneously. This creates dependence between the\nselection and inference stages. To address this issue, we propose a valid\nselective inference approach conditional on the selection event to account for\nthe selection effect. We formally define the selective type I error and\ndescribe how to efficiently compute the correct p-values for clusters obtained\nusing k-means clustering. Furthermore, the same idea can be extended to test\nfor differences in coefficients due to a single covariate and can be\nincorporated into the GMM estimation framework. Simulation studies show that\nour method has satisfactory finite sample performance. We apply this method to\nexplore the heterogeneous relationships between economic growth and the $CO_2$\nemission across countries for which some new findings are discovered. An R\npackage TestHomoPanel is provided to implement the proposed selective inference\nframework for panel data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of testing for differences in group-specific slopes\nbetween the selected groups in panel data identified via k-means clustering. In\nthis setting, the classical Wald-type test statistic is problematic because it\nproduces an extremely inflated type I error probability. The underlying reason\nis that the same dataset is used to identify the group structure and construct\nthe test statistic, simultaneously. This creates dependence between the\nselection and inference stages. To address this issue, we propose a valid\nselective inference approach conditional on the selection event to account for\nthe selection effect. We formally define the selective type I error and\ndescribe how to efficiently compute the correct p-values for clusters obtained\nusing k-means clustering. Furthermore, the same idea can be extended to test\nfor differences in coefficients due to a single covariate and can be\nincorporated into the GMM estimation framework. Simulation studies show that\nour method has satisfactory finite sample performance. We apply this method to\nexplore the heterogeneous relationships between economic growth and the $CO_2$\nemission across countries for which some new findings are discovered. An R\npackage TestHomoPanel is provided to implement the proposed selective inference\nframework for panel data."
                },
                "authors": [
                    {
                        "name": "Chuang Wan"
                    },
                    {
                        "name": "Jiajun Sun"
                    },
                    {
                        "name": "Xingbai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xingbai Xu"
                },
                "author": "Xingbai Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04464v1",
                "updated": "2025-11-06T15:37:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:37:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context"
                },
                "summary": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization."
                },
                "authors": [
                    {
                        "name": "Carnot Braun"
                    },
                    {
                        "name": "Rafael O. Jarczewski"
                    },
                    {
                        "name": "Gabriel U. Talasso"
                    },
                    {
                        "name": "Leandro A. Villas"
                    },
                    {
                        "name": "Allan M. de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Allan M. de Souza"
                },
                "author": "Allan M. de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02851v2",
                "updated": "2025-11-06T15:25:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    25,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-03T09:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    40,
                    39,
                    4,
                    276,
                    0
                ],
                "title": "Action Deviation-Aware Inference for Low-Latency Wireless Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Deviation-Aware Inference for Low-Latency Wireless Robots"
                },
                "summary": "To support latency-sensitive AI applications ranging from autonomous driving\nto industrial robot manipulation, 6G envisions distributed ML with\ncomputational resources in mobile, edge, and cloud connected over\nhyper-reliable low-latency communication (HRLLC). In this setting, speculative\ndecoding can facilitate collaborative inference of models distributively\ndeployed: a lightweight on-device model locally generates drafts while a more\ncapable remote target model on a server verifies and corrects them in parallel\nwith speculative sampling, thus resulting in lower latency without compromising\naccuracy. However, unlike autoregressive text generation, behavior cloning\npolicies, typically used for embodied AI applications, cannot parallelize\nverification and correction for multiple drafts as each generated action\ndepends on observation updated by a previous action. To this end, we propose\nAction Deviation-Aware Hybrid Inference (ADAHI), wherein drafts are selectively\ntransmitted and verified based on action deviation, which has a strong\ncorrelation with action's rejection probability by the target model. By\ninvoking server operation only when necessary, communication and computational\noverhead can be reduced while accuracy gain from speculative sampling is\npreserved. Experiments on our testbed show that ADAHI reduces transmission and\nserver operations by approximately 40%, lowers end-to-end latency by 39.2%, and\nattains up to 97.2% of the task-success rate of baseline that invokes\nspeculative sampling for every draft embedding vector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support latency-sensitive AI applications ranging from autonomous driving\nto industrial robot manipulation, 6G envisions distributed ML with\ncomputational resources in mobile, edge, and cloud connected over\nhyper-reliable low-latency communication (HRLLC). In this setting, speculative\ndecoding can facilitate collaborative inference of models distributively\ndeployed: a lightweight on-device model locally generates drafts while a more\ncapable remote target model on a server verifies and corrects them in parallel\nwith speculative sampling, thus resulting in lower latency without compromising\naccuracy. However, unlike autoregressive text generation, behavior cloning\npolicies, typically used for embodied AI applications, cannot parallelize\nverification and correction for multiple drafts as each generated action\ndepends on observation updated by a previous action. To this end, we propose\nAction Deviation-Aware Hybrid Inference (ADAHI), wherein drafts are selectively\ntransmitted and verified based on action deviation, which has a strong\ncorrelation with action's rejection probability by the target model. By\ninvoking server operation only when necessary, communication and computational\noverhead can be reduced while accuracy gain from speculative sampling is\npreserved. Experiments on our testbed show that ADAHI reduces transmission and\nserver operations by approximately 40%, lowers end-to-end latency by 39.2%, and\nattains up to 97.2% of the task-success rate of baseline that invokes\nspeculative sampling for every draft embedding vector."
                },
                "authors": [
                    {
                        "name": "Jeyoung Park"
                    },
                    {
                        "name": "Yeonsub Lim"
                    },
                    {
                        "name": "Seungeun Oh"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Seong-Lyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Lyun Kim"
                },
                "author": "Seong-Lyun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00783v2",
                "updated": "2025-11-06T15:24:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    24,
                    48,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-02T03:34:44Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    3,
                    34,
                    44,
                    6,
                    306,
                    0
                ],
                "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for\n  Cooperative Multi-Robot Underwater Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for\n  Cooperative Multi-Robot Underwater Coverage"
                },
                "summary": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions."
                },
                "authors": [
                    {
                        "name": "Jingzehua Xu"
                    },
                    {
                        "name": "Weihang Zhang"
                    },
                    {
                        "name": "Yangyang Li"
                    },
                    {
                        "name": "Hongmiaoyi Zhang"
                    },
                    {
                        "name": "Guanwen Xie"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "arxiv_comment": "This paper has been submitted to IEEE Transactions on Mobile\n  Computing. Jingzehua Xu, Weihang Zhang, and Yangyang Li contributed equally\n  to this work and are recognized as the co-first authors of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06991v2",
                "updated": "2025-11-06T15:24:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    24,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-08T04:38:39Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    4,
                    38,
                    39,
                    6,
                    159,
                    0
                ],
                "title": "Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth"
                },
                "summary": "The recent success of generative AI highlights the crucial role of\nhigh-quality human feedback in building trustworthy AI systems. However, the\nincreasing use of large language models (LLMs) by crowdsourcing workers poses a\nsignificant challenge: datasets intended to reflect human input may be\ncompromised by LLM-generated responses. Existing LLM detection approaches often\nrely on high-dimensional training data such as text, making them unsuitable for\nannotation tasks like multiple-choice labeling. In this work, we investigate\nthe potential of peer prediction -- a mechanism that evaluates the information\nwithin workers' responses without using ground truth -- to mitigate\nLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our\napproach quantifies the correlations between worker answers while conditioning\non (a subset of) LLM-generated labels available to the requester. Building on\nprior research, we propose a training-free scoring mechanism with theoretical\nguarantees under a crowdsourcing model that accounts for LLM collusion. We\nestablish conditions under which our method is effective and empirically\ndemonstrate its robustness in detecting low-effort cheating on real-world\ncrowdsourcing datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of generative AI highlights the crucial role of\nhigh-quality human feedback in building trustworthy AI systems. However, the\nincreasing use of large language models (LLMs) by crowdsourcing workers poses a\nsignificant challenge: datasets intended to reflect human input may be\ncompromised by LLM-generated responses. Existing LLM detection approaches often\nrely on high-dimensional training data such as text, making them unsuitable for\nannotation tasks like multiple-choice labeling. In this work, we investigate\nthe potential of peer prediction -- a mechanism that evaluates the information\nwithin workers' responses without using ground truth -- to mitigate\nLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our\napproach quantifies the correlations between worker answers while conditioning\non (a subset of) LLM-generated labels available to the requester. Building on\nprior research, we propose a training-free scoring mechanism with theoretical\nguarantees under a crowdsourcing model that accounts for LLM collusion. We\nestablish conditions under which our method is effective and empirically\ndemonstrate its robustness in detecting low-effort cheating on real-world\ncrowdsourcing datasets."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Jinlong Pang"
                    },
                    {
                        "name": "Zhaowei Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "32 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04453v1",
                "updated": "2025-11-06T15:23:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    23,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:23:50Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    23,
                    50,
                    3,
                    310,
                    0
                ],
                "title": "Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI\n  Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI\n  Tools"
                },
                "summary": "Social news platforms have become key launch outlets for open-source\nprojects, especially Hacker News (HN), though quantifying their immediate\nimpact remains challenging. This paper presents a reproducible demonstration\nsystem that tracks how HN exposure translates into GitHub star growth for AI\nand LLM tools. Built entirely on public APIs, our pipeline analyzes 138\nrepository launches from 2024-2025 and reveals substantial launch effects:\nrepositories gain an average of 121 stars within 24 hours, 189 stars within 48\nhours, and 289 stars within a week of HN exposure. Through machine learning\nmodels (Elastic Net) and non-linear approaches (Gradient Boosting), we identify\nkey predictors of viral growth. Posting timing appears as key factor--launching\nat optimal hours can mean hundreds of additional stars--while the \"Show HN\" tag\nshows no statistical advantage after controlling for other factors. The\ndemonstration completes in under five minutes on standard hardware,\nautomatically collecting data, training models, and generating visualizations\nthrough single-file scripts. This makes our findings immediately reproducible\nand the framework easily be extended to other platforms, providing both\nresearchers and developers with actionable insights into launch dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social news platforms have become key launch outlets for open-source\nprojects, especially Hacker News (HN), though quantifying their immediate\nimpact remains challenging. This paper presents a reproducible demonstration\nsystem that tracks how HN exposure translates into GitHub star growth for AI\nand LLM tools. Built entirely on public APIs, our pipeline analyzes 138\nrepository launches from 2024-2025 and reveals substantial launch effects:\nrepositories gain an average of 121 stars within 24 hours, 189 stars within 48\nhours, and 289 stars within a week of HN exposure. Through machine learning\nmodels (Elastic Net) and non-linear approaches (Gradient Boosting), we identify\nkey predictors of viral growth. Posting timing appears as key factor--launching\nat optimal hours can mean hundreds of additional stars--while the \"Show HN\" tag\nshows no statistical advantage after controlling for other factors. The\ndemonstration completes in under five minutes on standard hardware,\nautomatically collecting data, training models, and generating visualizations\nthrough single-file scripts. This makes our findings immediately reproducible\nand the framework easily be extended to other platforms, providing both\nresearchers and developers with actionable insights into launch dynamics."
                },
                "authors": [
                    {
                        "name": "Obada Kraishan"
                    }
                ],
                "author_detail": {
                    "name": "Obada Kraishan"
                },
                "author": "Obada Kraishan",
                "arxiv_comment": "7 pages, 3 figures. Reproducible demonstration system with public\n  code available at https://github.com/obadaKraishan/Launch-Day-Diffusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.3; K.6.3; D.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06446v2",
                "updated": "2025-11-06T15:22:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    22,
                    42,
                    3,
                    310,
                    0
                ],
                "published": "2025-02-10T13:23:27Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    23,
                    27,
                    0,
                    41,
                    0
                ],
                "title": "Grouped fixed effects regularization for binary choice models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grouped fixed effects regularization for binary choice models"
                },
                "summary": "We study the application of the grouped fixed effects approach to binary\nchoice models for panel data in presence of severe complete separation. Through\ndata loss, complete separation may lead to biased estimates of Average Partial\nEffects and imprecise inference. Moreover, forecasts are not available for\nunits without variability in the response configuration. The grouped fixed\neffects approach discretizes unobserved heterogeneity via k-means clustering,\nthus reducing the number of fixed effects to estimate. This regularization\nreduces complete separation, since it relies on within-cluster rather than\nwithin-subject response transitions. Drawing from asymptotic theory for the\nAPEs, we propose choosing a number of groups such that clustering delivers a\ngood approximation of the latent trait while keeping the incidental parameters\nproblem under control. The simulation results show that the proposed approach\ndelivers unbiased estimates and reliable inference for the APEs. Two empirical\napplications illustrate the sensitivity of the results to the choice of the\nnumber of groups and how nontrivial forecasts for a much larger number of units\ncan be obtained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the application of the grouped fixed effects approach to binary\nchoice models for panel data in presence of severe complete separation. Through\ndata loss, complete separation may lead to biased estimates of Average Partial\nEffects and imprecise inference. Moreover, forecasts are not available for\nunits without variability in the response configuration. The grouped fixed\neffects approach discretizes unobserved heterogeneity via k-means clustering,\nthus reducing the number of fixed effects to estimate. This regularization\nreduces complete separation, since it relies on within-cluster rather than\nwithin-subject response transitions. Drawing from asymptotic theory for the\nAPEs, we propose choosing a number of groups such that clustering delivers a\ngood approximation of the latent trait while keeping the incidental parameters\nproblem under control. The simulation results show that the proposed approach\ndelivers unbiased estimates and reliable inference for the APEs. Two empirical\napplications illustrate the sensitivity of the results to the choice of the\nnumber of groups and how nontrivial forecasts for a much larger number of units\ncan be obtained."
                },
                "authors": [
                    {
                        "name": "Claudia Pigini"
                    },
                    {
                        "name": "Alessandro Pionati"
                    },
                    {
                        "name": "Francesco Valentini"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Valentini"
                },
                "author": "Francesco Valentini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04439v1",
                "updated": "2025-11-06T15:12:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    12,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:12:50Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    12,
                    50,
                    3,
                    310,
                    0
                ],
                "title": "The Peril of Preference: Why GRPO fails on Ordinal Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Peril of Preference: Why GRPO fails on Ordinal Rewards"
                },
                "summary": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision."
                },
                "authors": [
                    {
                        "name": "Anisha Garg"
                    },
                    {
                        "name": "Ganesh Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Venkatesh"
                },
                "author": "Ganesh Venkatesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04434v1",
                "updated": "2025-11-06T15:08:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    8,
                    0,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:08:00Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    8,
                    0,
                    3,
                    310,
                    0
                ],
                "title": "Estimating ground-state properties in quantum simulators with global\n  control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating ground-state properties in quantum simulators with global\n  control"
                },
                "summary": "Accurately determining ground-state properties of quantum many-body systems\nremains one of the major challenges of quantum simulation. In this work, we\npresent a protocol for estimating the ground-state energy using only global\ntime evolution under a target Hamiltonian. This avoids the need for controlled\noperations that are typically required in conventional quantum phase estimation\nand extends the algorithm applicability to analog simulators. Our method\nextracts energy differences from measurements of the Loschmidt echo over an\ninitial ground-state approximation, combines them with direct energy\nmeasurements, and solves a set of equations to infer the individual\neigenenergies. We benchmark this protocol on free-fermion systems, showing\norders-of-magnitude precision gains over direct energy measurements on the\ninitial state, with accuracy improving rapidly with initial-state fidelity and\npersisting for hundreds of modes. We further demonstrate applicability to the\n2D Ising and Fermi-Hubbard models and show that the approach extends naturally\nto other observables such as order parameters. Finally, we analyze the effect\nof experimental imperfections and propose error-mitigation strategies. These\nresults establish a practical route to compute physically relevant quantities\nwith high precision using globally controlled quantum simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately determining ground-state properties of quantum many-body systems\nremains one of the major challenges of quantum simulation. In this work, we\npresent a protocol for estimating the ground-state energy using only global\ntime evolution under a target Hamiltonian. This avoids the need for controlled\noperations that are typically required in conventional quantum phase estimation\nand extends the algorithm applicability to analog simulators. Our method\nextracts energy differences from measurements of the Loschmidt echo over an\ninitial ground-state approximation, combines them with direct energy\nmeasurements, and solves a set of equations to infer the individual\neigenenergies. We benchmark this protocol on free-fermion systems, showing\norders-of-magnitude precision gains over direct energy measurements on the\ninitial state, with accuracy improving rapidly with initial-state fidelity and\npersisting for hundreds of modes. We further demonstrate applicability to the\n2D Ising and Fermi-Hubbard models and show that the approach extends naturally\nto other observables such as order parameters. Finally, we analyze the effect\nof experimental imperfections and propose error-mitigation strategies. These\nresults establish a practical route to compute physically relevant quantities\nwith high precision using globally controlled quantum simulators."
                },
                "authors": [
                    {
                        "name": "Cristian Tabares"
                    },
                    {
                        "name": "Dominik S. Wild"
                    },
                    {
                        "name": "J. Ignacio Cirac"
                    },
                    {
                        "name": "Peter Zoller"
                    },
                    {
                        "name": "Alejandro González-Tudela"
                    },
                    {
                        "name": "Daniel González-Cuadra"
                    }
                ],
                "author_detail": {
                    "name": "Daniel González-Cuadra"
                },
                "author": "Daniel González-Cuadra",
                "arxiv_comment": "12+10 pages, 5+5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04432v1",
                "updated": "2025-11-06T15:06:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    6,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:06:22Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    6,
                    22,
                    3,
                    310,
                    0
                ],
                "title": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning\n  Task for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning\n  Task for LLMs"
                },
                "summary": "In this study, we experiment with the ability of LLMs to do temporal\nreasoning. Using a Norwegian book from 1940 containing trivia questions, we\nprompt the LLMs to answer the questions as if it were 1940. We also pose the\nquestions in both English and Norwegian. Correct answers are often presented as\nsentences, and grading is done by means of LLM-as-judge, with sampled checks by\na native speaker. Prompting in English consistently gave better results than in\nNorwegian, an unexpected result. In contrast, using larger LLMs improved\nresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,\nand also the largest available LLM especially crafted for Norwegian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we experiment with the ability of LLMs to do temporal\nreasoning. Using a Norwegian book from 1940 containing trivia questions, we\nprompt the LLMs to answer the questions as if it were 1940. We also pose the\nquestions in both English and Norwegian. Correct answers are often presented as\nsentences, and grading is done by means of LLM-as-judge, with sampled checks by\na native speaker. Prompting in English consistently gave better results than in\nNorwegian, an unexpected result. In contrast, using larger LLMs improved\nresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,\nand also the largest available LLM especially crafted for Norwegian."
                },
                "authors": [
                    {
                        "name": "Lars Bungum"
                    },
                    {
                        "name": "Charles Yijia Huang"
                    },
                    {
                        "name": "Abeer Kashar"
                    }
                ],
                "author_detail": {
                    "name": "Abeer Kashar"
                },
                "author": "Abeer Kashar",
                "arxiv_comment": "8 pages, 1 figure, 3 tables, submitted to aconference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04427v1",
                "updated": "2025-11-06T15:00:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    0,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:00:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    0,
                    51,
                    3,
                    310,
                    0
                ],
                "title": "Speed at the Cost of Quality? The Impact of LLM Agent Assistance on\n  Software Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speed at the Cost of Quality? The Impact of LLM Agent Assistance on\n  Software Development"
                },
                "summary": "Large language models (LLMs) have demonstrated the promise to revolutionize\nthe field of software engineering. Among other things, LLM agents are rapidly\ngaining momentum in their application to software development, with\npractitioners claiming a multifold productivity increase after adoption. Yet,\nempirical evidence is lacking around these claims. In this paper, we estimate\nthe causal effect of adopting a widely popular LLM agent assistant, namely\nCursor, on development velocity and software quality. The estimation is enabled\nby a state-of-the-art difference-in-differences design comparing\nCursor-adopting GitHub projects with a matched control group of similar GitHub\nprojects that do not use Cursor. We find that the adoption of Cursor leads to a\nsignificant, large, but transient increase in project-level development\nvelocity, along with a significant and persistent increase in static analysis\nwarnings and code complexity. Further panel generalized method of moments\nestimation reveals that the increase in static analysis warnings and code\ncomplexity acts as a major factor causing long-term velocity slowdown. Our\nstudy carries implications for software engineering practitioners, LLM agent\nassistant designers, and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the promise to revolutionize\nthe field of software engineering. Among other things, LLM agents are rapidly\ngaining momentum in their application to software development, with\npractitioners claiming a multifold productivity increase after adoption. Yet,\nempirical evidence is lacking around these claims. In this paper, we estimate\nthe causal effect of adopting a widely popular LLM agent assistant, namely\nCursor, on development velocity and software quality. The estimation is enabled\nby a state-of-the-art difference-in-differences design comparing\nCursor-adopting GitHub projects with a matched control group of similar GitHub\nprojects that do not use Cursor. We find that the adoption of Cursor leads to a\nsignificant, large, but transient increase in project-level development\nvelocity, along with a significant and persistent increase in static analysis\nwarnings and code complexity. Further panel generalized method of moments\nestimation reveals that the increase in static analysis warnings and code\ncomplexity acts as a major factor causing long-term velocity slowdown. Our\nstudy carries implications for software engineering practitioners, LLM agent\nassistant designers, and researchers."
                },
                "authors": [
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Courtney Miller"
                    },
                    {
                        "name": "Shyam Agarwal"
                    },
                    {
                        "name": "Christian Kästner"
                    },
                    {
                        "name": "Bogdan Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Vasilescu"
                },
                "author": "Bogdan Vasilescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04418v1",
                "updated": "2025-11-06T14:46:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    46,
                    35,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:46:35Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    46,
                    35,
                    3,
                    310,
                    0
                ],
                "title": "The Illusion of Certainty: Uncertainty quantification for LLMs fails\n  under ambiguity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Certainty: Uncertainty quantification for LLMs fails\n  under ambiguity"
                },
                "summary": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is\ncritical for trustworthy deployment. While real-world language is inherently\nambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically\nbenchmarked against tasks with no ambiguity. In this work, we demonstrate that\nwhile current uncertainty estimators perform well under the restrictive\nassumption of no ambiguity, they degrade to close-to-random performance on\nambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first\nambiguous question-answering (QA) datasets equipped with ground-truth answer\ndistributions estimated from factual co-occurrence. We find this performance\ndeterioration to be consistent across different estimation paradigms: using the\npredictive distribution itself, internal representations throughout the model,\nand an ensemble of models. We show that this phenomenon can be theoretically\nexplained, revealing that predictive-distribution and ensemble-based estimators\nare fundamentally limited under ambiguity. Overall, our study reveals a key\nshortcoming of current UQ methods for LLMs and motivates a rethinking of\ncurrent modeling paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is\ncritical for trustworthy deployment. While real-world language is inherently\nambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically\nbenchmarked against tasks with no ambiguity. In this work, we demonstrate that\nwhile current uncertainty estimators perform well under the restrictive\nassumption of no ambiguity, they degrade to close-to-random performance on\nambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first\nambiguous question-answering (QA) datasets equipped with ground-truth answer\ndistributions estimated from factual co-occurrence. We find this performance\ndeterioration to be consistent across different estimation paradigms: using the\npredictive distribution itself, internal representations throughout the model,\nand an ensemble of models. We show that this phenomenon can be theoretically\nexplained, revealing that predictive-distribution and ensemble-based estimators\nare fundamentally limited under ambiguity. Overall, our study reveals a key\nshortcoming of current UQ methods for LLMs and motivates a rethinking of\ncurrent modeling paradigms."
                },
                "authors": [
                    {
                        "name": "Tim Tomov"
                    },
                    {
                        "name": "Dominik Fuchsgruber"
                    },
                    {
                        "name": "Tom Wollschläger"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07961v2",
                "updated": "2025-11-06T14:41:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    41,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2024-10-10T14:24:30Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    30,
                    3,
                    284,
                    0
                ],
                "title": "QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm\n  Design"
                },
                "summary": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitBench, the first benchmark dataset designed to evaluate AI's\ncapability in designing and implementing quantum algorithms using quantum\nprogramming languages. Unlike using AI for writing traditional codes, this task\nis fundamentally more complicated due to highly flexible design space. Our key\ncontributions include: 1. A general framework which formulates the key features\nof quantum algorithm design for Large Language Models. 2. Implementations for\nquantum algorithms from basic primitives to advanced applications, spanning 3\ntask suites, 25 algorithms, and 120,290 data points. 3. Automatic validation\nand verification functions, allowing for iterative evaluation and interactive\nreasoning without human inspection. 4. Promising potential as a training\ndataset through preliminary fine-tuning results. We observed several\ninteresting experimental phenomena: LLMs tend to exhibit consistent error\npatterns, and fine-tuning does not always outperform few-shot learning. In all,\nQCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm\ndesign, and it reveals limitations of LLMs in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitBench, the first benchmark dataset designed to evaluate AI's\ncapability in designing and implementing quantum algorithms using quantum\nprogramming languages. Unlike using AI for writing traditional codes, this task\nis fundamentally more complicated due to highly flexible design space. Our key\ncontributions include: 1. A general framework which formulates the key features\nof quantum algorithm design for Large Language Models. 2. Implementations for\nquantum algorithms from basic primitives to advanced applications, spanning 3\ntask suites, 25 algorithms, and 120,290 data points. 3. Automatic validation\nand verification functions, allowing for iterative evaluation and interactive\nreasoning without human inspection. 4. Promising potential as a training\ndataset through preliminary fine-tuning results. We observed several\ninteresting experimental phenomena: LLMs tend to exhibit consistent error\npatterns, and fine-tuning does not always outperform few-shot learning. In all,\nQCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm\ndesign, and it reveals limitations of LLMs in this domain."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ziruo Wang"
                    },
                    {
                        "name": "Yuntian Gu"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Tongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongyang Li"
                },
                "author": "Tongyang Li",
                "arxiv_comment": "45 pages, 17 figures, 15 tables, GitHub repository:\n  https://github.com/EstelYang/QCircuitBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04403v1",
                "updated": "2025-11-06T14:29:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    29,
                    5,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:29:05Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    29,
                    5,
                    3,
                    310,
                    0
                ],
                "title": "Online Bayesian Experimental Design for Partially Observed Dynamical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Bayesian Experimental Design for Partially Observed Dynamical\n  Systems"
                },
                "summary": "Bayesian experimental design (BED) provides a principled framework for\noptimizing data collection, but existing approaches do not apply to crucial\nreal-world settings such as dynamical systems with partial observability, where\nonly noisy and incomplete observations are available. These systems are\nnaturally modeled as state-space models (SSMs), where latent states mediate the\nlink between parameters and data, making the likelihood -- and thus\ninformation-theoretic objectives like the expected information gain (EIG) --\nintractable. In addition, the dynamical nature of the system requires online\nalgorithms that update posterior distributions and select designs sequentially\nin a computationally efficient manner. We address these challenges by deriving\nnew estimators of the EIG and its gradient that explicitly marginalize latent\nstates, enabling scalable stochastic optimization in nonlinear SSMs. Our\napproach leverages nested particle filters (NPFs) for efficient online\ninference with convergence guarantees. Applications to realistic models, such\nas the susceptible-infected-recovered (SIR) and a moving source location task,\nshow that our framework successfully handles both partial observability and\nonline computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian experimental design (BED) provides a principled framework for\noptimizing data collection, but existing approaches do not apply to crucial\nreal-world settings such as dynamical systems with partial observability, where\nonly noisy and incomplete observations are available. These systems are\nnaturally modeled as state-space models (SSMs), where latent states mediate the\nlink between parameters and data, making the likelihood -- and thus\ninformation-theoretic objectives like the expected information gain (EIG) --\nintractable. In addition, the dynamical nature of the system requires online\nalgorithms that update posterior distributions and select designs sequentially\nin a computationally efficient manner. We address these challenges by deriving\nnew estimators of the EIG and its gradient that explicitly marginalize latent\nstates, enabling scalable stochastic optimization in nonlinear SSMs. Our\napproach leverages nested particle filters (NPFs) for efficient online\ninference with convergence guarantees. Applications to realistic models, such\nas the susceptible-infected-recovered (SIR) and a moving source location task,\nshow that our framework successfully handles both partial observability and\nonline computation."
                },
                "authors": [
                    {
                        "name": "Sara Pérez-Vieites"
                    },
                    {
                        "name": "Sahel Iqbal"
                    },
                    {
                        "name": "Simo Särkkä"
                    },
                    {
                        "name": "Dominik Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Baumann"
                },
                "author": "Dominik Baumann",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04393v1",
                "updated": "2025-11-06T14:21:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    21,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:21:22Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    21,
                    22,
                    3,
                    310,
                    0
                ],
                "title": "Post-Training LLMs as Better Decision-Making Agents: A\n  Regret-Minimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training LLMs as Better Decision-Making Agents: A\n  Regret-Minimization Approach"
                },
                "summary": "Large language models (LLMs) are increasingly deployed as \"agents\" for\ndecision-making (DM) in interactive and dynamic environments. Yet, since they\nwere not originally designed for DM, recent studies show that LLMs can struggle\neven in basic online DM problems, failing to achieve low regret or an effective\nexploration-exploitation tradeoff. To address this, we introduce Iterative\nRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure\nthat repeatedly distills low-regret decision trajectories back into the base\nmodel. At each iteration, the model rolls out multiple decision trajectories,\nselects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior\nmethods that (a) distill action sequences from known DM algorithms or (b) rely\non manually crafted chain-of-thought templates, our approach leverages the\nregret metric to elicit the model's own DM ability and reasoning rationales.\nThis reliance on model-generated reasoning avoids rigid output engineering and\nprovides more flexible, natural-language training signals. Empirical results\nshow that Iterative RMFT improves LLMs' DM performance across diverse models -\nfrom Transformers with numerical input/output, to open-weight LLMs, and\nadvanced closed-weight models like GPT-4o mini. Its flexibility in output and\nreasoning formats enables generalization across tasks with varying horizons,\naction spaces, reward processes, and natural-language contexts. Finally, we\nprovide theoretical insight showing that a single-layer Transformer under this\nparadigm can act as a no-regret learner in a simplified setting. Overall,\nIterative RMFT offers a principled and general post-training framework for\nenhancing LLMs' decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed as \"agents\" for\ndecision-making (DM) in interactive and dynamic environments. Yet, since they\nwere not originally designed for DM, recent studies show that LLMs can struggle\neven in basic online DM problems, failing to achieve low regret or an effective\nexploration-exploitation tradeoff. To address this, we introduce Iterative\nRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure\nthat repeatedly distills low-regret decision trajectories back into the base\nmodel. At each iteration, the model rolls out multiple decision trajectories,\nselects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior\nmethods that (a) distill action sequences from known DM algorithms or (b) rely\non manually crafted chain-of-thought templates, our approach leverages the\nregret metric to elicit the model's own DM ability and reasoning rationales.\nThis reliance on model-generated reasoning avoids rigid output engineering and\nprovides more flexible, natural-language training signals. Empirical results\nshow that Iterative RMFT improves LLMs' DM performance across diverse models -\nfrom Transformers with numerical input/output, to open-weight LLMs, and\nadvanced closed-weight models like GPT-4o mini. Its flexibility in output and\nreasoning formats enables generalization across tasks with varying horizons,\naction spaces, reward processes, and natural-language contexts. Finally, we\nprovide theoretical insight showing that a single-layer Transformer under this\nparadigm can act as a no-regret learner in a simplified setting. Overall,\nIterative RMFT offers a principled and general post-training framework for\nenhancing LLMs' decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    },
                    {
                        "name": "Kaiqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqing Zhang"
                },
                "author": "Kaiqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01827v3",
                "updated": "2025-11-06T14:13:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    13,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-02T15:44:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    44,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search"
                },
                "summary": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04366v1",
                "updated": "2025-11-06T13:51:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    51,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:51:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    51,
                    51,
                    3,
                    310,
                    0
                ],
                "title": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on\n  Parent-Child Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on\n  Parent-Child Interaction"
                },
                "summary": "While multimodal large language models (MLLMs) are increasingly applied in\nhuman-centred AI systems, their ability to understand complex social\ninteractions remains uncertain. We present an exploratory study on aligning\nMLLMs with speech-language pathologists (SLPs) in analysing joint attention in\nparent-child interactions, a key construct in early social-communicative\ndevelopment. Drawing on interviews and video annotations with three SLPs, we\ncharacterise how observational cues of gaze, action, and vocalisation inform\ntheir reasoning processes. We then test whether an MLLM can approximate this\nworkflow through a two-stage prompting, separating observation from judgment.\nOur findings reveal that alignment is more robust at the observation layer,\nwhere experts share common descriptors, than at the judgement layer, where\ninterpretive criteria diverge. We position this work as a case-based probe into\nexpert-AI alignment in complex social behaviour, highlighting both the\nfeasibility and the challenges of applying MLLMs to socially situated\ninteraction analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal large language models (MLLMs) are increasingly applied in\nhuman-centred AI systems, their ability to understand complex social\ninteractions remains uncertain. We present an exploratory study on aligning\nMLLMs with speech-language pathologists (SLPs) in analysing joint attention in\nparent-child interactions, a key construct in early social-communicative\ndevelopment. Drawing on interviews and video annotations with three SLPs, we\ncharacterise how observational cues of gaze, action, and vocalisation inform\ntheir reasoning processes. We then test whether an MLLM can approximate this\nworkflow through a two-stage prompting, separating observation from judgment.\nOur findings reveal that alignment is more robust at the observation layer,\nwhere experts share common descriptors, than at the judgement layer, where\ninterpretive criteria diverge. We position this work as a case-based probe into\nexpert-AI alignment in complex social behaviour, highlighting both the\nfeasibility and the challenges of applying MLLMs to socially situated\ninteraction analysis."
                },
                "authors": [
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Kenny Tsu Wei Choo"
                    }
                ],
                "author_detail": {
                    "name": "Kenny Tsu Wei Choo"
                },
                "author": "Kenny Tsu Wei Choo",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18246v2",
                "updated": "2025-11-06T13:47:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    47,
                    8,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-23T17:02:04Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    4,
                    4,
                    143,
                    0
                ],
                "title": "Will Large Language Models Transform Clinical Prediction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will Large Language Models Transform Clinical Prediction?"
                },
                "summary": "Objective: Large language models (LLMs) are attracting increasing interest in\nhealthcare. This commentary evaluates the potential of LLMs to improve clinical\nprediction models (CPMs) for diagnostic and prognostic tasks, with a focus on\ntheir ability to process longitudinal electronic health record (EHR) data.\n  Findings: LLMs show promise in handling multimodal and longitudinal EHR data\nand can support multi-outcome predictions for diverse health conditions.\nHowever, methodological, validation, infrastructural, and regulatory chal-\nlenges remain. These include inadequate methods for time-to-event modelling,\npoor calibration of predictions, limited external validation, and bias\naffecting underrepresented groups. High infrastructure costs and the absence of\nclear regulatory frameworks further prevent adoption.\n  Implications: Further work and interdisciplinary collaboration are needed to\nsupport equitable and effective integra- tion into the clinical prediction.\nDeveloping temporally aware, fair, and explainable models should be a priority\nfocus for transforming clinical prediction workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Large language models (LLMs) are attracting increasing interest in\nhealthcare. This commentary evaluates the potential of LLMs to improve clinical\nprediction models (CPMs) for diagnostic and prognostic tasks, with a focus on\ntheir ability to process longitudinal electronic health record (EHR) data.\n  Findings: LLMs show promise in handling multimodal and longitudinal EHR data\nand can support multi-outcome predictions for diverse health conditions.\nHowever, methodological, validation, infrastructural, and regulatory chal-\nlenges remain. These include inadequate methods for time-to-event modelling,\npoor calibration of predictions, limited external validation, and bias\naffecting underrepresented groups. High infrastructure costs and the absence of\nclear regulatory frameworks further prevent adoption.\n  Implications: Further work and interdisciplinary collaboration are needed to\nsupport equitable and effective integra- tion into the clinical prediction.\nDeveloping temporally aware, fair, and explainable models should be a priority\nfocus for transforming clinical prediction workflow."
                },
                "authors": [
                    {
                        "name": "Yusuf Yildiz"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Meghna Jani"
                    },
                    {
                        "name": "David A. Jenkins"
                    }
                ],
                "author_detail": {
                    "name": "David A. Jenkins"
                },
                "author": "David A. Jenkins",
                "arxiv_doi": "10.1186/s41512-025-00211-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s41512-025-00211-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.18246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published: BMC Diagnostic and Prognostic Research",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04357v1",
                "updated": "2025-11-06T13:39:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    39,
                    38,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:39:38Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    39,
                    38,
                    3,
                    310,
                    0
                ],
                "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon\n  Planning with VLA Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon\n  Planning with VLA Policies"
                },
                "summary": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Maëlic Neau"
                    },
                    {
                        "name": "Zoe Falomir"
                    },
                    {
                        "name": "Paulo E. Santos"
                    },
                    {
                        "name": "Anne-Gwenn Bosser"
                    },
                    {
                        "name": "Cédric Buche"
                    }
                ],
                "author_detail": {
                    "name": "Cédric Buche"
                },
                "author": "Cédric Buche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04355v1",
                "updated": "2025-11-06T13:38:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    38,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:38:03Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    38,
                    3,
                    3,
                    310,
                    0
                ],
                "title": "Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation\n  Benchmarks"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration, and the race to improve their performance has become a central\nfocus of AI research. Benchmarks and leaderboards are increasingly popular,\noffering quantitative rankings of LLMs. However, they provide limited insight\ninto the tasks that LLMs consistently fail to solve - information that is\ncrucial for understanding current limitations and guiding the development of\nmore capable models. To address this gap, we examined code generation tasks\nacross four popular benchmarks, identifying those that major LLMs are most\nlikely to fail. To understand the causes of these failures, we investigated\nwhether the static complexity of solution code contributes to them, followed by\na systematic inspection of 114 tasks that LLMs consistently struggled with. Our\nanalysis revealed four recurring patterns of weaknesses in LLMs, as well as\ncommon complications within benchmark tasks that most often lead to failure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration, and the race to improve their performance has become a central\nfocus of AI research. Benchmarks and leaderboards are increasingly popular,\noffering quantitative rankings of LLMs. However, they provide limited insight\ninto the tasks that LLMs consistently fail to solve - information that is\ncrucial for understanding current limitations and guiding the development of\nmore capable models. To address this gap, we examined code generation tasks\nacross four popular benchmarks, identifying those that major LLMs are most\nlikely to fail. To understand the causes of these failures, we investigated\nwhether the static complexity of solution code contributes to them, followed by\na systematic inspection of 114 tasks that LLMs consistently struggled with. Our\nanalysis revealed four recurring patterns of weaknesses in LLMs, as well as\ncommon complications within benchmark tasks that most often lead to failure."
                },
                "authors": [
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Maedeh Heydari"
                    },
                    {
                        "name": "Parsa Kazerooni"
                    },
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2025 2nd IEEE/ACM International\n  Conference on AI-powered Software (AIware), Data & Benchmark Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24239v2",
                "updated": "2025-11-06T13:36:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    36,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-29T03:24:48Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    3,
                    24,
                    48,
                    0,
                    272,
                    0
                ],
                "title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning\n  Capabilities of Large Language Models"
                },
                "summary": "Recent large language models (LLMs) have shown strong reasoning capabilities.\nHowever, a critical question remains: do these models possess genuine reasoning\nskills particularly complex strategic reasoning or are they primarily excelling\nat sophisticated pattern recognition within their training data? To address\nthis question, this paper presents a chess testbed, ChessArena, to evaluate the\nstrategic reasoning capabilities of LLMs. Chess requires complex strategic\nreasoning capabilities including long-term planning, strict rule comprehension,\nand multi-turn conversation memorization. Specifically, ChessArena is a\ncompetitive framework where LLMs play against each other, under four different\nplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.\nThe testbed can also evaluate fine-grained capabilities including basic\nunderstanding, move selection, and puzzle solving. Over 13 LLMs with different\nmodes are evaluated in ChessArena, playing over 800 games. The results reveal\nsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chess\nengine at human amateur level), while some even failed to defeat a random\nplayer that selects moves arbitrarily. We also present a strong baseline to the\ntestbed: our fine-tuned Qwen3-8B substantially improved performance,\napproaching much larger state-of-the-art reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have shown strong reasoning capabilities.\nHowever, a critical question remains: do these models possess genuine reasoning\nskills particularly complex strategic reasoning or are they primarily excelling\nat sophisticated pattern recognition within their training data? To address\nthis question, this paper presents a chess testbed, ChessArena, to evaluate the\nstrategic reasoning capabilities of LLMs. Chess requires complex strategic\nreasoning capabilities including long-term planning, strict rule comprehension,\nand multi-turn conversation memorization. Specifically, ChessArena is a\ncompetitive framework where LLMs play against each other, under four different\nplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.\nThe testbed can also evaluate fine-grained capabilities including basic\nunderstanding, move selection, and puzzle solving. Over 13 LLMs with different\nmodes are evaluated in ChessArena, playing over 800 games. The results reveal\nsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chess\nengine at human amateur level), while some even failed to defeat a random\nplayer that selects moves arbitrarily. We also present a strong baseline to the\ntestbed: our fine-tuned Qwen3-8B substantially improved performance,\napproaching much larger state-of-the-art reasoning models."
                },
                "authors": [
                    {
                        "name": "Jincheng Liu"
                    },
                    {
                        "name": "Sijun He"
                    },
                    {
                        "name": "Jingjing Wu"
                    },
                    {
                        "name": "Xiangsen Wang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Zhaoqi Kuang"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Yuan Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Yao"
                },
                "author": "Yuan Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04351v1",
                "updated": "2025-11-06T13:32:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    32,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:32:50Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    32,
                    50,
                    3,
                    310,
                    0
                ],
                "title": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal\n  (RGB-D, Skeleton, Point Cloud) Action Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal\n  (RGB-D, Skeleton, Point Cloud) Action Understanding"
                },
                "summary": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Akira Yamamoto"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Maya Singh"
                    }
                ],
                "author_detail": {
                    "name": "Maya Singh"
                },
                "author": "Maya Singh",
                "arxiv_comment": "11 pages, 6 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04337v1",
                "updated": "2025-11-06T13:19:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    19,
                    31,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:19:31Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    19,
                    31,
                    3,
                    310,
                    0
                ],
                "title": "Massive stars exploding in a He-rich circumstellar medium XII. SN\n  2024acyl: A fast, linearly declining Type Ibn supernova with early\n  flash-ionisation features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive stars exploding in a He-rich circumstellar medium XII. SN\n  2024acyl: A fast, linearly declining Type Ibn supernova with early\n  flash-ionisation features"
                },
                "summary": "We present a photometric and spectroscopic analysis of the Type Ibn supernova\n(SN) 2024acyl. It rises to an absolute magnitude peak of about -17.58 mag in\n10.6 days, and displays a rapid linear post-peak light-curve decline in all\nbands, similar to most SNe Ibn. The optical pseudobolometric light curve peaks\nat ($3.5\\pm0.8) \\times 10^{42}$ erg s$^{-1}$, with a total radiated energy of\n$(5.0\\pm0.4) \\times 10^{48}$ erg. The spectra are dominated by a blue continuum\nat early stages, with narrow P-Cygni \\Hei~lines and flash-ionisation emission\nlines of C {\\sc iii}, N {\\sc iii}, and He {\\sc ii}. The P-Cygni \\Hei~features\ngradually evolve and become emission-dominated in late-time spectra. The\n\\Ha~line is detected throughout the entire spectral evolution, which indicates\nthat the CSM is helium-rich with some residual amount of H. Our multiband\nlight-curve modelling yields estimates of the ejecta mass of $M_{ej}$ =\n$0.98^{+0.30}_{-0.20} \\, \\msun$, with a kinetic energy of $E_{k} =\n0.13^{+0.03}_{-0.02} \\times 10^{51}$ erg, and a $^{56}Ni$ mass of\n$M_{\\mathrm{Ni}} = 0.017 \\, \\msun$. The inferred CSM properties are\ncharacterised by a mass of $M_{\\rm{CSM}} = 0.39^{+0.04}_{-0.04}$ \\msun, an\ninner radius of $R_0$=$15.6^{+1.9}_{-2.0}$ AU, and a density $\\rho_{CSM} =\n(1.32\\pm0.22)\\times10^{-11} \\, \\mathrm{g\\,cm^{-3}}$. The multi-epoch spectra\nare well reproduced by the CMFGEN/ \\texttt{he4p0} model, corresponding to a\nHe-ZAMS mass of 4~M$_\\odot$. These findings are consistent with a scenario of\nan SN powered by ejecta-CSM interaction, originating from a low-mass helium\nstar that evolved within an interacting binary system where the CSM with some\nresidual hydrogen may originate from the mass-transfer process. In addition, a\nchannel of core-collapse explosion of a late-type Wolf-Rayet star with H, or an\nOfpe/WN9 star with fallback accretion, cannot be entirely ruled out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a photometric and spectroscopic analysis of the Type Ibn supernova\n(SN) 2024acyl. It rises to an absolute magnitude peak of about -17.58 mag in\n10.6 days, and displays a rapid linear post-peak light-curve decline in all\nbands, similar to most SNe Ibn. The optical pseudobolometric light curve peaks\nat ($3.5\\pm0.8) \\times 10^{42}$ erg s$^{-1}$, with a total radiated energy of\n$(5.0\\pm0.4) \\times 10^{48}$ erg. The spectra are dominated by a blue continuum\nat early stages, with narrow P-Cygni \\Hei~lines and flash-ionisation emission\nlines of C {\\sc iii}, N {\\sc iii}, and He {\\sc ii}. The P-Cygni \\Hei~features\ngradually evolve and become emission-dominated in late-time spectra. The\n\\Ha~line is detected throughout the entire spectral evolution, which indicates\nthat the CSM is helium-rich with some residual amount of H. Our multiband\nlight-curve modelling yields estimates of the ejecta mass of $M_{ej}$ =\n$0.98^{+0.30}_{-0.20} \\, \\msun$, with a kinetic energy of $E_{k} =\n0.13^{+0.03}_{-0.02} \\times 10^{51}$ erg, and a $^{56}Ni$ mass of\n$M_{\\mathrm{Ni}} = 0.017 \\, \\msun$. The inferred CSM properties are\ncharacterised by a mass of $M_{\\rm{CSM}} = 0.39^{+0.04}_{-0.04}$ \\msun, an\ninner radius of $R_0$=$15.6^{+1.9}_{-2.0}$ AU, and a density $\\rho_{CSM} =\n(1.32\\pm0.22)\\times10^{-11} \\, \\mathrm{g\\,cm^{-3}}$. The multi-epoch spectra\nare well reproduced by the CMFGEN/ \\texttt{he4p0} model, corresponding to a\nHe-ZAMS mass of 4~M$_\\odot$. These findings are consistent with a scenario of\nan SN powered by ejecta-CSM interaction, originating from a low-mass helium\nstar that evolved within an interacting binary system where the CSM with some\nresidual hydrogen may originate from the mass-transfer process. In addition, a\nchannel of core-collapse explosion of a late-type Wolf-Rayet star with H, or an\nOfpe/WN9 star with fallback accretion, cannot be entirely ruled out."
                },
                "authors": [
                    {
                        "name": "Y. -Z. Cai"
                    },
                    {
                        "name": "A. Pastorello"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "J. -W. Zhao"
                    },
                    {
                        "name": "Z. -Y. Wang"
                    },
                    {
                        "name": "Z. -H. Peng"
                    },
                    {
                        "name": "A. Reguitti"
                    },
                    {
                        "name": "L. Tartaglia"
                    },
                    {
                        "name": "A. V. Filippenko"
                    },
                    {
                        "name": "Y. Pan"
                    },
                    {
                        "name": "G. Valerin"
                    },
                    {
                        "name": "B. Kumar"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "M. Fraser"
                    },
                    {
                        "name": "J. P. Anderson"
                    },
                    {
                        "name": "S. Benetti"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "T. G. Brink"
                    },
                    {
                        "name": "E. Cappellaro"
                    },
                    {
                        "name": "T. -W. Chen"
                    },
                    {
                        "name": "X. -L. Chen"
                    },
                    {
                        "name": "N. Elias-Rosa"
                    },
                    {
                        "name": "A. Esamdin"
                    },
                    {
                        "name": "A. Gal-Yam"
                    },
                    {
                        "name": "M. González-Bañuelos"
                    },
                    {
                        "name": "M. Gromadzki"
                    },
                    {
                        "name": "C. P. Gutiérrez"
                    },
                    {
                        "name": "A. Iskandar"
                    },
                    {
                        "name": "C. Inserra"
                    },
                    {
                        "name": "T. Kangas"
                    },
                    {
                        "name": "E. Kankare"
                    },
                    {
                        "name": "T. Kravtsov"
                    },
                    {
                        "name": "H. Kuncarayakti"
                    },
                    {
                        "name": "L. -P. Li"
                    },
                    {
                        "name": "C. -X. Liu"
                    },
                    {
                        "name": "X. -K. Liu"
                    },
                    {
                        "name": "P. Lundqvist"
                    },
                    {
                        "name": "K. Matilainen"
                    },
                    {
                        "name": "S. Mattila"
                    },
                    {
                        "name": "S. Moran"
                    },
                    {
                        "name": "T. E. Müller-Bravo"
                    },
                    {
                        "name": "T. Nagao"
                    },
                    {
                        "name": "T. Petrushevska"
                    },
                    {
                        "name": "G. Pignata"
                    },
                    {
                        "name": "I. Salmaso"
                    },
                    {
                        "name": "S. J. Smartt"
                    },
                    {
                        "name": "J. Sollerman"
                    },
                    {
                        "name": "M. D. Stritzinger"
                    },
                    {
                        "name": "S. Srivastav"
                    },
                    {
                        "name": "L. -T. Wang"
                    },
                    {
                        "name": "S. -Y. Yan"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Y. -P. Yang"
                    },
                    {
                        "name": "W. Zheng"
                    },
                    {
                        "name": "X. -Z. Zou"
                    },
                    {
                        "name": "L. -Y. Chen"
                    },
                    {
                        "name": "X. -L. Du"
                    },
                    {
                        "name": "Q. -L. Fang"
                    },
                    {
                        "name": "A. Fiore"
                    },
                    {
                        "name": "F. Ragosta"
                    },
                    {
                        "name": "S. Zha"
                    },
                    {
                        "name": "J. -J. Zhang"
                    },
                    {
                        "name": "X. -W. Liu"
                    },
                    {
                        "name": "J. -M. Bai"
                    },
                    {
                        "name": "B. Wang"
                    },
                    {
                        "name": "X. -F. Wang"
                    }
                ],
                "author_detail": {
                    "name": "X. -F. Wang"
                },
                "author": "X. -F. Wang",
                "arxiv_comment": "19 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23653v2",
                "updated": "2025-11-06T13:18:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    18,
                    58,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-29T17:02:49Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    2,
                    49,
                    3,
                    149,
                    0
                ],
                "title": "How do Transformers Learn Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Transformers Learn Implicit Reasoning?"
                },
                "summary": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models."
                },
                "authors": [
                    {
                        "name": "Jiaran Ye"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhidian Huang"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Weichuan Liu"
                    },
                    {
                        "name": "Xiaoyin Che"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Accepted as Spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04334v1",
                "updated": "2025-11-06T13:17:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    17,
                    16,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:17:16Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    17,
                    16,
                    3,
                    310,
                    0
                ],
                "title": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation\n  of Kidneys and Kidney Tumours in Computed Tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation\n  of Kidneys and Kidney Tumours in Computed Tomography"
                },
                "summary": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested."
                },
                "authors": [
                    {
                        "name": "Saúl Alonso-Monsalve"
                    },
                    {
                        "name": "Leigh H. Whitehead"
                    },
                    {
                        "name": "Adam Aurisano"
                    },
                    {
                        "name": "Lorena Escudero Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Lorena Escudero Sanchez"
                },
                "author": "Lorena Escudero Sanchez",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19604v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19604v3",
                "updated": "2025-11-06T13:15:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    15,
                    10,
                    3,
                    310,
                    0
                ],
                "published": "2024-04-30T14:53:07Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    14,
                    53,
                    7,
                    1,
                    121,
                    0
                ],
                "title": "X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image\n  Using Cross-Sectional Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image\n  Using Cross-Sectional Diffusion Models"
                },
                "summary": "Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but\nhigh-resolution scans are often slow and expensive due to extensive data\nacquisition requirements. Traditional MRI reconstruction methods aim to\nexpedite this process by filling in missing frequency components in the\nK-space, performing 3D-to-3D reconstructions that demand full 3D scans. In\ncontrast, we introduce X-Diffusion, a novel cross-sectional diffusion model\nthat reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain\ninputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI\nslice or few slices. A key aspect of X-Diffusion is that it models MRI data as\nholistic 3D volumes during the cross-sectional training and inference, unlike\nprevious learning approaches that treat MRI scans as collections of 2D slices\nin standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on\nbrain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank\ndataset. Our results demonstrate that X-Diffusion not only surpasses\nstate-of-the-art methods in quantitative accuracy (PSNR) on unseen data but\nalso preserves critical anatomical features such as tumor profiles, spine\ncurvature, and brain volume. Remarkably, the model generalizes beyond the\ntraining domain, successfully reconstructing knee MRIs despite being trained\nexclusively on brain data. Medical expert evaluations further confirm the\nclinical relevance and fidelity of the generated images.To our knowledge,\nX-Diffusion is the first method capable of producing detailed 3D MRIs from\nhighly limited 2D input data, potentially accelerating MRI acquisition and\nreducing associated costs. The code is available on the project website\nhttps://emmanuelleb985.github.io/XDiffusion/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but\nhigh-resolution scans are often slow and expensive due to extensive data\nacquisition requirements. Traditional MRI reconstruction methods aim to\nexpedite this process by filling in missing frequency components in the\nK-space, performing 3D-to-3D reconstructions that demand full 3D scans. In\ncontrast, we introduce X-Diffusion, a novel cross-sectional diffusion model\nthat reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain\ninputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI\nslice or few slices. A key aspect of X-Diffusion is that it models MRI data as\nholistic 3D volumes during the cross-sectional training and inference, unlike\nprevious learning approaches that treat MRI scans as collections of 2D slices\nin standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on\nbrain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank\ndataset. Our results demonstrate that X-Diffusion not only surpasses\nstate-of-the-art methods in quantitative accuracy (PSNR) on unseen data but\nalso preserves critical anatomical features such as tumor profiles, spine\ncurvature, and brain volume. Remarkably, the model generalizes beyond the\ntraining domain, successfully reconstructing knee MRIs despite being trained\nexclusively on brain data. Medical expert evaluations further confirm the\nclinical relevance and fidelity of the generated images.To our knowledge,\nX-Diffusion is the first method capable of producing detailed 3D MRIs from\nhighly limited 2D input data, potentially accelerating MRI acquisition and\nreducing associated costs. The code is available on the project website\nhttps://emmanuelleb985.github.io/XDiffusion/ ."
                },
                "authors": [
                    {
                        "name": "Emmanuelle Bourigault"
                    },
                    {
                        "name": "Abdullah Hamdi"
                    },
                    {
                        "name": "Amir Jamaludin"
                    }
                ],
                "author_detail": {
                    "name": "Amir Jamaludin"
                },
                "author": "Amir Jamaludin",
                "arxiv_comment": "accepted at ICCV 2025 GAIA workshop\n  https://era-ai-biomed.github.io/GAIA/ , project website:\n  https://emmanuelleb985.github.io/XDiffusion/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19604v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19604v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04333v1",
                "updated": "2025-11-06T13:13:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    13,
                    39,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:13:39Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    13,
                    39,
                    3,
                    310,
                    0
                ],
                "title": "LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in\n  Intensive Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in\n  Intensive Care"
                },
                "summary": "Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to\ntheir ability to model complex temporal relationships in patient data while\nmaintaining interpretability, an essential feature for clinical\ndecision-making. However, existing approaches to handling missing data in\nlongitudinal clinical datasets are largely derived from static Bayesian\nnetworks literature, failing to properly account for the temporal nature of the\ndata. This gap limits the ability to quantify uncertainty over time, which is\nparticularly critical in settings such as intensive care, where understanding\nthe temporal dynamics is fundamental for model trustworthiness and\napplicability across diverse patient groups. Despite the potential of DBNs, a\nfull Bayesian framework that integrates missing data handling remains\nunderdeveloped. In this work, we propose a novel Gibbs sampling-based method\nfor learning DBNs from incomplete data. Our method treats each missing value as\nan unknown parameter following a Gaussian distribution. At each iteration, the\nunobserved values are sampled from their full conditional distributions,\nallowing for principled imputation and uncertainty estimation. We evaluate our\nmethod on both simulated datasets and real-world intensive care data from\ncritically ill patients. Compared to standard model-agnostic techniques such as\nMICE, our Bayesian approach demonstrates superior reconstruction accuracy and\nconvergence properties. These results highlight the clinical relevance of\nincorporating full Bayesian inference in temporal models, providing more\nreliable imputations and offering deeper insight into model behavior. Our\napproach supports safer and more informed clinical decision-making,\nparticularly in settings where missing data are frequent and potentially\nimpactful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to\ntheir ability to model complex temporal relationships in patient data while\nmaintaining interpretability, an essential feature for clinical\ndecision-making. However, existing approaches to handling missing data in\nlongitudinal clinical datasets are largely derived from static Bayesian\nnetworks literature, failing to properly account for the temporal nature of the\ndata. This gap limits the ability to quantify uncertainty over time, which is\nparticularly critical in settings such as intensive care, where understanding\nthe temporal dynamics is fundamental for model trustworthiness and\napplicability across diverse patient groups. Despite the potential of DBNs, a\nfull Bayesian framework that integrates missing data handling remains\nunderdeveloped. In this work, we propose a novel Gibbs sampling-based method\nfor learning DBNs from incomplete data. Our method treats each missing value as\nan unknown parameter following a Gaussian distribution. At each iteration, the\nunobserved values are sampled from their full conditional distributions,\nallowing for principled imputation and uncertainty estimation. We evaluate our\nmethod on both simulated datasets and real-world intensive care data from\ncritically ill patients. Compared to standard model-agnostic techniques such as\nMICE, our Bayesian approach demonstrates superior reconstruction accuracy and\nconvergence properties. These results highlight the clinical relevance of\nincorporating full Bayesian inference in temporal models, providing more\nreliable imputations and offering deeper insight into model behavior. Our\napproach supports safer and more informed clinical decision-making,\nparticularly in settings where missing data are frequent and potentially\nimpactful."
                },
                "authors": [
                    {
                        "name": "Federico Pirola"
                    },
                    {
                        "name": "Fabio Stella"
                    },
                    {
                        "name": "Marco Grzegorczyk"
                    }
                ],
                "author_detail": {
                    "name": "Marco Grzegorczyk"
                },
                "author": "Marco Grzegorczyk",
                "arxiv_comment": "27 pages, 8 figures, 3 tables, presented at HC@AIxIA + HYDRA 2025\n  Workshop located at ECAI 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04332v1",
                "updated": "2025-11-06T13:06:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    6,
                    37,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:06:37Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    6,
                    37,
                    3,
                    310,
                    0
                ],
                "title": "Differentially Private In-Context Learning with Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private In-Context Learning with Nearest Neighbor Search"
                },
                "summary": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Antti Koskela"
                    },
                    {
                        "name": "Tejas Kulkarni"
                    },
                    {
                        "name": "Laith Zumot"
                    }
                ],
                "author_detail": {
                    "name": "Laith Zumot"
                },
                "author": "Laith Zumot",
                "arxiv_comment": "NeurIPS Lock-LLM Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03604v2",
                "updated": "2025-11-06T13:06:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    6,
                    25,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T16:24:53Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    16,
                    24,
                    53,
                    2,
                    309,
                    0
                ],
                "title": "The first year of LISA Galactic foreground",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first year of LISA Galactic foreground"
                },
                "summary": "Galactic white-dwarf binaries play a central role in the inference model for\nthe Laser Interferometer Space Antenna. In this manuscript, we employ the\n$\\texttt{bahamas}$ codebase to characterize, in a global-fit fashion, the\nreconstruction of the Galactic foreground during the first year of observation.\nTo account for its statistical properties, we represent the data in\ntime--frequency domain, and characterize the effectiveness of multiple\napproaches, e.g. statistically viable likelihoods, sampling schemes,\nsegmentation widths, and gaps density. Our analysis yields consistent results\nacross, with overwhelming evidence in favor of a non-stationary model in less\nthan a month of data. Moreover, we show robustness against the presence of\nadditional extragalactic foregrounds, and test the suitability of our\napproximations on the more complex simulated data in the $\\textit{Yorsh}$ data\nchallenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galactic white-dwarf binaries play a central role in the inference model for\nthe Laser Interferometer Space Antenna. In this manuscript, we employ the\n$\\texttt{bahamas}$ codebase to characterize, in a global-fit fashion, the\nreconstruction of the Galactic foreground during the first year of observation.\nTo account for its statistical properties, we represent the data in\ntime--frequency domain, and characterize the effectiveness of multiple\napproaches, e.g. statistically viable likelihoods, sampling schemes,\nsegmentation widths, and gaps density. Our analysis yields consistent results\nacross, with overwhelming evidence in favor of a non-stationary model in less\nthan a month of data. Moreover, we show robustness against the presence of\nadditional extragalactic foregrounds, and test the suitability of our\napproximations on the more complex simulated data in the $\\textit{Yorsh}$ data\nchallenge."
                },
                "authors": [
                    {
                        "name": "Riccardo Buscicchio"
                    },
                    {
                        "name": "Federico Pozzoli"
                    },
                    {
                        "name": "Daniele Chirico"
                    },
                    {
                        "name": "Alberto Sesana"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Sesana"
                },
                "author": "Alberto Sesana",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04328v1",
                "updated": "2025-11-06T12:56:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    56,
                    34,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:56:34Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    56,
                    34,
                    3,
                    310,
                    0
                ],
                "title": "RxSafeBench: Identifying Medication Safety Issues of Large Language\n  Models in Simulated Consultation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RxSafeBench: Identifying Medication Safety Issues of Large Language\n  Models in Simulated Consultation"
                },
                "summary": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Luxin Xu"
                    },
                    {
                        "name": "Minghuan Tan"
                    },
                    {
                        "name": "Lichao Zhang"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "arxiv_comment": "To appear in BIBM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13956v2",
                "updated": "2025-11-06T12:53:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    53,
                    4,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-18T14:21:24Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    21,
                    24,
                    4,
                    199,
                    0
                ],
                "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction"
                },
                "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multi-modal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causality-inspired\nframework named Cross-modal Causal Intervention with Mediator for Alzheimer's\nDisease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large\nLanguage Models (LLMs) to summarize clinical data under strict templates,\ntherefore enriching textual inputs. The MediAD model utilizes Magnetic\nResonance Imaging (MRI), clinical data, and textual data enriched by LLMs to\nclassify participants into Cognitively Normal (CN), MCI, and AD categories.\nBecause of the presence of confounders, such as cerebral vascular lesions and\nage-related biomarkers, non-causal models are likely to capture spurious\ninput-output correlations, generating less reliable results. Our framework\nimplicitly mitigates the effect of both observable and unobservable confounders\nthrough a unified causal intervention method. Experimental results demonstrate\nthe outstanding performance of our method in distinguishing CN/MCI/AD cases,\noutperforming other methods in most evaluation metrics. The study showcases the\npotential of integrating causal reasoning with multi-modal learning for\nneurological disease diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multi-modal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causality-inspired\nframework named Cross-modal Causal Intervention with Mediator for Alzheimer's\nDisease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large\nLanguage Models (LLMs) to summarize clinical data under strict templates,\ntherefore enriching textual inputs. The MediAD model utilizes Magnetic\nResonance Imaging (MRI), clinical data, and textual data enriched by LLMs to\nclassify participants into Cognitively Normal (CN), MCI, and AD categories.\nBecause of the presence of confounders, such as cerebral vascular lesions and\nage-related biomarkers, non-causal models are likely to capture spurious\ninput-output correlations, generating less reliable results. Our framework\nimplicitly mitigates the effect of both observable and unobservable confounders\nthrough a unified causal intervention method. Experimental results demonstrate\nthe outstanding performance of our method in distinguishing CN/MCI/AD cases,\noutperforming other methods in most evaluation metrics. The study showcases the\npotential of integrating causal reasoning with multi-modal learning for\nneurological disease diagnosis."
                },
                "authors": [
                    {
                        "name": "Yutao Jin"
                    },
                    {
                        "name": "Haowen Xiao"
                    },
                    {
                        "name": "Junyong Zhai"
                    },
                    {
                        "name": "Yuxiao Li"
                    },
                    {
                        "name": "Jielei Chu"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Yuxiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Li"
                },
                "author": "Yuxiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01250v2",
                "updated": "2025-11-06T12:45:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    45,
                    44,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-03T05:44:07Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    5,
                    44,
                    7,
                    0,
                    307,
                    0
                ],
                "title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop"
                },
                "summary": "LiDAR semantic segmentation degrades in adverse weather because refraction,\nscattering, and point dropouts corrupt geometry. Prior work in weather\nsimulation, mixing-based augmentation, domain randomization, and uncertainty or\nboundary regularization improves robustness but still overlooks structural\nvulnerabilities near boundaries, corners, and sparse regions. We present a\nLight Geometry-aware adapter. The module aligns azimuth and applies horizontal\ncircular padding to preserve neighbor continuity across the 0~360 degree\nwrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points\nand computes simple local statistics, which are compressed into compact\ngeometry-aware cues. During training, these cues drive region-aware\nregularization that stabilizes predictions in structurally fragile areas. The\nadapter is plug and play, complements augmentation, and can be enabled only\nduring training with negligible inference cost. We adopt a source-only\ncross-weather setup where models train on SemanticKITTI and are evaluated on\nSemanticSTF without target labels or fine-tuning. The adapter improves mIoU by\n7.9 percentage points over the data-centric augmentation baseline and by 0.6\npoints over the class-centric regularization baseline. These results indicate\nthat geometry-driven regularization is a key direction for all-weather LiDAR\nsegmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR semantic segmentation degrades in adverse weather because refraction,\nscattering, and point dropouts corrupt geometry. Prior work in weather\nsimulation, mixing-based augmentation, domain randomization, and uncertainty or\nboundary regularization improves robustness but still overlooks structural\nvulnerabilities near boundaries, corners, and sparse regions. We present a\nLight Geometry-aware adapter. The module aligns azimuth and applies horizontal\ncircular padding to preserve neighbor continuity across the 0~360 degree\nwrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points\nand computes simple local statistics, which are compressed into compact\ngeometry-aware cues. During training, these cues drive region-aware\nregularization that stabilizes predictions in structurally fragile areas. The\nadapter is plug and play, complements augmentation, and can be enabled only\nduring training with negligible inference cost. We adopt a source-only\ncross-weather setup where models train on SemanticKITTI and are evaluated on\nSemanticSTF without target labels or fine-tuning. The adapter improves mIoU by\n7.9 percentage points over the data-centric augmentation baseline and by 0.6\npoints over the class-centric regularization baseline. These results indicate\nthat geometry-driven regularization is a key direction for all-weather LiDAR\nsegmentation."
                },
                "authors": [
                    {
                        "name": "YoungJae Cheong"
                    },
                    {
                        "name": "Jhonghyun An"
                    }
                ],
                "author_detail": {
                    "name": "Jhonghyun An"
                },
                "author": "Jhonghyun An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04317v1",
                "updated": "2025-11-06T12:42:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    42,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:42:03Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    42,
                    3,
                    3,
                    310,
                    0
                ],
                "title": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive\n  Text-to-Video Generation"
                },
                "summary": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io."
                },
                "authors": [
                    {
                        "name": "Xiangjun Zhang"
                    },
                    {
                        "name": "Litong Gong"
                    },
                    {
                        "name": "Yinglin Zheng"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Mingyi Xu"
                    },
                    {
                        "name": "Biao Wang"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Ming Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zeng"
                },
                "author": "Ming Zeng",
                "arxiv_comment": "17 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04316v1",
                "updated": "2025-11-06T12:38:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    38,
                    9,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:38:09Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    38,
                    9,
                    3,
                    310,
                    0
                ],
                "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research"
                },
                "summary": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety."
                },
                "authors": [
                    {
                        "name": "Tim Beyer"
                    },
                    {
                        "name": "Jonas Dornbusch"
                    },
                    {
                        "name": "Jakob Steimle"
                    },
                    {
                        "name": "Moritz Ladenburger"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14133v3",
                "updated": "2025-11-06T12:34:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    34,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2024-11-21T14:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    0,
                    1,
                    3,
                    326,
                    0
                ],
                "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs"
                },
                "summary": "LLMs have shown impressive capabilities across various natural language\nprocessing tasks, yet remain vulnerable to input prompts, known as jailbreak\nattacks, carefully designed to bypass safety guardrails and elicit harmful\nresponses. Traditional methods rely on manual heuristics but suffer from\nlimited generalizability. Despite being automatic, optimization-based attacks\noften produce unnatural prompts that can be easily detected by safety filters\nor require high computational costs due to discrete token optimization. In this\npaper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel\nautomated framework that can efficiently generate human-readable jailbreak\nprompts in a fully black-box setting. In particular, GASP leverages latent\nBayesian optimization to craft adversarial suffixes by efficiently exploring\ncontinuous latent embedding spaces, gradually optimizing the suffix prompter to\nimprove attack efficacy while balancing prompt coherence via a targeted\niterative refinement procedure. Through comprehensive experiments, we show that\nGASP can produce natural adversarial prompts, significantly improving jailbreak\nsuccess over baselines, reducing training times, and accelerating inference\nspeed, thus making it an efficient and scalable solution for red-teaming LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown impressive capabilities across various natural language\nprocessing tasks, yet remain vulnerable to input prompts, known as jailbreak\nattacks, carefully designed to bypass safety guardrails and elicit harmful\nresponses. Traditional methods rely on manual heuristics but suffer from\nlimited generalizability. Despite being automatic, optimization-based attacks\noften produce unnatural prompts that can be easily detected by safety filters\nor require high computational costs due to discrete token optimization. In this\npaper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel\nautomated framework that can efficiently generate human-readable jailbreak\nprompts in a fully black-box setting. In particular, GASP leverages latent\nBayesian optimization to craft adversarial suffixes by efficiently exploring\ncontinuous latent embedding spaces, gradually optimizing the suffix prompter to\nimprove attack efficacy while balancing prompt coherence via a targeted\niterative refinement procedure. Through comprehensive experiments, we show that\nGASP can produce natural adversarial prompts, significantly improving jailbreak\nsuccess over baselines, reducing training times, and accelerating inference\nspeed, thus making it an efficient and scalable solution for red-teaming LLMs."
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025. Project page and demos:\n  https://air-ml.org/project/gasp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10641v2",
                "updated": "2025-11-06T12:24:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    24,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-12T18:58:42Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    18,
                    58,
                    42,
                    4,
                    255,
                    0
                ],
                "title": "Test-Time Warmup for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Warmup for Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) hold great promise for advanced\nreasoning at the intersection of text and images, yet they have not fully\nrealized this potential. MLLMs typically integrate an LLM, a vision encoder,\nand a connector that maps the vision encoder's embeddings into the LLM's text\nembedding space. Although each component is pretrained on massive datasets with\nbillions of samples, the entire multimodal model is typically trained on only\nthousands (or a few million) samples, which can result in weak performance on\ncomplex reasoning tasks. To address these shortcomings, instead of relying on\nextensive labeled datasets for fine-tuning, we propose a Test-Time Warmup\nmethod that adapts the MLLM per test instance by leveraging data from weakly\nsupervised auxiliary tasks. With our approach, we observe a relative\nperformance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on\nthe Llama-Vision-Instruct model. Our method demonstrates that 'warming up'\nbefore inference can enhance MLLMs' robustness across diverse reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) hold great promise for advanced\nreasoning at the intersection of text and images, yet they have not fully\nrealized this potential. MLLMs typically integrate an LLM, a vision encoder,\nand a connector that maps the vision encoder's embeddings into the LLM's text\nembedding space. Although each component is pretrained on massive datasets with\nbillions of samples, the entire multimodal model is typically trained on only\nthousands (or a few million) samples, which can result in weak performance on\ncomplex reasoning tasks. To address these shortcomings, instead of relying on\nextensive labeled datasets for fine-tuning, we propose a Test-Time Warmup\nmethod that adapts the MLLM per test instance by leveraging data from weakly\nsupervised auxiliary tasks. With our approach, we observe a relative\nperformance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on\nthe Llama-Vision-Instruct model. Our method demonstrates that 'warming up'\nbefore inference can enhance MLLMs' robustness across diverse reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Nikita Rajaneesh"
                    },
                    {
                        "name": "Thomas Zollo"
                    },
                    {
                        "name": "Richard Zemel"
                    }
                ],
                "author_detail": {
                    "name": "Richard Zemel"
                },
                "author": "Richard Zemel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02578v3",
                "updated": "2025-11-06T12:23:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    23,
                    24,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-02T21:38:26Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    21,
                    38,
                    26,
                    3,
                    275,
                    0
                ],
                "title": "FLOWR.root: A flow matching based foundation model for joint\n  multi-purpose structure-aware 3D ligand generation and affinity prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLOWR.root: A flow matching based foundation model for joint\n  multi-purpose structure-aware 3D ligand generation and affinity prediction"
                },
                "summary": "We present FLOWR:root, an equivariant flow-matching model for pocket-aware 3D\nligand generation with joint binding affinity prediction and confidence\nestimation. The model supports de novo generation, pharmacophore-conditional\nsampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50,\npKi, pKd, pEC50). Training combines large-scale ligand libraries with\nmixed-fidelity protein-ligand complexes, followed by refinement on curated\nco-crystal datasets and parameter-efficient finetuning for project-specific\nadaptation. FLOWR:root achieves state-of-the-art performance in unconditional\n3D molecule generation and pocket-conditional ligand design, producing\ngeometrically realistic, low-strain structures. The integrated affinity\nprediction module demonstrates superior accuracy on the SPINDR test set and\noutperforms recent models on the Schrodinger FEP+/OpenFE benchmark with\nsubstantial speed advantages. As a foundation model, FLOWR:root requires\nfinetuning on project-specific datasets to account for unseen\nstructure-activity landscapes, yielding strong correlation with experimental\ndata. Joint generation and affinity prediction enable inference-time scaling\nthrough importance sampling, steering molecular design toward higher-affinity\ncompounds. Case studies validate this: selective CK2$\\alpha$ ligand generation\nagainst CLK3 shows significant correlation between predicted and\nquantum-mechanical binding energies, while ER$\\alpha$, TYK2 and BACE1 scaffold\nelaboration demonstrates strong agreement with QM calculations. By integrating\nstructure-aware generation, affinity estimation, and property-guided sampling,\nFLOWR:root provides a comprehensive foundation for structure-based drug design\nspanning hit identification through lead optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FLOWR:root, an equivariant flow-matching model for pocket-aware 3D\nligand generation with joint binding affinity prediction and confidence\nestimation. The model supports de novo generation, pharmacophore-conditional\nsampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50,\npKi, pKd, pEC50). Training combines large-scale ligand libraries with\nmixed-fidelity protein-ligand complexes, followed by refinement on curated\nco-crystal datasets and parameter-efficient finetuning for project-specific\nadaptation. FLOWR:root achieves state-of-the-art performance in unconditional\n3D molecule generation and pocket-conditional ligand design, producing\ngeometrically realistic, low-strain structures. The integrated affinity\nprediction module demonstrates superior accuracy on the SPINDR test set and\noutperforms recent models on the Schrodinger FEP+/OpenFE benchmark with\nsubstantial speed advantages. As a foundation model, FLOWR:root requires\nfinetuning on project-specific datasets to account for unseen\nstructure-activity landscapes, yielding strong correlation with experimental\ndata. Joint generation and affinity prediction enable inference-time scaling\nthrough importance sampling, steering molecular design toward higher-affinity\ncompounds. Case studies validate this: selective CK2$\\alpha$ ligand generation\nagainst CLK3 shows significant correlation between predicted and\nquantum-mechanical binding energies, while ER$\\alpha$, TYK2 and BACE1 scaffold\nelaboration demonstrates strong agreement with QM calculations. By integrating\nstructure-aware generation, affinity estimation, and property-guided sampling,\nFLOWR:root provides a comprehensive foundation for structure-based drug design\nspanning hit identification through lead optimization."
                },
                "authors": [
                    {
                        "name": "Julian Cremer"
                    },
                    {
                        "name": "Tuan Le"
                    },
                    {
                        "name": "Mohammad M. Ghahremanpour"
                    },
                    {
                        "name": "Emilia Sługocka"
                    },
                    {
                        "name": "Filipe Menezes"
                    },
                    {
                        "name": "Djork-Arné Clevert"
                    }
                ],
                "author_detail": {
                    "name": "Djork-Arné Clevert"
                },
                "author": "Djork-Arné Clevert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04307v1",
                "updated": "2025-11-06T12:19:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    19,
                    2,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:19:02Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    19,
                    2,
                    3,
                    310,
                    0
                ],
                "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents"
                },
                "summary": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360."
                },
                "authors": [
                    {
                        "name": "Jian Mu"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Chiming Ni"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Bo Qiao"
                    },
                    {
                        "name": "Kartik Mathur"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Xiaojun Ma"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04737v2",
                "updated": "2025-11-06T12:02:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    2,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-04-07T05:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    5,
                    27,
                    32,
                    0,
                    97,
                    0
                ],
                "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction\n  and Explanation in the Indian Legal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction\n  and Explanation in the Indian Legal Context"
                },
                "summary": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Paper accepted in the AACL-IJCNLP 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21928v2",
                "updated": "2025-11-06T12:01:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    1,
                    57,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-29T15:44:55Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    15,
                    44,
                    55,
                    1,
                    210,
                    0
                ],
                "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software\n  Development: Definition, Implications, and Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vibe Coding as a Reconfiguration of Intent Mediation in Software\n  Development: Definition, Implications, and Research Agenda"
                },
                "summary": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm."
                },
                "authors": [
                    {
                        "name": "Christian Meske"
                    },
                    {
                        "name": "Tobias Hermanns"
                    },
                    {
                        "name": "Esther von der Weiden"
                    },
                    {
                        "name": "Kai-Uwe Loser"
                    },
                    {
                        "name": "Thorsten Berger"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Berger"
                },
                "author": "Thorsten Berger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15980v2",
                "updated": "2025-11-06T11:55:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    55,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-19T02:56:06Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    56,
                    6,
                    3,
                    170,
                    0
                ],
                "title": "Advanced Sign Language Video Generation with Compressed and Quantized\n  Multi-Condition Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Sign Language Video Generation with Compressed and Quantized\n  Multi-Condition Tokenization"
                },
                "summary": "Sign Language Video Generation (SLVG) seeks to generate identity-preserving\nsign language videos from spoken language texts. Existing methods primarily\nrely on the single coarse condition (\\eg, skeleton sequences) as the\nintermediary to bridge the translation model and the video generation model,\nwhich limits both the naturalness and expressiveness of the generated videos.\nTo overcome these limitations, we propose SignViP, a novel SLVG framework that\nincorporates multiple fine-grained conditions for improved generation fidelity.\nRather than directly translating error-prone high-dimensional conditions,\nSignViP adopts a discrete tokenization paradigm to integrate and represent\nfine-grained conditions (\\ie, fine-grained poses and 3D hands). SignViP\ncontains three core components. (1) Sign Video Diffusion Model is jointly\ntrained with a multi-condition encoder to learn continuous embeddings that\nencapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization\n(FSQ) Autoencoder is further trained to compress and quantize these embeddings\ninto discrete tokens for compact representation of the conditions. (3)\nMulti-Condition Token Translator is trained to translate spoken language text\nto discrete multi-condition tokens. During inference, Multi-Condition Token\nTranslator first translates the spoken language text into discrete\nmulti-condition tokens. These tokens are then decoded to continuous embeddings\nby FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion\nModel to guide video generation. Experimental results show that SignViP\nachieves state-of-the-art performance across metrics, including video quality,\ntemporal coherence, and semantic fidelity. The code is available at\nhttps://github.com/umnooob/signvip/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Video Generation (SLVG) seeks to generate identity-preserving\nsign language videos from spoken language texts. Existing methods primarily\nrely on the single coarse condition (\\eg, skeleton sequences) as the\nintermediary to bridge the translation model and the video generation model,\nwhich limits both the naturalness and expressiveness of the generated videos.\nTo overcome these limitations, we propose SignViP, a novel SLVG framework that\nincorporates multiple fine-grained conditions for improved generation fidelity.\nRather than directly translating error-prone high-dimensional conditions,\nSignViP adopts a discrete tokenization paradigm to integrate and represent\nfine-grained conditions (\\ie, fine-grained poses and 3D hands). SignViP\ncontains three core components. (1) Sign Video Diffusion Model is jointly\ntrained with a multi-condition encoder to learn continuous embeddings that\nencapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization\n(FSQ) Autoencoder is further trained to compress and quantize these embeddings\ninto discrete tokens for compact representation of the conditions. (3)\nMulti-Condition Token Translator is trained to translate spoken language text\nto discrete multi-condition tokens. During inference, Multi-Condition Token\nTranslator first translates the spoken language text into discrete\nmulti-condition tokens. These tokens are then decoded to continuous embeddings\nby FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion\nModel to guide video generation. Experimental results show that SignViP\nachieves state-of-the-art performance across metrics, including video quality,\ntemporal coherence, and semantic fidelity. The code is available at\nhttps://github.com/umnooob/signvip/."
                },
                "authors": [
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zexuan Deng"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Fei Shen"
                    },
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Shiping Ge"
                    },
                    {
                        "name": "Shiwei Gan"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.04668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04668v1",
                "updated": "2025-11-06T18:53:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    53,
                    31,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:53:31Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    53,
                    31,
                    3,
                    310,
                    0
                ],
                "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding"
                },
                "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks."
                },
                "authors": [
                    {
                        "name": "Ellis Brown"
                    },
                    {
                        "name": "Arijit Ray"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ross Girshick"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Project page: https://ellisbrown.github.io/sims-v",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04662v1",
                "updated": "2025-11-06T18:50:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    50,
                    8,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:50:08Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    50,
                    8,
                    3,
                    310,
                    0
                ],
                "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks"
                },
                "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Nathaniel Weir"
                    },
                    {
                        "name": "Kaj Bostrom"
                    },
                    {
                        "name": "Sam Bayless"
                    },
                    {
                        "name": "Darion Cassel"
                    },
                    {
                        "name": "Sapana Chaudhary"
                    },
                    {
                        "name": "Benjamin Kiesl-Reiter"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    }
                ],
                "author_detail": {
                    "name": "Huzefa Rangwala"
                },
                "author": "Huzefa Rangwala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04658v1",
                "updated": "2025-11-06T18:44:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    44,
                    21,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:44:21Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    44,
                    21,
                    3,
                    310,
                    0
                ],
                "title": "Where to Experiment? Site Selection Under Distribution Shift via Optimal\n  Transport and Wasserstein DRO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Experiment? Site Selection Under Distribution Shift via Optimal\n  Transport and Wasserstein DRO"
                },
                "summary": "How should researchers select experimental sites when the deployment\npopulation differs from observed data? I formulate the problem of experimental\nsite selection as an optimal transport problem, developing methods to minimize\ndownstream estimation error by choosing sites that minimize the Wasserstein\ndistance between population and sample covariate distributions. I develop new\ntheoretical upper bounds on PATE and CATE estimation errors, and show that\nthese different objectives lead to different site selection strategies. I\nextend this approach by using Wasserstein Distributionally Robust Optimization\nto develop a site selection procedure robust to adversarial perturbations of\ncovariate information: a specific model of distribution shift. I also propose a\nnovel data-driven procedure for selecting the uncertainty radius the\nWasserstein DRO problem, which allows the user to benchmark robustness levels\nagainst observed variation in their data. Simulation evidence, and a reanalysis\nof a randomized microcredit experiment in Morocco (Cr\\'epon et al.), show that\nthese methods outperform random and stratified sampling of sites when\ncovariates have prognostic R-squared > .5, and alternative optimization methods\ni) for moderate-to-large size problem instances ii) when covariates are\nmoderately informative about treatment effects, and iii) under induced\ndistribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How should researchers select experimental sites when the deployment\npopulation differs from observed data? I formulate the problem of experimental\nsite selection as an optimal transport problem, developing methods to minimize\ndownstream estimation error by choosing sites that minimize the Wasserstein\ndistance between population and sample covariate distributions. I develop new\ntheoretical upper bounds on PATE and CATE estimation errors, and show that\nthese different objectives lead to different site selection strategies. I\nextend this approach by using Wasserstein Distributionally Robust Optimization\nto develop a site selection procedure robust to adversarial perturbations of\ncovariate information: a specific model of distribution shift. I also propose a\nnovel data-driven procedure for selecting the uncertainty radius the\nWasserstein DRO problem, which allows the user to benchmark robustness levels\nagainst observed variation in their data. Simulation evidence, and a reanalysis\nof a randomized microcredit experiment in Morocco (Cr\\'epon et al.), show that\nthese methods outperform random and stratified sampling of sites when\ncovariates have prognostic R-squared > .5, and alternative optimization methods\ni) for moderate-to-large size problem instances ii) when covariates are\nmoderately informative about treatment effects, and iii) under induced\ndistribution shift."
                },
                "authors": [
                    {
                        "name": "Adam Bouyamourn"
                    }
                ],
                "author_detail": {
                    "name": "Adam Bouyamourn"
                },
                "author": "Adam Bouyamourn",
                "arxiv_comment": "71 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07325v2",
                "updated": "2025-11-06T18:38:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    38,
                    30,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-09T01:49:29Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    1,
                    49,
                    29,
                    1,
                    252,
                    0
                ],
                "title": "CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement\n  Estimation"
                },
                "summary": "The National Comprehensive Cancer Network (NCCN) provides evidence-based\nguidelines for cancer treatment. Translating complex patient presentations into\nguideline-compliant treatment recommendations is time-intensive, requires\nspecialized expertise, and is prone to error. Advances in large language model\n(LLM) capabilities promise to reduce the time required to generate treatment\nrecommendations and improve accuracy. We present an LLM agent-based approach to\nautomatically generate guideline-concordant treatment trajectories for patients\nwith non-small cell lung cancer (NSCLC). Our contributions are threefold.\nFirst, we construct a novel longitudinal dataset of 121 cases of NSCLC patients\nthat includes clinical encounters, diagnostic results, and medical histories,\neach expertly annotated with the corresponding NCCN guideline trajectories by\nboard-certified oncologists. Second, we demonstrate that existing LLMs possess\ndomain-specific knowledge that enables high-quality proxy benchmark generation\nfor both model development and evaluation, achieving strong correlation\n(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.\nThird, we develop a hybrid approach combining expensive human annotations with\nmodel consistency information to create both the agent framework that predicts\nthe relevant guidelines for a patient, as well as a meta-classifier that\nverifies prediction accuracy with calibrated confidence scores for treatment\nrecommendations (AUROC=0.800), a critical capability for communicating the\naccuracy of outputs, custom-tailoring tradeoffs in performance, and supporting\nregulatory compliance. This work establishes a framework for clinically viable\nLLM-based guideline adherence systems that balance accuracy, interpretability,\nand regulatory requirements while reducing annotation costs, providing a\nscalable pathway toward automated clinical decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The National Comprehensive Cancer Network (NCCN) provides evidence-based\nguidelines for cancer treatment. Translating complex patient presentations into\nguideline-compliant treatment recommendations is time-intensive, requires\nspecialized expertise, and is prone to error. Advances in large language model\n(LLM) capabilities promise to reduce the time required to generate treatment\nrecommendations and improve accuracy. We present an LLM agent-based approach to\nautomatically generate guideline-concordant treatment trajectories for patients\nwith non-small cell lung cancer (NSCLC). Our contributions are threefold.\nFirst, we construct a novel longitudinal dataset of 121 cases of NSCLC patients\nthat includes clinical encounters, diagnostic results, and medical histories,\neach expertly annotated with the corresponding NCCN guideline trajectories by\nboard-certified oncologists. Second, we demonstrate that existing LLMs possess\ndomain-specific knowledge that enables high-quality proxy benchmark generation\nfor both model development and evaluation, achieving strong correlation\n(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.\nThird, we develop a hybrid approach combining expensive human annotations with\nmodel consistency information to create both the agent framework that predicts\nthe relevant guidelines for a patient, as well as a meta-classifier that\nverifies prediction accuracy with calibrated confidence scores for treatment\nrecommendations (AUROC=0.800), a critical capability for communicating the\naccuracy of outputs, custom-tailoring tradeoffs in performance, and supporting\nregulatory compliance. This work establishes a framework for clinically viable\nLLM-based guideline adherence systems that balance accuracy, interpretability,\nand regulatory requirements while reducing annotation costs, providing a\nscalable pathway toward automated clinical decision support."
                },
                "authors": [
                    {
                        "name": "Alyssa Unell"
                    },
                    {
                        "name": "Noel C. F. Codella"
                    },
                    {
                        "name": "Sam Preston"
                    },
                    {
                        "name": "Peniel Argaw"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Zelalem Gero"
                    },
                    {
                        "name": "Cliff Wong"
                    },
                    {
                        "name": "Rajesh Jena"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Amanda K. Hall"
                    },
                    {
                        "name": "Ruican Rachel Zhong"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Mu Wei"
                    },
                    {
                        "name": "Matthew Lungren"
                    },
                    {
                        "name": "Hoifung Poon"
                    }
                ],
                "author_detail": {
                    "name": "Hoifung Poon"
                },
                "author": "Hoifung Poon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04646v1",
                "updated": "2025-11-06T18:37:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    37,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:37:18Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    37,
                    18,
                    3,
                    310,
                    0
                ],
                "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for\n  Embodied LLM-Based Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for\n  Embodied LLM-Based Multi-Agent Collaboration"
                },
                "summary": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies."
                },
                "authors": [
                    {
                        "name": "Narjes Nourzad"
                    },
                    {
                        "name": "Hanqing Yang"
                    },
                    {
                        "name": "Shiyu Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04643v1",
                "updated": "2025-11-06T18:35:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    35,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T18:35:45Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    35,
                    45,
                    3,
                    310,
                    0
                ],
                "title": "When retrieval outperforms generation: Dense evidence retrieval for\n  scalable fake news detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When retrieval outperforms generation: Dense evidence retrieval for\n  scalable fake news detection"
                },
                "summary": "The proliferation of misinformation necessitates robust yet computationally\nefficient fact verification systems. While current state-of-the-art approaches\nleverage Large Language Models (LLMs) for generating explanatory rationales,\nthese methods face significant computational barriers and hallucination risks\nin real-world deployments. We present DeReC (Dense Retrieval Classification), a\nlightweight framework that demonstrates how general-purpose text embeddings can\neffectively replace autoregressive LLM-based approaches in fact verification\ntasks. By combining dense retrieval with specialized classification, our system\nachieves better accuracy while being significantly more efficient. DeReC\noutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%\non RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%\non LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),\nshowcasing its effectiveness across varying dataset sizes. On the RAWFC\ndataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art\nmethod L-Defense (61.20%). Our results demonstrate that carefully engineered\nretrieval-based systems can match or exceed LLM performance in specialized\ntasks while being significantly more practical for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation necessitates robust yet computationally\nefficient fact verification systems. While current state-of-the-art approaches\nleverage Large Language Models (LLMs) for generating explanatory rationales,\nthese methods face significant computational barriers and hallucination risks\nin real-world deployments. We present DeReC (Dense Retrieval Classification), a\nlightweight framework that demonstrates how general-purpose text embeddings can\neffectively replace autoregressive LLM-based approaches in fact verification\ntasks. By combining dense retrieval with specialized classification, our system\nachieves better accuracy while being significantly more efficient. DeReC\noutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%\non RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%\non LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),\nshowcasing its effectiveness across varying dataset sizes. On the RAWFC\ndataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art\nmethod L-Defense (61.20%). Our results demonstrate that carefully engineered\nretrieval-based systems can match or exceed LLM performance in specialized\ntasks while being significantly more practical for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Alamgir Munir Qazi"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Jamal Abdul Nasir"
                    }
                ],
                "author_detail": {
                    "name": "Jamal Abdul Nasir"
                },
                "author": "Jamal Abdul Nasir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03092v2",
                "updated": "2025-11-06T18:27:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    27,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching."
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Haggstrom"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Hakan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08604v2",
                "updated": "2025-11-06T18:15:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    15,
                    30,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-10T14:02:18Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    18,
                    2,
                    253,
                    0
                ],
                "title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information."
                },
                "authors": [
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Mengmeng Du"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Yihang Fu"
                    },
                    {
                        "name": "Erica Stutz"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Siru Liu"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    },
                    {
                        "name": "Hyunghoon Cho"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Qingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingyu Chen"
                },
                "author": "Qingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07110v2",
                "updated": "2025-11-06T18:08:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    8,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-03-18T20:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    38,
                    31,
                    1,
                    77,
                    0
                ],
                "title": "DashCLIP: Leveraging multimodal models for generating semantic\n  embeddings for DoorDash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DashCLIP: Leveraging multimodal models for generating semantic\n  embeddings for DoorDash"
                },
                "summary": "Despite the success of vision-language models in various generative tasks,\nobtaining high-quality semantic representations for products and user intents\nis still challenging due to the inability of off-the-shelf models to capture\nnuanced relationships between the entities. In this paper, we introduce a joint\ntraining framework for product and user queries by aligning uni-modal and\nmulti-modal encoders through contrastive learning on image-text data. Our novel\napproach trains a query encoder with an LLM-curated relevance dataset,\neliminating the reliance on engagement history. These embeddings demonstrate\nstrong generalization capabilities and improve performance across applications,\nincluding product categorization and relevance prediction. For personalized ads\nrecommendation, a significant uplift in the click-through rate and conversion\nrate after the deployment further confirms the impact on key business metrics.\nWe believe that the flexibility of our framework makes it a promising solution\ntoward enriching the user experience across the e-commerce landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of vision-language models in various generative tasks,\nobtaining high-quality semantic representations for products and user intents\nis still challenging due to the inability of off-the-shelf models to capture\nnuanced relationships between the entities. In this paper, we introduce a joint\ntraining framework for product and user queries by aligning uni-modal and\nmulti-modal encoders through contrastive learning on image-text data. Our novel\napproach trains a query encoder with an LLM-curated relevance dataset,\neliminating the reliance on engagement history. These embeddings demonstrate\nstrong generalization capabilities and improve performance across applications,\nincluding product categorization and relevance prediction. For personalized ads\nrecommendation, a significant uplift in the click-through rate and conversion\nrate after the deployment further confirms the impact on key business metrics.\nWe believe that the flexibility of our framework makes it a promising solution\ntoward enriching the user experience across the e-commerce landscape."
                },
                "authors": [
                    {
                        "name": "Omkar Gurjar"
                    },
                    {
                        "name": "Kin Sum Liu"
                    },
                    {
                        "name": "Praveen Kolli"
                    },
                    {
                        "name": "Utsaw Kumar"
                    },
                    {
                        "name": "Mandar Rahurkar"
                    }
                ],
                "author_detail": {
                    "name": "Mandar Rahurkar"
                },
                "author": "Mandar Rahurkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04601v1",
                "updated": "2025-11-06T17:54:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    54,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:54:12Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    54,
                    12,
                    3,
                    310,
                    0
                ],
                "title": "PixCLIP: Achieving Fine-grained Visual Language Understanding via\n  Any-granularity Pixel-Text Alignment Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PixCLIP: Achieving Fine-grained Visual Language Understanding via\n  Any-granularity Pixel-Text Alignment Learning"
                },
                "summary": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved\nremarkable success in a variety of downstream vison language understanding\ntasks, enhancing its capability for fine-grained image-text alignment remains\nan active research focus. To this end, most existing works adopt the strategy\nof explicitly increasing the granularity of visual information processing,\ne.g., incorporating visual prompts to guide the model focus on specific local\nregions within the image. Meanwhile, researches on Multimodal Large Language\nModels(MLLMs) have demonstrated that training with long and detailed textual\ndescriptions can effectively improve the model's fine-grained vision-language\nalignment. However, the inherent token length limitation of CLIP's text encoder\nfundamentally limits CLIP to process more granular textual information embedded\nin long text sequences. To synergistically leverage the advantages of enhancing\nboth visual and textual content processing granularity, we propose PixCLIP, a\nnovel framework designed to concurrently accommodate visual prompt inputs and\nprocess lengthy textual descriptions. Specifically, we first establish an\nautomated annotation pipeline capable of generating pixel-level localized,\nlong-form textual descriptions for images. Utilizing this pipeline, we\nconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 million\nsamples. Secondly, we replace CLIP's original text encoder with the LLM and\npropose a three-branch pixel-text alignment learning framework, facilitating\nfine-grained alignment between image regions and corresponding textual\ndescriptions at arbitrary granularity. Experiments demonstrate that PixCLIP\nshowcases breakthroughs in pixel-level interaction and handling long-form\ntexts, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved\nremarkable success in a variety of downstream vison language understanding\ntasks, enhancing its capability for fine-grained image-text alignment remains\nan active research focus. To this end, most existing works adopt the strategy\nof explicitly increasing the granularity of visual information processing,\ne.g., incorporating visual prompts to guide the model focus on specific local\nregions within the image. Meanwhile, researches on Multimodal Large Language\nModels(MLLMs) have demonstrated that training with long and detailed textual\ndescriptions can effectively improve the model's fine-grained vision-language\nalignment. However, the inherent token length limitation of CLIP's text encoder\nfundamentally limits CLIP to process more granular textual information embedded\nin long text sequences. To synergistically leverage the advantages of enhancing\nboth visual and textual content processing granularity, we propose PixCLIP, a\nnovel framework designed to concurrently accommodate visual prompt inputs and\nprocess lengthy textual descriptions. Specifically, we first establish an\nautomated annotation pipeline capable of generating pixel-level localized,\nlong-form textual descriptions for images. Utilizing this pipeline, we\nconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 million\nsamples. Secondly, we replace CLIP's original text encoder with the LLM and\npropose a three-branch pixel-text alignment learning framework, facilitating\nfine-grained alignment between image regions and corresponding textual\ndescriptions at arbitrary granularity. Experiments demonstrate that PixCLIP\nshowcases breakthroughs in pixel-level interaction and handling long-form\ntexts, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Caorui Li"
                    },
                    {
                        "name": "Lingxiang Wu"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04588v1",
                "updated": "2025-11-06T17:45:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    45,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:45:12Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    45,
                    12,
                    3,
                    310,
                    0
                ],
                "title": "Question the Questions: Auditing Representation in Online Deliberative\n  Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question the Questions: Auditing Representation in Online Deliberative\n  Processes"
                },
                "summary": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations."
                },
                "authors": [
                    {
                        "name": "Soham De"
                    },
                    {
                        "name": "Lodewijk Gelauff"
                    },
                    {
                        "name": "Ashish Goel"
                    },
                    {
                        "name": "Smitha Milli"
                    },
                    {
                        "name": "Ariel Procaccia"
                    },
                    {
                        "name": "Alice Siu"
                    }
                ],
                "author_detail": {
                    "name": "Alice Siu"
                },
                "author": "Alice Siu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27052v2",
                "updated": "2025-11-06T17:44:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    44,
                    55,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-30T23:45:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    23,
                    45,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "VISTA Score: Verification In Sequential Turn-based Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA Score: Verification In Sequential Turn-based Assessment"
                },
                "summary": "Hallucination--defined here as generating statements unsupported or\ncontradicted by available evidence or conversational context--remains a major\nobstacle to deploying conversational AI systems in settings that demand factual\nreliability. Existing metrics either evaluate isolated responses or treat\nunverifiable content as errors, limiting their use for multi-turn dialogue. We\nintroduce VISTA (Verification In Sequential Turn-based Assessment), a framework\nfor evaluating conversational factuality through claim-level verification and\nsequential consistency tracking. VISTA decomposes each assistant turn into\natomic factual claims, verifies them against trusted sources and dialogue\nhistory, and categorizes unverifiable statements (subjective, contradicted,\nlacking evidence, or abstaining). Across eight large language models and four\ndialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA\nsubstantially improves hallucination detection over FACTSCORE and LLM-as-Judge\nbaselines. Human evaluation confirms that VISTA's decomposition improves\nannotator agreement and reveals inconsistencies in existing benchmarks. By\nmodeling factuality as a dynamic property of conversation, VISTA offers a more\ntransparent, human-aligned measure of truthfulness in dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination--defined here as generating statements unsupported or\ncontradicted by available evidence or conversational context--remains a major\nobstacle to deploying conversational AI systems in settings that demand factual\nreliability. Existing metrics either evaluate isolated responses or treat\nunverifiable content as errors, limiting their use for multi-turn dialogue. We\nintroduce VISTA (Verification In Sequential Turn-based Assessment), a framework\nfor evaluating conversational factuality through claim-level verification and\nsequential consistency tracking. VISTA decomposes each assistant turn into\natomic factual claims, verifies them against trusted sources and dialogue\nhistory, and categorizes unverifiable statements (subjective, contradicted,\nlacking evidence, or abstaining). Across eight large language models and four\ndialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA\nsubstantially improves hallucination detection over FACTSCORE and LLM-as-Judge\nbaselines. Human evaluation confirms that VISTA's decomposition improves\nannotator agreement and reveals inconsistencies in existing benchmarks. By\nmodeling factuality as a dynamic property of conversation, VISTA offers a more\ntransparent, human-aligned measure of truthfulness in dialogue systems."
                },
                "authors": [
                    {
                        "name": "Ashley Lewis"
                    },
                    {
                        "name": "Andrew Perrault"
                    },
                    {
                        "name": "Eric Fosler-Lussier"
                    },
                    {
                        "name": "Michael White"
                    }
                ],
                "author_detail": {
                    "name": "Michael White"
                },
                "author": "Michael White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03325v2",
                "updated": "2025-11-06T17:28:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    28,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T09:40:16Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    40,
                    16,
                    2,
                    309,
                    0
                ],
                "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding"
                },
                "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA."
                },
                "authors": [
                    {
                        "name": "Mauro Orazio Drago"
                    },
                    {
                        "name": "Luca Carlini"
                    },
                    {
                        "name": "Pelinsu Celebi Balyemez"
                    },
                    {
                        "name": "Dennis Pierantozzi"
                    },
                    {
                        "name": "Chiara Lena"
                    },
                    {
                        "name": "Cesare Hassan"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Elena De Momi"
                    },
                    {
                        "name": "Sophia Bano"
                    },
                    {
                        "name": "Mobarak I. Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Mobarak I. Hoque"
                },
                "author": "Mobarak I. Hoque",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04570v1",
                "updated": "2025-11-06T17:25:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    25,
                    23,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:25:23Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    25,
                    23,
                    3,
                    310,
                    0
                ],
                "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm"
                },
                "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Yongzhuo Yang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Xinchi Chen"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "36 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v2",
                "updated": "2025-11-06T17:09:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    9,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures;Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04555v1",
                "updated": "2025-11-06T17:07:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    7,
                    49,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:07:49Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    7,
                    49,
                    3,
                    310,
                    0
                ],
                "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic\n  Alignment"
                },
                "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models."
                },
                "authors": [
                    {
                        "name": "Tao Lin"
                    },
                    {
                        "name": "Yilei Zhong"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Jingjing Zhang"
                    },
                    {
                        "name": "Jiting Liu"
                    },
                    {
                        "name": "Yinxinyu Chen"
                    },
                    {
                        "name": "Encheng Gu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Yanwen Zou"
                    },
                    {
                        "name": "Lixing Zou"
                    },
                    {
                        "name": "Zhaoye Zhou"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "arxiv_comment": "Github: https://github.com/MINT-SJTU/Evo-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04550v1",
                "updated": "2025-11-06T17:03:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    3,
                    33,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T17:03:33Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    3,
                    33,
                    3,
                    310,
                    0
                ],
                "title": "Confidential Computing for Cloud Security: Exploring Hardware based\n  Encryption Using Trusted Execution Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing for Cloud Security: Exploring Hardware based\n  Encryption Using Trusted Execution Environments"
                },
                "summary": "The growth of cloud computing has revolutionized data processing and storage\ncapacities to another levels of scalability and flexibility. But in the\nprocess, it has created a huge challenge of security, especially in terms of\nsafeguarding sensitive data. Classical security practices, including encryption\nat rest and during transit, fail to protect data in use and expose it to\nvarious possible breaches. In response to this problem , Confidential Computing\nhas been a tool ,seeking to secure data in processing by usage of\nhardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's\nSoftware Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts\nwithin the processor, where data is kept confidential ,intact and secure , even\nwith malicious software or compromised operating systems. In this research, we\nhave explored the architecture and security features of TEEs like Intel SGX and\nARM TrustZone, and their effectiveness in improving cloud data security. From a\nthorough literature survey ,we have analyzed the deployment strategies,\nperformance indicators, and practical uses of these TEEs for the same purpose.\nIn addition, we have discussed the issues regarding deployment, possible\nweaknesses, scalability issues, and integration issues. Our results focuses on\nthe central position of TEEs in strengthening and advancing cloud security\ninfrastructures, pointing towards their ability to create a secure foundation\nfor Confidential Computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of cloud computing has revolutionized data processing and storage\ncapacities to another levels of scalability and flexibility. But in the\nprocess, it has created a huge challenge of security, especially in terms of\nsafeguarding sensitive data. Classical security practices, including encryption\nat rest and during transit, fail to protect data in use and expose it to\nvarious possible breaches. In response to this problem , Confidential Computing\nhas been a tool ,seeking to secure data in processing by usage of\nhardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's\nSoftware Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts\nwithin the processor, where data is kept confidential ,intact and secure , even\nwith malicious software or compromised operating systems. In this research, we\nhave explored the architecture and security features of TEEs like Intel SGX and\nARM TrustZone, and their effectiveness in improving cloud data security. From a\nthorough literature survey ,we have analyzed the deployment strategies,\nperformance indicators, and practical uses of these TEEs for the same purpose.\nIn addition, we have discussed the issues regarding deployment, possible\nweaknesses, scalability issues, and integration issues. Our results focuses on\nthe central position of TEEs in strengthening and advancing cloud security\ninfrastructures, pointing towards their ability to create a secure foundation\nfor Confidential Computing."
                },
                "authors": [
                    {
                        "name": "Dhruv Deepak Agarwal"
                    },
                    {
                        "name": "Aswani Kumar Cherukuri"
                    }
                ],
                "author_detail": {
                    "name": "Aswani Kumar Cherukuri"
                },
                "author": "Aswani Kumar Cherukuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04541v1",
                "updated": "2025-11-06T16:54:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    54,
                    54,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:54:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    54,
                    54,
                    3,
                    310,
                    0
                ],
                "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems"
                },
                "summary": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems."
                },
                "authors": [
                    {
                        "name": "Baptiste Bonin"
                    },
                    {
                        "name": "Maxime Heuillet"
                    },
                    {
                        "name": "Audrey Durand"
                    }
                ],
                "author_detail": {
                    "name": "Audrey Durand"
                },
                "author": "Audrey Durand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03179v2",
                "updated": "2025-11-06T16:54:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    54,
                    41,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T04:55:25Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    4,
                    55,
                    25,
                    2,
                    309,
                    0
                ],
                "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent\n  Framework"
                },
                "summary": "The engineering design process often demands expertise from multiple domains,\nleading to complex collaborations and iterative refinements. Traditional\nmethods can be resource-intensive and prone to inefficiencies. To address this,\nwe formalize the engineering design process through a multi-agent AI framework\nthat integrates structured design and review loops. The framework introduces\nspecialized knowledge-driven agents that collaborate to generate and refine\ndesign candidates. As an exemplar, we demonstrate its application to the\naerodynamic optimization of 4-digit NACA airfoils. The framework consists of\nthree key AI agents: a Graph Ontologist, a Design Engineer, and a Systems\nEngineer. The Graph Ontologist employs a Large Language Model (LLM) to\nconstruct two domain-specific knowledge graphs from airfoil design literature.\nThe Systems Engineer, informed by a human manager, formulates technical\nrequirements that guide design generation and evaluation. The Design Engineer\nleverages the design knowledge graph and computational tools to propose\ncandidate airfoils meeting these requirements. The Systems Engineer reviews and\nprovides feedback both qualitative and quantitative using its own knowledge\ngraph, forming an iterative feedback loop until a design is validated by the\nmanager. The final design is then optimized to maximize performance metrics\nsuch as the lift-to-drag ratio. Overall, this work demonstrates how\ncollaborative AI agents equipped with structured knowledge representations can\nenhance efficiency, consistency, and quality in the engineering design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The engineering design process often demands expertise from multiple domains,\nleading to complex collaborations and iterative refinements. Traditional\nmethods can be resource-intensive and prone to inefficiencies. To address this,\nwe formalize the engineering design process through a multi-agent AI framework\nthat integrates structured design and review loops. The framework introduces\nspecialized knowledge-driven agents that collaborate to generate and refine\ndesign candidates. As an exemplar, we demonstrate its application to the\naerodynamic optimization of 4-digit NACA airfoils. The framework consists of\nthree key AI agents: a Graph Ontologist, a Design Engineer, and a Systems\nEngineer. The Graph Ontologist employs a Large Language Model (LLM) to\nconstruct two domain-specific knowledge graphs from airfoil design literature.\nThe Systems Engineer, informed by a human manager, formulates technical\nrequirements that guide design generation and evaluation. The Design Engineer\nleverages the design knowledge graph and computational tools to propose\ncandidate airfoils meeting these requirements. The Systems Engineer reviews and\nprovides feedback both qualitative and quantitative using its own knowledge\ngraph, forming an iterative feedback loop until a design is validated by the\nmanager. The final design is then optimized to maximize performance metrics\nsuch as the lift-to-drag ratio. Overall, this work demonstrates how\ncollaborative AI agents equipped with structured knowledge representations can\nenhance efficiency, consistency, and quality in the engineering design process."
                },
                "authors": [
                    {
                        "name": "Varun Kumar"
                    },
                    {
                        "name": "George Em Karniadakis"
                    }
                ],
                "author_detail": {
                    "name": "George Em Karniadakis"
                },
                "author": "George Em Karniadakis",
                "arxiv_comment": "Revised to fix typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01019v2",
                "updated": "2025-11-06T16:53:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    53,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-02T17:23:58Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    17,
                    23,
                    58,
                    6,
                    306,
                    0
                ],
                "title": "OceanAI: A Conversational Platform for Accurate, Transparent,\n  Near-Real-Time Oceanographic Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OceanAI: A Conversational Platform for Accurate, Transparent,\n  Near-Real-Time Oceanographic Insights"
                },
                "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz."
                },
                "authors": [
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Jayesh Gajbhar"
                    },
                    {
                        "name": "Gregory Dusek"
                    },
                    {
                        "name": "Rob Redmon"
                    },
                    {
                        "name": "Patrick Hogan"
                    },
                    {
                        "name": "Paul Liu"
                    },
                    {
                        "name": "DelWayne Bohnenstiehl"
                    },
                    {
                        "name": "Dongkuan Xu"
                    },
                    {
                        "name": "Ruoying He"
                    }
                ],
                "author_detail": {
                    "name": "Ruoying He"
                },
                "author": "Ruoying He",
                "arxiv_comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.01254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.01254v2",
                "updated": "2025-11-06T16:52:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    52,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2021-06-02T16:07:32Z",
                "published_parsed": [
                    2021,
                    6,
                    2,
                    16,
                    7,
                    32,
                    2,
                    153,
                    0
                ],
                "title": "Rater Equivalence: Evaluating Classifiers in Human Judgment Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rater Equivalence: Evaluating Classifiers in Human Judgment Settings"
                },
                "summary": "In many decision settings, the definitive ground truth is either non-existent\nor inaccessible. We introduce a framework for evaluating classifiers based\nsolely on human judgments. In such cases, it is helpful to compare automated\nclassifiers to human judgment. We quantify a classifier's performance by its\nrater equivalence: the smallest number of human raters whose combined judgment\nmatches the classifier's performance. Our framework uses human-generated labels\nboth to construct benchmark panels and to evaluate performance. We distinguish\nbetween two models of utility: one based on agreement with the assumed but\ninaccessible ground truth, and one based on matching individual human\njudgments. Using case studies and formal analysis, we demonstrate how this\nframework can inform the evaluation and deployment of AI systems in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many decision settings, the definitive ground truth is either non-existent\nor inaccessible. We introduce a framework for evaluating classifiers based\nsolely on human judgments. In such cases, it is helpful to compare automated\nclassifiers to human judgment. We quantify a classifier's performance by its\nrater equivalence: the smallest number of human raters whose combined judgment\nmatches the classifier's performance. Our framework uses human-generated labels\nboth to construct benchmark panels and to evaluate performance. We distinguish\nbetween two models of utility: one based on agreement with the assumed but\ninaccessible ground truth, and one based on matching individual human\njudgments. Using case studies and formal analysis, we demonstrate how this\nframework can inform the evaluation and deployment of AI systems in practice."
                },
                "authors": [
                    {
                        "name": "Paul Resnick"
                    },
                    {
                        "name": "Yuqing Kong"
                    },
                    {
                        "name": "Grant Schoenebeck"
                    },
                    {
                        "name": "Tim Weninger"
                    }
                ],
                "author_detail": {
                    "name": "Tim Weninger"
                },
                "author": "Tim Weninger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2106.01254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.01254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04538v1",
                "updated": "2025-11-06T16:52:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    52,
                    27,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:52:27Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    52,
                    27,
                    3,
                    310,
                    0
                ],
                "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities\n  Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities\n  Reporting"
                },
                "summary": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates."
                },
                "authors": [
                    {
                        "name": "Cyril Vallez"
                    },
                    {
                        "name": "Alexander Sternfeld"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Dolamic"
                },
                "author": "Ljiljana Dolamic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04528v1",
                "updated": "2025-11-06T16:43:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    43,
                    37,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:43:37Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    43,
                    37,
                    3,
                    310,
                    0
                ],
                "title": "IntelliProof: An Argumentation Network-based Conversational Helper for\n  Organized Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntelliProof: An Argumentation Network-based Conversational Helper for\n  Organized Reflection"
                },
                "summary": "We present IntelliProof, an interactive system for analyzing argumentative\nessays through LLMs. IntelliProof structures an essay as an argumentation\ngraph, where claims are represented as nodes, supporting evidence is attached\nas node properties, and edges encode supporting or attacking relations. Unlike\nexisting automated essay scoring systems, IntelliProof emphasizes the user\nexperience: each relation is initially classified and scored by an LLM, then\nvisualized for enhanced understanding. The system provides justifications for\nclassifications and produces quantitative measures for essay coherence. It\nenables rapid exploration of argumentative quality while retaining human\noversight. In addition, IntelliProof provides a set of tools for a better\nunderstanding of an argumentative essay and its corresponding graph in natural\nlanguage, bridging the gap between the structural semantics of argumentative\nessays and the user's understanding of a given text. A live demo and the system\nare available here to try: \\textbf{https://intelliproof.vercel.app}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present IntelliProof, an interactive system for analyzing argumentative\nessays through LLMs. IntelliProof structures an essay as an argumentation\ngraph, where claims are represented as nodes, supporting evidence is attached\nas node properties, and edges encode supporting or attacking relations. Unlike\nexisting automated essay scoring systems, IntelliProof emphasizes the user\nexperience: each relation is initially classified and scored by an LLM, then\nvisualized for enhanced understanding. The system provides justifications for\nclassifications and produces quantitative measures for essay coherence. It\nenables rapid exploration of argumentative quality while retaining human\noversight. In addition, IntelliProof provides a set of tools for a better\nunderstanding of an argumentative essay and its corresponding graph in natural\nlanguage, bridging the gap between the structural semantics of argumentative\nessays and the user's understanding of a given text. A live demo and the system\nare available here to try: \\textbf{https://intelliproof.vercel.app}"
                },
                "authors": [
                    {
                        "name": "Kaveh Eskandari Miandoab"
                    },
                    {
                        "name": "Katharine Kowalyshyn"
                    },
                    {
                        "name": "Kabir Pamnani"
                    },
                    {
                        "name": "Anesu Gavhera"
                    },
                    {
                        "name": "Vasanth Sarathy"
                    },
                    {
                        "name": "Matthias Scheutz"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Scheutz"
                },
                "author": "Matthias Scheutz",
                "arxiv_comment": "Accepted for the 40th Annual AAAI Conference on Artificial\n  Intelligence (2026) - Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18989v2",
                "updated": "2025-11-06T16:26:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    26,
                    13,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-25T06:34:59Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    6,
                    34,
                    59,
                    4,
                    206,
                    0
                ],
                "title": "GENIAL: Generative Design Space Exploration via Network Inversion for\n  Low Power Algorithmic Logic Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENIAL: Generative Design Space Exploration via Network Inversion for\n  Low Power Algorithmic Logic Units"
                },
                "summary": "As AI workloads proliferate, optimizing arithmetic units is becoming\nincreasingly important for reducing the footprint of digital systems.\nConventional design flows, which often rely on manual or heuristic-based\noptimization, are limited in their ability to thoroughly explore the vast\ndesign space. In this paper, we introduce GENIAL, a machine learning-based\nframework for the automatic generation and optimization of arithmetic units,\nwith a focus on multipliers.\n  At the core of GENIAL is a Transformer-based surrogate model trained in two\nstages, involving self-supervised pretraining followed by supervised\nfinetuning, to robustly forecast key hardware metrics such as power and area\nfrom abstracted design representations. By inverting the surrogate model,\nGENIAL efficiently searches for new operand encodings that directly minimize\npower consumption in arithmetic units for specific input data distributions.\nExtensive experiments on large datasets demonstrate that GENIAL is consistently\nmore sample efficient than other methods, and converges faster towards\noptimized designs. This enables deployment of a high-effort logic synthesis\noptimization flow in the loop, improving the accuracy of the surrogate model.\nNotably, GENIAL automatically discovers encodings that achieve up to 18%\nswitching activity savings within multipliers on representative AI workloads\ncompared with the conventional two's complement. We also demonstrate the\nversatility of our approach by achieving significant improvements on Finite\nState Machines, highlighting GENIAL's applicability for a wide spectrum of\nlogic functions. Together, these advances mark a significant step toward\nautomated Quality-of-Results-optimized combinational circuit generation for\ndigital systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads proliferate, optimizing arithmetic units is becoming\nincreasingly important for reducing the footprint of digital systems.\nConventional design flows, which often rely on manual or heuristic-based\noptimization, are limited in their ability to thoroughly explore the vast\ndesign space. In this paper, we introduce GENIAL, a machine learning-based\nframework for the automatic generation and optimization of arithmetic units,\nwith a focus on multipliers.\n  At the core of GENIAL is a Transformer-based surrogate model trained in two\nstages, involving self-supervised pretraining followed by supervised\nfinetuning, to robustly forecast key hardware metrics such as power and area\nfrom abstracted design representations. By inverting the surrogate model,\nGENIAL efficiently searches for new operand encodings that directly minimize\npower consumption in arithmetic units for specific input data distributions.\nExtensive experiments on large datasets demonstrate that GENIAL is consistently\nmore sample efficient than other methods, and converges faster towards\noptimized designs. This enables deployment of a high-effort logic synthesis\noptimization flow in the loop, improving the accuracy of the surrogate model.\nNotably, GENIAL automatically discovers encodings that achieve up to 18%\nswitching activity savings within multipliers on representative AI workloads\ncompared with the conventional two's complement. We also demonstrate the\nversatility of our approach by achieving significant improvements on Finite\nState Machines, highlighting GENIAL's applicability for a wide spectrum of\nlogic functions. Together, these advances mark a significant step toward\nautomated Quality-of-Results-optimized combinational circuit generation for\ndigital systems."
                },
                "authors": [
                    {
                        "name": "Maxence Bouvier"
                    },
                    {
                        "name": "Ryan Amaudruz"
                    },
                    {
                        "name": "Felix Arnold"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "Accepted at the 2026 31st Asia and South Pacific Design Automation\n  Conference (ASP-DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04508v1",
                "updated": "2025-11-06T16:25:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    25,
                    35,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:25:35Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    25,
                    35,
                    3,
                    310,
                    0
                ],
                "title": "Large Language Models for Cyber Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cyber Security"
                },
                "summary": "This paper studies the integration off Large Language Models into\ncybersecurity tools and protocols. The main issue discussed in this paper is\nhow traditional rule-based and signature based security systems are not enough\nto deal with modern AI powered cyber threats. Cybersecurity industry is\nchanging as threats are becoming more dangerous and adaptive in nature by\nlevering the features provided by AI tools. By integrating LLMs into these\ntools and protocols, make the systems scalable, context-aware and intelligent.\nThus helping it to mitigate these evolving cyber threats. The paper studies the\narchitecture and functioning of LLMs, its integration into Encrypted prompts to\nprevent prompt injection attacks. It also studies the integration of LLMs into\ncybersecurity tools using a four layered architecture. At last, the paper has\ntried to explain various ways of integration LLMs into traditional Intrusion\nDetection System and enhancing its original abilities in various dimensions.\nThe key findings of this paper has been (i)Encrypted Prompt with LLM is an\neffective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber\nsecurity tools are more accurate, scalable and adaptable to new threats as\ncompared to traditional models, (iii) The decoupled model approach for LLM\nintegration into IDS is the best way as it is the most accurate way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the integration off Large Language Models into\ncybersecurity tools and protocols. The main issue discussed in this paper is\nhow traditional rule-based and signature based security systems are not enough\nto deal with modern AI powered cyber threats. Cybersecurity industry is\nchanging as threats are becoming more dangerous and adaptive in nature by\nlevering the features provided by AI tools. By integrating LLMs into these\ntools and protocols, make the systems scalable, context-aware and intelligent.\nThus helping it to mitigate these evolving cyber threats. The paper studies the\narchitecture and functioning of LLMs, its integration into Encrypted prompts to\nprevent prompt injection attacks. It also studies the integration of LLMs into\ncybersecurity tools using a four layered architecture. At last, the paper has\ntried to explain various ways of integration LLMs into traditional Intrusion\nDetection System and enhancing its original abilities in various dimensions.\nThe key findings of this paper has been (i)Encrypted Prompt with LLM is an\neffective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber\nsecurity tools are more accurate, scalable and adaptable to new threats as\ncompared to traditional models, (iii) The decoupled model approach for LLM\nintegration into IDS is the best way as it is the most accurate way."
                },
                "authors": [
                    {
                        "name": "Raunak Somani"
                    },
                    {
                        "name": "Aswani Kumar Cherukuri"
                    }
                ],
                "author_detail": {
                    "name": "Aswani Kumar Cherukuri"
                },
                "author": "Aswani Kumar Cherukuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04506v1",
                "updated": "2025-11-06T16:24:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    24,
                    53,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:24:53Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    24,
                    53,
                    3,
                    310,
                    0
                ],
                "title": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit\n  Uncertainty Markers to Implicit Reasoning Pathways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit\n  Uncertainty Markers to Implicit Reasoning Pathways"
                },
                "summary": "Radiology reports are invaluable for clinical decision-making and hold great\npotential for automated analysis when structured into machine-readable formats.\nThese reports often contain uncertainty, which we categorize into two distinct\ntypes: (i) Explicit uncertainty reflects doubt about the presence or absence of\nfindings, conveyed through hedging phrases. These vary in meaning depending on\nthe context, making rule-based systems insufficient to quantify the level of\nuncertainty for specific findings; (ii) Implicit uncertainty arises when\nradiologists omit parts of their reasoning, recording only key findings or\ndiagnoses. Here, it is often unclear whether omitted findings are truly absent\nor simply unmentioned for brevity. We address these challenges with a two-part\nframework. We quantify explicit uncertainty by creating an expert-validated,\nLLM-based reference ranking of common hedging phrases, and mapping each finding\nto a probability value based on this reference. In addition, we model implicit\nuncertainty through an expansion framework that systematically adds\ncharacteristic sub-findings derived from expert-defined diagnostic pathways for\n14 common diagnoses. Using these methods, we release Lunguage++, an expanded,\nuncertainty-aware version of the Lunguage benchmark of fine-grained structured\nradiology reports. This enriched resource enables uncertainty-aware image\nclassification, faithful diagnostic reasoning, and new investigations into the\nclinical impact of diagnostic uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology reports are invaluable for clinical decision-making and hold great\npotential for automated analysis when structured into machine-readable formats.\nThese reports often contain uncertainty, which we categorize into two distinct\ntypes: (i) Explicit uncertainty reflects doubt about the presence or absence of\nfindings, conveyed through hedging phrases. These vary in meaning depending on\nthe context, making rule-based systems insufficient to quantify the level of\nuncertainty for specific findings; (ii) Implicit uncertainty arises when\nradiologists omit parts of their reasoning, recording only key findings or\ndiagnoses. Here, it is often unclear whether omitted findings are truly absent\nor simply unmentioned for brevity. We address these challenges with a two-part\nframework. We quantify explicit uncertainty by creating an expert-validated,\nLLM-based reference ranking of common hedging phrases, and mapping each finding\nto a probability value based on this reference. In addition, we model implicit\nuncertainty through an expansion framework that systematically adds\ncharacteristic sub-findings derived from expert-defined diagnostic pathways for\n14 common diagnoses. Using these methods, we release Lunguage++, an expanded,\nuncertainty-aware version of the Lunguage benchmark of fine-grained structured\nradiology reports. This enriched resource enables uncertainty-aware image\nclassification, faithful diagnostic reasoning, and new investigations into the\nclinical impact of diagnostic uncertainty."
                },
                "authors": [
                    {
                        "name": "Paloma Rabaey"
                    },
                    {
                        "name": "Jong Hak Moon"
                    },
                    {
                        "name": "Jung-Oh Lee"
                    },
                    {
                        "name": "Min Gwan Kim"
                    },
                    {
                        "name": "Hangyul Yoon"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04505v1",
                "updated": "2025-11-06T16:24:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    24,
                    43,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:24:43Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    24,
                    43,
                    3,
                    310,
                    0
                ],
                "title": "Alternative Fairness and Accuracy Optimization in Criminal Justice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alternative Fairness and Accuracy Optimization in Criminal Justice"
                },
                "summary": "Algorithmic fairness has grown rapidly as a research area, yet key concepts\nremain unsettled, especially in criminal justice. We review group, individual,\nand process fairness and map the conditions under which they conflict. We then\ndevelop a simple modification to standard group fairness. Rather than exact\nparity across protected groups, we minimize a weighted error loss while keeping\ndifferences in false negative rates within a small tolerance. This makes\nsolutions easier to find, can raise predictive accuracy, and surfaces the\nethical choice of error costs. We situate this proposal within three classes of\ncritique: biased and incomplete data, latent affirmative action, and the\nexplosion of subgroup constraints. Finally, we offer a practical framework for\ndeployment in public decision systems built on three pillars: need-based\ndecisions, Transparency and accountability, and narrowly tailored definitions\nand solutions. Together, these elements link technical design to legitimacy and\nprovide actionable guidance for agencies that use risk assessment and related\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic fairness has grown rapidly as a research area, yet key concepts\nremain unsettled, especially in criminal justice. We review group, individual,\nand process fairness and map the conditions under which they conflict. We then\ndevelop a simple modification to standard group fairness. Rather than exact\nparity across protected groups, we minimize a weighted error loss while keeping\ndifferences in false negative rates within a small tolerance. This makes\nsolutions easier to find, can raise predictive accuracy, and surfaces the\nethical choice of error costs. We situate this proposal within three classes of\ncritique: biased and incomplete data, latent affirmative action, and the\nexplosion of subgroup constraints. Finally, we offer a practical framework for\ndeployment in public decision systems built on three pillars: need-based\ndecisions, Transparency and accountability, and narrowly tailored definitions\nand solutions. Together, these elements link technical design to legitimacy and\nprovide actionable guidance for agencies that use risk assessment and related\ntools."
                },
                "authors": [
                    {
                        "name": "Shaolong Wu"
                    },
                    {
                        "name": "James Blume"
                    },
                    {
                        "name": "Geshi Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Geshi Yeung"
                },
                "author": "Geshi Yeung",
                "arxiv_comment": "Accepted for presentation at the AAAI 2026 AI Governance Workshop\n  (AIGOV). 24 pages",
                "arxiv_journal_ref": "Proceedings of the AAAI 2026 AI Governance Workshop (AIGOV),\n  Singapore, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04502v1",
                "updated": "2025-11-06T16:22:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:22:52Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    52,
                    3,
                    310,
                    0
                ],
                "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific\n  RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github."
                },
                "authors": [
                    {
                        "name": "Joshua Gao"
                    },
                    {
                        "name": "Quoc Huy Pham"
                    },
                    {
                        "name": "Subin Varghese"
                    },
                    {
                        "name": "Silwal Saurav"
                    },
                    {
                        "name": "Vedhus Hoskere"
                    }
                ],
                "author_detail": {
                    "name": "Vedhus Hoskere"
                },
                "author": "Vedhus Hoskere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22879v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22879v4",
                "updated": "2025-11-06T16:22:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    33,
                    3,
                    310,
                    0
                ],
                "published": "2025-03-28T21:10:39Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    10,
                    39,
                    4,
                    87,
                    0
                ],
                "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models"
                },
                "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms two state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms two state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba."
                },
                "authors": [
                    {
                        "name": "Hung-Yueh Chiang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Natalia Frumkin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22879v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22879v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18597v3",
                "updated": "2025-11-06T16:22:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    22,
                    17,
                    3,
                    310,
                    0
                ],
                "published": "2025-03-24T11:55:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Testora: Using Natural Language Intent to Detect Behavioral Regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testora: Using Natural Language Intent to Detect Behavioral Regressions"
                },
                "summary": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, the first automated approach that\ndetects regressions by comparing the intentions of a code change against\nbehavioral differences caused by the code change. Given a pull request (PR),\nTestora queries an LLM to generate tests that exercise the modified code,\ncompares the behavior of the original and modified code, and classifies any\nbehavioral differences as intended or unintended. For the classification, we\npresent an LLM-based technique that leverages the natural language information\nassociated with the PR, such as the title, description, and commit messages --\neffectively using the natural language intent to detect behavioral regressions.\nApplying Testora to PRs of complex and popular Python projects, we find 19\nregression bugs and 11 PRs that, despite having another intention,\ncoincidentally fix a bug. Out of 13 regressions reported to the developers, 11\nhave been confirmed and 9 have already been fixed. The costs of using Testora\nare acceptable for real-world deployment, with 12.3 minutes to check a PR and\nLLM costs of only $0.003 per PR. We envision our approach to be used before or\nshortly after a code change gets merged into a code base, providing a way to\nearly on detect regressions that are not caught by traditional approaches."
                },
                "authors": [
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted at IEEE/ACM International Conference on Software Engineering\n  (ICSE) 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04500v1",
                "updated": "2025-11-06T16:21:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    21,
                    27,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:21:27Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    21,
                    27,
                    3,
                    310,
                    0
                ],
                "title": "Large language models replicate and predict human cooperation across\n  experiments in game theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models replicate and predict human cooperation across\n  experiments in game theory"
                },
                "summary": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making."
                },
                "authors": [
                    {
                        "name": "Andrea Cera Palatsi"
                    },
                    {
                        "name": "Samuel Martin-Gutierrez"
                    },
                    {
                        "name": "Ana S. Cardenal"
                    },
                    {
                        "name": "Max Pellert"
                    }
                ],
                "author_detail": {
                    "name": "Max Pellert"
                },
                "author": "Max Pellert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04499v1",
                "updated": "2025-11-06T16:20:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    20,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:20:52Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    20,
                    52,
                    3,
                    310,
                    0
                ],
                "title": "Decoding Emergent Big Five Traits in Large Language Models:\n  Temperature-Dependent Expression and Architectural Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Emergent Big Five Traits in Large Language Models:\n  Temperature-Dependent Expression and Architectural Clustering"
                },
                "summary": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1"
                },
                "authors": [
                    {
                        "name": "Christos-Nikolaos Zacharopoulos"
                    },
                    {
                        "name": "Revekka Kyriakoglou"
                    }
                ],
                "author_detail": {
                    "name": "Revekka Kyriakoglou"
                },
                "author": "Revekka Kyriakoglou",
                "arxiv_comment": "Accepted at IJCNLP-AACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04495v1",
                "updated": "2025-11-06T16:16:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    16,
                    32,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:16:32Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    16,
                    32,
                    3,
                    310,
                    0
                ],
                "title": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code\n  Generation"
                },
                "summary": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance."
                },
                "authors": [
                    {
                        "name": "Cuong Huynh"
                    },
                    {
                        "name": "Jie Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jie Cao"
                },
                "author": "Jie Cao",
                "arxiv_comment": "Accepted to TSAR 2025 Workshop at EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17737v2",
                "updated": "2025-11-06T16:16:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    16,
                    5,
                    3,
                    310,
                    0
                ],
                "published": "2024-06-25T17:24:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    24,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable\n  Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable\n  Users"
                },
                "summary": "While state-of-the-art large language models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While state-of-the-art large language models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users."
                },
                "authors": [
                    {
                        "name": "Elinor Poole-Dayan"
                    },
                    {
                        "name": "Deb Roy"
                    },
                    {
                        "name": "Jad Kabbara"
                    }
                ],
                "author_detail": {
                    "name": "Jad Kabbara"
                },
                "author": "Jad Kabbara",
                "arxiv_comment": "Paper accepted at AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04491v1",
                "updated": "2025-11-06T16:10:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    10,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:10:03Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    10,
                    3,
                    3,
                    310,
                    0
                ],
                "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within\n  Structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within\n  Structured Tables"
                },
                "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research."
                },
                "authors": [
                    {
                        "name": "Nikhil Abhyankar"
                    },
                    {
                        "name": "Purvi Chaurasia"
                    },
                    {
                        "name": "Sanchit Kabra"
                    },
                    {
                        "name": "Ananya Srivastava"
                    },
                    {
                        "name": "Vivek Gupta"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04486v1",
                "updated": "2025-11-06T16:05:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    28,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:05:28Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    28,
                    3,
                    310,
                    0
                ],
                "title": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed\n  Code Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed\n  Code Edits"
                },
                "summary": "Instructed code editing, where LLMs directly modify a developer's existing\ncode based on a user instruction, is becoming a widely used interaction mode in\nAI coding assistants. However, few benchmarks directly evaluate this capability\nand current datasets often rely on artificial sources. We introduce EDIT-Bench,\na benchmark for evaluating LLM code editing capabilities grounded in real-world\nusage, i.e., user instructions and code contexts collected in the wild.\nEDIT-Bench comprises of 545 problems, multiple natural and programming\nlanguages, and a diverse set of real-world use cases, ranging from resolving\nerrors to adding features. EDIT-Bench introduces context-dependent problems\nthat require the model to understand code context, highlighted code, and cursor\nposition in addition to the user instruction. We evaluate 40 diverse LLMs and\nobserve that EDIT-Bench is a challenging set of problems where only 5 models\nscore over 60%. We find that model performance varies across different\ncategories of user instructions. Further, we find that varying levels of\ncontextual information greatly affect task success rate, with performance\nvarying up to 11%, indicating the importance of evaluating with realistic\ncontext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructed code editing, where LLMs directly modify a developer's existing\ncode based on a user instruction, is becoming a widely used interaction mode in\nAI coding assistants. However, few benchmarks directly evaluate this capability\nand current datasets often rely on artificial sources. We introduce EDIT-Bench,\na benchmark for evaluating LLM code editing capabilities grounded in real-world\nusage, i.e., user instructions and code contexts collected in the wild.\nEDIT-Bench comprises of 545 problems, multiple natural and programming\nlanguages, and a diverse set of real-world use cases, ranging from resolving\nerrors to adding features. EDIT-Bench introduces context-dependent problems\nthat require the model to understand code context, highlighted code, and cursor\nposition in addition to the user instruction. We evaluate 40 diverse LLMs and\nobserve that EDIT-Bench is a challenging set of problems where only 5 models\nscore over 60%. We find that model performance varies across different\ncategories of user instructions. Further, we find that varying levels of\ncontextual information greatly affect task success rate, with performance\nvarying up to 11%, indicating the importance of evaluating with realistic\ncontext."
                },
                "authors": [
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Jenny Liang"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04481v1",
                "updated": "2025-11-06T15:59:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    59,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:59:59Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    59,
                    59,
                    3,
                    310,
                    0
                ],
                "title": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy\n  Consumption through Empirical and Theoretical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy\n  Consumption through Empirical and Theoretical Analysis"
                },
                "summary": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful\nagentic systems pushing the boundaries of Large Language Models (LLM). They can\nautonomously interact with the internet at the user's behest, such as\nnavigating websites, filling search masks, and comparing price lists. Though\nweb agent research is thriving, induced sustainability issues remain largely\nunexplored. To highlight the urgency of this issue, we provide an initial\nexploration of the energy and $CO_2$ cost associated with web agents from both\na theoretical -via estimation- and an empirical perspective -by benchmarking.\nOur results show how different philosophies in web agent creation can severely\nimpact the associated expended energy, and that more energy consumed does not\nnecessarily equate to better results. We highlight a lack of transparency\nregarding disclosing model parameters and processes used for some web agents as\na limiting factor when estimating energy consumption. Our work contributes\ntowards a change in thinking of how we evaluate web agents, advocating for\ndedicated metrics measuring energy consumption in benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful\nagentic systems pushing the boundaries of Large Language Models (LLM). They can\nautonomously interact with the internet at the user's behest, such as\nnavigating websites, filling search masks, and comparing price lists. Though\nweb agent research is thriving, induced sustainability issues remain largely\nunexplored. To highlight the urgency of this issue, we provide an initial\nexploration of the energy and $CO_2$ cost associated with web agents from both\na theoretical -via estimation- and an empirical perspective -by benchmarking.\nOur results show how different philosophies in web agent creation can severely\nimpact the associated expended energy, and that more energy consumed does not\nnecessarily equate to better results. We highlight a lack of transparency\nregarding disclosing model parameters and processes used for some web agents as\na limiting factor when estimating energy consumption. Our work contributes\ntowards a change in thinking of how we evaluate web agents, advocating for\ndedicated metrics measuring energy consumption in benchmarks."
                },
                "authors": [
                    {
                        "name": "Lars Krupp"
                    },
                    {
                        "name": "Daniel Geißler"
                    },
                    {
                        "name": "Vishal Banwari"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Jakob Karolus"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Karolus"
                },
                "author": "Jakob Karolus",
                "arxiv_comment": "Accepted by AAAI 2026 AISI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01409v2",
                "updated": "2025-11-06T15:57:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-03T10:00:49Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    0,
                    49,
                    0,
                    307,
                    0
                ],
                "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval\n  and Reasoning over Dynamic Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval\n  and Reasoning over Dynamic Knowledge"
                },
                "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge."
                },
                "authors": [
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Ao Yu"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Jianing Shi"
                    },
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Hejia Geng"
                    },
                    {
                        "name": "Yongting Zhang"
                    },
                    {
                        "name": "Yutao Fan"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Tiancheng He"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhenfei Yin"
                },
                "author": "Zhenfei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04478v1",
                "updated": "2025-11-06T15:57:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    19,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:57:19Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    19,
                    3,
                    310,
                    0
                ],
                "title": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop\n  Refinement of LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop\n  Refinement of LLM Judges"
                },
                "summary": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but\nits effectiveness is often limited by the scarcity of diverse, representative\ndata for refining criteria. We present a tool that integrates synthetic data\ngeneration into the LLM-as-a-judge workflow, empowering users to create\ntailored and challenging test cases with configurable domains, personas,\nlengths, and desired outcomes, including borderline cases. The tool also\nsupports AI-assisted inline editing of existing test cases. To enhance\ntransparency and interpretability, it reveals the prompts and explanations\nbehind each generation. In a user study (N=24), 83% of participants preferred\nthe tool over manually creating or selecting test cases, as it allowed them to\nrapidly generate diverse synthetic data without additional workload. The\ngenerated synthetic data proved as effective as hand-crafted data for both\nrefining evaluation criteria and aligning with human preferences. These\nfindings highlight synthetic data as a promising alternative, particularly in\ncontexts where efficiency and scalability are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but\nits effectiveness is often limited by the scarcity of diverse, representative\ndata for refining criteria. We present a tool that integrates synthetic data\ngeneration into the LLM-as-a-judge workflow, empowering users to create\ntailored and challenging test cases with configurable domains, personas,\nlengths, and desired outcomes, including borderline cases. The tool also\nsupports AI-assisted inline editing of existing test cases. To enhance\ntransparency and interpretability, it reveals the prompts and explanations\nbehind each generation. In a user study (N=24), 83% of participants preferred\nthe tool over manually creating or selecting test cases, as it allowed them to\nrapidly generate diverse synthetic data without additional workload. The\ngenerated synthetic data proved as effective as hand-crafted data for both\nrefining evaluation criteria and aligning with human preferences. These\nfindings highlight synthetic data as a promising alternative, particularly in\ncontexts where efficiency and scalability are critical."
                },
                "authors": [
                    {
                        "name": "Hyo Jin Do"
                    },
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Jasmina Gajcin"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Martín Santillán Cooper"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Werner Geyer"
                    }
                ],
                "author_detail": {
                    "name": "Werner Geyer"
                },
                "author": "Werner Geyer",
                "arxiv_comment": "29 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04477v1",
                "updated": "2025-11-06T15:57:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:57:18Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    57,
                    18,
                    3,
                    310,
                    0
                ],
                "title": "Enabling Dynamic Sparsity in Quantized LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Dynamic Sparsity in Quantized LLM Inference"
                },
                "summary": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs."
                },
                "authors": [
                    {
                        "name": "Rongxiang Wang"
                    },
                    {
                        "name": "Kangyuan Shu"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12474v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12474v3",
                "updated": "2025-11-06T15:56:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    56,
                    42,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-18T15:52:24Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    15,
                    52,
                    24,
                    6,
                    138,
                    0
                ],
                "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization"
                },
                "summary": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration."
                },
                "authors": [
                    {
                        "name": "Weixiao Zhou"
                    },
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Gengyao Li"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Xinnian Liang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "Accepted to AACL-IJCNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12474v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12474v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04847v2",
                "updated": "2025-11-06T15:46:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    46,
                    58,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-07T22:50:33Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    22,
                    50,
                    33,
                    2,
                    127,
                    0
                ],
                "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards"
                },
                "summary": "Retrieval-augmented generation (RAG) aims to reduce hallucinations by\ngrounding responses in external context, yet large language models (LLMs) still\nfrequently introduce unsupported information or contradictions even when\nprovided with relevant context. This paper presents two complementary efforts\nat Vectara to measure and benchmark LLM faithfulness in RAG. First, we describe\nour original hallucination leaderboard, which has tracked hallucination rates\nfor LLMs since 2023 using our HHEM hallucination detection model. Motivated by\nlimitations observed in current hallucination detection methods, we introduce\nFaithJudge, an LLM-as-a-judge framework that leverages a pool of diverse\nhuman-annotated hallucination examples to substantially improve the automated\nhallucination evaluation of LLMs. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness in\nsummarization, question-answering, and data-to-text generation tasks.\nFaithJudge enables a more reliable benchmarking of LLM hallucinations in RAG\nand supports the development of more trustworthy generative AI systems:\nhttps://github.com/vectara/FaithJudge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) aims to reduce hallucinations by\ngrounding responses in external context, yet large language models (LLMs) still\nfrequently introduce unsupported information or contradictions even when\nprovided with relevant context. This paper presents two complementary efforts\nat Vectara to measure and benchmark LLM faithfulness in RAG. First, we describe\nour original hallucination leaderboard, which has tracked hallucination rates\nfor LLMs since 2023 using our HHEM hallucination detection model. Motivated by\nlimitations observed in current hallucination detection methods, we introduce\nFaithJudge, an LLM-as-a-judge framework that leverages a pool of diverse\nhuman-annotated hallucination examples to substantially improve the automated\nhallucination evaluation of LLMs. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness in\nsummarization, question-answering, and data-to-text generation tasks.\nFaithJudge enables a more reliable benchmarking of LLM hallucinations in RAG\nand supports the development of more trustworthy generative AI systems:\nhttps://github.com/vectara/FaithJudge."
                },
                "authors": [
                    {
                        "name": "Manveer Singh Tamber"
                    },
                    {
                        "name": "Forrest Sheng Bao"
                    },
                    {
                        "name": "Chenyu Xu"
                    },
                    {
                        "name": "Ge Luo"
                    },
                    {
                        "name": "Suleman Kazi"
                    },
                    {
                        "name": "Minseok Bae"
                    },
                    {
                        "name": "Miaoran Li"
                    },
                    {
                        "name": "Ofer Mendelevitch"
                    },
                    {
                        "name": "Renyi Qu"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "EMNLP Industry Track 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04473v1",
                "updated": "2025-11-06T15:45:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    45,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:45:18Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    45,
                    18,
                    3,
                    310,
                    0
                ],
                "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge\n  Graph Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge\n  Graph Augmented LLMs"
                },
                "summary": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval of information from graph-structured knowledge bases represents a\npromising direction for improving the factuality of LLMs. While various\nsolutions have been proposed, a comparison of methods is difficult due to the\nlack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a framework for generating high-quality synthetic\nKnowledge Graph Question Answering datasets from any Knowledge Graph, providing\nthe full set of ground-truth facts in the KG to reason over each question. We\nshow how, in addition to enabling more informative benchmarking of KG\nretrievers, the data produced with SynthKGQA also allows us to train better\nmodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset\ndesigned to test zero-shot generalization abilities of KG retrievers with\nrespect to unseen graph structures and relation types, and benchmark popular\nsolutions for KG-augmented LLMs on it."
                },
                "authors": [
                    {
                        "name": "Alberto Cattaneo"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Daniel Justus"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Justus"
                },
                "author": "Daniel Justus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04464v1",
                "updated": "2025-11-06T15:37:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:37:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context"
                },
                "summary": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization."
                },
                "authors": [
                    {
                        "name": "Carnot Braun"
                    },
                    {
                        "name": "Rafael O. Jarczewski"
                    },
                    {
                        "name": "Gabriel U. Talasso"
                    },
                    {
                        "name": "Leandro A. Villas"
                    },
                    {
                        "name": "Allan M. de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Allan M. de Souza"
                },
                "author": "Allan M. de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00783v2",
                "updated": "2025-11-06T15:24:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    24,
                    48,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-02T03:34:44Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    3,
                    34,
                    44,
                    6,
                    306,
                    0
                ],
                "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for\n  Cooperative Multi-Robot Underwater Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for\n  Cooperative Multi-Robot Underwater Coverage"
                },
                "summary": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions."
                },
                "authors": [
                    {
                        "name": "Jingzehua Xu"
                    },
                    {
                        "name": "Weihang Zhang"
                    },
                    {
                        "name": "Yangyang Li"
                    },
                    {
                        "name": "Hongmiaoyi Zhang"
                    },
                    {
                        "name": "Guanwen Xie"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "arxiv_comment": "This paper has been submitted to IEEE Transactions on Mobile\n  Computing. Jingzehua Xu, Weihang Zhang, and Yangyang Li contributed equally\n  to this work and are recognized as the co-first authors of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06991v2",
                "updated": "2025-11-06T15:24:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    24,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-08T04:38:39Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    4,
                    38,
                    39,
                    6,
                    159,
                    0
                ],
                "title": "Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth"
                },
                "summary": "The recent success of generative AI highlights the crucial role of\nhigh-quality human feedback in building trustworthy AI systems. However, the\nincreasing use of large language models (LLMs) by crowdsourcing workers poses a\nsignificant challenge: datasets intended to reflect human input may be\ncompromised by LLM-generated responses. Existing LLM detection approaches often\nrely on high-dimensional training data such as text, making them unsuitable for\nannotation tasks like multiple-choice labeling. In this work, we investigate\nthe potential of peer prediction -- a mechanism that evaluates the information\nwithin workers' responses without using ground truth -- to mitigate\nLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our\napproach quantifies the correlations between worker answers while conditioning\non (a subset of) LLM-generated labels available to the requester. Building on\nprior research, we propose a training-free scoring mechanism with theoretical\nguarantees under a crowdsourcing model that accounts for LLM collusion. We\nestablish conditions under which our method is effective and empirically\ndemonstrate its robustness in detecting low-effort cheating on real-world\ncrowdsourcing datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of generative AI highlights the crucial role of\nhigh-quality human feedback in building trustworthy AI systems. However, the\nincreasing use of large language models (LLMs) by crowdsourcing workers poses a\nsignificant challenge: datasets intended to reflect human input may be\ncompromised by LLM-generated responses. Existing LLM detection approaches often\nrely on high-dimensional training data such as text, making them unsuitable for\nannotation tasks like multiple-choice labeling. In this work, we investigate\nthe potential of peer prediction -- a mechanism that evaluates the information\nwithin workers' responses without using ground truth -- to mitigate\nLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our\napproach quantifies the correlations between worker answers while conditioning\non (a subset of) LLM-generated labels available to the requester. Building on\nprior research, we propose a training-free scoring mechanism with theoretical\nguarantees under a crowdsourcing model that accounts for LLM collusion. We\nestablish conditions under which our method is effective and empirically\ndemonstrate its robustness in detecting low-effort cheating on real-world\ncrowdsourcing datasets."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Jinlong Pang"
                    },
                    {
                        "name": "Zhaowei Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "32 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04453v1",
                "updated": "2025-11-06T15:23:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    23,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:23:50Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    23,
                    50,
                    3,
                    310,
                    0
                ],
                "title": "Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI\n  Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI\n  Tools"
                },
                "summary": "Social news platforms have become key launch outlets for open-source\nprojects, especially Hacker News (HN), though quantifying their immediate\nimpact remains challenging. This paper presents a reproducible demonstration\nsystem that tracks how HN exposure translates into GitHub star growth for AI\nand LLM tools. Built entirely on public APIs, our pipeline analyzes 138\nrepository launches from 2024-2025 and reveals substantial launch effects:\nrepositories gain an average of 121 stars within 24 hours, 189 stars within 48\nhours, and 289 stars within a week of HN exposure. Through machine learning\nmodels (Elastic Net) and non-linear approaches (Gradient Boosting), we identify\nkey predictors of viral growth. Posting timing appears as key factor--launching\nat optimal hours can mean hundreds of additional stars--while the \"Show HN\" tag\nshows no statistical advantage after controlling for other factors. The\ndemonstration completes in under five minutes on standard hardware,\nautomatically collecting data, training models, and generating visualizations\nthrough single-file scripts. This makes our findings immediately reproducible\nand the framework easily be extended to other platforms, providing both\nresearchers and developers with actionable insights into launch dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social news platforms have become key launch outlets for open-source\nprojects, especially Hacker News (HN), though quantifying their immediate\nimpact remains challenging. This paper presents a reproducible demonstration\nsystem that tracks how HN exposure translates into GitHub star growth for AI\nand LLM tools. Built entirely on public APIs, our pipeline analyzes 138\nrepository launches from 2024-2025 and reveals substantial launch effects:\nrepositories gain an average of 121 stars within 24 hours, 189 stars within 48\nhours, and 289 stars within a week of HN exposure. Through machine learning\nmodels (Elastic Net) and non-linear approaches (Gradient Boosting), we identify\nkey predictors of viral growth. Posting timing appears as key factor--launching\nat optimal hours can mean hundreds of additional stars--while the \"Show HN\" tag\nshows no statistical advantage after controlling for other factors. The\ndemonstration completes in under five minutes on standard hardware,\nautomatically collecting data, training models, and generating visualizations\nthrough single-file scripts. This makes our findings immediately reproducible\nand the framework easily be extended to other platforms, providing both\nresearchers and developers with actionable insights into launch dynamics."
                },
                "authors": [
                    {
                        "name": "Obada Kraishan"
                    }
                ],
                "author_detail": {
                    "name": "Obada Kraishan"
                },
                "author": "Obada Kraishan",
                "arxiv_comment": "7 pages, 3 figures. Reproducible demonstration system with public\n  code available at https://github.com/obadaKraishan/Launch-Day-Diffusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.3; K.6.3; D.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04439v1",
                "updated": "2025-11-06T15:12:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    12,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:12:50Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    12,
                    50,
                    3,
                    310,
                    0
                ],
                "title": "The Peril of Preference: Why GRPO fails on Ordinal Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Peril of Preference: Why GRPO fails on Ordinal Rewards"
                },
                "summary": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision."
                },
                "authors": [
                    {
                        "name": "Anisha Garg"
                    },
                    {
                        "name": "Ganesh Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Venkatesh"
                },
                "author": "Ganesh Venkatesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04432v1",
                "updated": "2025-11-06T15:06:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    6,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:06:22Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    6,
                    22,
                    3,
                    310,
                    0
                ],
                "title": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning\n  Task for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning\n  Task for LLMs"
                },
                "summary": "In this study, we experiment with the ability of LLMs to do temporal\nreasoning. Using a Norwegian book from 1940 containing trivia questions, we\nprompt the LLMs to answer the questions as if it were 1940. We also pose the\nquestions in both English and Norwegian. Correct answers are often presented as\nsentences, and grading is done by means of LLM-as-judge, with sampled checks by\na native speaker. Prompting in English consistently gave better results than in\nNorwegian, an unexpected result. In contrast, using larger LLMs improved\nresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,\nand also the largest available LLM especially crafted for Norwegian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we experiment with the ability of LLMs to do temporal\nreasoning. Using a Norwegian book from 1940 containing trivia questions, we\nprompt the LLMs to answer the questions as if it were 1940. We also pose the\nquestions in both English and Norwegian. Correct answers are often presented as\nsentences, and grading is done by means of LLM-as-judge, with sampled checks by\na native speaker. Prompting in English consistently gave better results than in\nNorwegian, an unexpected result. In contrast, using larger LLMs improved\nresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,\nand also the largest available LLM especially crafted for Norwegian."
                },
                "authors": [
                    {
                        "name": "Lars Bungum"
                    },
                    {
                        "name": "Charles Yijia Huang"
                    },
                    {
                        "name": "Abeer Kashar"
                    }
                ],
                "author_detail": {
                    "name": "Abeer Kashar"
                },
                "author": "Abeer Kashar",
                "arxiv_comment": "8 pages, 1 figure, 3 tables, submitted to aconference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04427v1",
                "updated": "2025-11-06T15:00:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    0,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:00:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    0,
                    51,
                    3,
                    310,
                    0
                ],
                "title": "Speed at the Cost of Quality? The Impact of LLM Agent Assistance on\n  Software Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speed at the Cost of Quality? The Impact of LLM Agent Assistance on\n  Software Development"
                },
                "summary": "Large language models (LLMs) have demonstrated the promise to revolutionize\nthe field of software engineering. Among other things, LLM agents are rapidly\ngaining momentum in their application to software development, with\npractitioners claiming a multifold productivity increase after adoption. Yet,\nempirical evidence is lacking around these claims. In this paper, we estimate\nthe causal effect of adopting a widely popular LLM agent assistant, namely\nCursor, on development velocity and software quality. The estimation is enabled\nby a state-of-the-art difference-in-differences design comparing\nCursor-adopting GitHub projects with a matched control group of similar GitHub\nprojects that do not use Cursor. We find that the adoption of Cursor leads to a\nsignificant, large, but transient increase in project-level development\nvelocity, along with a significant and persistent increase in static analysis\nwarnings and code complexity. Further panel generalized method of moments\nestimation reveals that the increase in static analysis warnings and code\ncomplexity acts as a major factor causing long-term velocity slowdown. Our\nstudy carries implications for software engineering practitioners, LLM agent\nassistant designers, and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the promise to revolutionize\nthe field of software engineering. Among other things, LLM agents are rapidly\ngaining momentum in their application to software development, with\npractitioners claiming a multifold productivity increase after adoption. Yet,\nempirical evidence is lacking around these claims. In this paper, we estimate\nthe causal effect of adopting a widely popular LLM agent assistant, namely\nCursor, on development velocity and software quality. The estimation is enabled\nby a state-of-the-art difference-in-differences design comparing\nCursor-adopting GitHub projects with a matched control group of similar GitHub\nprojects that do not use Cursor. We find that the adoption of Cursor leads to a\nsignificant, large, but transient increase in project-level development\nvelocity, along with a significant and persistent increase in static analysis\nwarnings and code complexity. Further panel generalized method of moments\nestimation reveals that the increase in static analysis warnings and code\ncomplexity acts as a major factor causing long-term velocity slowdown. Our\nstudy carries implications for software engineering practitioners, LLM agent\nassistant designers, and researchers."
                },
                "authors": [
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Courtney Miller"
                    },
                    {
                        "name": "Shyam Agarwal"
                    },
                    {
                        "name": "Christian Kästner"
                    },
                    {
                        "name": "Bogdan Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Vasilescu"
                },
                "author": "Bogdan Vasilescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04418v1",
                "updated": "2025-11-06T14:46:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    46,
                    35,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:46:35Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    46,
                    35,
                    3,
                    310,
                    0
                ],
                "title": "The Illusion of Certainty: Uncertainty quantification for LLMs fails\n  under ambiguity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Certainty: Uncertainty quantification for LLMs fails\n  under ambiguity"
                },
                "summary": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is\ncritical for trustworthy deployment. While real-world language is inherently\nambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically\nbenchmarked against tasks with no ambiguity. In this work, we demonstrate that\nwhile current uncertainty estimators perform well under the restrictive\nassumption of no ambiguity, they degrade to close-to-random performance on\nambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first\nambiguous question-answering (QA) datasets equipped with ground-truth answer\ndistributions estimated from factual co-occurrence. We find this performance\ndeterioration to be consistent across different estimation paradigms: using the\npredictive distribution itself, internal representations throughout the model,\nand an ensemble of models. We show that this phenomenon can be theoretically\nexplained, revealing that predictive-distribution and ensemble-based estimators\nare fundamentally limited under ambiguity. Overall, our study reveals a key\nshortcoming of current UQ methods for LLMs and motivates a rethinking of\ncurrent modeling paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is\ncritical for trustworthy deployment. While real-world language is inherently\nambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically\nbenchmarked against tasks with no ambiguity. In this work, we demonstrate that\nwhile current uncertainty estimators perform well under the restrictive\nassumption of no ambiguity, they degrade to close-to-random performance on\nambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first\nambiguous question-answering (QA) datasets equipped with ground-truth answer\ndistributions estimated from factual co-occurrence. We find this performance\ndeterioration to be consistent across different estimation paradigms: using the\npredictive distribution itself, internal representations throughout the model,\nand an ensemble of models. We show that this phenomenon can be theoretically\nexplained, revealing that predictive-distribution and ensemble-based estimators\nare fundamentally limited under ambiguity. Overall, our study reveals a key\nshortcoming of current UQ methods for LLMs and motivates a rethinking of\ncurrent modeling paradigms."
                },
                "authors": [
                    {
                        "name": "Tim Tomov"
                    },
                    {
                        "name": "Dominik Fuchsgruber"
                    },
                    {
                        "name": "Tom Wollschläger"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07961v2",
                "updated": "2025-11-06T14:41:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    41,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2024-10-10T14:24:30Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    24,
                    30,
                    3,
                    284,
                    0
                ],
                "title": "QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm\n  Design"
                },
                "summary": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitBench, the first benchmark dataset designed to evaluate AI's\ncapability in designing and implementing quantum algorithms using quantum\nprogramming languages. Unlike using AI for writing traditional codes, this task\nis fundamentally more complicated due to highly flexible design space. Our key\ncontributions include: 1. A general framework which formulates the key features\nof quantum algorithm design for Large Language Models. 2. Implementations for\nquantum algorithms from basic primitives to advanced applications, spanning 3\ntask suites, 25 algorithms, and 120,290 data points. 3. Automatic validation\nand verification functions, allowing for iterative evaluation and interactive\nreasoning without human inspection. 4. Promising potential as a training\ndataset through preliminary fine-tuning results. We observed several\ninteresting experimental phenomena: LLMs tend to exhibit consistent error\npatterns, and fine-tuning does not always outperform few-shot learning. In all,\nQCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm\ndesign, and it reveals limitations of LLMs in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitBench, the first benchmark dataset designed to evaluate AI's\ncapability in designing and implementing quantum algorithms using quantum\nprogramming languages. Unlike using AI for writing traditional codes, this task\nis fundamentally more complicated due to highly flexible design space. Our key\ncontributions include: 1. A general framework which formulates the key features\nof quantum algorithm design for Large Language Models. 2. Implementations for\nquantum algorithms from basic primitives to advanced applications, spanning 3\ntask suites, 25 algorithms, and 120,290 data points. 3. Automatic validation\nand verification functions, allowing for iterative evaluation and interactive\nreasoning without human inspection. 4. Promising potential as a training\ndataset through preliminary fine-tuning results. We observed several\ninteresting experimental phenomena: LLMs tend to exhibit consistent error\npatterns, and fine-tuning does not always outperform few-shot learning. In all,\nQCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm\ndesign, and it reveals limitations of LLMs in this domain."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ziruo Wang"
                    },
                    {
                        "name": "Yuntian Gu"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Tongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongyang Li"
                },
                "author": "Tongyang Li",
                "arxiv_comment": "45 pages, 17 figures, 15 tables, GitHub repository:\n  https://github.com/EstelYang/QCircuitBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04394v1",
                "updated": "2025-11-06T14:22:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    22,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:22:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    22,
                    51,
                    3,
                    310,
                    0
                ],
                "title": "DORAEMON: A Unified Library for Visual Object Modeling and\n  Representation Learning at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DORAEMON: A Unified Library for Visual Object Modeling and\n  Representation Learning at Scale"
                },
                "summary": "DORAEMON is an open-source PyTorch library that unifies visual object\nmodeling and representation learning across diverse scales. A single\nYAML-driven workflow covers classification, retrieval and metric learning; more\nthan 1000 pretrained backbones are exposed through a timm-compatible interface,\ntogether with modular losses, augmentations and distributed-training utilities.\nReproducible recipes match or exceed reference results on ImageNet-1K,\nMS-Celeb-1M and Stanford online products, while one-command export to ONNX or\nHuggingFace bridges research and deployment. By consolidating datasets, models,\nand training techniques into one platform, DORAEMON offers a scalable\nfoundation for rapid experimentation in visual recognition and representation\nlearning, enabling efficient transfer of research advances to real-world\napplications. The repository is available at https://github.com/wuji3/DORAEMON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DORAEMON is an open-source PyTorch library that unifies visual object\nmodeling and representation learning across diverse scales. A single\nYAML-driven workflow covers classification, retrieval and metric learning; more\nthan 1000 pretrained backbones are exposed through a timm-compatible interface,\ntogether with modular losses, augmentations and distributed-training utilities.\nReproducible recipes match or exceed reference results on ImageNet-1K,\nMS-Celeb-1M and Stanford online products, while one-command export to ONNX or\nHuggingFace bridges research and deployment. By consolidating datasets, models,\nand training techniques into one platform, DORAEMON offers a scalable\nfoundation for rapid experimentation in visual recognition and representation\nlearning, enabling efficient transfer of research advances to real-world\napplications. The repository is available at https://github.com/wuji3/DORAEMON."
                },
                "authors": [
                    {
                        "name": "Ke Du"
                    },
                    {
                        "name": "Yimin Peng"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Siqiao Xue"
                    }
                ],
                "author_detail": {
                    "name": "Siqiao Xue"
                },
                "author": "Siqiao Xue",
                "arxiv_comment": "code: https://github.com/wuji3/DORAEMON",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04393v1",
                "updated": "2025-11-06T14:21:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    21,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:21:22Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    21,
                    22,
                    3,
                    310,
                    0
                ],
                "title": "Post-Training LLMs as Better Decision-Making Agents: A\n  Regret-Minimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training LLMs as Better Decision-Making Agents: A\n  Regret-Minimization Approach"
                },
                "summary": "Large language models (LLMs) are increasingly deployed as \"agents\" for\ndecision-making (DM) in interactive and dynamic environments. Yet, since they\nwere not originally designed for DM, recent studies show that LLMs can struggle\neven in basic online DM problems, failing to achieve low regret or an effective\nexploration-exploitation tradeoff. To address this, we introduce Iterative\nRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure\nthat repeatedly distills low-regret decision trajectories back into the base\nmodel. At each iteration, the model rolls out multiple decision trajectories,\nselects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior\nmethods that (a) distill action sequences from known DM algorithms or (b) rely\non manually crafted chain-of-thought templates, our approach leverages the\nregret metric to elicit the model's own DM ability and reasoning rationales.\nThis reliance on model-generated reasoning avoids rigid output engineering and\nprovides more flexible, natural-language training signals. Empirical results\nshow that Iterative RMFT improves LLMs' DM performance across diverse models -\nfrom Transformers with numerical input/output, to open-weight LLMs, and\nadvanced closed-weight models like GPT-4o mini. Its flexibility in output and\nreasoning formats enables generalization across tasks with varying horizons,\naction spaces, reward processes, and natural-language contexts. Finally, we\nprovide theoretical insight showing that a single-layer Transformer under this\nparadigm can act as a no-regret learner in a simplified setting. Overall,\nIterative RMFT offers a principled and general post-training framework for\nenhancing LLMs' decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed as \"agents\" for\ndecision-making (DM) in interactive and dynamic environments. Yet, since they\nwere not originally designed for DM, recent studies show that LLMs can struggle\neven in basic online DM problems, failing to achieve low regret or an effective\nexploration-exploitation tradeoff. To address this, we introduce Iterative\nRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure\nthat repeatedly distills low-regret decision trajectories back into the base\nmodel. At each iteration, the model rolls out multiple decision trajectories,\nselects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior\nmethods that (a) distill action sequences from known DM algorithms or (b) rely\non manually crafted chain-of-thought templates, our approach leverages the\nregret metric to elicit the model's own DM ability and reasoning rationales.\nThis reliance on model-generated reasoning avoids rigid output engineering and\nprovides more flexible, natural-language training signals. Empirical results\nshow that Iterative RMFT improves LLMs' DM performance across diverse models -\nfrom Transformers with numerical input/output, to open-weight LLMs, and\nadvanced closed-weight models like GPT-4o mini. Its flexibility in output and\nreasoning formats enables generalization across tasks with varying horizons,\naction spaces, reward processes, and natural-language contexts. Finally, we\nprovide theoretical insight showing that a single-layer Transformer under this\nparadigm can act as a no-regret learner in a simplified setting. Overall,\nIterative RMFT offers a principled and general post-training framework for\nenhancing LLMs' decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    },
                    {
                        "name": "Kaiqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqing Zhang"
                },
                "author": "Kaiqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01827v3",
                "updated": "2025-11-06T14:13:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    13,
                    45,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-02T15:44:12Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    44,
                    12,
                    2,
                    183,
                    0
                ],
                "title": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search"
                },
                "summary": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04366v1",
                "updated": "2025-11-06T13:51:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    51,
                    51,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:51:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    51,
                    51,
                    3,
                    310,
                    0
                ],
                "title": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on\n  Parent-Child Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on\n  Parent-Child Interaction"
                },
                "summary": "While multimodal large language models (MLLMs) are increasingly applied in\nhuman-centred AI systems, their ability to understand complex social\ninteractions remains uncertain. We present an exploratory study on aligning\nMLLMs with speech-language pathologists (SLPs) in analysing joint attention in\nparent-child interactions, a key construct in early social-communicative\ndevelopment. Drawing on interviews and video annotations with three SLPs, we\ncharacterise how observational cues of gaze, action, and vocalisation inform\ntheir reasoning processes. We then test whether an MLLM can approximate this\nworkflow through a two-stage prompting, separating observation from judgment.\nOur findings reveal that alignment is more robust at the observation layer,\nwhere experts share common descriptors, than at the judgement layer, where\ninterpretive criteria diverge. We position this work as a case-based probe into\nexpert-AI alignment in complex social behaviour, highlighting both the\nfeasibility and the challenges of applying MLLMs to socially situated\ninteraction analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal large language models (MLLMs) are increasingly applied in\nhuman-centred AI systems, their ability to understand complex social\ninteractions remains uncertain. We present an exploratory study on aligning\nMLLMs with speech-language pathologists (SLPs) in analysing joint attention in\nparent-child interactions, a key construct in early social-communicative\ndevelopment. Drawing on interviews and video annotations with three SLPs, we\ncharacterise how observational cues of gaze, action, and vocalisation inform\ntheir reasoning processes. We then test whether an MLLM can approximate this\nworkflow through a two-stage prompting, separating observation from judgment.\nOur findings reveal that alignment is more robust at the observation layer,\nwhere experts share common descriptors, than at the judgement layer, where\ninterpretive criteria diverge. We position this work as a case-based probe into\nexpert-AI alignment in complex social behaviour, highlighting both the\nfeasibility and the challenges of applying MLLMs to socially situated\ninteraction analysis."
                },
                "authors": [
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Kenny Tsu Wei Choo"
                    }
                ],
                "author_detail": {
                    "name": "Kenny Tsu Wei Choo"
                },
                "author": "Kenny Tsu Wei Choo",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18246v2",
                "updated": "2025-11-06T13:47:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    47,
                    8,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-23T17:02:04Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    4,
                    4,
                    143,
                    0
                ],
                "title": "Will Large Language Models Transform Clinical Prediction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will Large Language Models Transform Clinical Prediction?"
                },
                "summary": "Objective: Large language models (LLMs) are attracting increasing interest in\nhealthcare. This commentary evaluates the potential of LLMs to improve clinical\nprediction models (CPMs) for diagnostic and prognostic tasks, with a focus on\ntheir ability to process longitudinal electronic health record (EHR) data.\n  Findings: LLMs show promise in handling multimodal and longitudinal EHR data\nand can support multi-outcome predictions for diverse health conditions.\nHowever, methodological, validation, infrastructural, and regulatory chal-\nlenges remain. These include inadequate methods for time-to-event modelling,\npoor calibration of predictions, limited external validation, and bias\naffecting underrepresented groups. High infrastructure costs and the absence of\nclear regulatory frameworks further prevent adoption.\n  Implications: Further work and interdisciplinary collaboration are needed to\nsupport equitable and effective integra- tion into the clinical prediction.\nDeveloping temporally aware, fair, and explainable models should be a priority\nfocus for transforming clinical prediction workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Large language models (LLMs) are attracting increasing interest in\nhealthcare. This commentary evaluates the potential of LLMs to improve clinical\nprediction models (CPMs) for diagnostic and prognostic tasks, with a focus on\ntheir ability to process longitudinal electronic health record (EHR) data.\n  Findings: LLMs show promise in handling multimodal and longitudinal EHR data\nand can support multi-outcome predictions for diverse health conditions.\nHowever, methodological, validation, infrastructural, and regulatory chal-\nlenges remain. These include inadequate methods for time-to-event modelling,\npoor calibration of predictions, limited external validation, and bias\naffecting underrepresented groups. High infrastructure costs and the absence of\nclear regulatory frameworks further prevent adoption.\n  Implications: Further work and interdisciplinary collaboration are needed to\nsupport equitable and effective integra- tion into the clinical prediction.\nDeveloping temporally aware, fair, and explainable models should be a priority\nfocus for transforming clinical prediction workflow."
                },
                "authors": [
                    {
                        "name": "Yusuf Yildiz"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Meghna Jani"
                    },
                    {
                        "name": "David A. Jenkins"
                    }
                ],
                "author_detail": {
                    "name": "David A. Jenkins"
                },
                "author": "David A. Jenkins",
                "arxiv_doi": "10.1186/s41512-025-00211-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s41512-025-00211-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.18246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published: BMC Diagnostic and Prognostic Research",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04355v1",
                "updated": "2025-11-06T13:38:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    38,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:38:03Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    38,
                    3,
                    3,
                    310,
                    0
                ],
                "title": "Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation\n  Benchmarks"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration, and the race to improve their performance has become a central\nfocus of AI research. Benchmarks and leaderboards are increasingly popular,\noffering quantitative rankings of LLMs. However, they provide limited insight\ninto the tasks that LLMs consistently fail to solve - information that is\ncrucial for understanding current limitations and guiding the development of\nmore capable models. To address this gap, we examined code generation tasks\nacross four popular benchmarks, identifying those that major LLMs are most\nlikely to fail. To understand the causes of these failures, we investigated\nwhether the static complexity of solution code contributes to them, followed by\na systematic inspection of 114 tasks that LLMs consistently struggled with. Our\nanalysis revealed four recurring patterns of weaknesses in LLMs, as well as\ncommon complications within benchmark tasks that most often lead to failure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration, and the race to improve their performance has become a central\nfocus of AI research. Benchmarks and leaderboards are increasingly popular,\noffering quantitative rankings of LLMs. However, they provide limited insight\ninto the tasks that LLMs consistently fail to solve - information that is\ncrucial for understanding current limitations and guiding the development of\nmore capable models. To address this gap, we examined code generation tasks\nacross four popular benchmarks, identifying those that major LLMs are most\nlikely to fail. To understand the causes of these failures, we investigated\nwhether the static complexity of solution code contributes to them, followed by\na systematic inspection of 114 tasks that LLMs consistently struggled with. Our\nanalysis revealed four recurring patterns of weaknesses in LLMs, as well as\ncommon complications within benchmark tasks that most often lead to failure."
                },
                "authors": [
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Maedeh Heydari"
                    },
                    {
                        "name": "Parsa Kazerooni"
                    },
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2025 2nd IEEE/ACM International\n  Conference on AI-powered Software (AIware), Data & Benchmark Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24239v2",
                "updated": "2025-11-06T13:36:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    36,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-29T03:24:48Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    3,
                    24,
                    48,
                    0,
                    272,
                    0
                ],
                "title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning\n  Capabilities of Large Language Models"
                },
                "summary": "Recent large language models (LLMs) have shown strong reasoning capabilities.\nHowever, a critical question remains: do these models possess genuine reasoning\nskills particularly complex strategic reasoning or are they primarily excelling\nat sophisticated pattern recognition within their training data? To address\nthis question, this paper presents a chess testbed, ChessArena, to evaluate the\nstrategic reasoning capabilities of LLMs. Chess requires complex strategic\nreasoning capabilities including long-term planning, strict rule comprehension,\nand multi-turn conversation memorization. Specifically, ChessArena is a\ncompetitive framework where LLMs play against each other, under four different\nplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.\nThe testbed can also evaluate fine-grained capabilities including basic\nunderstanding, move selection, and puzzle solving. Over 13 LLMs with different\nmodes are evaluated in ChessArena, playing over 800 games. The results reveal\nsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chess\nengine at human amateur level), while some even failed to defeat a random\nplayer that selects moves arbitrarily. We also present a strong baseline to the\ntestbed: our fine-tuned Qwen3-8B substantially improved performance,\napproaching much larger state-of-the-art reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have shown strong reasoning capabilities.\nHowever, a critical question remains: do these models possess genuine reasoning\nskills particularly complex strategic reasoning or are they primarily excelling\nat sophisticated pattern recognition within their training data? To address\nthis question, this paper presents a chess testbed, ChessArena, to evaluate the\nstrategic reasoning capabilities of LLMs. Chess requires complex strategic\nreasoning capabilities including long-term planning, strict rule comprehension,\nand multi-turn conversation memorization. Specifically, ChessArena is a\ncompetitive framework where LLMs play against each other, under four different\nplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.\nThe testbed can also evaluate fine-grained capabilities including basic\nunderstanding, move selection, and puzzle solving. Over 13 LLMs with different\nmodes are evaluated in ChessArena, playing over 800 games. The results reveal\nsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chess\nengine at human amateur level), while some even failed to defeat a random\nplayer that selects moves arbitrarily. We also present a strong baseline to the\ntestbed: our fine-tuned Qwen3-8B substantially improved performance,\napproaching much larger state-of-the-art reasoning models."
                },
                "authors": [
                    {
                        "name": "Jincheng Liu"
                    },
                    {
                        "name": "Sijun He"
                    },
                    {
                        "name": "Jingjing Wu"
                    },
                    {
                        "name": "Xiangsen Wang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Zhaoqi Kuang"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Yuan Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Yao"
                },
                "author": "Yuan Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04351v1",
                "updated": "2025-11-06T13:32:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    32,
                    50,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:32:50Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    32,
                    50,
                    3,
                    310,
                    0
                ],
                "title": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal\n  (RGB-D, Skeleton, Point Cloud) Action Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal\n  (RGB-D, Skeleton, Point Cloud) Action Understanding"
                },
                "summary": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Akira Yamamoto"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Maya Singh"
                    }
                ],
                "author_detail": {
                    "name": "Maya Singh"
                },
                "author": "Maya Singh",
                "arxiv_comment": "11 pages, 6 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23653v2",
                "updated": "2025-11-06T13:18:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    18,
                    58,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-29T17:02:49Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    2,
                    49,
                    3,
                    149,
                    0
                ],
                "title": "How do Transformers Learn Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Transformers Learn Implicit Reasoning?"
                },
                "summary": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models."
                },
                "authors": [
                    {
                        "name": "Jiaran Ye"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhidian Huang"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Weichuan Liu"
                    },
                    {
                        "name": "Xiaoyin Che"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Accepted as Spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18793v2",
                "updated": "2025-11-06T13:18:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    18,
                    7,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-23T08:36:08Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    36,
                    8,
                    1,
                    266,
                    0
                ],
                "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments\n  and Reconfigurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments\n  and Reconfigurations"
                },
                "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager."
                },
                "authors": [
                    {
                        "name": "Lukas Zanger"
                    },
                    {
                        "name": "Bastian Lampe"
                    },
                    {
                        "name": "Lennart Reiher"
                    },
                    {
                        "name": "Lutz Eckstein"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Eckstein"
                },
                "author": "Lutz Eckstein",
                "arxiv_comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04334v1",
                "updated": "2025-11-06T13:17:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    17,
                    16,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:17:16Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    17,
                    16,
                    3,
                    310,
                    0
                ],
                "title": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation\n  of Kidneys and Kidney Tumours in Computed Tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation\n  of Kidneys and Kidney Tumours in Computed Tomography"
                },
                "summary": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested."
                },
                "authors": [
                    {
                        "name": "Saúl Alonso-Monsalve"
                    },
                    {
                        "name": "Leigh H. Whitehead"
                    },
                    {
                        "name": "Adam Aurisano"
                    },
                    {
                        "name": "Lorena Escudero Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Lorena Escudero Sanchez"
                },
                "author": "Lorena Escudero Sanchez",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04332v1",
                "updated": "2025-11-06T13:06:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    6,
                    37,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T13:06:37Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    6,
                    37,
                    3,
                    310,
                    0
                ],
                "title": "Differentially Private In-Context Learning with Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private In-Context Learning with Nearest Neighbor Search"
                },
                "summary": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Antti Koskela"
                    },
                    {
                        "name": "Tejas Kulkarni"
                    },
                    {
                        "name": "Laith Zumot"
                    }
                ],
                "author_detail": {
                    "name": "Laith Zumot"
                },
                "author": "Laith Zumot",
                "arxiv_comment": "NeurIPS Lock-LLM Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04328v1",
                "updated": "2025-11-06T12:56:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    56,
                    34,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:56:34Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    56,
                    34,
                    3,
                    310,
                    0
                ],
                "title": "RxSafeBench: Identifying Medication Safety Issues of Large Language\n  Models in Simulated Consultation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RxSafeBench: Identifying Medication Safety Issues of Large Language\n  Models in Simulated Consultation"
                },
                "summary": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Luxin Xu"
                    },
                    {
                        "name": "Minghuan Tan"
                    },
                    {
                        "name": "Lichao Zhang"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "arxiv_comment": "To appear in BIBM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13956v2",
                "updated": "2025-11-06T12:53:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    53,
                    4,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-18T14:21:24Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    21,
                    24,
                    4,
                    199,
                    0
                ],
                "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction"
                },
                "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multi-modal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causality-inspired\nframework named Cross-modal Causal Intervention with Mediator for Alzheimer's\nDisease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large\nLanguage Models (LLMs) to summarize clinical data under strict templates,\ntherefore enriching textual inputs. The MediAD model utilizes Magnetic\nResonance Imaging (MRI), clinical data, and textual data enriched by LLMs to\nclassify participants into Cognitively Normal (CN), MCI, and AD categories.\nBecause of the presence of confounders, such as cerebral vascular lesions and\nage-related biomarkers, non-causal models are likely to capture spurious\ninput-output correlations, generating less reliable results. Our framework\nimplicitly mitigates the effect of both observable and unobservable confounders\nthrough a unified causal intervention method. Experimental results demonstrate\nthe outstanding performance of our method in distinguishing CN/MCI/AD cases,\noutperforming other methods in most evaluation metrics. The study showcases the\npotential of integrating causal reasoning with multi-modal learning for\nneurological disease diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multi-modal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causality-inspired\nframework named Cross-modal Causal Intervention with Mediator for Alzheimer's\nDisease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large\nLanguage Models (LLMs) to summarize clinical data under strict templates,\ntherefore enriching textual inputs. The MediAD model utilizes Magnetic\nResonance Imaging (MRI), clinical data, and textual data enriched by LLMs to\nclassify participants into Cognitively Normal (CN), MCI, and AD categories.\nBecause of the presence of confounders, such as cerebral vascular lesions and\nage-related biomarkers, non-causal models are likely to capture spurious\ninput-output correlations, generating less reliable results. Our framework\nimplicitly mitigates the effect of both observable and unobservable confounders\nthrough a unified causal intervention method. Experimental results demonstrate\nthe outstanding performance of our method in distinguishing CN/MCI/AD cases,\noutperforming other methods in most evaluation metrics. The study showcases the\npotential of integrating causal reasoning with multi-modal learning for\nneurological disease diagnosis."
                },
                "authors": [
                    {
                        "name": "Yutao Jin"
                    },
                    {
                        "name": "Haowen Xiao"
                    },
                    {
                        "name": "Junyong Zhai"
                    },
                    {
                        "name": "Yuxiao Li"
                    },
                    {
                        "name": "Jielei Chu"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Yuxiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Li"
                },
                "author": "Yuxiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04320v1",
                "updated": "2025-11-06T12:47:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    47,
                    33,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:47:33Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    47,
                    33,
                    3,
                    310,
                    0
                ],
                "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient\n  Navigation in Unknown Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MacroNav: Multi-Task Context Representation Learning Enables Efficient\n  Navigation in Unknown Environments"
                },
                "summary": "Autonomous navigation in unknown environments requires compact yet expressive\nspatial understanding under partial observability to support high-level\ndecision making. Existing approaches struggle to balance rich contextual\nrepresentation with navigation efficiency. We present MacroNav, a\nlearning-based navigation framework featuring two key components: (1) a\nlightweight context encoder trained via multi-task self-supervised learning to\ncapture multi-scale, navigation-centric spatial representations; and (2) a\nreinforcement learning policy that seamlessly integrates these representations\nwith graph-based reasoning for efficient action selection. Extensive\nexperiments demonstrate the context encoder's efficient and robust\nenvironmental understanding. Real-world deployments further validate MacroNav's\neffectiveness, yielding significant gains over state-of-the-art navigation\nmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),\nwhile maintaining low computational cost. Code will be released upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in unknown environments requires compact yet expressive\nspatial understanding under partial observability to support high-level\ndecision making. Existing approaches struggle to balance rich contextual\nrepresentation with navigation efficiency. We present MacroNav, a\nlearning-based navigation framework featuring two key components: (1) a\nlightweight context encoder trained via multi-task self-supervised learning to\ncapture multi-scale, navigation-centric spatial representations; and (2) a\nreinforcement learning policy that seamlessly integrates these representations\nwith graph-based reasoning for efficient action selection. Extensive\nexperiments demonstrate the context encoder's efficient and robust\nenvironmental understanding. Real-world deployments further validate MacroNav's\neffectiveness, yielding significant gains over state-of-the-art navigation\nmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),\nwhile maintaining low computational cost. Code will be released upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Kuankuan Sima"
                    },
                    {
                        "name": "Longbin Tang"
                    },
                    {
                        "name": "Haozhe Ma"
                    },
                    {
                        "name": "Lin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhao"
                },
                "author": "Lin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04317v1",
                "updated": "2025-11-06T12:42:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    42,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:42:03Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    42,
                    3,
                    3,
                    310,
                    0
                ],
                "title": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive\n  Text-to-Video Generation"
                },
                "summary": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io."
                },
                "authors": [
                    {
                        "name": "Xiangjun Zhang"
                    },
                    {
                        "name": "Litong Gong"
                    },
                    {
                        "name": "Yinglin Zheng"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Mingyi Xu"
                    },
                    {
                        "name": "Biao Wang"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Ming Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zeng"
                },
                "author": "Ming Zeng",
                "arxiv_comment": "17 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04316v1",
                "updated": "2025-11-06T12:38:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    38,
                    9,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:38:09Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    38,
                    9,
                    3,
                    310,
                    0
                ],
                "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research"
                },
                "summary": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety."
                },
                "authors": [
                    {
                        "name": "Tim Beyer"
                    },
                    {
                        "name": "Jonas Dornbusch"
                    },
                    {
                        "name": "Jakob Steimle"
                    },
                    {
                        "name": "Moritz Ladenburger"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14133v3",
                "updated": "2025-11-06T12:34:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    34,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2024-11-21T14:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    0,
                    1,
                    3,
                    326,
                    0
                ],
                "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs"
                },
                "summary": "LLMs have shown impressive capabilities across various natural language\nprocessing tasks, yet remain vulnerable to input prompts, known as jailbreak\nattacks, carefully designed to bypass safety guardrails and elicit harmful\nresponses. Traditional methods rely on manual heuristics but suffer from\nlimited generalizability. Despite being automatic, optimization-based attacks\noften produce unnatural prompts that can be easily detected by safety filters\nor require high computational costs due to discrete token optimization. In this\npaper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel\nautomated framework that can efficiently generate human-readable jailbreak\nprompts in a fully black-box setting. In particular, GASP leverages latent\nBayesian optimization to craft adversarial suffixes by efficiently exploring\ncontinuous latent embedding spaces, gradually optimizing the suffix prompter to\nimprove attack efficacy while balancing prompt coherence via a targeted\niterative refinement procedure. Through comprehensive experiments, we show that\nGASP can produce natural adversarial prompts, significantly improving jailbreak\nsuccess over baselines, reducing training times, and accelerating inference\nspeed, thus making it an efficient and scalable solution for red-teaming LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown impressive capabilities across various natural language\nprocessing tasks, yet remain vulnerable to input prompts, known as jailbreak\nattacks, carefully designed to bypass safety guardrails and elicit harmful\nresponses. Traditional methods rely on manual heuristics but suffer from\nlimited generalizability. Despite being automatic, optimization-based attacks\noften produce unnatural prompts that can be easily detected by safety filters\nor require high computational costs due to discrete token optimization. In this\npaper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel\nautomated framework that can efficiently generate human-readable jailbreak\nprompts in a fully black-box setting. In particular, GASP leverages latent\nBayesian optimization to craft adversarial suffixes by efficiently exploring\ncontinuous latent embedding spaces, gradually optimizing the suffix prompter to\nimprove attack efficacy while balancing prompt coherence via a targeted\niterative refinement procedure. Through comprehensive experiments, we show that\nGASP can produce natural adversarial prompts, significantly improving jailbreak\nsuccess over baselines, reducing training times, and accelerating inference\nspeed, thus making it an efficient and scalable solution for red-teaming LLMs."
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025. Project page and demos:\n  https://air-ml.org/project/gasp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10641v2",
                "updated": "2025-11-06T12:24:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    24,
                    59,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-12T18:58:42Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    18,
                    58,
                    42,
                    4,
                    255,
                    0
                ],
                "title": "Test-Time Warmup for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Warmup for Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) hold great promise for advanced\nreasoning at the intersection of text and images, yet they have not fully\nrealized this potential. MLLMs typically integrate an LLM, a vision encoder,\nand a connector that maps the vision encoder's embeddings into the LLM's text\nembedding space. Although each component is pretrained on massive datasets with\nbillions of samples, the entire multimodal model is typically trained on only\nthousands (or a few million) samples, which can result in weak performance on\ncomplex reasoning tasks. To address these shortcomings, instead of relying on\nextensive labeled datasets for fine-tuning, we propose a Test-Time Warmup\nmethod that adapts the MLLM per test instance by leveraging data from weakly\nsupervised auxiliary tasks. With our approach, we observe a relative\nperformance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on\nthe Llama-Vision-Instruct model. Our method demonstrates that 'warming up'\nbefore inference can enhance MLLMs' robustness across diverse reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) hold great promise for advanced\nreasoning at the intersection of text and images, yet they have not fully\nrealized this potential. MLLMs typically integrate an LLM, a vision encoder,\nand a connector that maps the vision encoder's embeddings into the LLM's text\nembedding space. Although each component is pretrained on massive datasets with\nbillions of samples, the entire multimodal model is typically trained on only\nthousands (or a few million) samples, which can result in weak performance on\ncomplex reasoning tasks. To address these shortcomings, instead of relying on\nextensive labeled datasets for fine-tuning, we propose a Test-Time Warmup\nmethod that adapts the MLLM per test instance by leveraging data from weakly\nsupervised auxiliary tasks. With our approach, we observe a relative\nperformance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on\nthe Llama-Vision-Instruct model. Our method demonstrates that 'warming up'\nbefore inference can enhance MLLMs' robustness across diverse reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Nikita Rajaneesh"
                    },
                    {
                        "name": "Thomas Zollo"
                    },
                    {
                        "name": "Richard Zemel"
                    }
                ],
                "author_detail": {
                    "name": "Richard Zemel"
                },
                "author": "Richard Zemel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04307v1",
                "updated": "2025-11-06T12:19:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    19,
                    2,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T12:19:02Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    19,
                    2,
                    3,
                    310,
                    0
                ],
                "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents"
                },
                "summary": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360."
                },
                "authors": [
                    {
                        "name": "Jian Mu"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Chiming Ni"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Bo Qiao"
                    },
                    {
                        "name": "Kartik Mathur"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Xiaojun Ma"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04737v2",
                "updated": "2025-11-06T12:02:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    12,
                    2,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-04-07T05:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    5,
                    27,
                    32,
                    0,
                    97,
                    0
                ],
                "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction\n  and Explanation in the Indian Legal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction\n  and Explanation in the Indian Legal Context"
                },
                "summary": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Paper accepted in the AACL-IJCNLP 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15835v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15835v3",
                "updated": "2025-11-06T11:42:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    42,
                    22,
                    3,
                    310,
                    0
                ],
                "published": "2025-02-20T12:44:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    44,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "Pragmatic Reasoning improves LLM Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatic Reasoning improves LLM Code Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed approaches that produce multiple\ncandidates of the program code and then rerank them to identify the best\nsolution. In this paper, we propose CodeRSA, a novel code candidate reranking\nmechanism built upon the Rational Speech Act (RSA) framework, designed to guide\nLLMs toward more comprehensive pragmatic reasoning about user intent. We\nevaluate CodeRSA using Llama-3-8B-Instruct and Qwen-2.5-7B-Instruct on two\nwidely used code generation benchmarks, HumanEval and MBPP. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed approaches that produce multiple\ncandidates of the program code and then rerank them to identify the best\nsolution. In this paper, we propose CodeRSA, a novel code candidate reranking\nmechanism built upon the Rational Speech Act (RSA) framework, designed to guide\nLLMs toward more comprehensive pragmatic reasoning about user intent. We\nevaluate CodeRSA using Llama-3-8B-Instruct and Qwen-2.5-7B-Instruct on two\nwidely used code generation benchmarks, HumanEval and MBPP. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuchen Cao"
                    },
                    {
                        "name": "Sven Apel"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15835v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15835v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04286v1",
                "updated": "2025-11-06T11:27:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    27,
                    38,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T11:27:38Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    27,
                    38,
                    3,
                    310,
                    0
                ],
                "title": "Efficient Reinforcement Learning from Human Feedback via Bayesian\n  Preference Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reinforcement Learning from Human Feedback via Bayesian\n  Preference Inference"
                },
                "summary": "Learning from human preferences is a cornerstone of aligning machine learning\nmodels with subjective human judgments. Yet, collecting such preference data is\noften costly and time-consuming, motivating the need for more efficient\nlearning paradigms. Two established approaches offer complementary advantages:\nRLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,\nwhile PBO achieves greater sample efficiency through active querying. We\npropose a hybrid framework that unifies RLHF's scalability with PBO's query\nefficiency by integrating an acquisition-driven module into the RLHF pipeline,\nthereby enabling active and sample-efficient preference gathering. We validate\nthe proposed approach on two representative domains: (i) high-dimensional\npreference optimization and (ii) LLM fine-tuning. Experimental results\ndemonstrate consistent improvements in both sample efficiency and overall\nperformance across these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human preferences is a cornerstone of aligning machine learning\nmodels with subjective human judgments. Yet, collecting such preference data is\noften costly and time-consuming, motivating the need for more efficient\nlearning paradigms. Two established approaches offer complementary advantages:\nRLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,\nwhile PBO achieves greater sample efficiency through active querying. We\npropose a hybrid framework that unifies RLHF's scalability with PBO's query\nefficiency by integrating an acquisition-driven module into the RLHF pipeline,\nthereby enabling active and sample-efficient preference gathering. We validate\nthe proposed approach on two representative domains: (i) high-dimensional\npreference optimization and (ii) LLM fine-tuning. Experimental results\ndemonstrate consistent improvements in both sample efficiency and overall\nperformance across these tasks."
                },
                "authors": [
                    {
                        "name": "Matteo Cercola"
                    },
                    {
                        "name": "Valeria Capretti"
                    },
                    {
                        "name": "Simone Formentin"
                    }
                ],
                "author_detail": {
                    "name": "Simone Formentin"
                },
                "author": "Simone Formentin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09334v2",
                "updated": "2025-11-06T11:24:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    24,
                    32,
                    3,
                    310,
                    0
                ],
                "published": "2025-02-13T13:53:32Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    53,
                    32,
                    3,
                    44,
                    0
                ],
                "title": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud\n  Environments"
                },
                "summary": "Recent developments in large language models (LLMs) have demonstrated their\nremarkable proficiency in a range of tasks. Compared to in-house homogeneous\nGPU clusters, deploying LLMs in cloud environments with diverse types of GPUs\nis crucial for addressing the GPU shortage problem and being more\ncost-effective. However, the diversity of network environments and various GPU\ntypes on the cloud bring difficulties to achieving high-performance serving. In\nthis work, we propose ThunderServe, a high-performance and cost-efficient LLM\nserving system for heterogeneous cloud environments. We introduce a novel\nscheduling algorithm, which optimizes the deployment plan of LLM serving to\naccommodate the heterogeneous resource and network bandwidth conditions in\ncloud environments. Furthermore, we propose a lightweight re-scheduling\nmechanism, designed to adapt to fluctuating online conditions (e.g., node\nfailures, workload shifts) without the need for costly restarts of ongoing\nservices. Empirical results in both heterogeneous cloud and homogeneous\nin-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and\non average a $1.7\\times$ increase in throughput and achieves up to a\n2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines\ncompared with state-of-the-art systems given the same price budget, suggesting\nopting for cloud services provides a more cost-efficient solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have demonstrated their\nremarkable proficiency in a range of tasks. Compared to in-house homogeneous\nGPU clusters, deploying LLMs in cloud environments with diverse types of GPUs\nis crucial for addressing the GPU shortage problem and being more\ncost-effective. However, the diversity of network environments and various GPU\ntypes on the cloud bring difficulties to achieving high-performance serving. In\nthis work, we propose ThunderServe, a high-performance and cost-efficient LLM\nserving system for heterogeneous cloud environments. We introduce a novel\nscheduling algorithm, which optimizes the deployment plan of LLM serving to\naccommodate the heterogeneous resource and network bandwidth conditions in\ncloud environments. Furthermore, we propose a lightweight re-scheduling\nmechanism, designed to adapt to fluctuating online conditions (e.g., node\nfailures, workload shifts) without the need for costly restarts of ongoing\nservices. Empirical results in both heterogeneous cloud and homogeneous\nin-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and\non average a $1.7\\times$ increase in throughput and achieves up to a\n2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines\ncompared with state-of-the-art systems given the same price budget, suggesting\nopting for cloud services provides a more cost-efficient solution."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Taiyi Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Ana Klimovic"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18112v2",
                "updated": "2025-11-06T11:12:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    12,
                    5,
                    3,
                    310,
                    0
                ],
                "published": "2025-09-09T22:04:33Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    22,
                    4,
                    33,
                    1,
                    252,
                    0
                ],
                "title": "Large language models surpass domain-specific architectures for\n  antepartum electronic fetal monitoring analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models surpass domain-specific architectures for\n  antepartum electronic fetal monitoring analysis"
                },
                "summary": "Foundation models (FMs) and large language models (LLMs) have demonstrated\npromising generalization across diverse domains for time-series analysis, yet\ntheir potential for electronic fetal monitoring (EFM) and cardiotocography\n(CTG) analysis remains underexplored. Most existing CTG studies relied on\ndomain-specific models and lack systematic comparisons with modern foundation\nor language models, limiting our understanding of whether these models can\noutperform specialized systems in fetal health assessment. In this study, we\npresent the first comprehensive benchmark of state-of-the-art architectures for\nautomated antepartum CTG classification. Over 2,500 20-minutes recordings were\nused to evaluate over 15 models spanning domain-specific, time-series,\nfoundation, and language-model categories under a unified framework. Fine-tuned\nLLMs consistently outperformed both foundation and domain-specific models\nacross data-availability scenarios, except when uterine-activity signals were\nabsent, where domain-specific models showed greater robustness. These\nperformance gains, however, required substantially higher computational\nresources. Our results highlight that while fine-tuned LLMs achieved\nstate-of-the-art performance for CTG classification, practical deployment must\nbalance performance with computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) and large language models (LLMs) have demonstrated\npromising generalization across diverse domains for time-series analysis, yet\ntheir potential for electronic fetal monitoring (EFM) and cardiotocography\n(CTG) analysis remains underexplored. Most existing CTG studies relied on\ndomain-specific models and lack systematic comparisons with modern foundation\nor language models, limiting our understanding of whether these models can\noutperform specialized systems in fetal health assessment. In this study, we\npresent the first comprehensive benchmark of state-of-the-art architectures for\nautomated antepartum CTG classification. Over 2,500 20-minutes recordings were\nused to evaluate over 15 models spanning domain-specific, time-series,\nfoundation, and language-model categories under a unified framework. Fine-tuned\nLLMs consistently outperformed both foundation and domain-specific models\nacross data-availability scenarios, except when uterine-activity signals were\nabsent, where domain-specific models showed greater robustness. These\nperformance gains, however, required substantially higher computational\nresources. Our results highlight that while fine-tuned LLMs achieved\nstate-of-the-art performance for CTG classification, practical deployment must\nbalance performance with computational efficiency."
                },
                "authors": [
                    {
                        "name": "Sheng Wong"
                    },
                    {
                        "name": "Ravi Shankar"
                    },
                    {
                        "name": "Beth Albert"
                    },
                    {
                        "name": "Gabriel Davis Jones"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Davis Jones"
                },
                "author": "Gabriel Davis Jones",
                "arxiv_comment": "Preparing for journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21849v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21849v3",
                "updated": "2025-11-06T11:09:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    9,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-22T17:02:48Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    2,
                    48,
                    2,
                    295,
                    0
                ],
                "title": "TowerVision: Understanding and Improving Multilinguality in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TowerVision: Understanding and Improving Multilinguality in\n  Vision-Language Models"
                },
                "summary": "Despite significant advances in vision-language models (VLMs), most existing\nwork follows an English-centric design process, limiting their effectiveness in\nmultilingual settings. In this work, we provide a comprehensive empirical study\nanalyzing the impact of several multilingual design choices, such as training\ndata composition, encoder selection, and text backbones. The result is\nTowerVision, a family of open multilingual VLMs for both image-text and\nvideo-text tasks, built upon the multilingual text-only model Tower+.\nTowerVision achieves competitive performance on multiple multimodal\nmultilingual benchmarks and shows particular strength in culturally grounded\ntasks and multimodal translation. By incorporating visual and cultural context\nduring fine-tuning, our models surpass existing approaches trained on\nsubstantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image\ntasks) and ViMUL-Bench (video tasks). Alongside the models, we release\nVisionBlocks, a high-quality, curated vision-language dataset. Our findings\nhighlight that multilingual vision-language training data substantially\nimproves cross-lingual generalization -- both from high-resource to\nunderrepresented languages and vice versa -- and that instruction-tuned LLMs\nare not always the optimal initialization point. To support further research,\nwe publicly release all models, data, and training recipes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advances in vision-language models (VLMs), most existing\nwork follows an English-centric design process, limiting their effectiveness in\nmultilingual settings. In this work, we provide a comprehensive empirical study\nanalyzing the impact of several multilingual design choices, such as training\ndata composition, encoder selection, and text backbones. The result is\nTowerVision, a family of open multilingual VLMs for both image-text and\nvideo-text tasks, built upon the multilingual text-only model Tower+.\nTowerVision achieves competitive performance on multiple multimodal\nmultilingual benchmarks and shows particular strength in culturally grounded\ntasks and multimodal translation. By incorporating visual and cultural context\nduring fine-tuning, our models surpass existing approaches trained on\nsubstantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image\ntasks) and ViMUL-Bench (video tasks). Alongside the models, we release\nVisionBlocks, a high-quality, curated vision-language dataset. Our findings\nhighlight that multilingual vision-language training data substantially\nimproves cross-lingual generalization -- both from high-resource to\nunderrepresented languages and vice versa -- and that instruction-tuned LLMs\nare not always the optimal initialization point. To support further research,\nwe publicly release all models, data, and training recipes."
                },
                "authors": [
                    {
                        "name": "André G. Viveiros"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Saul Santos"
                    },
                    {
                        "name": "Sonal Sannigrahi"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "15 pages, 7 figures, submitted to arXiv October 2025. All models,\n  datasets, and training code will be released at\n  https://huggingface.co/collections/utter-project/towervision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21849v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21849v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T45, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17760v2",
                "updated": "2025-11-06T11:07:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    7,
                    41,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-23T11:34:02Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    34,
                    2,
                    4,
                    143,
                    0
                ],
                "title": "But what is your honest answer? Aiding LLM-judges with honest\n  alternatives using steering vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "But what is your honest answer? Aiding LLM-judges with honest\n  alternatives using steering vectors"
                },
                "summary": "Detecting subtle forms of dishonesty like sycophancy and manipulation in\nLarge Language Models (LLMs) remains challenging for both humans and automated\nevaluators, as these behaviors often appear through small biases rather than\nclear false statements. We introduce Judge Using Safety-Steered Alternatives\n(JUSSA), a novel framework that employs steering vectors not to improve model\nbehavior directly, but to enhance LLM judges' evaluation capabilities. JUSSA\napplies steering vectors during inference to generate more honest alternatives,\nproviding judges with contrastive examples that make subtle dishonest patterns\neasier to detect. While existing evaluation methods rely on black-box\nevaluation, JUSSA leverages model internals to create targeted comparisons from\nsingle examples. We evaluate our method on sycophancy detection and introduce a\nnew manipulation dataset covering multiple types of manipulation. Our results\ndemonstrate that JUSSA effectively improves detection accuracy over\nsingle-response evaluation in various cases. Analysis across judge models\nreveals that JUSSA helps weaker judges on easier dishonesty detection tasks,\nand stronger judges on harder tasks. Layer-wise experiments show how dishonest\nprompts cause representations to diverge from honest ones in middle layers,\nrevealing where steering interventions are most effective for generating\ncontrastive examples. By demonstrating that steering vectors can enhance safety\nevaluation rather than just modify behavior, our work opens new directions for\nscalable model auditing as systems become increasingly sophisticated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting subtle forms of dishonesty like sycophancy and manipulation in\nLarge Language Models (LLMs) remains challenging for both humans and automated\nevaluators, as these behaviors often appear through small biases rather than\nclear false statements. We introduce Judge Using Safety-Steered Alternatives\n(JUSSA), a novel framework that employs steering vectors not to improve model\nbehavior directly, but to enhance LLM judges' evaluation capabilities. JUSSA\napplies steering vectors during inference to generate more honest alternatives,\nproviding judges with contrastive examples that make subtle dishonest patterns\neasier to detect. While existing evaluation methods rely on black-box\nevaluation, JUSSA leverages model internals to create targeted comparisons from\nsingle examples. We evaluate our method on sycophancy detection and introduce a\nnew manipulation dataset covering multiple types of manipulation. Our results\ndemonstrate that JUSSA effectively improves detection accuracy over\nsingle-response evaluation in various cases. Analysis across judge models\nreveals that JUSSA helps weaker judges on easier dishonesty detection tasks,\nand stronger judges on harder tasks. Layer-wise experiments show how dishonest\nprompts cause representations to diverge from honest ones in middle layers,\nrevealing where steering interventions are most effective for generating\ncontrastive examples. By demonstrating that steering vectors can enhance safety\nevaluation rather than just modify behavior, our work opens new directions for\nscalable model auditing as systems become increasingly sophisticated."
                },
                "authors": [
                    {
                        "name": "Leon Eshuijs"
                    },
                    {
                        "name": "Archie Chaudhury"
                    },
                    {
                        "name": "Alan McBeth"
                    },
                    {
                        "name": "Ethan Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Nguyen"
                },
                "author": "Ethan Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03441v2",
                "updated": "2025-11-06T11:06:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    6,
                    10,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T13:02:06Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    2,
                    6,
                    2,
                    309,
                    0
                ],
                "title": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the\n  Biomedical Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the\n  Biomedical Field"
                },
                "summary": "Critical appraisal of scientific literature is an essential skill in the\nbiomedical field. While large language models (LLMs) can offer promising\nsupport in this task, their reliability remains limited, particularly for\ncritical reasoning in specialized domains. We introduce CareMedEval, an\noriginal dataset designed to evaluate LLMs on biomedical critical appraisal and\nreasoning tasks. Derived from authentic exams taken by French medical students,\nthe dataset contains 534 questions based on 37 scientific articles. Unlike\nexisting benchmarks, CareMedEval explicitly evaluates critical reading and\nreasoning grounded in scientific papers. Benchmarking state-of-the-art\ngeneralist and biomedical-specialized LLMs under various context conditions\nreveals the difficulty of the task: open and commercial models fail to exceed\nan Exact Match Rate of 0.5 even though generating intermediate reasoning tokens\nconsiderably improves the results. Yet, models remain challenged especially on\nquestions about study limitations and statistical analysis. CareMedEval\nprovides a challenging benchmark for grounded reasoning, exposing current LLM\nlimitations and paving the way for future development of automated support for\ncritical appraisal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical appraisal of scientific literature is an essential skill in the\nbiomedical field. While large language models (LLMs) can offer promising\nsupport in this task, their reliability remains limited, particularly for\ncritical reasoning in specialized domains. We introduce CareMedEval, an\noriginal dataset designed to evaluate LLMs on biomedical critical appraisal and\nreasoning tasks. Derived from authentic exams taken by French medical students,\nthe dataset contains 534 questions based on 37 scientific articles. Unlike\nexisting benchmarks, CareMedEval explicitly evaluates critical reading and\nreasoning grounded in scientific papers. Benchmarking state-of-the-art\ngeneralist and biomedical-specialized LLMs under various context conditions\nreveals the difficulty of the task: open and commercial models fail to exceed\nan Exact Match Rate of 0.5 even though generating intermediate reasoning tokens\nconsiderably improves the results. Yet, models remain challenged especially on\nquestions about study limitations and statistical analysis. CareMedEval\nprovides a challenging benchmark for grounded reasoning, exposing current LLM\nlimitations and paving the way for future development of automated support for\ncritical appraisal."
                },
                "authors": [
                    {
                        "name": "Doria Bonzi"
                    },
                    {
                        "name": "Alexandre Guiggi"
                    },
                    {
                        "name": "Frédéric Béchet"
                    },
                    {
                        "name": "Carlos Ramisch"
                    },
                    {
                        "name": "Benoit Favre"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Favre"
                },
                "author": "Benoit Favre",
                "arxiv_comment": "Preprint submitted to LREC 2026 (under review) To access the dataset,\n  see https://github.com/bonzid/CareMedEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04267v1",
                "updated": "2025-11-06T11:02:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    2,
                    4,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T11:02:04Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    11,
                    2,
                    4,
                    3,
                    310,
                    0
                ],
                "title": "A Tool for Benchmarking Large Language Models' Robustness in Assessing\n  the Realism of Driving Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tool for Benchmarking Large Language Models' Robustness in Assessing\n  the Realism of Driving Scenarios"
                },
                "summary": "In recent years, autonomous driving systems have made significant progress,\nyet ensuring their safety remains a key challenge. To this end, scenario-based\ntesting offers a practical solution, and simulation-based methods have gained\ntraction due to the high cost and risk of real-world testing. However,\nevaluating the realism of simulated scenarios remains difficult, creating\ndemand for effective assessment methods. Recent advances show that Large\nLanguage Models (LLMs) possess strong reasoning and generalization\ncapabilities, suggesting their potential in assessing scenario realism through\nscenario-related textual prompts. Motivated by this, we propose DriveRLR, a\nbenchmark tool to assess the robustness of LLMs in evaluating the realism of\ndriving scenarios. DriveRLR generates mutated scenario variants, constructs\nprompts, which are then used to assess a given LLM's ability and robustness in\ndetermining the realism of driving scenarios. We validate DriveRLR on the\nDeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4\nMaverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals\ndifferences in the robustness of various LLMs, demonstrating its effectiveness\nand practical value in scenario realism assessment. Beyond LLM robustness\nevaluation, DriveRLR can serve as a practical component in applications such as\nan objective function to guide scenario generation, supporting simulation-based\nADS testing workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, autonomous driving systems have made significant progress,\nyet ensuring their safety remains a key challenge. To this end, scenario-based\ntesting offers a practical solution, and simulation-based methods have gained\ntraction due to the high cost and risk of real-world testing. However,\nevaluating the realism of simulated scenarios remains difficult, creating\ndemand for effective assessment methods. Recent advances show that Large\nLanguage Models (LLMs) possess strong reasoning and generalization\ncapabilities, suggesting their potential in assessing scenario realism through\nscenario-related textual prompts. Motivated by this, we propose DriveRLR, a\nbenchmark tool to assess the robustness of LLMs in evaluating the realism of\ndriving scenarios. DriveRLR generates mutated scenario variants, constructs\nprompts, which are then used to assess a given LLM's ability and robustness in\ndetermining the realism of driving scenarios. We validate DriveRLR on the\nDeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4\nMaverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals\ndifferences in the robustness of various LLMs, demonstrating its effectiveness\nand practical value in scenario realism assessment. Beyond LLM robustness\nevaluation, DriveRLR can serve as a practical component in applications such as\nan objective function to guide scenario generation, supporting simulation-based\nADS testing workflows."
                },
                "authors": [
                    {
                        "name": "Jiahui Wu"
                    },
                    {
                        "name": "Chengjie Lu"
                    },
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Shaukat Ali"
                    }
                ],
                "author_detail": {
                    "name": "Shaukat Ali"
                },
                "author": "Shaukat Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02531v2",
                "updated": "2025-11-06T10:52:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    52,
                    31,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-04T12:34:46Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    34,
                    46,
                    1,
                    308,
                    0
                ],
                "title": "Causal Graph Neural Networks for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Graph Neural Networks for Healthcare"
                },
                "summary": "Healthcare artificial intelligence systems routinely fail when deployed\nacross institutions, with documented performance drops and perpetuation of\ndiscriminatory patterns embedded in historical data. This brittleness stems, in\npart, from learning statistical associations rather than causal mechanisms.\nCausal graph neural networks address this triple crisis of distribution shift,\ndiscrimination, and inscrutability by combining graph-based representations of\nbiomedical data with causal inference principles to learn invariant mechanisms\nrather than spurious correlations. This Review examines methodological\nfoundations spanning structural causal models, disentangled causal\nrepresentation learning, and techniques for interventional prediction and\ncounterfactual reasoning on graphs. We analyse applications demonstrating\nclinical value across psychiatric diagnosis through brain network analysis,\ncancer subtyping via multi-omics causal integration, continuous physiological\nmonitoring with mechanistic interpretation, and drug recommendation correcting\nprescription bias. These advances establish foundations for patient-specific\nCausal Digital Twins, enabling in silico clinical experimentation, with\nintegration of large language models for hypothesis generation and causal graph\nneural networks for mechanistic validation. Substantial barriers remain,\nincluding computational requirements precluding real-time deployment,\nvalidation challenges demanding multi-modal evidence triangulation beyond\ncross-validation, and risks of causal-washing where methods employ causal\nterminology without rigorous evidentiary support. We propose tiered frameworks\ndistinguishing causally-inspired architectures from causally-validated\ndiscoveries and identify critical research priorities making causal rather than\npurely associational claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare artificial intelligence systems routinely fail when deployed\nacross institutions, with documented performance drops and perpetuation of\ndiscriminatory patterns embedded in historical data. This brittleness stems, in\npart, from learning statistical associations rather than causal mechanisms.\nCausal graph neural networks address this triple crisis of distribution shift,\ndiscrimination, and inscrutability by combining graph-based representations of\nbiomedical data with causal inference principles to learn invariant mechanisms\nrather than spurious correlations. This Review examines methodological\nfoundations spanning structural causal models, disentangled causal\nrepresentation learning, and techniques for interventional prediction and\ncounterfactual reasoning on graphs. We analyse applications demonstrating\nclinical value across psychiatric diagnosis through brain network analysis,\ncancer subtyping via multi-omics causal integration, continuous physiological\nmonitoring with mechanistic interpretation, and drug recommendation correcting\nprescription bias. These advances establish foundations for patient-specific\nCausal Digital Twins, enabling in silico clinical experimentation, with\nintegration of large language models for hypothesis generation and causal graph\nneural networks for mechanistic validation. Substantial barriers remain,\nincluding computational requirements precluding real-time deployment,\nvalidation challenges demanding multi-modal evidence triangulation beyond\ncross-validation, and risks of causal-washing where methods employ causal\nterminology without rigorous evidentiary support. We propose tiered frameworks\ndistinguishing causally-inspired architectures from causally-validated\ndiscoveries and identify critical research priorities making causal rather than\npurely associational claims."
                },
                "authors": [
                    {
                        "name": "Munib Mesinovic"
                    },
                    {
                        "name": "Max Buhlan"
                    },
                    {
                        "name": "Tingting Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Zhu"
                },
                "author": "Tingting Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04261v1",
                "updated": "2025-11-06T10:51:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    51,
                    20,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T10:51:20Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    51,
                    20,
                    3,
                    310,
                    0
                ],
                "title": "A Parallel Region-Adaptive Differential Privacy Framework for Image\n  Pixelization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Parallel Region-Adaptive Differential Privacy Framework for Image\n  Pixelization"
                },
                "summary": "The widespread deployment of high-resolution visual sensing systems, coupled\nwith the rise of foundation models, has amplified privacy risks in video-based\napplications. Differentially private pixelization offers mathematically\nguaranteed protection for visual data through grid-based noise addition, but\nchallenges remain in preserving task-relevant fidelity, achieving scalability,\nand enabling efficient real-time deployment. To address this, we propose a\nnovel parallel, region-adaptive pixelization framework that combines the\ntheoretical rigor of differential privacy with practical efficiency. Our method\nadaptively adjusts grid sizes and noise scales based on regional complexity,\nleveraging GPU parallelism to achieve significant runtime acceleration compared\nto the classical baseline. A lightweight storage scheme is introduced by\nretaining only essential noisy statistics, significantly reducing space\noverhead. Formal privacy analysis is provided under the Laplace mechanism and\nparallel composition theorem. Extensive experiments on the PETS, Venice-2, and\nPPM-100 datasets demonstrate favorable privacy-utility trade-offs and\nsignificant runtime/storage reductions. A face re-identification attack\nexperiment on CelebA further confirms the method's effectiveness in preventing\nidentity inference. This validates its suitability for real-time\nprivacy-critical applications such as elderly care, smart home monitoring,\ndriver behavior analysis, and crowd behavior monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of high-resolution visual sensing systems, coupled\nwith the rise of foundation models, has amplified privacy risks in video-based\napplications. Differentially private pixelization offers mathematically\nguaranteed protection for visual data through grid-based noise addition, but\nchallenges remain in preserving task-relevant fidelity, achieving scalability,\nand enabling efficient real-time deployment. To address this, we propose a\nnovel parallel, region-adaptive pixelization framework that combines the\ntheoretical rigor of differential privacy with practical efficiency. Our method\nadaptively adjusts grid sizes and noise scales based on regional complexity,\nleveraging GPU parallelism to achieve significant runtime acceleration compared\nto the classical baseline. A lightweight storage scheme is introduced by\nretaining only essential noisy statistics, significantly reducing space\noverhead. Formal privacy analysis is provided under the Laplace mechanism and\nparallel composition theorem. Extensive experiments on the PETS, Venice-2, and\nPPM-100 datasets demonstrate favorable privacy-utility trade-offs and\nsignificant runtime/storage reductions. A face re-identification attack\nexperiment on CelebA further confirms the method's effectiveness in preventing\nidentity inference. This validates its suitability for real-time\nprivacy-critical applications such as elderly care, smart home monitoring,\ndriver behavior analysis, and crowd behavior monitoring."
                },
                "authors": [
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04256v1",
                "updated": "2025-11-06T10:48:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    48,
                    31,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T10:48:31Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    48,
                    31,
                    3,
                    310,
                    0
                ],
                "title": "SSPO: Subsentence-level Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSPO: Subsentence-level Policy Optimization"
                },
                "summary": "As a significant part of post-training of the Large Language Models (LLMs),\nReinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'\nreasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative\nPolicy Optimization) and GSPO (Group Sequence Policy Optimization), are\nobserved to suffer from unstable policy updates and low usage of sampling data,\nrespectively. The importance ratio of GRPO is calculated at the token level,\nwhich focuses more on optimizing a single token. This will be easily affected\nby outliers, leading to model training collapse. GSPO proposed the calculation\nof the response level importance ratio, which solves the problem of high\nvariance and training noise accumulation in the calculation of the GRPO\nimportance ratio. However, since all the response tokens share a common\nimportance ratio, extreme values can easily raise or lower the overall mean,\nleading to the entire response being mistakenly discarded, resulting in a\ndecrease in the utilization of sampled data. This paper introduces SSPO, which\napplies sentence-level importance ratio, taking the balance between GRPO and\nGSPO. SSPO not only avoids training collapse and high variance, but also\nprevents the whole response tokens from being abandoned by the clipping\nmechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily\nadjust the clipping bounds, encouraging high-entropy tokens to explore and\nnarrow the clipping range of low-entropy tokens. In particular, SSPO achieves\nan average score of 46.57 across five datasets, surpassing GRPO (43.01) and\nGSPO (44.42), and wins state-of-the-art performance on three datasets. These\nresults highlight SSPO's effectiveness in leveraging generated data by taking\nthe essence of GSPO but rejecting its shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a significant part of post-training of the Large Language Models (LLMs),\nReinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'\nreasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative\nPolicy Optimization) and GSPO (Group Sequence Policy Optimization), are\nobserved to suffer from unstable policy updates and low usage of sampling data,\nrespectively. The importance ratio of GRPO is calculated at the token level,\nwhich focuses more on optimizing a single token. This will be easily affected\nby outliers, leading to model training collapse. GSPO proposed the calculation\nof the response level importance ratio, which solves the problem of high\nvariance and training noise accumulation in the calculation of the GRPO\nimportance ratio. However, since all the response tokens share a common\nimportance ratio, extreme values can easily raise or lower the overall mean,\nleading to the entire response being mistakenly discarded, resulting in a\ndecrease in the utilization of sampled data. This paper introduces SSPO, which\napplies sentence-level importance ratio, taking the balance between GRPO and\nGSPO. SSPO not only avoids training collapse and high variance, but also\nprevents the whole response tokens from being abandoned by the clipping\nmechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily\nadjust the clipping bounds, encouraging high-entropy tokens to explore and\nnarrow the clipping range of low-entropy tokens. In particular, SSPO achieves\nan average score of 46.57 across five datasets, surpassing GRPO (43.01) and\nGSPO (44.42), and wins state-of-the-art performance on three datasets. These\nresults highlight SSPO's effectiveness in leveraging generated data by taking\nthe essence of GSPO but rejecting its shortcomings."
                },
                "authors": [
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Zikang chen"
                    },
                    {
                        "name": "Yanmeng Wang"
                    },
                    {
                        "name": "Zhigen Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhigen Li"
                },
                "author": "Zhigen Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00709v2",
                "updated": "2025-11-06T10:32:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    32,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-08-01T15:23:20Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    23,
                    20,
                    4,
                    213,
                    0
                ],
                "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System"
                },
                "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Ajay Varghese Thomas"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Paper accepted in the AACL-IJCNLP 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00955v3",
                "updated": "2025-11-06T10:24:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    24,
                    23,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-02T14:32:21Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    14,
                    32,
                    21,
                    6,
                    306,
                    0
                ],
                "title": "Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins"
                },
                "summary": "The proliferation of IoT devices in smart cities challenges 6G networks with\nconflicting energy-latency requirements across heterogeneous slices. Existing\napproaches struggle with the energy-latency trade-off, particularly for massive\nscale deployments exceeding 50,000 devices km. This paper proposes an\nedge-aware CyberTwin framework integrating hybrid federated learning for\nenergy-latency co-optimization in 6G network slicing. Our approach combines\ncentralized Artificial Intelligence scheduling for latency-sensitive slices\nwith distributed federated learning for non-critical slices, enhanced by\ncompressive sensing-based digital twins and renewable energy-aware resource\nallocation. The hybrid scheduler leverages a three-tier architecture with\nPhysical Unclonable Function (PUF) based security attestation achieving 99.7%\nattack detection accuracy. Comprehensive simulations demonstrate 52% energy\nreduction for non-real-time slices compared to Diffusion-Reinforcement Learning\nbaselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA\ncompliance. The framework scales to 50,000 devices km with CPU overhead below\n25%, validated through NS-3 hybrid simulations across realistic smart city\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of IoT devices in smart cities challenges 6G networks with\nconflicting energy-latency requirements across heterogeneous slices. Existing\napproaches struggle with the energy-latency trade-off, particularly for massive\nscale deployments exceeding 50,000 devices km. This paper proposes an\nedge-aware CyberTwin framework integrating hybrid federated learning for\nenergy-latency co-optimization in 6G network slicing. Our approach combines\ncentralized Artificial Intelligence scheduling for latency-sensitive slices\nwith distributed federated learning for non-critical slices, enhanced by\ncompressive sensing-based digital twins and renewable energy-aware resource\nallocation. The hybrid scheduler leverages a three-tier architecture with\nPhysical Unclonable Function (PUF) based security attestation achieving 99.7%\nattack detection accuracy. Comprehensive simulations demonstrate 52% energy\nreduction for non-real-time slices compared to Diffusion-Reinforcement Learning\nbaselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA\ncompliance. The framework scales to 50,000 devices km with CPU overhead below\n25%, validated through NS-3 hybrid simulations across realistic smart city\nscenarios."
                },
                "authors": [
                    {
                        "name": "Amine Abouaomar"
                    },
                    {
                        "name": "Badr Ben Elallid"
                    },
                    {
                        "name": "Nabil Benamar"
                    }
                ],
                "author_detail": {
                    "name": "Nabil Benamar"
                },
                "author": "Nabil Benamar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10628v2",
                "updated": "2025-11-06T10:08:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    8,
                    44,
                    3,
                    310,
                    0
                ],
                "published": "2024-10-14T15:35:44Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    35,
                    44,
                    0,
                    288,
                    0
                ],
                "title": "Test smells in LLM-Generated Unit Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells in LLM-Generated Unit Tests"
                },
                "summary": "LLMs promise to transform unit test generation from a manual burden into an\nautomated solution. Yet, beyond metrics such as compilability or coverage,\nlittle is known about the quality of LLM-generated tests, particularly their\nsusceptibility to test smells, design flaws that undermine readability and\nmaintainability. This paper presents the first multi-benchmark, large-scale\nanalysis of test smell diffusion in LLM-generated unit tests. We contrast LLM\noutputs with human-written suites (as the reference for real-world practices)\nand SBST-generated tests from EvoSuite (as the automated baseline),\ndisentangling whether LLMs reproduce human-like flaws or artifacts of synthetic\ngeneration. Our study draws on 20,505 class-level suites from four LLMs\n(GPT-3.5, GPT-4, Mistral 7B, Mixtral 8x7B), 972 method-level cases from\nTestBench, 14,469 EvoSuite tests, and 779,585 human-written tests from 34,635\nopen-source Java projects. Using two complementary detection tools (TsDetect\nand JNose), we analyze prevalence, co-occurrence, and correlations with\nsoftware attributes and generation parameters. Results show that LLM-generated\ntests consistently manifest smells such as Assertion Roulette and Magic Number\nTest, with patterns strongly influenced by prompting strategy, context length,\nand model scale. Comparisons reveal overlaps with human-written tests, raising\nconcerns of potential data leakage from training corpora while EvoSuite\nexhibits distinct, generator-specific flaws. These findings highlight both the\npromise and the risks of LLM-based test generation, and call for the design of\nsmell-aware generation frameworks, prompt engineering strategies, and enhanced\ndetection tools to ensure maintainable, high-quality test code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs promise to transform unit test generation from a manual burden into an\nautomated solution. Yet, beyond metrics such as compilability or coverage,\nlittle is known about the quality of LLM-generated tests, particularly their\nsusceptibility to test smells, design flaws that undermine readability and\nmaintainability. This paper presents the first multi-benchmark, large-scale\nanalysis of test smell diffusion in LLM-generated unit tests. We contrast LLM\noutputs with human-written suites (as the reference for real-world practices)\nand SBST-generated tests from EvoSuite (as the automated baseline),\ndisentangling whether LLMs reproduce human-like flaws or artifacts of synthetic\ngeneration. Our study draws on 20,505 class-level suites from four LLMs\n(GPT-3.5, GPT-4, Mistral 7B, Mixtral 8x7B), 972 method-level cases from\nTestBench, 14,469 EvoSuite tests, and 779,585 human-written tests from 34,635\nopen-source Java projects. Using two complementary detection tools (TsDetect\nand JNose), we analyze prevalence, co-occurrence, and correlations with\nsoftware attributes and generation parameters. Results show that LLM-generated\ntests consistently manifest smells such as Assertion Roulette and Magic Number\nTest, with patterns strongly influenced by prompting strategy, context length,\nand model scale. Comparisons reveal overlaps with human-written tests, raising\nconcerns of potential data leakage from training corpora while EvoSuite\nexhibits distinct, generator-specific flaws. These findings highlight both the\npromise and the risks of LLM-based test generation, and call for the design of\nsmell-aware generation frameworks, prompt engineering strategies, and enhanced\ndetection tools to ensure maintainable, high-quality test code."
                },
                "authors": [
                    {
                        "name": "Wendkûuni C. Ouédraogo"
                    },
                    {
                        "name": "Yinghua Li"
                    },
                    {
                        "name": "Xueqi Dang"
                    },
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Anil Koyuncu"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    }
                ],
                "author_detail": {
                    "name": "Tegawendé F. Bissyandé"
                },
                "author": "Tegawendé F. Bissyandé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03480v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03480v3",
                "updated": "2025-11-06T10:06:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    10,
                    6,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-03-05T13:16:55Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    13,
                    16,
                    55,
                    2,
                    64,
                    0
                ],
                "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via\n  Constrained Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via\n  Constrained Learning"
                },
                "summary": "Vision-language-action models (VLAs) show potential as generalist robot\npolicies. However, these models pose extreme safety challenges during\nreal-world deployment, including the risk of harm to the environment, the robot\nitself, and humans. How can safety constraints be explicitly integrated into\nVLAs? We address this by exploring an integrated safety approach (ISA),\nsystematically modeling safety requirements, then actively eliciting diverse\nunsafe behaviors, effectively constraining VLA policies via safe reinforcement\nlearning, and rigorously assuring their safety through targeted evaluations.\nLeveraging the constrained Markov decision process (CMDP) paradigm, ISA\noptimizes VLAs from a min-max perspective against elicited safety risks. Thus,\npolicies aligned through this comprehensive approach achieve the following key\nfeatures: (I) effective safety-performance trade-offs, reducing the cumulative\ncost of safety violations by 83.58% compared to the state-of-the-art method,\nwhile also maintaining task success rate (+3.85%). (II) strong safety\nassurance, with the ability to mitigate long-tail risks and handle extreme\nfailure scenarios. (III) robust generalization of learned safety behaviors to\nvarious out-of-distribution perturbations. The effectiveness is evaluated on\nlong-horizon mobile manipulation tasks. Our data, models and newly proposed\nbenchmark environment are available at https://pku-safevla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action models (VLAs) show potential as generalist robot\npolicies. However, these models pose extreme safety challenges during\nreal-world deployment, including the risk of harm to the environment, the robot\nitself, and humans. How can safety constraints be explicitly integrated into\nVLAs? We address this by exploring an integrated safety approach (ISA),\nsystematically modeling safety requirements, then actively eliciting diverse\nunsafe behaviors, effectively constraining VLA policies via safe reinforcement\nlearning, and rigorously assuring their safety through targeted evaluations.\nLeveraging the constrained Markov decision process (CMDP) paradigm, ISA\noptimizes VLAs from a min-max perspective against elicited safety risks. Thus,\npolicies aligned through this comprehensive approach achieve the following key\nfeatures: (I) effective safety-performance trade-offs, reducing the cumulative\ncost of safety violations by 83.58% compared to the state-of-the-art method,\nwhile also maintaining task success rate (+3.85%). (II) strong safety\nassurance, with the ability to mitigate long-tail risks and handle extreme\nfailure scenarios. (III) robust generalization of learned safety behaviors to\nvarious out-of-distribution perturbations. The effectiveness is evaluated on\nlong-horizon mobile manipulation tasks. Our data, models and newly proposed\nbenchmark environment are available at https://pku-safevla.github.io."
                },
                "authors": [
                    {
                        "name": "Borong Zhang"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Yingshan Lei"
                    },
                    {
                        "name": "Josef Dai"
                    },
                    {
                        "name": "Yuanpei Chen"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Accepted by NeurIPS 2025 Spotlight Presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03480v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11321v2",
                "updated": "2025-11-06T09:59:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    59,
                    35,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-13T12:19:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    19,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled\n  Multi-Modal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled\n  Multi-Modal Data"
                },
                "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Ruizhe Liu"
                    },
                    {
                        "name": "Pei Zhou"
                    },
                    {
                        "name": "Qian Luo"
                    },
                    {
                        "name": "Li Sun"
                    },
                    {
                        "name": "Jun Cen"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Yanchao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yanchao Yang"
                },
                "author": "Yanchao Yang",
                "arxiv_comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04228v1",
                "updated": "2025-11-06T09:58:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    58,
                    19,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T09:58:19Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    58,
                    19,
                    3,
                    310,
                    0
                ],
                "title": "REMIND: Input Loss Landscapes Reveal Residual Memorization in\n  Post-Unlearning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REMIND: Input Loss Landscapes Reveal Residual Memorization in\n  Post-Unlearning LLMs"
                },
                "summary": "Machine unlearning aims to remove the influence of specific training data\nfrom a model without requiring full retraining. This capability is crucial for\nensuring privacy, safety, and regulatory compliance. Therefore, verifying\nwhether a model has truly forgotten target data is essential for maintaining\nreliability and trustworthiness. However, existing evaluation methods often\nassess forgetting at the level of individual inputs. This approach may overlook\nresidual influence present in semantically similar examples. Such influence can\ncompromise privacy and lead to indirect information leakage. We propose REMIND\n(Residual Memorization In Neighborhood Dynamics), a novel evaluation method\naiming to detect the subtle remaining influence of unlearned data and classify\nwhether the data has been effectively forgotten. REMIND analyzes the model's\nloss over small input variations and reveals patterns unnoticed by single-point\nevaluations. We show that unlearned data yield flatter, less steep loss\nlandscapes, while retained or unrelated data exhibit sharper, more volatile\npatterns. REMIND requires only query-based access, outperforms existing methods\nunder similar constraints, and demonstrates robustness across different models,\ndatasets, and paraphrased inputs, making it practical for real-world\ndeployment. By providing a more sensitive and interpretable measure of\nunlearning effectiveness, REMIND provides a reliable framework to assess\nunlearning in language models. As a result, REMIND offers a novel perspective\non memorization and unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning aims to remove the influence of specific training data\nfrom a model without requiring full retraining. This capability is crucial for\nensuring privacy, safety, and regulatory compliance. Therefore, verifying\nwhether a model has truly forgotten target data is essential for maintaining\nreliability and trustworthiness. However, existing evaluation methods often\nassess forgetting at the level of individual inputs. This approach may overlook\nresidual influence present in semantically similar examples. Such influence can\ncompromise privacy and lead to indirect information leakage. We propose REMIND\n(Residual Memorization In Neighborhood Dynamics), a novel evaluation method\naiming to detect the subtle remaining influence of unlearned data and classify\nwhether the data has been effectively forgotten. REMIND analyzes the model's\nloss over small input variations and reveals patterns unnoticed by single-point\nevaluations. We show that unlearned data yield flatter, less steep loss\nlandscapes, while retained or unrelated data exhibit sharper, more volatile\npatterns. REMIND requires only query-based access, outperforms existing methods\nunder similar constraints, and demonstrates robustness across different models,\ndatasets, and paraphrased inputs, making it practical for real-world\ndeployment. By providing a more sensitive and interpretable measure of\nunlearning effectiveness, REMIND provides a reliable framework to assess\nunlearning in language models. As a result, REMIND offers a novel perspective\non memorization and unlearning."
                },
                "authors": [
                    {
                        "name": "Liran Cohen"
                    },
                    {
                        "name": "Yaniv Nemcovesky"
                    },
                    {
                        "name": "Avi Mendelson"
                    }
                ],
                "author_detail": {
                    "name": "Avi Mendelson"
                },
                "author": "Avi Mendelson",
                "arxiv_comment": "Pre-print version under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26510v2",
                "updated": "2025-11-06T09:42:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    42,
                    34,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-30T14:04:25Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    4,
                    25,
                    3,
                    303,
                    0
                ],
                "title": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection"
                },
                "summary": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization."
                },
                "authors": [
                    {
                        "name": "Youssef Attia El Hili"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Malik Tiomoko"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Corinne Ancourt"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "arxiv_comment": "27 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11916v2",
                "updated": "2025-11-06T09:41:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    41,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-17T09:00:09Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    9,
                    0,
                    9,
                    5,
                    137,
                    0
                ],
                "title": "Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference\n  Architecture"
                },
                "summary": "Existing large language model (LLM) serving systems typically employ\nPrefill-Decode disaggregated architecture to prevent computational interference\nbetween the prefill and decode phases. However, in real-world LLM serving\nscenarios, significant fluctuations in request input/output lengths lead to\nimbalanced computational loads between prefill and decode nodes under\ntraditional static node allocation strategies, consequently preventing\nefficient utilization of computing resources to improve the system's goodput.\nTo address this challenge, we design and implement Arrow, an adaptive scheduler\nthat leverages stateless instances and latency characteristics of prefill and\ndecode tasks to achieve efficient adaptive request and instance scheduling.\nArrow dynamically adjusts the number of instances handling prefill and decode\ntasks based on real-time cluster performance metrics, substantially enhancing\nthe system's capability to handle traffic spikes and load variations. Our\nevaluation under diverse real-world workloads shows that Arrow achieves up to\n$2.55 \\times$ higher request serving rates compared to state-of-the-art\nPrefill-Decode disaggregated serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems typically employ\nPrefill-Decode disaggregated architecture to prevent computational interference\nbetween the prefill and decode phases. However, in real-world LLM serving\nscenarios, significant fluctuations in request input/output lengths lead to\nimbalanced computational loads between prefill and decode nodes under\ntraditional static node allocation strategies, consequently preventing\nefficient utilization of computing resources to improve the system's goodput.\nTo address this challenge, we design and implement Arrow, an adaptive scheduler\nthat leverages stateless instances and latency characteristics of prefill and\ndecode tasks to achieve efficient adaptive request and instance scheduling.\nArrow dynamically adjusts the number of instances handling prefill and decode\ntasks based on real-time cluster performance metrics, substantially enhancing\nthe system's capability to handle traffic spikes and load variations. Our\nevaluation under diverse real-world workloads shows that Arrow achieves up to\n$2.55 \\times$ higher request serving rates compared to state-of-the-art\nPrefill-Decode disaggregated serving systems."
                },
                "authors": [
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26374v2",
                "updated": "2025-11-06T09:27:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    27,
                    20,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-30T11:15:23Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    15,
                    23,
                    3,
                    303,
                    0
                ],
                "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM\n  Reinforcement Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM\n  Reinforcement Finetuning"
                },
                "summary": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce BOTS, a unified framework for Bayesian Online Task Selection in LLM\nreinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively\nmaintains posterior estimates of task difficulty as the model evolves. It\njointly incorporates explicit evidence from direct evaluations of selected\ntasks and implicit evidence inferred from these evaluations for unselected\ntasks, with Thompson sampling ensuring a principled balance between exploration\nand exploitation. To make implicit evidence practical, we instantiate it with\nan ultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce BOTS, a unified framework for Bayesian Online Task Selection in LLM\nreinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively\nmaintains posterior estimates of task difficulty as the model evolves. It\njointly incorporates explicit evidence from direct evaluations of selected\ntasks and implicit evidence inferred from these evaluations for unselected\ntasks, with Thompson sampling ensuring a principled balance between exploration\nand exploitation. To make implicit evidence practical, we instantiate it with\nan ultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT."
                },
                "authors": [
                    {
                        "name": "Qianli Shen"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Yilun Huang"
                    },
                    {
                        "name": "Zhenqing Ling"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04215v1",
                "updated": "2025-11-06T09:24:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    24,
                    49,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T09:24:49Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    24,
                    49,
                    3,
                    310,
                    0
                ],
                "title": "Black-Box Guardrail Reverse-engineering Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box Guardrail Reverse-engineering Attack"
                },
                "summary": "Large language models (LLMs) increasingly employ guardrails to enforce\nethical, legal, and application-specific constraints on their outputs. While\neffective at mitigating harmful responses, these guardrails introduce a new\nclass of vulnerabilities by exposing observable decision patterns. In this\nwork, we present the first study of black-box LLM guardrail reverse-engineering\nattacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement\nlearning-based framework that leverages genetic algorithm-driven data\naugmentation to approximate the decision-making policy of victim guardrails. By\niteratively collecting input-output pairs, prioritizing divergence cases, and\napplying targeted mutations and crossovers, our method incrementally converges\ntoward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on\nthree widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,\nand demonstrate that it achieves an rule matching rate exceeding 0.92 while\nrequiring less than $85 in API costs. These findings underscore the practical\nfeasibility of guardrail extraction and highlight significant security risks\nfor current LLM safety mechanisms. Our findings expose critical vulnerabilities\nin current guardrail designs and highlight the urgent need for more robust\ndefense mechanisms in LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly employ guardrails to enforce\nethical, legal, and application-specific constraints on their outputs. While\neffective at mitigating harmful responses, these guardrails introduce a new\nclass of vulnerabilities by exposing observable decision patterns. In this\nwork, we present the first study of black-box LLM guardrail reverse-engineering\nattacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement\nlearning-based framework that leverages genetic algorithm-driven data\naugmentation to approximate the decision-making policy of victim guardrails. By\niteratively collecting input-output pairs, prioritizing divergence cases, and\napplying targeted mutations and crossovers, our method incrementally converges\ntoward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on\nthree widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,\nand demonstrate that it achieves an rule matching rate exceeding 0.92 while\nrequiring less than $85 in API costs. These findings underscore the practical\nfeasibility of guardrail extraction and highlight significant security risks\nfor current LLM safety mechanisms. Our findings expose critical vulnerabilities\nin current guardrail designs and highlight the urgent need for more robust\ndefense mechanisms in LLM deployment."
                },
                "authors": [
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Yun Xia"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Haoran Shi"
                    },
                    {
                        "name": "Tong Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04214v1",
                "updated": "2025-11-06T09:22:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    22,
                    31,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T09:22:31Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    22,
                    31,
                    3,
                    310,
                    0
                ],
                "title": "Block Rotation is All You Need for MXFP4 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Rotation is All You Need for MXFP4 Quantization"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success, but their\nrapidly growing scale imposes prohibitive costs in memory, computation, and\nenergy. Post-training quantization (PTQ) is a promising solution for efficient\ndeployment, yet achieving accurate W4A4 quantization remains an open challenge.\nWhile most existing methods are designed for INT4 formats, the emergence of\nMXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--\nraises questions about the applicability of current techniques. In this work,\nwe establish a comprehensive benchmark of PTQ methods under the MXFP4 format.\nThrough systematic evaluation, we find that methods like GPTQ consistently\ndeliver strong performance, whereas rotation-based approaches, which are almost\nused by all state-of-the-art approaches, suffer from severe incompatibility\nwith MXFP4. We further provide the first in-depth analysis of this conflict,\ntracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)\nblock scaling and the redistribution of outlier energy via global rotation.\nBuilding on this insight, we propose a simple yet effective block rotation\nstrategy that adapts rotation-based methods to MXFP4, leading to substantial\naccuracy improvements across diverse LLMs. Our findings not only offer clear\nguidance for practitioners but also set a foundation for advancing PTQ research\nunder emerging low-precision formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success, but their\nrapidly growing scale imposes prohibitive costs in memory, computation, and\nenergy. Post-training quantization (PTQ) is a promising solution for efficient\ndeployment, yet achieving accurate W4A4 quantization remains an open challenge.\nWhile most existing methods are designed for INT4 formats, the emergence of\nMXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--\nraises questions about the applicability of current techniques. In this work,\nwe establish a comprehensive benchmark of PTQ methods under the MXFP4 format.\nThrough systematic evaluation, we find that methods like GPTQ consistently\ndeliver strong performance, whereas rotation-based approaches, which are almost\nused by all state-of-the-art approaches, suffer from severe incompatibility\nwith MXFP4. We further provide the first in-depth analysis of this conflict,\ntracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)\nblock scaling and the redistribution of outlier energy via global rotation.\nBuilding on this insight, we propose a simple yet effective block rotation\nstrategy that adapts rotation-based methods to MXFP4, leading to substantial\naccuracy improvements across diverse LLMs. Our findings not only offer clear\nguidance for practitioners but also set a foundation for advancing PTQ research\nunder emerging low-precision formats."
                },
                "authors": [
                    {
                        "name": "Yuantian Shao"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Yuanteng Chen"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Zhihui Wei"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "arxiv_comment": "9 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04213v1",
                "updated": "2025-11-06T09:18:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    18,
                    54,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T09:18:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    18,
                    54,
                    3,
                    310,
                    0
                ],
                "title": "Can we trust LLMs as a tutor for our students? Evaluating the Quality of\n  LLM-generated Feedback in Statistics Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we trust LLMs as a tutor for our students? Evaluating the Quality of\n  LLM-generated Feedback in Statistics Exams"
                },
                "summary": "One of the central challenges for instructors is offering meaningful\nindividual feedback, especially in large courses. Faced with limited time and\nresources, educators are often forced to rely on generalized feedback, even\nwhen more personalized support would be pedagogically valuable. To overcome\nthis limitation, one potential technical solution is to utilize large language\nmodels (LLMs). For an exploratory study using a new platform connected with\nLLMs, we conducted a LLM-corrected mock exam during the \"Introduction to\nStatistics\" lecture at the University of Munich (Germany). The online platform\nallows instructors to upload exercises along with the correct solutions.\nStudents complete these exercises and receive overall feedback on their\nresults, as well as individualized feedback generated by GPT-4 based on the\ncorrect answers provided by the lecturers. The resulting dataset comprised\ntask-level information for all participating students, including individual\nresponses and the corresponding LLM-generated feedback. Our systematic analysis\nrevealed that approximately 7 \\% of the 2,389 feedback instances contained\nerrors, ranging from minor technical inaccuracies to conceptually misleading\nexplanations. Further, using a combined feedback framework approach, we found\nthat the feedback predominantly focused on explaining why an answer was correct\nor incorrect, with fewer instances providing deeper conceptual insights,\nlearning strategies or self-regulatory advice. These findings highlight both\nthe potential and the limitations of deploying LLMs as scalable feedback tools\nin higher education, emphasizing the need for careful quality monitoring and\nprompt design to maximize their pedagogical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the central challenges for instructors is offering meaningful\nindividual feedback, especially in large courses. Faced with limited time and\nresources, educators are often forced to rely on generalized feedback, even\nwhen more personalized support would be pedagogically valuable. To overcome\nthis limitation, one potential technical solution is to utilize large language\nmodels (LLMs). For an exploratory study using a new platform connected with\nLLMs, we conducted a LLM-corrected mock exam during the \"Introduction to\nStatistics\" lecture at the University of Munich (Germany). The online platform\nallows instructors to upload exercises along with the correct solutions.\nStudents complete these exercises and receive overall feedback on their\nresults, as well as individualized feedback generated by GPT-4 based on the\ncorrect answers provided by the lecturers. The resulting dataset comprised\ntask-level information for all participating students, including individual\nresponses and the corresponding LLM-generated feedback. Our systematic analysis\nrevealed that approximately 7 \\% of the 2,389 feedback instances contained\nerrors, ranging from minor technical inaccuracies to conceptually misleading\nexplanations. Further, using a combined feedback framework approach, we found\nthat the feedback predominantly focused on explaining why an answer was correct\nor incorrect, with fewer instances providing deeper conceptual insights,\nlearning strategies or self-regulatory advice. These findings highlight both\nthe potential and the limitations of deploying LLMs as scalable feedback tools\nin higher education, emphasizing the need for careful quality monitoring and\nprompt design to maximize their pedagogical value."
                },
                "authors": [
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Niklas Ippisch"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    }
                ],
                "author_detail": {
                    "name": "Anna-Carolina Haensch"
                },
                "author": "Anna-Carolina Haensch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17923v2",
                "updated": "2025-11-06T09:14:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    14,
                    10,
                    3,
                    310,
                    0
                ],
                "published": "2025-10-20T07:53:51Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    53,
                    51,
                    0,
                    293,
                    0
                ],
                "title": "Rewarding the Journey, Not Just the Destination: A Composite Path and\n  Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding the Journey, Not Just the Destination: A Composite Path and\n  Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing\nLarge Language Models (LLMs), achieving remarkable performance in complex\nreasoning domains such as mathematics and code generation. However, current RL\nmethods face a fundamental scalability bottleneck due to their heavy reliance\non human-curated preference data or labeled datasets for reward modeling. To\novercome this limitation, we explore RL on unlabeled data where models learn\nautonomously from continuous experience streams. The core challenge in this\nsetting lies in reliable reward estimation without ground-truth supervision.\nExisting approaches like Test-Time RL address this through self-consistent\nconsensus, but risk reinforcing incorrect pseudo-labels derived from majority\nvoting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel\ntest-time reward mechanism that operates without external supervision. COMPASS\nintegrates two complementary components: the Dual-Calibration Answer Reward\n(DCAR), which stabilizes training by establishing trustworthy pseudo-labels\nthrough confidence and credibility calibration, and the Decisive Path Reward\n(DPR), which directly optimizes the reasoning process quality beyond mere\noutcome supervision. By jointly reinforcing trustworthy consensus answers and\nhighly decisive reasoning chains, the COMPASS systematically enhances the\nmodel's analytical capabilities. Extensive experiments show that COMPASS\nachieves significant and consistent performance gains across diverse reasoning\ntasks and model architectures, advancing a more scalable direction for LLMs to\nlearn from continuous experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing\nLarge Language Models (LLMs), achieving remarkable performance in complex\nreasoning domains such as mathematics and code generation. However, current RL\nmethods face a fundamental scalability bottleneck due to their heavy reliance\non human-curated preference data or labeled datasets for reward modeling. To\novercome this limitation, we explore RL on unlabeled data where models learn\nautonomously from continuous experience streams. The core challenge in this\nsetting lies in reliable reward estimation without ground-truth supervision.\nExisting approaches like Test-Time RL address this through self-consistent\nconsensus, but risk reinforcing incorrect pseudo-labels derived from majority\nvoting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel\ntest-time reward mechanism that operates without external supervision. COMPASS\nintegrates two complementary components: the Dual-Calibration Answer Reward\n(DCAR), which stabilizes training by establishing trustworthy pseudo-labels\nthrough confidence and credibility calibration, and the Decisive Path Reward\n(DPR), which directly optimizes the reasoning process quality beyond mere\noutcome supervision. By jointly reinforcing trustworthy consensus answers and\nhighly decisive reasoning chains, the COMPASS systematically enhances the\nmodel's analytical capabilities. Extensive experiments show that COMPASS\nachieves significant and consistent performance gains across diverse reasoning\ntasks and model architectures, advancing a more scalable direction for LLMs to\nlearn from continuous experience."
                },
                "authors": [
                    {
                        "name": "Chenwei Tang"
                    },
                    {
                        "name": "Jingyu Xing"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Jiancheng Lv"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Deng Xiong"
                    },
                    {
                        "name": "Ziyue Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Qiao"
                },
                "author": "Ziyue Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04205v1",
                "updated": "2025-11-06T09:11:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    11,
                    20,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T09:11:20Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    11,
                    20,
                    3,
                    310,
                    0
                ],
                "title": "LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for\n  the Member of the Polish National Board of Appeal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for\n  the Member of the Polish National Board of Appeal"
                },
                "summary": "This study provides an empirical assessment of whether current large language\nmodels (LLMs) can pass the official qualifying examination for membership in\nPoland's National Appeal Chamber (Krajowa Izba Odwo{\\l}awcza). The authors\nexamine two related ideas: using LLM as actual exam candidates and applying the\n'LLM-as-a-judge' approach, in which model-generated answers are automatically\nevaluated by other models. The paper describes the structure of the exam, which\nincludes a multiple-choice knowledge test on public procurement law and a\nwritten judgment, and presents the hybrid information recovery and extraction\npipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4\nSonnet and Bielik-11B-v2.6) were tested in closed-book and various\nRetrieval-Augmented Generation settings. The results show that although the\nmodels achieved satisfactory scores in the knowledge test, none met the passing\nthreshold in the practical written part, and the evaluations of the\n'LLM-as-a-judge' often diverged from the judgments of the official examining\ncommittee. The authors highlight key limitations: susceptibility to\nhallucinations, incorrect citation of legal provisions, weaknesses in logical\nargumentation, and the need for close collaboration between legal experts and\ntechnical teams. The findings indicate that, despite rapid technological\nprogress, current LLMs cannot yet replace human judges or independent examiners\nin Polish public procurement adjudication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides an empirical assessment of whether current large language\nmodels (LLMs) can pass the official qualifying examination for membership in\nPoland's National Appeal Chamber (Krajowa Izba Odwo{\\l}awcza). The authors\nexamine two related ideas: using LLM as actual exam candidates and applying the\n'LLM-as-a-judge' approach, in which model-generated answers are automatically\nevaluated by other models. The paper describes the structure of the exam, which\nincludes a multiple-choice knowledge test on public procurement law and a\nwritten judgment, and presents the hybrid information recovery and extraction\npipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4\nSonnet and Bielik-11B-v2.6) were tested in closed-book and various\nRetrieval-Augmented Generation settings. The results show that although the\nmodels achieved satisfactory scores in the knowledge test, none met the passing\nthreshold in the practical written part, and the evaluations of the\n'LLM-as-a-judge' often diverged from the judgments of the official examining\ncommittee. The authors highlight key limitations: susceptibility to\nhallucinations, incorrect citation of legal provisions, weaknesses in logical\nargumentation, and the need for close collaboration between legal experts and\ntechnical teams. The findings indicate that, despite rapid technological\nprogress, current LLMs cannot yet replace human judges or independent examiners\nin Polish public procurement adjudication."
                },
                "authors": [
                    {
                        "name": "Michał Karp"
                    },
                    {
                        "name": "Anna Kubaszewska"
                    },
                    {
                        "name": "Magdalena Król"
                    },
                    {
                        "name": "Robert Król"
                    },
                    {
                        "name": "Aleksander Smywiński-Pohl"
                    },
                    {
                        "name": "Mateusz Szymański"
                    },
                    {
                        "name": "Witold Wydmański"
                    }
                ],
                "author_detail": {
                    "name": "Witold Wydmański"
                },
                "author": "Witold Wydmański",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24630v2",
                "updated": "2025-11-06T09:04:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    9,
                    4,
                    26,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-30T14:23:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    23,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement\n  Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement\n  Learning for Large Reasoning Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "arxiv_comment": "accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04195v1",
                "updated": "2025-11-06T08:56:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    8,
                    56,
                    37,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T08:56:37Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    8,
                    56,
                    37,
                    3,
                    310,
                    0
                ],
                "title": "Computational Turing Test Reveals Systematic Differences Between Human\n  and AI Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Turing Test Reveals Systematic Differences Between Human\n  and AI Language"
                },
                "summary": "Large language models (LLMs) are increasingly used in the social sciences to\nsimulate human behavior, based on the assumption that they can generate\nrealistic, human-like text. Yet this assumption remains largely untested.\nExisting validation efforts rely heavily on human-judgment-based evaluations --\ntesting whether humans can distinguish AI from human output -- despite evidence\nthat such judgments are blunt and unreliable. As a result, the field lacks\nrobust tools for assessing the realism of LLM-generated text or for calibrating\nmodels to real-world data. This paper makes two contributions. First, we\nintroduce a computational Turing test: a validation framework that integrates\naggregate metrics (BERT-based detectability and semantic similarity) with\ninterpretable linguistic features (stylistic markers and topical patterns) to\nassess how closely LLMs approximate human language within a given dataset.\nSecond, we systematically compare nine open-weight LLMs across five calibration\nstrategies -- including fine-tuning, stylistic prompting, and context retrieval\n-- benchmarking their ability to reproduce user interactions on X (formerly\nTwitter), Bluesky, and Reddit. Our findings challenge core assumptions in the\nliterature. Even after calibration, LLM outputs remain clearly distinguishable\nfrom human text, particularly in affective tone and emotional expression.\nInstruction-tuned models underperform their base counterparts, and scaling up\nmodel size does not enhance human-likeness. Crucially, we identify a trade-off:\noptimizing for human-likeness often comes at the cost of semantic fidelity, and\nvice versa. These results provide a much-needed scalable framework for\nvalidation and calibration in LLM simulations -- and offer a cautionary note\nabout their current limitations in capturing human communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in the social sciences to\nsimulate human behavior, based on the assumption that they can generate\nrealistic, human-like text. Yet this assumption remains largely untested.\nExisting validation efforts rely heavily on human-judgment-based evaluations --\ntesting whether humans can distinguish AI from human output -- despite evidence\nthat such judgments are blunt and unreliable. As a result, the field lacks\nrobust tools for assessing the realism of LLM-generated text or for calibrating\nmodels to real-world data. This paper makes two contributions. First, we\nintroduce a computational Turing test: a validation framework that integrates\naggregate metrics (BERT-based detectability and semantic similarity) with\ninterpretable linguistic features (stylistic markers and topical patterns) to\nassess how closely LLMs approximate human language within a given dataset.\nSecond, we systematically compare nine open-weight LLMs across five calibration\nstrategies -- including fine-tuning, stylistic prompting, and context retrieval\n-- benchmarking their ability to reproduce user interactions on X (formerly\nTwitter), Bluesky, and Reddit. Our findings challenge core assumptions in the\nliterature. Even after calibration, LLM outputs remain clearly distinguishable\nfrom human text, particularly in affective tone and emotional expression.\nInstruction-tuned models underperform their base counterparts, and scaling up\nmodel size does not enhance human-likeness. Crucially, we identify a trade-off:\noptimizing for human-likeness often comes at the cost of semantic fidelity, and\nvice versa. These results provide a much-needed scalable framework for\nvalidation and calibration in LLM simulations -- and offer a cautionary note\nabout their current limitations in capturing human communication."
                },
                "authors": [
                    {
                        "name": "Nicolò Pagan"
                    },
                    {
                        "name": "Petter Törnberg"
                    },
                    {
                        "name": "Christopher A. Bail"
                    },
                    {
                        "name": "Anikó Hannák"
                    },
                    {
                        "name": "Christopher Barrie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Barrie"
                },
                "author": "Christopher Barrie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]