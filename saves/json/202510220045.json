[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v1",
                "updated": "2025-10-19T12:17:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15590v1",
                "updated": "2025-10-17T12:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T12:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "title": "A single optically detectable tumbling spin in silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single optically detectable tumbling spin in silicon"
                },
                "summary": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial."
                },
                "authors": [
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Frédéric Milési"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Isabelle Robert-Philip"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    },
                    {
                        "name": "Guillaume Cassabois"
                    },
                    {
                        "name": "Vincent Jacques"
                    },
                    {
                        "name": "Anaïs Dréau"
                    }
                ],
                "author_detail": {
                    "name": "Anaïs Dréau"
                },
                "author": "Anaïs Dréau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15413v1",
                "updated": "2025-10-17T08:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T08:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL: Fully Homomorphic Encrypted SQL Database"
                },
                "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."
                },
                "authors": [
                    {
                        "name": "Po-Yu Tseng"
                    },
                    {
                        "name": "Po-Chu Hsu"
                    },
                    {
                        "name": "Shih-Wei Liao"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Wei Liao"
                },
                "author": "Shih-Wei Liao",
                "arxiv_comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v4",
                "updated": "2025-10-17T06:54:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    54,
                    10,
                    4,
                    290,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "NeurIPS 2025 Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v2",
                "updated": "2025-10-17T06:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    45,
                    17,
                    4,
                    290,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Pre-trained Large Language Models"
                },
                "summary": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "A version accepted by TACL, prior to its publication by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15152v1",
                "updated": "2025-10-16T21:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T21:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "Tail-Optimized Caching for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-Optimized Caching for LLM Inference"
                },
                "summary": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15095v1",
                "updated": "2025-10-16T19:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T19:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs"
                },
                "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing."
                },
                "authors": [
                    {
                        "name": "Md Sabbir Hossain Polak"
                    },
                    {
                        "name": "David Troendle"
                    },
                    {
                        "name": "Byunghyun Jang"
                    }
                ],
                "author_detail": {
                    "name": "Byunghyun Jang"
                },
                "author": "Byunghyun Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14622v1",
                "updated": "2025-10-16T12:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:32:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems"
                },
                "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Hyein Woo"
                    },
                    {
                        "name": "Junhee Kim"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Kyungkuk Nam"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Hanyeoreum Bae"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14531v1",
                "updated": "2025-10-16T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "title": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation"
                },
                "summary": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona."
                },
                "authors": [
                    {
                        "name": "Sebastian Onder"
                    },
                    {
                        "name": "Philipp Gaggl"
                    },
                    {
                        "name": "Jürgen Burin"
                    },
                    {
                        "name": "Andreas Gsponer"
                    },
                    {
                        "name": "Matthias Knopf"
                    },
                    {
                        "name": "Simon Waid"
                    },
                    {
                        "name": "Neil Moffat"
                    },
                    {
                        "name": "Giulio Pellegrini"
                    },
                    {
                        "name": "Thomas Bergauer"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bergauer"
                },
                "author": "Thomas Bergauer",
                "arxiv_doi": "10.1016/j.nima.2025.170740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16040v1",
                "updated": "2025-10-16T07:12:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T07:12:08Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing"
                },
                "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."
                },
                "authors": [
                    {
                        "name": "Tianhua Xia"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14126v1",
                "updated": "2025-10-15T21:49:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T21:49:38Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving"
                },
                "summary": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\""
                },
                "authors": [
                    {
                        "name": "Nikos Pagonas"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Kostis Kaffes"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13940v1",
                "updated": "2025-10-15T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention"
                },
                "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "José Cano"
                    }
                ],
                "author_detail": {
                    "name": "José Cano"
                },
                "author": "José Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13334v1",
                "updated": "2025-10-15T09:18:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:18:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Fragility of KV Cache Eviction in LLM Inference"
                },
                "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Haoyu Guo"
                    },
                    {
                        "name": "JunLin Lv"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Xike Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xike Xie"
                },
                "author": "Xike Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13279v1",
                "updated": "2025-10-15T08:25:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T08:25:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time"
                },
                "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets."
                },
                "authors": [
                    {
                        "name": "Fuma Omori"
                    },
                    {
                        "name": "Atsushi Yano"
                    },
                    {
                        "name": "Takuya Azumi"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Azumi"
                },
                "author": "Takuya Azumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13223v1",
                "updated": "2025-10-15T07:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T07:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13084v1",
                "updated": "2025-10-15T01:55:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T01:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation"
                },
                "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Yi Zuo"
                    },
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12889v1",
                "updated": "2025-10-14T18:04:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:04:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters"
                },
                "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "single column,20 pages and 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v1",
                "updated": "2025-10-14T18:00:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v2",
                "updated": "2025-10-13T20:40:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    20,
                    40,
                    32,
                    0,
                    286,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v2",
                "updated": "2025-10-13T10:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    39,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15980v1",
                "updated": "2025-10-13T09:04:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    4,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T09:04:19Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    4,
                    19,
                    0,
                    286,
                    0
                ],
                "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model\n  Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model\n  Cognition"
                },
                "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxuan Yu"
                },
                "author": "Yanxuan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13860v1",
                "updated": "2025-10-13T04:04:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T04:04:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing"
                },
                "summary": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint."
                },
                "authors": [
                    {
                        "name": "Shivanshu Kumar"
                    },
                    {
                        "name": "Gopalakrishnan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Gopalakrishnan Srinivasan"
                },
                "author": "Gopalakrishnan Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v2",
                "updated": "2025-10-10T16:56:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    56,
                    23,
                    4,
                    283,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_doi": "10.1145/3769780",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769780",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v2",
                "updated": "2025-10-10T16:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    8,
                    26,
                    4,
                    283,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Kühn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Kühn"
                },
                "author": "Martin J. Kühn",
                "arxiv_comment": "29 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09477v1",
                "updated": "2025-10-10T15:32:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T15:32:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Autoregressive Inference for Transformer Probabilistic Models"
                },
                "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."
                },
                "authors": [
                    {
                        "name": "Conor Hassan"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09409v1",
                "updated": "2025-10-10T14:03:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T14:03:42Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "title": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network"
                },
                "summary": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC."
                },
                "authors": [
                    {
                        "name": "Chongxiao Cai"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Min Sheng"
                    },
                    {
                        "name": "Jiandong Li"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Di Zhou"
                    },
                    {
                        "name": "Ziwen Xie"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v2",
                "updated": "2025-10-10T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    15,
                    40,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "Cédrick Austa"
                    },
                    {
                        "name": "Jan Tobias Mühlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v2",
                "updated": "2025-10-10T13:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    8,
                    39,
                    4,
                    283,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The paper is currently under investigation regarding concerns of\n  potential academic misconduct. While the investigation is ongoing, the\n  authors have voluntarily requested to withdraw the manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09309v1",
                "updated": "2025-10-10T12:01:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T12:01:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference"
                },
                "summary": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV"
                },
                "authors": [
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09182v1",
                "updated": "2025-10-10T09:24:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T09:24:53Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption"
                },
                "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware."
                },
                "authors": [
                    {
                        "name": "Johann-Friedrich Feiden"
                    },
                    {
                        "name": "Tim Küchler"
                    },
                    {
                        "name": "Denis Zavadski"
                    },
                    {
                        "name": "Bogdan Savchynskyy"
                    },
                    {
                        "name": "Carsten Rother"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rother"
                },
                "author": "Carsten Rother",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09154v1",
                "updated": "2025-10-10T08:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T08:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications"
                },
                "summary": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications."
                },
                "authors": [
                    {
                        "name": "Tanjim Rahman"
                    },
                    {
                        "name": "Trupti Ranjan Lenka"
                    }
                ],
                "author_detail": {
                    "name": "Trupti Ranjan Lenka"
                },
                "author": "Trupti Ranjan Lenka",
                "arxiv_comment": "13 pages, 13 figures including DC, RF, and breakdown analysis of\n  field-plated AlGaN/GaN HEMT using TCAD simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v3",
                "updated": "2025-10-09T20:37:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    37,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08803v1",
                "updated": "2025-10-09T20:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T20:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Man-Made Heuristics Are Dead. Long Live Code Generators!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Man-Made Heuristics Are Dead. Long Live Code Generators!"
                },
                "summary": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    },
                    {
                        "name": "Daehyeok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daehyeok Kim"
                },
                "author": "Daehyeok Kim",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. To be presented at HotNets 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08774v1",
                "updated": "2025-10-09T19:45:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T19:45:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings"
                },
                "summary": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models."
                },
                "authors": [
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08669v1",
                "updated": "2025-10-09T17:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching"
                },
                "summary": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Deyang Kong"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Yupei Pan"
                    },
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08351v1",
                "updated": "2025-10-09T15:38:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "FMCache: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMCache: File-System Metadata Caching in Programmable Switches"
                },
                "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "arxiv_affiliation": "The Chinese University of Hong Kong",
                "author": "Patrick P. C. Lee",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08180v1",
                "updated": "2025-10-09T13:06:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:06:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Energy-Efficient Serverless Computing with Hardware Isolation"
                },
                "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW."
                },
                "authors": [
                    {
                        "name": "Natalie Carl"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v4",
                "updated": "2025-10-09T12:05:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    5,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.17803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17803v1",
                "updated": "2025-10-20T17:59:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    59,
                    52,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:59:52Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    59,
                    52,
                    0,
                    293,
                    0
                ],
                "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing"
                },
                "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control."
                },
                "authors": [
                    {
                        "name": "Zixin Yin"
                    },
                    {
                        "name": "Ling-Hao Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Xili Dai"
                    }
                ],
                "author_detail": {
                    "name": "Xili Dai"
                },
                "author": "Xili Dai",
                "arxiv_comment": "SIGGRAPH Asia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17802v1",
                "updated": "2025-10-20T17:59:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    59,
                    25,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:59:25Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    59,
                    25,
                    0,
                    293,
                    0
                ],
                "title": "Unbiased Gradient Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased Gradient Low-Rank Projection"
                },
                "summary": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yuxing Liu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17800v1",
                "updated": "2025-10-20T17:58:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    58,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:58:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    58,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glyph: Scaling Context Windows via Visual-Text Compression"
                },
                "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yusen Liu"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Yulin Fei"
                    },
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17795v1",
                "updated": "2025-10-20T17:53:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    53,
                    23,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:53:23Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    53,
                    23,
                    0,
                    293,
                    0
                ],
                "title": "Executable Knowledge Graphs for Replicating AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executable Knowledge Graphs for Replicating AI Research"
                },
                "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG."
                },
                "authors": [
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Zhuoyun Yu"
                    },
                    {
                        "name": "Xuehai Wang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17793v1",
                "updated": "2025-10-20T17:52:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    52,
                    6,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:52:06Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    52,
                    6,
                    0,
                    293,
                    0
                ],
                "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative\n  Evaluator Training for Reasoning-Centric Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Automatic Evaluators: Scaling Multi-Task Generative\n  Evaluator Training for Reasoning-Centric Domains"
                },
                "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality."
                },
                "authors": [
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "29 pages, 9 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07578v2",
                "updated": "2025-10-20T17:51:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    51,
                    25,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-09T09:23:09Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    23,
                    9,
                    0,
                    160,
                    0
                ],
                "title": "Denoising the Future: Top-p Distributions for Moving Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising the Future: Top-p Distributions for Moving Through Time"
                },
                "summary": "Inference in dynamic probabilistic models is a complex task involving\nexpensive operations. In particular, for Hidden Markov Models, the whole state\nspace has to be enumerated for advancing in time. Even states with negligible\nprobabilities are considered, resulting in computational inefficiency and\nincreased noise due to the propagation of unlikely probability mass. We propose\nto denoise the future and speed up inference by using only the top-p states,\ni.e., the most probable states with accumulated probability p. We show that the\nerror introduced by using only the top-p states is bound by p and the so-called\nminimal mixing rate of the underlying model. Moreover, in our empirical\nevaluation, we show that we can expect speedups of at least an order of\nmagnitude, while the error in terms of total variation distance is below 0.09.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in dynamic probabilistic models is a complex task involving\nexpensive operations. In particular, for Hidden Markov Models, the whole state\nspace has to be enumerated for advancing in time. Even states with negligible\nprobabilities are considered, resulting in computational inefficiency and\nincreased noise due to the propagation of unlikely probability mass. We propose\nto denoise the future and speed up inference by using only the top-p states,\ni.e., the most probable states with accumulated probability p. We show that the\nerror introduced by using only the top-p states is bound by p and the so-called\nminimal mixing rate of the underlying model. Moreover, in our empirical\nevaluation, we show that we can expect speedups of at least an order of\nmagnitude, while the error in terms of total variation distance is below 0.09."
                },
                "authors": [
                    {
                        "name": "Florian Andreas Marwitz"
                    },
                    {
                        "name": "Ralf Möller"
                    },
                    {
                        "name": "Magnus Bender"
                    },
                    {
                        "name": "Marcel Gehrke"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Gehrke"
                },
                "author": "Marcel Gehrke",
                "arxiv_comment": "Accepted at ECSQARU 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10815v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10815v3",
                "updated": "2025-10-20T17:46:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    46,
                    57,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-12T21:42:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    21,
                    42,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems"
                },
                "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities."
                },
                "authors": [
                    {
                        "name": "Meiru Zhang"
                    },
                    {
                        "name": "Philipp Borchert"
                    },
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    }
                ],
                "author_detail": {
                    "name": "Gerasimos Lampouras"
                },
                "author": "Gerasimos Lampouras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10815v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10815v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17787v1",
                "updated": "2025-10-20T17:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    46,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:46:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    46,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "Comprehensive analysis of time-domain overlapping gravitational wave\n  transients: A Lensing Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive analysis of time-domain overlapping gravitational wave\n  transients: A Lensing Study"
                },
                "summary": "Next-generation GW detectors will produce a high rate of temporally\noverlapping signals from unrelated compact binary coalescences. Such overlaps\ncan bias parameter estimation (PE) and mimic signatures of other physical\neffects, such as gravitational lensing. In this work, we investigate how\noverlapping signals can be degenerate with gravitational lensing by focusing on\ntwo scenarios: Type-II strong lensing and microlensing by an isolated\npoint-mass lens. We simulate quasicircular binary black-hole pairs with\nchirp-mass ratios $\\mathscr{M}_{\\rm B}/\\mathscr{M}_{\\rm A}\\in\\{0.5,\\,1,\\,2\\}$,\nSNR ratios $\\mathrm{SNR}_{\\rm B}/\\mathrm{SNR}_{\\rm A}\\in\\{0.5,\\,1\\}$, and\ncoalescence-time offsets $\\Delta t_{\\rm c}\\in[-0.1,\\,0.1]~\\mathrm{s}$. Bayesian\nPE and fitting-factor studies show that the Type-II lensing hypothesis is\nfavored over the unlensed quasicircular hypothesis ($\\log_{10}\\mathscr{B}^{\\rm\nL}_{\\rm U}>1$) only in a small region of the overlapping parameter space with\n$\\mathscr{M}_{\\rm B}/\\mathscr{M}_{\\rm A}\\gtrsim1$ and $|\\Delta t_{\\rm\nc}|\\leq0.03~\\rm{s}$.. Meanwhile, false evidence for microlensing signatures can\narise because, to a reasonable approximation, the model produces two\nsuperimposed images whose time delay can closely match $|\\Delta t_{\\rm c}|$.\nOverall, the inferred Bayes factor depends on relative chirp-mass ratios,\nrelative loudness, difference in coalescence times, and also the absolute SNRs\nof the overlapping signals. Cumulatively, our results indicate that overlapping\nblack-hole binaries with nearly equal chirp masses and comparable loudness are\nlikely to be falsely identified as lensed. Such misidentifications are expected\nto become more common as detector sensitivities improve. While our study\nfocuses on ground-based detectors using appropriate detectability thresholds,\nthe findings naturally extend to next-generation GW observatories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation GW detectors will produce a high rate of temporally\noverlapping signals from unrelated compact binary coalescences. Such overlaps\ncan bias parameter estimation (PE) and mimic signatures of other physical\neffects, such as gravitational lensing. In this work, we investigate how\noverlapping signals can be degenerate with gravitational lensing by focusing on\ntwo scenarios: Type-II strong lensing and microlensing by an isolated\npoint-mass lens. We simulate quasicircular binary black-hole pairs with\nchirp-mass ratios $\\mathscr{M}_{\\rm B}/\\mathscr{M}_{\\rm A}\\in\\{0.5,\\,1,\\,2\\}$,\nSNR ratios $\\mathrm{SNR}_{\\rm B}/\\mathrm{SNR}_{\\rm A}\\in\\{0.5,\\,1\\}$, and\ncoalescence-time offsets $\\Delta t_{\\rm c}\\in[-0.1,\\,0.1]~\\mathrm{s}$. Bayesian\nPE and fitting-factor studies show that the Type-II lensing hypothesis is\nfavored over the unlensed quasicircular hypothesis ($\\log_{10}\\mathscr{B}^{\\rm\nL}_{\\rm U}>1$) only in a small region of the overlapping parameter space with\n$\\mathscr{M}_{\\rm B}/\\mathscr{M}_{\\rm A}\\gtrsim1$ and $|\\Delta t_{\\rm\nc}|\\leq0.03~\\rm{s}$.. Meanwhile, false evidence for microlensing signatures can\narise because, to a reasonable approximation, the model produces two\nsuperimposed images whose time delay can closely match $|\\Delta t_{\\rm c}|$.\nOverall, the inferred Bayes factor depends on relative chirp-mass ratios,\nrelative loudness, difference in coalescence times, and also the absolute SNRs\nof the overlapping signals. Cumulatively, our results indicate that overlapping\nblack-hole binaries with nearly equal chirp masses and comparable loudness are\nlikely to be falsely identified as lensed. Such misidentifications are expected\nto become more common as detector sensitivities improve. While our study\nfocuses on ground-based detectors using appropriate detectability thresholds,\nthe findings naturally extend to next-generation GW observatories."
                },
                "authors": [
                    {
                        "name": "Nishkal Rao"
                    },
                    {
                        "name": "Anuj Mishra"
                    },
                    {
                        "name": "Apratim Ganguly"
                    },
                    {
                        "name": "Anupreeta More"
                    }
                ],
                "author_detail": {
                    "name": "Anupreeta More"
                },
                "author": "Anupreeta More",
                "arxiv_comment": "14 pages, 11 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02058v3",
                "updated": "2025-10-20T17:45:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    45,
                    23,
                    0,
                    293,
                    0
                ],
                "published": "2024-11-04T13:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    1,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning: A Linear Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning: A Linear Approach"
                },
                "summary": "A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimension $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength. By\nestimating the intrinsic dimension $m^{\\ast}$ using multiple methods\n(participation ratio, Kaiser rule, and the Kneedle algorithm), it is found that\n$m^{\\ast}$ increases with the model nonlinearity. Interestingly, in the weakly\nnonlinear regime, for trajectories initialized by exciting the first mode, the\nparticipation ratio estimates $m^{\\ast} = 2, 3$, strongly suggesting that\nquasi-periodic motion on a low-dimensional Riemannian manifold underlies the\ncharacteristic energy recurrences observed in the FPUT model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimension $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength. By\nestimating the intrinsic dimension $m^{\\ast}$ using multiple methods\n(participation ratio, Kaiser rule, and the Kneedle algorithm), it is found that\n$m^{\\ast}$ increases with the model nonlinearity. Interestingly, in the weakly\nnonlinear regime, for trajectories initialized by exciting the first mode, the\nparticipation ratio estimates $m^{\\ast} = 2, 3$, strongly suggesting that\nquasi-periodic motion on a low-dimensional Riemannian manifold underlies the\ncharacteristic energy recurrences observed in the FPUT model."
                },
                "authors": [
                    {
                        "name": "Gionni Marchetti"
                    }
                ],
                "author_detail": {
                    "name": "Gionni Marchetti"
                },
                "author": "Gionni Marchetti",
                "arxiv_doi": "10.1063/5.0293702",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0293702",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 15 figures. This version matches the article published in\n  Chaos 35, 103118 (2025)",
                "arxiv_journal_ref": "Chaos 35, 103118 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17786v1",
                "updated": "2025-10-20T17:44:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    44,
                    17,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:44:17Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    44,
                    17,
                    0,
                    293,
                    0
                ],
                "title": "Inference-Time Compute Scaling For Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Compute Scaling For Flow Matching"
                },
                "summary": "Allocating extra computation at inference time has recently improved sample\nquality in large language models and diffusion-based image generation. In\nparallel, Flow Matching (FM) has gained traction in language, vision, and\nscientific domains, but inference-time scaling methods for it remain\nunder-explored. Concurrently, Kim et al., 2025 approach this problem but\nreplace the linear interpolant with a non-linear variance-preserving (VP)\ninterpolant at inference, sacrificing FM's efficient and straight sampling.\nAdditionally, inference-time compute scaling for flow matching has only been\napplied to visual tasks, like image generation. We introduce novel\ninference-time scaling procedures for FM that preserve the linear interpolant\nduring sampling. Evaluations of our method on image generation, and for the\nfirst time (to the best of our knowledge), unconditional protein generation,\nshow that I) sample quality consistently improves as inference compute\nincreases, and II) flow matching inference-time scaling can be applied to\nscientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Allocating extra computation at inference time has recently improved sample\nquality in large language models and diffusion-based image generation. In\nparallel, Flow Matching (FM) has gained traction in language, vision, and\nscientific domains, but inference-time scaling methods for it remain\nunder-explored. Concurrently, Kim et al., 2025 approach this problem but\nreplace the linear interpolant with a non-linear variance-preserving (VP)\ninterpolant at inference, sacrificing FM's efficient and straight sampling.\nAdditionally, inference-time compute scaling for flow matching has only been\napplied to visual tasks, like image generation. We introduce novel\ninference-time scaling procedures for FM that preserve the linear interpolant\nduring sampling. Evaluations of our method on image generation, and for the\nfirst time (to the best of our knowledge), unconditional protein generation,\nshow that I) sample quality consistently improves as inference compute\nincreases, and II) flow matching inference-time scaling can be applied to\nscientific domains."
                },
                "authors": [
                    {
                        "name": "Adam Stecklov"
                    },
                    {
                        "name": "Noah El Rimawi-Fine"
                    },
                    {
                        "name": "Mathieu Blanchette"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Blanchette"
                },
                "author": "Mathieu Blanchette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17774v1",
                "updated": "2025-10-20T17:34:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    34,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:34:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    34,
                    40,
                    0,
                    293,
                    0
                ],
                "title": "A High-Resolution Spectroscopic Survey of Directly Imaged Companion\n  Hosts: II. Diversity in C/O Ratios among Host Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A High-Resolution Spectroscopic Survey of Directly Imaged Companion\n  Hosts: II. Diversity in C/O Ratios among Host Stars"
                },
                "summary": "The era of JWST has enabled measurements of abundances of elements such as C,\nO, and even Na, S, K, and Fe in planetary atmospheres to very high precisions\n($\\sim$0.1 dex). Accurate inference of planet formation using these elemental\nabundances require the corresponding abundance measurements for the host star.\nWe present the second set of results from our high-resolution spectroscopic\nsurvey of directly imaged companion host stars, measuring abundances of 16\nelements (including C, O, Na, Mg, Si, S, K and Fe) for five directly imaged\ncompanion host stars. Using both the spectral fitting and the equivalent width\nmethods, we find solar C/O ratios for HR 2562 (0.58 $\\pm$ 0.09), AB Pic (0.50\n$\\pm$ 0.14), and YSES 1 (0.45 $\\pm$ 0.05), and sub-solar C/O ratios for PZ Tel\n(0.28 $\\pm$ 0.05) and $\\beta$ Pic (0.22 $\\pm$ 0.06). The $4\\sigma$ sub-solar\nC/O detections for PZ Tel and $\\beta$ Pic highlight the importance of accurate\nstellar C/O estimates for constraining planet formation. Subsequently, we\ncombine our abundances with those from our previous work to measure\npopulation-level average elemental abundances. We find super-solar carbon and\noxygen for this stellar population, indicating that the protoplanetary disks\naround these stars were potentially rich in volatiles. We compare stellar C/O\nto those of their companions, revealing super-stellar C/O for several objects\nthat suggest planet-like formation mechanisms. We also compare the C/O of our\ndirectly imaged companion host star population with other planet host stars\nusing the Kolmogorov-Smirnov Test, which indicates insufficient evidence to\ndifferentiate between the various stellar populations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of JWST has enabled measurements of abundances of elements such as C,\nO, and even Na, S, K, and Fe in planetary atmospheres to very high precisions\n($\\sim$0.1 dex). Accurate inference of planet formation using these elemental\nabundances require the corresponding abundance measurements for the host star.\nWe present the second set of results from our high-resolution spectroscopic\nsurvey of directly imaged companion host stars, measuring abundances of 16\nelements (including C, O, Na, Mg, Si, S, K and Fe) for five directly imaged\ncompanion host stars. Using both the spectral fitting and the equivalent width\nmethods, we find solar C/O ratios for HR 2562 (0.58 $\\pm$ 0.09), AB Pic (0.50\n$\\pm$ 0.14), and YSES 1 (0.45 $\\pm$ 0.05), and sub-solar C/O ratios for PZ Tel\n(0.28 $\\pm$ 0.05) and $\\beta$ Pic (0.22 $\\pm$ 0.06). The $4\\sigma$ sub-solar\nC/O detections for PZ Tel and $\\beta$ Pic highlight the importance of accurate\nstellar C/O estimates for constraining planet formation. Subsequently, we\ncombine our abundances with those from our previous work to measure\npopulation-level average elemental abundances. We find super-solar carbon and\noxygen for this stellar population, indicating that the protoplanetary disks\naround these stars were potentially rich in volatiles. We compare stellar C/O\nto those of their companions, revealing super-stellar C/O for several objects\nthat suggest planet-like formation mechanisms. We also compare the C/O of our\ndirectly imaged companion host star population with other planet host stars\nusing the Kolmogorov-Smirnov Test, which indicates insufficient evidence to\ndifferentiate between the various stellar populations"
                },
                "authors": [
                    {
                        "name": "Aneesh Baburaj"
                    },
                    {
                        "name": "Quinn M. Konopacky"
                    },
                    {
                        "name": "Christopher A. Theissen"
                    },
                    {
                        "name": "Roman Gerasimov"
                    },
                    {
                        "name": "Kielan K. W. Hoch"
                    }
                ],
                "author_detail": {
                    "name": "Kielan K. W. Hoch"
                },
                "author": "Kielan K. W. Hoch",
                "arxiv_comment": "37 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17771v1",
                "updated": "2025-10-20T17:31:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    31,
                    9,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:31:09Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    31,
                    9,
                    0,
                    293,
                    0
                ],
                "title": "Seeing but Not Believing: Probing the Disconnect Between Visual\n  Attention and Answer Correctness in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing but Not Believing: Probing the Disconnect Between Visual\n  Attention and Answer Correctness in VLMs"
                },
                "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs."
                },
                "authors": [
                    {
                        "name": "Zhining Liu"
                    },
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Joy Zeng"
                    },
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Benoit Dumoulin"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "21 pages, 10 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17764v1",
                "updated": "2025-10-20T17:22:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    22,
                    32,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:22:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    22,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from\n  Benchmarks to Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from\n  Benchmarks to Applications"
                },
                "summary": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use."
                },
                "authors": [
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Weiyu Chen"
                    },
                    {
                        "name": "Shijie Lu"
                    },
                    {
                        "name": "Yuxi Huang"
                    },
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Phu Tran"
                    },
                    {
                        "name": "Ji-Eun Irene Yum"
                    },
                    {
                        "name": "Muhammad Ali Khan"
                    },
                    {
                        "name": "Muhammad Umar Afzal"
                    },
                    {
                        "name": "Irbaz Bin Riaz"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "author": "Ben Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18196v3",
                "updated": "2025-10-20T17:16:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    16,
                    38,
                    0,
                    293,
                    0
                ],
                "published": "2024-12-24T06:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by\n  Pseudo Gradient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by\n  Pseudo Gradient"
                },
                "summary": "While automatic prompt generation methods have recently received significant\nattention, their robustness remains poorly understood. In this paper, we\nintroduce PertBench, a comprehensive benchmark dataset that includes a wide\nrange of input perturbations, designed to systematically evaluate the\nrobustness of current auto-prompting techniques. Our analysis reveals\nsubstantial vulnerabilities in existing prompt generation strategies, where\neven minor modifications to the prompt can lead to significant differences in\nmodel output. To address this issue, we propose PGO, a gradient-free prompt\ngeneration framework that leverages perturbation types as pseudo-gradient\nsignals to guide LLMs in producing more robust prompts. In contrast to existing\nmethods that assess prompt quality only on clean, well-structured inputs, our\napproach explicitly emphasizes robustness under noisy and perturbed conditions.\nExtensive experiments across diverse tasks and multiple LLMs show PGO\nconsistently outperforms previous methods in maintaining performance under\ninput perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automatic prompt generation methods have recently received significant\nattention, their robustness remains poorly understood. In this paper, we\nintroduce PertBench, a comprehensive benchmark dataset that includes a wide\nrange of input perturbations, designed to systematically evaluate the\nrobustness of current auto-prompting techniques. Our analysis reveals\nsubstantial vulnerabilities in existing prompt generation strategies, where\neven minor modifications to the prompt can lead to significant differences in\nmodel output. To address this issue, we propose PGO, a gradient-free prompt\ngeneration framework that leverages perturbation types as pseudo-gradient\nsignals to guide LLMs in producing more robust prompts. In contrast to existing\nmethods that assess prompt quality only on clean, well-structured inputs, our\napproach explicitly emphasizes robustness under noisy and perturbed conditions.\nExtensive experiments across diverse tasks and multiple LLMs show PGO\nconsistently outperforms previous methods in maintaining performance under\ninput perturbations."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17761v1",
                "updated": "2025-10-20T17:13:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    13,
                    11,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:13:11Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    13,
                    11,
                    0,
                    293,
                    0
                ],
                "title": "QUIJOTE scientific results XIX. New constraints on the synchrotron\n  spectral index using a semi-blind component separation method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUIJOTE scientific results XIX. New constraints on the synchrotron\n  spectral index using a semi-blind component separation method"
                },
                "summary": "We introduce a novel approach to estimate the spectral index, $\\beta_s$, of\npolarised synchrotron emission, combining the moment expansion of CMB\nforegrounds and the constrained-ILC method. We reconstruct the maps of the\nfirst two synchrotron moments, combining multi-frequency data, and apply the\n`T-T plot' technique between two moment maps to estimate the synchrotron\nspectral index. This approach offers a new technique for mapping the foreground\nspectral parameters, complementing the model-based parametric component\nseparation methods. Applying this technique, we derive a new constraint on the\nspectral index of polarised synchrotron emission using QUIJOTE MFI wide-survey\n11 and 13 GHz data, Wilkinson Microwave Anisotropy Probe (WMAP) data at K and\nKa bands, and Planck LFI 30 GHz data. In the Galactic plane and North Polar\nSpur regions, we obtain an inverse-variance-weighted mean synchrotron index of\n$\\beta_s = -3.11$ with a standard deviation of $0.21$ due to intrinsic scatter,\nconsistent with previous results based on parametric methods using the same\ndataset. We find that the inverse-variance-weighted mean spectral index,\nincluding both statistical and systematic uncertainties, is $\\beta_s^{\\rm\nplane} = -3.05 \\pm 0.01$ in the Galactic plane and $\\beta_s^{\\rm\nhigh\\text{-}lat} = -3.13 \\pm 0.02$ at high latitudes, indicating a moderate\nsteepening of the spectral index from low to high Galactic latitudes. Our\nanalysis indicates that, within the current upper limit on the AME polarisation\nfraction, our results are not subject to any appreciable bias. Furthermore, we\ninfer the spectral index over the entire QUIJOTE survey region, partitioning\nthe sky into 21 patches. This technique can be further extended to constrain\nthe synchrotron spectral curvature by reconstructing higher-order moments when\nbetter-quality data become available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel approach to estimate the spectral index, $\\beta_s$, of\npolarised synchrotron emission, combining the moment expansion of CMB\nforegrounds and the constrained-ILC method. We reconstruct the maps of the\nfirst two synchrotron moments, combining multi-frequency data, and apply the\n`T-T plot' technique between two moment maps to estimate the synchrotron\nspectral index. This approach offers a new technique for mapping the foreground\nspectral parameters, complementing the model-based parametric component\nseparation methods. Applying this technique, we derive a new constraint on the\nspectral index of polarised synchrotron emission using QUIJOTE MFI wide-survey\n11 and 13 GHz data, Wilkinson Microwave Anisotropy Probe (WMAP) data at K and\nKa bands, and Planck LFI 30 GHz data. In the Galactic plane and North Polar\nSpur regions, we obtain an inverse-variance-weighted mean synchrotron index of\n$\\beta_s = -3.11$ with a standard deviation of $0.21$ due to intrinsic scatter,\nconsistent with previous results based on parametric methods using the same\ndataset. We find that the inverse-variance-weighted mean spectral index,\nincluding both statistical and systematic uncertainties, is $\\beta_s^{\\rm\nplane} = -3.05 \\pm 0.01$ in the Galactic plane and $\\beta_s^{\\rm\nhigh\\text{-}lat} = -3.13 \\pm 0.02$ at high latitudes, indicating a moderate\nsteepening of the spectral index from low to high Galactic latitudes. Our\nanalysis indicates that, within the current upper limit on the AME polarisation\nfraction, our results are not subject to any appreciable bias. Furthermore, we\ninfer the spectral index over the entire QUIJOTE survey region, partitioning\nthe sky into 21 patches. This technique can be further extended to constrain\nthe synchrotron spectral curvature by reconstructing higher-order moments when\nbetter-quality data become available."
                },
                "authors": [
                    {
                        "name": "Debabrata Adak"
                    },
                    {
                        "name": "J. A. Rubiño-Martín"
                    },
                    {
                        "name": "R. T. Génova-Santos"
                    },
                    {
                        "name": "M. Remazeilles"
                    },
                    {
                        "name": "A. Almeida"
                    },
                    {
                        "name": "K. Aryan"
                    },
                    {
                        "name": "M. Ashdown"
                    },
                    {
                        "name": "R. B. Barreiro"
                    },
                    {
                        "name": "U. Bose"
                    },
                    {
                        "name": "R. Cepeda-Arroita"
                    },
                    {
                        "name": "J. M. Casas"
                    },
                    {
                        "name": "M. Fernández-Torreiro"
                    },
                    {
                        "name": "E. Martínez-Gonzalez"
                    },
                    {
                        "name": "F. Poidevin"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "P. Vielva"
                    }
                ],
                "author_detail": {
                    "name": "P. Vielva"
                },
                "author": "P. Vielva",
                "arxiv_comment": "15 pages, 15 figures, 1 table, submitted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17759v1",
                "updated": "2025-10-20T17:12:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    12,
                    10,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:12:10Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    12,
                    10,
                    0,
                    293,
                    0
                ],
                "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language\n  Models"
                },
                "summary": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o."
                },
                "authors": [
                    {
                        "name": "Qilin Liao"
                    },
                    {
                        "name": "Anamika Lochab"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "arxiv_comment": "18 pages, 7 Figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10939v2",
                "updated": "2025-10-20T17:02:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    41,
                    0,
                    293,
                    0
                ],
                "published": "2025-02-16T00:51:04Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    0,
                    51,
                    4,
                    6,
                    47,
                    0
                ],
                "title": "Model-assisted inference for dynamic causal effects in staggered rollout\n  cluster randomized experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-assisted inference for dynamic causal effects in staggered rollout\n  cluster randomized experiments"
                },
                "summary": "Staggered rollout cluster randomized experiments (SR-CREs) involve sequential\ntreatment adoption across clusters, requiring analysis methods that address a\ngeneral class of dynamic causal effects, anticipation, and non-ignorable\ncluster-period sizes. Without imposing any outcome modeling assumptions, we\nstudy regression estimators using individual data, cluster-period averages, and\nscaled cluster-period totals, with and without covariate adjustment from a\ndesign-based perspective. We establish consistency and asymptotic normality of\neach estimator under a randomization-based framework and prove that the\nassociated variance estimators are asymptotically conservative in the\nL\\\"{o}wner ordering. Furthermore, we conduct a unified efficiency comparison of\nthe estimators and provide recommendations. We highlight the efficiency\nadvantage of using estimators based on scaled cluster-period totals with\ncovariate adjustment over their counterparts using individual-level data and\ncluster-period averages. Our results rigorously justify linear regression\nestimators as model-assisted methods to address an entire class of dynamic\ncausal effects in SR-CREs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staggered rollout cluster randomized experiments (SR-CREs) involve sequential\ntreatment adoption across clusters, requiring analysis methods that address a\ngeneral class of dynamic causal effects, anticipation, and non-ignorable\ncluster-period sizes. Without imposing any outcome modeling assumptions, we\nstudy regression estimators using individual data, cluster-period averages, and\nscaled cluster-period totals, with and without covariate adjustment from a\ndesign-based perspective. We establish consistency and asymptotic normality of\neach estimator under a randomization-based framework and prove that the\nassociated variance estimators are asymptotically conservative in the\nL\\\"{o}wner ordering. Furthermore, we conduct a unified efficiency comparison of\nthe estimators and provide recommendations. We highlight the efficiency\nadvantage of using estimators based on scaled cluster-period totals with\ncovariate adjustment over their counterparts using individual-level data and\ncluster-period averages. Our results rigorously justify linear regression\nestimators as model-assisted methods to address an entire class of dynamic\ncausal effects in SR-CREs."
                },
                "authors": [
                    {
                        "name": "Xinyuan Chen"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17748v1",
                "updated": "2025-10-20T17:02:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:02:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    40,
                    0,
                    293,
                    0
                ],
                "title": "This is Going to Sound Crazy, But What If We Used Large Language Models\n  to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We\n  Will Find Better Configurations More Quickly Than Retraining From Scratch!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is Going to Sound Crazy, But What If We Used Large Language Models\n  to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We\n  Will Find Better Configurations More Quickly Than Retraining From Scratch!"
                },
                "summary": "Tuning database management systems (DBMSs) is challenging due to trillions of\npossible configurations and evolving workloads. Recent advances in tuning have\nled to breakthroughs in optimizing over the possible configurations. However,\ndue to their design and inability to leverage query-level historical insights,\nexisting automated tuners struggle to adapt and re-optimize the DBMS when the\nenvironment changes (e.g., workload drift, schema transfer).\n  This paper presents the Booster framework that assists existing tuners in\nadapting to environment changes (e.g., drift, cross-schema transfer). Booster\nstructures historical artifacts into query-configuration contexts, prompts\nlarge language models (LLMs) to suggest configurations for each query based on\nrelevant contexts, and then composes the query-level suggestions into a\nholistic configuration with beam search. With multiple OLAP workloads, we\nevaluate Booster's ability to assist different state-of-the-art tuners (e.g.,\ncost-/machine learning-/LLM-based) in adapting to environment changes. By\ncomposing recommendations derived from query-level insights, Booster assists\ntuners in discovering configurations that are up to 74% better and in up to\n4.7x less time than the alternative approach of continuing to tune from\nhistorical configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning database management systems (DBMSs) is challenging due to trillions of\npossible configurations and evolving workloads. Recent advances in tuning have\nled to breakthroughs in optimizing over the possible configurations. However,\ndue to their design and inability to leverage query-level historical insights,\nexisting automated tuners struggle to adapt and re-optimize the DBMS when the\nenvironment changes (e.g., workload drift, schema transfer).\n  This paper presents the Booster framework that assists existing tuners in\nadapting to environment changes (e.g., drift, cross-schema transfer). Booster\nstructures historical artifacts into query-configuration contexts, prompts\nlarge language models (LLMs) to suggest configurations for each query based on\nrelevant contexts, and then composes the query-level suggestions into a\nholistic configuration with beam search. With multiple OLAP workloads, we\nevaluate Booster's ability to assist different state-of-the-art tuners (e.g.,\ncost-/machine learning-/LLM-based) in adapting to environment changes. By\ncomposing recommendations derived from query-level insights, Booster assists\ntuners in discovering configurations that are up to 74% better and in up to\n4.7x less time than the alternative approach of continuing to tune from\nhistorical configurations."
                },
                "authors": [
                    {
                        "name": "William Zhang"
                    },
                    {
                        "name": "Wan Shen Lim"
                    },
                    {
                        "name": "Andrew Pavlo"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Pavlo"
                },
                "author": "Andrew Pavlo",
                "arxiv_comment": "Accepted to SIGMOD2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17746v1",
                "updated": "2025-10-20T17:02:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    29,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:02:29Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    29,
                    0,
                    293,
                    0
                ],
                "title": "A search for black holes with metal-poor stellar companions: I. Survey\n  sample selection and single epoch radial velocity follow-up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A search for black holes with metal-poor stellar companions: I. Survey\n  sample selection and single epoch radial velocity follow-up"
                },
                "summary": "Stellar-mass black holes (BHs) above $30 M_\\odot$ are predicted to form from\nlow-metallicity progenitors, but direct detections of such systems in the Milky\nWay remain scarce. Motivated by the recent discovery of Gaia BH3, a $33\nM_\\odot$ BH with a very metal-poor giant companion, we conduct a systematic\nsearch for additional systems. Approximately 900 candidates are identified with\nGaia as having significant deviations from single-star astrometric motion,\nevidence of RV variability, and low metallicities inferred from Gaia XP\nspectra. We obtain single epoch high-resolution spectra for over 600 of these\nsources with Magellan/MIKE and Lick/APF and measure independent RVs with\n$\\approx 1$ km s$^{-1}$ precision. After removing contaminants such as hot\nstars, pulsators, eclipsing binaries, and hierarchical triples, we identify\nabout 15 promising candidates with large RV amplitudes or offsets from the Gaia\nreported values. This program establishes a well-characterized sample of BH\ncandidates for detailed orbital modeling once Gaia DR4 epoch astrometry and RVs\nare released in late 2026; multi-epoch RV follow-up is ongoing. Together, the\nGaia and ground-based data will place new constraints on the demographics of\nBHs with metal-poor companions and test theoretical predictions linking low\nmetallicity to the formation of the most massive stellar remnants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar-mass black holes (BHs) above $30 M_\\odot$ are predicted to form from\nlow-metallicity progenitors, but direct detections of such systems in the Milky\nWay remain scarce. Motivated by the recent discovery of Gaia BH3, a $33\nM_\\odot$ BH with a very metal-poor giant companion, we conduct a systematic\nsearch for additional systems. Approximately 900 candidates are identified with\nGaia as having significant deviations from single-star astrometric motion,\nevidence of RV variability, and low metallicities inferred from Gaia XP\nspectra. We obtain single epoch high-resolution spectra for over 600 of these\nsources with Magellan/MIKE and Lick/APF and measure independent RVs with\n$\\approx 1$ km s$^{-1}$ precision. After removing contaminants such as hot\nstars, pulsators, eclipsing binaries, and hierarchical triples, we identify\nabout 15 promising candidates with large RV amplitudes or offsets from the Gaia\nreported values. This program establishes a well-characterized sample of BH\ncandidates for detailed orbital modeling once Gaia DR4 epoch astrometry and RVs\nare released in late 2026; multi-epoch RV follow-up is ongoing. Together, the\nGaia and ground-based data will place new constraints on the demographics of\nBHs with metal-poor companions and test theoretical predictions linking low\nmetallicity to the formation of the most massive stellar remnants."
                },
                "authors": [
                    {
                        "name": "Casey Y. Lam"
                    },
                    {
                        "name": "Joshua D. Simon"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Howard Isaacson"
                    },
                    {
                        "name": "Daniel D. Kelson"
                    },
                    {
                        "name": "Jessica Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Lu"
                },
                "author": "Jessica Lu",
                "arxiv_comment": "25 pages, 9 figures, 9 tables, 3 appendices. Submitted to ApJ. Tables\n  1 - 3 are available here:\n  https://drive.google.com/drive/folders/1vbxLSUFfvu1q9nrJ8MFQ4KOxpQOKvkiJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17727v2",
                "updated": "2025-10-21T05:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    5,
                    22,
                    16,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T16:43:06Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    43,
                    6,
                    0,
                    293,
                    0
                ],
                "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Fine-Grained Operating Points for Black-Box LLMs"
                },
                "summary": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs."
                },
                "authors": [
                    {
                        "name": "Ege Beyazit"
                    },
                    {
                        "name": "KL Navaneet"
                    },
                    {
                        "name": "Prashant Mathur"
                    },
                    {
                        "name": "Roi Blanco"
                    },
                    {
                        "name": "Vidit Bansal"
                    },
                    {
                        "name": "Karim Bouyarmane"
                    }
                ],
                "author_detail": {
                    "name": "Karim Bouyarmane"
                },
                "author": "Karim Bouyarmane",
                "arxiv_comment": "Under review at ICLR 2026. 36 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17726v1",
                "updated": "2025-10-20T16:42:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    49,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:42:49Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    49,
                    0,
                    293,
                    0
                ],
                "title": "Rethinking Search: A Study of University Students' Perspectives on Using\n  LLMs and Traditional Search Engines in Academic Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Search: A Study of University Students' Perspectives on Using\n  LLMs and Traditional Search Engines in Academic Problem Solving"
                },
                "summary": "With the increasing integration of Artificial Intelligence (AI) in academic\nproblem solving, university students frequently alternate between traditional\nsearch engines like Google and large language models (LLMs) for information\nretrieval. This study explores students' perceptions of both tools, emphasizing\nusability, efficiency, and their integration into academic workflows. Employing\na mixed-methods approach, we surveyed 109 students from diverse disciplines and\nconducted in-depth interviews with 12 participants. Quantitative analyses,\nincluding ANOVA and chi-square tests, were used to assess differences in\nefficiency, satisfaction, and tool preference. Qualitative insights revealed\nthat students commonly switch between GPT and Google: using Google for\ncredible, multi-source information and GPT for summarization, explanation, and\ndrafting. While neither tool proved sufficient on its own, there was a strong\ndemand for a hybrid solution. In response, we developed a prototype, a chatbot\nembedded within the search interface, that combines GPT's conversational\ncapabilities with Google's reliability to enhance academic research and reduce\ncognitive load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing integration of Artificial Intelligence (AI) in academic\nproblem solving, university students frequently alternate between traditional\nsearch engines like Google and large language models (LLMs) for information\nretrieval. This study explores students' perceptions of both tools, emphasizing\nusability, efficiency, and their integration into academic workflows. Employing\na mixed-methods approach, we surveyed 109 students from diverse disciplines and\nconducted in-depth interviews with 12 participants. Quantitative analyses,\nincluding ANOVA and chi-square tests, were used to assess differences in\nefficiency, satisfaction, and tool preference. Qualitative insights revealed\nthat students commonly switch between GPT and Google: using Google for\ncredible, multi-source information and GPT for summarization, explanation, and\ndrafting. While neither tool proved sufficient on its own, there was a strong\ndemand for a hybrid solution. In response, we developed a prototype, a chatbot\nembedded within the search interface, that combines GPT's conversational\ncapabilities with Google's reliability to enhance academic research and reduce\ncognitive load."
                },
                "authors": [
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Md. Sadman Haque"
                    },
                    {
                        "name": "Zobaer Ibn Razzaque"
                    },
                    {
                        "name": "Robiul Awoul Robin"
                    },
                    {
                        "name": "Sabila Nawshin"
                    }
                ],
                "author_detail": {
                    "name": "Sabila Nawshin"
                },
                "author": "Sabila Nawshin",
                "arxiv_comment": "Acctepted at the EMNLP 2025 HCI+NLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17725v1",
                "updated": "2025-10-20T16:42:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    30,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:42:30Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    30,
                    0,
                    293,
                    0
                ],
                "title": "AcademicEval: Live Long-Context LLM Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcademicEval: Live Long-Context LLM Benchmark"
                },
                "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval"
                },
                "authors": [
                    {
                        "name": "Haozhen Zhang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "arxiv_comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17722v1",
                "updated": "2025-10-20T16:38:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    38,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:38:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    38,
                    40,
                    0,
                    293,
                    0
                ],
                "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues"
                },
                "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research."
                },
                "authors": [
                    {
                        "name": "Yaning Pan"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Yongqian Wen"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Guohui Zhang"
                    },
                    {
                        "name": "Haoxuan Hu"
                    },
                    {
                        "name": "Zhiyu Pan"
                    },
                    {
                        "name": "Yibing Huang"
                    },
                    {
                        "name": "Zhidong Gan"
                    },
                    {
                        "name": "Yonghong Lin"
                    },
                    {
                        "name": "An Ping"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu",
                "arxiv_comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17720v1",
                "updated": "2025-10-20T16:36:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    36,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:36:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    36,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity\n  Recognition"
                },
                "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power."
                },
                "authors": [
                    {
                        "name": "Nanda Kumar Rengarajan"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Chun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Wang"
                },
                "author": "Chun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05605v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05605v5",
                "updated": "2025-10-20T16:35:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    35,
                    34,
                    0,
                    293,
                    0
                ],
                "published": "2025-02-08T15:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    15,
                    21,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "Evolving LLMs' Self-Refinement Capability via Iterative Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving LLMs' Self-Refinement Capability via Iterative Preference\n  Optimization"
                },
                "summary": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinyu Cui"
                    },
                    {
                        "name": "Xuanfa Jin"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05605v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05605v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17715v1",
                "updated": "2025-10-20T16:29:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    29,
                    53,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:29:53Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    29,
                    53,
                    0,
                    293,
                    0
                ],
                "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QueST: Incentivizing LLMs to Generate Difficult Problems"
                },
                "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17705v1",
                "updated": "2025-10-20T16:19:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    19,
                    27,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:19:27Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    19,
                    27,
                    0,
                    293,
                    0
                ],
                "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM."
                },
                "authors": [
                    {
                        "name": "Dayan Pan"
                    },
                    {
                        "name": "Zhaoyang Fu"
                    },
                    {
                        "name": "Jingyuan Wang"
                    },
                    {
                        "name": "Xiao Han"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_doi": "10.1145/3746252.3761289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.17705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM' 25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17700v1",
                "updated": "2025-10-20T16:15:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    15,
                    3,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:15:03Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    15,
                    3,
                    0,
                    293,
                    0
                ],
                "title": "Elastic ViTs from Pretrained Models without Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic ViTs from Pretrained Models without Retraining"
                },
                "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/"
                },
                "authors": [
                    {
                        "name": "Walter Simoncini"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17698v1",
                "updated": "2025-10-20T16:11:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    11,
                    34,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:11:34Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    11,
                    34,
                    0,
                    293,
                    0
                ],
                "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM\n  Educational Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mining Effective Pedagogical Strategies from Learner-LLM\n  Educational Dialogues"
                },
                "summary": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies."
                },
                "authors": [
                    {
                        "name": "Liqun He"
                    },
                    {
                        "name": "Manolis Mavrikis"
                    },
                    {
                        "name": "Mutlu Cukurova"
                    }
                ],
                "author_detail": {
                    "name": "Mutlu Cukurova"
                },
                "author": "Mutlu Cukurova",
                "arxiv_doi": "10.1007/978-3-031-99261-2_42",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99261-2_42",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.17698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17697v1",
                "updated": "2025-10-20T16:10:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    10,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:10:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    10,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Principle of Targeted Intervention for Multi-Agent Reinforcement\n  Learning"
                },
                "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis."
                },
                "authors": [
                    {
                        "name": "Anjie Liu"
                    },
                    {
                        "name": "Jianhong Wang"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Mengyue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Yang"
                },
                "author": "Mengyue Yang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12943v2",
                "updated": "2025-10-20T15:55:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    55,
                    28,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-14T19:42:24Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    19,
                    42,
                    24,
                    1,
                    287,
                    0
                ],
                "title": "The Curious Case of Curiosity across Human Cultures and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curious Case of Curiosity across Human Cultures and LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have expanded their role in\nhuman interaction, yet curiosity -- a central driver of inquiry -- remains\nunderexplored in these systems, particularly across cultural contexts. In this\nwork, we investigate cultural variation in curiosity using Yahoo! Answers, a\nreal-world multi-country dataset spanning diverse topics. We introduce CUEST\n(CUriosity Evaluation across SocieTies), an evaluation framework that measures\nhuman-model alignment in curiosity through linguistic (style), topic preference\n(content) analysis and grounding insights in social science constructs. Across\nopen- and closed-source models, we find that LLMs flatten cross-cultural\ndiversity, aligning more closely with how curiosity is expressed in Western\ncountries. We then explore fine-tuning strategies to induce curiosity in LLMs,\nnarrowing the human-model alignment gap by up to 50%. Finally, we demonstrate\nthe practical value of curiosity for LLM adaptability across cultures, showing\nits importance for future NLP research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have expanded their role in\nhuman interaction, yet curiosity -- a central driver of inquiry -- remains\nunderexplored in these systems, particularly across cultural contexts. In this\nwork, we investigate cultural variation in curiosity using Yahoo! Answers, a\nreal-world multi-country dataset spanning diverse topics. We introduce CUEST\n(CUriosity Evaluation across SocieTies), an evaluation framework that measures\nhuman-model alignment in curiosity through linguistic (style), topic preference\n(content) analysis and grounding insights in social science constructs. Across\nopen- and closed-source models, we find that LLMs flatten cross-cultural\ndiversity, aligning more closely with how curiosity is expressed in Western\ncountries. We then explore fine-tuning strategies to induce curiosity in LLMs,\nnarrowing the human-model alignment gap by up to 50%. Finally, we demonstrate\nthe practical value of curiosity for LLM adaptability across cultures, showing\nits importance for future NLP research."
                },
                "authors": [
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "arxiv_comment": "Preprint (Paper under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15217v2",
                "updated": "2025-10-20T15:51:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    51,
                    30,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-17T00:54:03Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    0,
                    54,
                    3,
                    4,
                    290,
                    0
                ],
                "title": "Reflections from Research Roundtables at the Conference on Health,\n  Inference, and Learning (CHIL) 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections from Research Roundtables at the Conference on Health,\n  Inference, and Learning (CHIL) 2025"
                },
                "summary": "The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),\nhosted by the Association for Health Learning and Inference (AHLI), was held in\nperson on June 25-27, 2025, at the University of California, Berkeley, in\nBerkeley, California, USA. As part of this year's program, we hosted Research\nRoundtables to catalyze collaborative, small-group dialogue around critical,\ntimely topics at the intersection of machine learning and healthcare. Each\nroundtable was moderated by a team of senior and junior chairs who fostered\nopen exchange, intellectual curiosity, and inclusive engagement. The sessions\nemphasized rigorous discussion of key challenges, exploration of emerging\nopportunities, and collective ideation toward actionable directions in the\nfield. In total, eight roundtables were held by 19 roundtable chairs on topics\nof \"Explainability, Interpretability, and Transparency,\" \"Uncertainty, Bias,\nand Fairness,\" \"Causality,\" \"Domain Adaptation,\" \"Foundation Models,\" \"Learning\nfrom Small Medical Data,\" \"Multimodal Methods,\" and \"Scalable, Translational\nHealthcare Solutions.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),\nhosted by the Association for Health Learning and Inference (AHLI), was held in\nperson on June 25-27, 2025, at the University of California, Berkeley, in\nBerkeley, California, USA. As part of this year's program, we hosted Research\nRoundtables to catalyze collaborative, small-group dialogue around critical,\ntimely topics at the intersection of machine learning and healthcare. Each\nroundtable was moderated by a team of senior and junior chairs who fostered\nopen exchange, intellectual curiosity, and inclusive engagement. The sessions\nemphasized rigorous discussion of key challenges, exploration of emerging\nopportunities, and collective ideation toward actionable directions in the\nfield. In total, eight roundtables were held by 19 roundtable chairs on topics\nof \"Explainability, Interpretability, and Transparency,\" \"Uncertainty, Bias,\nand Fairness,\" \"Causality,\" \"Domain Adaptation,\" \"Foundation Models,\" \"Learning\nfrom Small Medical Data,\" \"Multimodal Methods,\" and \"Scalable, Translational\nHealthcare Solutions.\""
                },
                "authors": [
                    {
                        "name": "Emily Alsentzer"
                    },
                    {
                        "name": "Marie-Laure Charpignon"
                    },
                    {
                        "name": "Bill Chen"
                    },
                    {
                        "name": "Niharika D'Souza"
                    },
                    {
                        "name": "Jason Fries"
                    },
                    {
                        "name": "Yixing Jiang"
                    },
                    {
                        "name": "Aparajita Kashyap"
                    },
                    {
                        "name": "Chanwoo Kim"
                    },
                    {
                        "name": "Simon Lee"
                    },
                    {
                        "name": "Aishwarya Mandyam"
                    },
                    {
                        "name": "Ashery Christopher Mbilinyi"
                    },
                    {
                        "name": "Nikita Mehandru"
                    },
                    {
                        "name": "Nitish Nagesh"
                    },
                    {
                        "name": "Brighton Nuwagira"
                    },
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Arvind Pillai"
                    },
                    {
                        "name": "Akane Sano"
                    },
                    {
                        "name": "Tanveer Syeda-Mahmood"
                    },
                    {
                        "name": "Shashank Yadav"
                    },
                    {
                        "name": "Elias Adhanom"
                    },
                    {
                        "name": "Muhammad Umar Afza"
                    },
                    {
                        "name": "Amelia Archer"
                    },
                    {
                        "name": "Suhana Bedi"
                    },
                    {
                        "name": "Vasiliki Bikia"
                    },
                    {
                        "name": "Trenton Chang"
                    },
                    {
                        "name": "George H. Chen"
                    },
                    {
                        "name": "Winston Chen"
                    },
                    {
                        "name": "Erica Chiang"
                    },
                    {
                        "name": "Edward Choi"
                    },
                    {
                        "name": "Octavia Ciora"
                    },
                    {
                        "name": "Paz Dozie-Nnamah"
                    },
                    {
                        "name": "Shaza Elsharief"
                    },
                    {
                        "name": "Matthew Engelhard"
                    },
                    {
                        "name": "Ali Eshragh"
                    },
                    {
                        "name": "Jean Feng"
                    },
                    {
                        "name": "Josh Fessel"
                    },
                    {
                        "name": "Scott Fleming"
                    },
                    {
                        "name": "Kei Sen Fong"
                    },
                    {
                        "name": "Thomas Frost"
                    },
                    {
                        "name": "Soham Gadgil"
                    },
                    {
                        "name": "Judy Gichoya"
                    },
                    {
                        "name": "Leeor Hershkovich"
                    },
                    {
                        "name": "Sujeong Im"
                    },
                    {
                        "name": "Bhavya Jain"
                    },
                    {
                        "name": "Vincent Jeanselme"
                    },
                    {
                        "name": "Furong Jia"
                    },
                    {
                        "name": "Qixuan Jin"
                    },
                    {
                        "name": "Yuxuan Jin"
                    },
                    {
                        "name": "Daniel Kapash"
                    },
                    {
                        "name": "Geetika Kapoor"
                    },
                    {
                        "name": "Behdokht Kiafar"
                    },
                    {
                        "name": "Matthias Kleiner"
                    },
                    {
                        "name": "Stefan Kraft"
                    },
                    {
                        "name": "Annika Kumar"
                    },
                    {
                        "name": "Daeun Kyung"
                    },
                    {
                        "name": "Zhongyuan Liang"
                    },
                    {
                        "name": "Joanna Lin"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Hongzhou Luan"
                    },
                    {
                        "name": "Chris Lunt"
                    },
                    {
                        "name": "Leopoldo Julían Lechuga López"
                    },
                    {
                        "name": "Matthew B. A. McDermott"
                    },
                    {
                        "name": "Shahriar Noroozizadeh"
                    },
                    {
                        "name": "Connor O'Brien"
                    },
                    {
                        "name": "YongKyung Oh"
                    },
                    {
                        "name": "Mixail Ota"
                    },
                    {
                        "name": "Stephen Pfohl"
                    },
                    {
                        "name": "Meagan Pi"
                    },
                    {
                        "name": "Tanmoy Sarkar Pias"
                    },
                    {
                        "name": "Emma Rocheteau"
                    },
                    {
                        "name": "Avishaan Sethi"
                    },
                    {
                        "name": "Toru Shirakawa"
                    },
                    {
                        "name": "Anita Silver"
                    },
                    {
                        "name": "Neha Simha"
                    },
                    {
                        "name": "Kamile Stankeviciute"
                    },
                    {
                        "name": "Max Sunog"
                    },
                    {
                        "name": "Peter Szolovits"
                    },
                    {
                        "name": "Shengpu Tang"
                    },
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Aaron Tierney"
                    },
                    {
                        "name": "John Valdovinos"
                    },
                    {
                        "name": "Byron Wallace"
                    },
                    {
                        "name": "Will Ke Wang"
                    },
                    {
                        "name": "Peter Washington"
                    },
                    {
                        "name": "Jeremy Weiss"
                    },
                    {
                        "name": "Daniel Wolfe"
                    },
                    {
                        "name": "Emily Wong"
                    },
                    {
                        "name": "Hye Sun Yun"
                    },
                    {
                        "name": "Xiaoman Zhang"
                    },
                    {
                        "name": "Xiao Yu Cindy Zhang"
                    },
                    {
                        "name": "Hayoung Jeong"
                    },
                    {
                        "name": "Kaveri A. Thakoor"
                    }
                ],
                "author_detail": {
                    "name": "Kaveri A. Thakoor"
                },
                "author": "Kaveri A. Thakoor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18963v3",
                "updated": "2025-10-20T15:50:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    50,
                    29,
                    0,
                    293,
                    0
                ],
                "published": "2025-01-31T08:49:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    8,
                    49,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "Time-Varying Bayesian Optimization Without a Metronome",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Varying Bayesian Optimization Without a Metronome"
                },
                "summary": "Time-Varying Bayesian Optimization (TVBO) is the go-to framework for\noptimizing a time-varying, expensive, noisy black-box function $f$. However,\nmost of the asymptotic guarantees offered by TVBO algorithms rely on the\nassumption that observations are acquired at a constant frequency. As the GP\ninference complexity scales with the cube of its dataset size, this assumption\nis unrealistic in the long run. In this paper, we relax this assumption and\nderive the first upper regret bound that explicitly accounts for changes in the\nobservations sampling frequency. Based on this analysis, we formulate practical\nrecommendations about dataset sizes and stale data policies of TVBO algorithms.\nWe illustrate how an algorithm (BOLT) that follows these recommendations\nperforms better than the state-of-the-art of TVBO through experiments on\nsynthetic and real-world problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Varying Bayesian Optimization (TVBO) is the go-to framework for\noptimizing a time-varying, expensive, noisy black-box function $f$. However,\nmost of the asymptotic guarantees offered by TVBO algorithms rely on the\nassumption that observations are acquired at a constant frequency. As the GP\ninference complexity scales with the cube of its dataset size, this assumption\nis unrealistic in the long run. In this paper, we relax this assumption and\nderive the first upper regret bound that explicitly accounts for changes in the\nobservations sampling frequency. Based on this analysis, we formulate practical\nrecommendations about dataset sizes and stale data policies of TVBO algorithms.\nWe illustrate how an algorithm (BOLT) that follows these recommendations\nperforms better than the state-of-the-art of TVBO through experiments on\nsynthetic and real-world problems."
                },
                "authors": [
                    {
                        "name": "Anthony Bardou"
                    },
                    {
                        "name": "Patrick Thiran"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Thiran"
                },
                "author": "Patrick Thiran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17671v1",
                "updated": "2025-10-20T15:41:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    41,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:41:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    41,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LILO: Bayesian Optimization with Interactive Natural Language Feedback"
                },
                "summary": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes."
                },
                "authors": [
                    {
                        "name": "Katarzyna Kobalczyk"
                    },
                    {
                        "name": "Zhiyuan Jerry Lin"
                    },
                    {
                        "name": "Benjamin Letham"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Maximilian Balandat"
                    },
                    {
                        "name": "Eytan Bakshy"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Bakshy"
                },
                "author": "Eytan Bakshy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17667v1",
                "updated": "2025-10-20T15:39:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    39,
                    4,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:39:04Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    39,
                    4,
                    0,
                    293,
                    0
                ],
                "title": "A performance evaluation of integrating machine learning schemes\n  utilizing fluidic lenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A performance evaluation of integrating machine learning schemes\n  utilizing fluidic lenses"
                },
                "summary": "A combination of statistical inference and machine learning (ML) schemes has\nbeen utilized to create a thorough understanding of coarse experimental data\nbased on Zernike variables characterizing optical aberrations in fluidic\nlenses. A classification of surplus-response variables through tolerance\nmanipulation was included to unravel the dimensional aspect of the data.\nSimilarly, the impact of the exclusion of supererogatory variables through the\nidentification of clustering movements of constituents is examined. The method\nof constructing a spectrum of collaborative results through the application of\nsimilar techniques has been tested. To evaluate the suitability of each\nstatistical method before its application on a large dataset, a selection of ML\nschemes has been proposed. The supervised learning tools principal component\nanalysis (PCA), factor analysis (FA), and hierarchical clustering (HC) were\nemployed to define the elemental characteristics of Zernike variables. PCA\nenabled to reduce the dimensionality of the system by identifying two principal\ncomponents which collectively account for 95\\% of the total variance. The\nexecution of FA indicated that a specific tolerance of independent variability\nof 0.005 could be used to reduce the dimensionality of the system without\nlosing essential data information. A high cophenetic coefficient value of\nc=0.9629 validated an accurate clustering division of variables with similar\ncharacteristics. The current approach of mutually validating ML and statistical\nanalysis methods will aid in laying the foundation for state-of-the-art (SOTA)\nanalysis. The benefit of our approach can be assessed by considering that the\nassociated SOTA will enhance the predictive accuracy between two comparable\nmethods, in contrast to the SOTA analysis conducted between two arbitrary ML\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A combination of statistical inference and machine learning (ML) schemes has\nbeen utilized to create a thorough understanding of coarse experimental data\nbased on Zernike variables characterizing optical aberrations in fluidic\nlenses. A classification of surplus-response variables through tolerance\nmanipulation was included to unravel the dimensional aspect of the data.\nSimilarly, the impact of the exclusion of supererogatory variables through the\nidentification of clustering movements of constituents is examined. The method\nof constructing a spectrum of collaborative results through the application of\nsimilar techniques has been tested. To evaluate the suitability of each\nstatistical method before its application on a large dataset, a selection of ML\nschemes has been proposed. The supervised learning tools principal component\nanalysis (PCA), factor analysis (FA), and hierarchical clustering (HC) were\nemployed to define the elemental characteristics of Zernike variables. PCA\nenabled to reduce the dimensionality of the system by identifying two principal\ncomponents which collectively account for 95\\% of the total variance. The\nexecution of FA indicated that a specific tolerance of independent variability\nof 0.005 could be used to reduce the dimensionality of the system without\nlosing essential data information. A high cophenetic coefficient value of\nc=0.9629 validated an accurate clustering division of variables with similar\ncharacteristics. The current approach of mutually validating ML and statistical\nanalysis methods will aid in laying the foundation for state-of-the-art (SOTA)\nanalysis. The benefit of our approach can be assessed by considering that the\nassociated SOTA will enhance the predictive accuracy between two comparable\nmethods, in contrast to the SOTA analysis conducted between two arbitrary ML\nmethods."
                },
                "authors": [
                    {
                        "name": "Graciana Puentes"
                    }
                ],
                "author_detail": {
                    "name": "Graciana Puentes"
                },
                "author": "Graciana Puentes",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17664v1",
                "updated": "2025-10-20T15:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    37,
                    49,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    37,
                    49,
                    0,
                    293,
                    0
                ],
                "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads"
                },
                "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes."
                },
                "authors": [
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Li Yi"
                    }
                ],
                "author_detail": {
                    "name": "Li Yi"
                },
                "author": "Li Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13339v2",
                "updated": "2025-10-20T15:31:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    31,
                    24,
                    0,
                    293,
                    0
                ],
                "published": "2025-07-17T17:57:18Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    57,
                    18,
                    3,
                    198,
                    0
                ],
                "title": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution"
                },
                "summary": "High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks."
                },
                "authors": [
                    {
                        "name": "Ritik Shah"
                    },
                    {
                        "name": "Marco F. Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Marco F. Duarte"
                },
                "author": "Marco F. Duarte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17652v1",
                "updated": "2025-10-20T15:27:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    27,
                    53,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:27:53Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    27,
                    53,
                    0,
                    293,
                    0
                ],
                "title": "Qomhra: A Bilingual Irish-English Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qomhra: A Bilingual Irish-English Large Language Model"
                },
                "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality."
                },
                "authors": [
                    {
                        "name": "Joseph McInerney"
                    }
                ],
                "author_detail": {
                    "name": "Joseph McInerney"
                },
                "author": "Joseph McInerney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17651v1",
                "updated": "2025-10-20T15:26:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    26,
                    43,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:26:43Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    26,
                    43,
                    0,
                    293,
                    0
                ],
                "title": "Frugal Federated Learning for Violence Detection: A Comparison of\n  LoRA-Tuned VLMs and Personalized CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frugal Federated Learning for Violence Detection: A Comparison of\n  LoRA-Tuned VLMs and Personalized CNNs"
                },
                "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems."
                },
                "authors": [
                    {
                        "name": "Sébastien Thuau"
                    },
                    {
                        "name": "Siba Haidar"
                    },
                    {
                        "name": "Ayush Bajracharya"
                    },
                    {
                        "name": "Rachid Chelouah"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Chelouah"
                },
                "author": "Rachid Chelouah",
                "arxiv_comment": "7 pages, 1 figure, FLTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17648v1",
                "updated": "2025-10-20T15:24:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    24,
                    43,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:24:43Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    24,
                    43,
                    0,
                    293,
                    0
                ],
                "title": "Wild regenerative block bootstrap for Harris recurrent Markov chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wild regenerative block bootstrap for Harris recurrent Markov chains"
                },
                "summary": "We consider Gaussian and bootstrap approximations for the supremum of\nadditive functionals of aperiodic Harris recurrent Markov chains. The supremum\nis taken over a function class that may depend on the sample size, which allows\nfor non-Donsker settings; that is, the empirical process need not have a weak\nlimit in the space of bounded functions. We first establish a non-asymptotic\nGaussian approximation error, which holds at rates comparable to those for sums\nof high-dimensional independent or one-dependent vectors. Key to our derivation\nis the Nummelin splitting technique, which enables us to decompose the chain\ninto either independent or one-dependent random blocks. Additionally, building\nupon the Nummelin splitting, we propose a Gaussian multiplier bootstrap for\npractical inference and establish its finite-sample guarantees in the strongly\naperiodic case. Finally, we apply our bootstrap to construct a uniform\nconfidence band for an invariant density within a certain class of diffusion\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider Gaussian and bootstrap approximations for the supremum of\nadditive functionals of aperiodic Harris recurrent Markov chains. The supremum\nis taken over a function class that may depend on the sample size, which allows\nfor non-Donsker settings; that is, the empirical process need not have a weak\nlimit in the space of bounded functions. We first establish a non-asymptotic\nGaussian approximation error, which holds at rates comparable to those for sums\nof high-dimensional independent or one-dependent vectors. Key to our derivation\nis the Nummelin splitting technique, which enables us to decompose the chain\ninto either independent or one-dependent random blocks. Additionally, building\nupon the Nummelin splitting, we propose a Gaussian multiplier bootstrap for\npractical inference and establish its finite-sample guarantees in the strongly\naperiodic case. Finally, we apply our bootstrap to construct a uniform\nconfidence band for an invariant density within a certain class of diffusion\nprocesses."
                },
                "authors": [
                    {
                        "name": "Kyuseong Choi"
                    },
                    {
                        "name": "Gabriella Ciolek"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Ciolek"
                },
                "author": "Gabriella Ciolek",
                "arxiv_comment": "47 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E17",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16239v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16239v5",
                "updated": "2025-10-20T15:20:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    20,
                    57,
                    0,
                    293,
                    0
                ],
                "published": "2024-07-23T07:26:38Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    7,
                    26,
                    38,
                    1,
                    205,
                    0
                ],
                "title": "Identifiable Latent Bandits: Leveraging observational data for\n  personalized decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifiable Latent Bandits: Leveraging observational data for\n  personalized decision-making"
                },
                "summary": "Sequential decision-making algorithms such as multi-armed bandits can find\noptimal personalized decisions, but are notoriously sample-hungry. In\npersonalized medicine, for example, training a bandit from scratch for every\npatient is typically infeasible, as the number of trials required is much\nlarger than the number of decision points for a single patient. To combat this,\nlatent bandits offer rapid exploration and personalization beyond what context\nvariables alone can offer, provided that a latent variable model of problem\ninstances can be learned consistently. However, existing works give no guidance\nas to how such a model can be found. In this work, we propose an identifiable\nlatent bandit framework that leads to optimal decision-making with a shorter\nexploration time than classical bandits by learning from historical records of\ndecisions and outcomes. Our method is based on nonlinear independent component\nanalysis that provably identifies representations from observational data\nsufficient to infer optimal actions in new bandit instances. We verify this\nstrategy in simulated and semi-synthetic environments, showing substantial\nimprovement over online and offline learning baselines when identifying\nconditions are satisfied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential decision-making algorithms such as multi-armed bandits can find\noptimal personalized decisions, but are notoriously sample-hungry. In\npersonalized medicine, for example, training a bandit from scratch for every\npatient is typically infeasible, as the number of trials required is much\nlarger than the number of decision points for a single patient. To combat this,\nlatent bandits offer rapid exploration and personalization beyond what context\nvariables alone can offer, provided that a latent variable model of problem\ninstances can be learned consistently. However, existing works give no guidance\nas to how such a model can be found. In this work, we propose an identifiable\nlatent bandit framework that leads to optimal decision-making with a shorter\nexploration time than classical bandits by learning from historical records of\ndecisions and outcomes. Our method is based on nonlinear independent component\nanalysis that provably identifies representations from observational data\nsufficient to infer optimal actions in new bandit instances. We verify this\nstrategy in simulated and semi-synthetic environments, showing substantial\nimprovement over online and offline learning baselines when identifying\nconditions are satisfied."
                },
                "authors": [
                    {
                        "name": "Ahmet Zahid Balcıoğlu"
                    },
                    {
                        "name": "Newton Mwai"
                    },
                    {
                        "name": "Emil Carlsson"
                    },
                    {
                        "name": "Fredrik D. Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik D. Johansson"
                },
                "author": "Fredrik D. Johansson",
                "arxiv_comment": "29 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16239v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16239v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17638v1",
                "updated": "2025-10-20T15:20:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    20,
                    5,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:20:05Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    20,
                    5,
                    0,
                    293,
                    0
                ],
                "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet\n  Arena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet\n  Arena"
                },
                "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears."
                },
                "authors": [
                    {
                        "name": "Qingchuan Yang"
                    },
                    {
                        "name": "Simon Mahns"
                    },
                    {
                        "name": "Sida Li"
                    },
                    {
                        "name": "Anri Gu"
                    },
                    {
                        "name": "Jibang Wu"
                    },
                    {
                        "name": "Haifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Xu"
                },
                "author": "Haifeng Xu",
                "arxiv_comment": "https://www.prophetarena.co/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17633v1",
                "updated": "2025-10-20T15:14:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    14,
                    25,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:14:25Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    14,
                    25,
                    0,
                    293,
                    0
                ],
                "title": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated\n  Refusal Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated\n  Refusal Steering"
                },
                "summary": "Large Audio-Language Models (LALMs) are becoming essential as a powerful\nmultimodal backbone for real-world applications. However, recent studies show\nthat audio inputs can more easily elicit harmful responses than text, exposing\nnew risks toward deployment. While safety alignment has made initial advances\nin LLMs and Large Vision-Language Models (LVLMs), we find that vanilla\nadaptation of these approaches to LALMs faces two key limitations: 1) LLM-based\nsteering fails under audio input due to the large distributional gap between\nactivations, and 2) prompt-based defenses induce over-refusals on benign-speech\nqueries. To address these challenges, we propose Safe-Ablated Refusal Steering\n(SARSteer), the first inference-time defense framework for LALMs. Specifically,\nSARSteer leverages text-derived refusal steering to enforce rejection without\nmanipulating audio inputs and introduces decomposed safe-space ablation to\nmitigate over-refusal. Extensive experiments demonstrate that SARSteer\nsignificantly improves harmful-query refusal while preserving benign responses,\nestablishing a principled step toward safety alignment in LALMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) are becoming essential as a powerful\nmultimodal backbone for real-world applications. However, recent studies show\nthat audio inputs can more easily elicit harmful responses than text, exposing\nnew risks toward deployment. While safety alignment has made initial advances\nin LLMs and Large Vision-Language Models (LVLMs), we find that vanilla\nadaptation of these approaches to LALMs faces two key limitations: 1) LLM-based\nsteering fails under audio input due to the large distributional gap between\nactivations, and 2) prompt-based defenses induce over-refusals on benign-speech\nqueries. To address these challenges, we propose Safe-Ablated Refusal Steering\n(SARSteer), the first inference-time defense framework for LALMs. Specifically,\nSARSteer leverages text-derived refusal steering to enforce rejection without\nmanipulating audio inputs and introduces decomposed safe-space ablation to\nmitigate over-refusal. Extensive experiments demonstrate that SARSteer\nsignificantly improves harmful-query refusal while preserving benign responses,\nestablishing a principled step toward safety alignment in LALMs."
                },
                "authors": [
                    {
                        "name": "Weilin Lin"
                    },
                    {
                        "name": "Jianze Li"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Li Liu"
                    }
                ],
                "author_detail": {
                    "name": "Li Liu"
                },
                "author": "Li Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11302v2",
                "updated": "2025-10-20T15:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    9,
                    23,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-13T11:48:48Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    48,
                    48,
                    0,
                    286,
                    0
                ],
                "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object\n  Detection in the Era of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Does Supervised Training Pay Off? The Hidden Economics of Object\n  Detection in the Era of Vision-Language Models"
                },
                "summary": "Object detection traditionally relies on costly manual annotation. We present\nthe first comprehensive cost-effectiveness analysis comparing supervised YOLO\nand zero-shot vision-language models (Gemini Flash 2.5 and GPT-4). Evaluated on\n5,000 stratified COCO images and 500 diverse product images, combined with\nTotal Cost of Ownership modeling, we derive break-even thresholds for\narchitecture selection. Results show supervised YOLO attains 91.2% accuracy\nversus 68.5% for Gemini and 71.3% for GPT-4 on standard categories; the\nannotation expense for a 100-category system is $10,800, and the accuracy\nadvantage only pays off beyond 55 million inferences (151,000 images/day for\none year). On diverse product categories Gemini achieves 52.3% and GPT-4 55.1%,\nwhile supervised YOLO cannot detect untrained classes.\nCost-per-correct-detection favors Gemini ($0.00050) and GPT-4 ($0.00067) over\nYOLO ($0.143) at 100,000 inferences. We provide decision frameworks showing\nthat optimal architecture choice depends on inference volume, category\nstability, budget, and accuracy requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection traditionally relies on costly manual annotation. We present\nthe first comprehensive cost-effectiveness analysis comparing supervised YOLO\nand zero-shot vision-language models (Gemini Flash 2.5 and GPT-4). Evaluated on\n5,000 stratified COCO images and 500 diverse product images, combined with\nTotal Cost of Ownership modeling, we derive break-even thresholds for\narchitecture selection. Results show supervised YOLO attains 91.2% accuracy\nversus 68.5% for Gemini and 71.3% for GPT-4 on standard categories; the\nannotation expense for a 100-category system is $10,800, and the accuracy\nadvantage only pays off beyond 55 million inferences (151,000 images/day for\none year). On diverse product categories Gemini achieves 52.3% and GPT-4 55.1%,\nwhile supervised YOLO cannot detect untrained classes.\nCost-per-correct-detection favors Gemini ($0.00050) and GPT-4 ($0.00067) over\nYOLO ($0.143) at 100,000 inferences. We provide decision frameworks showing\nthat optimal architecture choice depends on inference volume, category\nstability, budget, and accuracy requirements."
                },
                "authors": [
                    {
                        "name": "Samer Al-Hamadani"
                    }
                ],
                "author_detail": {
                    "name": "Samer Al-Hamadani"
                },
                "author": "Samer Al-Hamadani",
                "arxiv_comment": "30 pages, 12 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17621v1",
                "updated": "2025-10-20T15:04:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    4,
                    29,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:04:29Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    4,
                    29,
                    0,
                    293,
                    0
                ],
                "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with\n  Denoising Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with\n  Denoising Models"
                },
                "summary": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric."
                },
                "authors": [
                    {
                        "name": "Vincenzo Carletti"
                    },
                    {
                        "name": "Pasquale Foggia"
                    },
                    {
                        "name": "Carlo Mazzocca"
                    },
                    {
                        "name": "Giuseppe Parrella"
                    },
                    {
                        "name": "Mario Vento"
                    }
                ],
                "author_detail": {
                    "name": "Mario Vento"
                },
                "author": "Mario Vento",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23156v2",
                "updated": "2025-10-20T15:01:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    1,
                    20,
                    0,
                    293,
                    0
                ],
                "published": "2025-03-29T17:24:48Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    17,
                    24,
                    48,
                    5,
                    88,
                    0
                ],
                "title": "Neural Bayes estimation and selection for complex bivariate extremal\n  dependence models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Bayes estimation and selection for complex bivariate extremal\n  dependence models"
                },
                "summary": "Likelihood-free approaches are appealing for performing inference on complex\ndependence models, either because it is not possible to formulate a likelihood\nfunction, or its evaluation is very computationally costly. This is the case\nfor several models available in the multivariate extremes literature,\nparticularly for the most flexible tail models, including those that\ninterpolate between the two key dependence classes of `asymptotic dependence'\nand `asymptotic independence'. We focus on approaches that leverage neural\nnetworks to approximate Bayes estimators. In particular, we explore the\nproperties of neural Bayes estimators for parameter inference for several\nflexible but computationally expensive models to fit, with a view to aiding\ntheir routine implementation. Owing to the absence of likelihood evaluation in\nthe inference procedure, classical information criteria such as the Bayesian\ninformation criterion cannot be used to select the most appropriate model.\nInstead, we propose using neural networks as neural Bayes classifiers for model\nselection. Our goal is to provide a toolbox for simple, fast fitting and\ncomparison of complex extreme-value dependence models, where the best model is\nselected for a given data set and its parameters subsequently estimated using\nneural Bayes estimation. We apply our classifiers and estimators to analyse the\npairwise extremal behaviour of changes in horizontal geomagnetic field\nfluctuations at three different locations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-free approaches are appealing for performing inference on complex\ndependence models, either because it is not possible to formulate a likelihood\nfunction, or its evaluation is very computationally costly. This is the case\nfor several models available in the multivariate extremes literature,\nparticularly for the most flexible tail models, including those that\ninterpolate between the two key dependence classes of `asymptotic dependence'\nand `asymptotic independence'. We focus on approaches that leverage neural\nnetworks to approximate Bayes estimators. In particular, we explore the\nproperties of neural Bayes estimators for parameter inference for several\nflexible but computationally expensive models to fit, with a view to aiding\ntheir routine implementation. Owing to the absence of likelihood evaluation in\nthe inference procedure, classical information criteria such as the Bayesian\ninformation criterion cannot be used to select the most appropriate model.\nInstead, we propose using neural networks as neural Bayes classifiers for model\nselection. Our goal is to provide a toolbox for simple, fast fitting and\ncomparison of complex extreme-value dependence models, where the best model is\nselected for a given data set and its parameters subsequently estimated using\nneural Bayes estimation. We apply our classifiers and estimators to analyse the\npairwise extremal behaviour of changes in horizontal geomagnetic field\nfluctuations at three different locations."
                },
                "authors": [
                    {
                        "name": "Lídia M. André"
                    },
                    {
                        "name": "Jennifer L. Wadsworth"
                    },
                    {
                        "name": "Raphaël Huser"
                    }
                ],
                "author_detail": {
                    "name": "Raphaël Huser"
                },
                "author": "Raphaël Huser",
                "arxiv_comment": "48 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H05, 62H12, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01611v3",
                "updated": "2025-10-20T14:59:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    59,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-02T02:49:06Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    2,
                    49,
                    6,
                    3,
                    275,
                    0
                ],
                "title": "PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: \\textit{Can LLMs be effectively applied to\npsychological counseling?} To determine whether an LLM can effectively take on\nthe role of a psychological counselor, the first step is to assess whether it\nmeets the qualifications required for such a role, namely the ability to pass\nthe U.S. National Counselor Certification Exam (NCE). This is because, just as\na human counselor must pass a certification exam to practice, an LLM must\ndemonstrate sufficient psychological knowledge to meet the standards required\nfor such a role. To address this, we introduce PsychCounsel-Bench, a benchmark\ngrounded in U.S.national counselor examinations, a licensure test for\nprofessional counselors that requires about 70\\% accuracy to pass.\nPsychCounsel-Bench comprises approximately 2,252 carefully curated\nsingle-choice questions, crafted to require deep understanding and broad enough\nto cover various sub-disciplines of psychology. This benchmark provides a\ncomprehensive assessment of an LLM's ability to function as a counselor. Our\nevaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and\nGemma3-27B achieve well above the passing threshold, while smaller open-source\nmodels (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results\nsuggest that only frontier LLMs are currently capable of meeting counseling\nexam standards, highlighting both the promise and the challenges of developing\npsychology-oriented LLMs. We release the proposed dataset for public use:\nhttps://github.com/cloversjtu/PsychCounsel-Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: \\textit{Can LLMs be effectively applied to\npsychological counseling?} To determine whether an LLM can effectively take on\nthe role of a psychological counselor, the first step is to assess whether it\nmeets the qualifications required for such a role, namely the ability to pass\nthe U.S. National Counselor Certification Exam (NCE). This is because, just as\na human counselor must pass a certification exam to practice, an LLM must\ndemonstrate sufficient psychological knowledge to meet the standards required\nfor such a role. To address this, we introduce PsychCounsel-Bench, a benchmark\ngrounded in U.S.national counselor examinations, a licensure test for\nprofessional counselors that requires about 70\\% accuracy to pass.\nPsychCounsel-Bench comprises approximately 2,252 carefully curated\nsingle-choice questions, crafted to require deep understanding and broad enough\nto cover various sub-disciplines of psychology. This benchmark provides a\ncomprehensive assessment of an LLM's ability to function as a counselor. Our\nevaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and\nGemma3-27B achieve well above the passing threshold, while smaller open-source\nmodels (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results\nsuggest that only frontier LLMs are currently capable of meeting counseling\nexam standards, highlighting both the promise and the challenges of developing\npsychology-oriented LLMs. We release the proposed dataset for public use:\nhttps://github.com/cloversjtu/PsychCounsel-Bench"
                },
                "authors": [
                    {
                        "name": "Min Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Min Zeng"
                },
                "author": "Min Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17611v1",
                "updated": "2025-10-20T14:57:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    57,
                    52,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:57:52Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    57,
                    52,
                    0,
                    293,
                    0
                ],
                "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum\n  Unsupervised Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum\n  Unsupervised Anomaly Detection"
                },
                "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications."
                },
                "authors": [
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Lei Fan"
                    },
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Weihang Zhang"
                    },
                    {
                        "name": "Wenbing Zhu"
                    },
                    {
                        "name": "Hong Yan"
                    },
                    {
                        "name": "Fang Chen"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Hongen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Hongen Liao"
                },
                "author": "Hongen Liao",
                "arxiv_comment": "Extended version of CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17604v1",
                "updated": "2025-10-20T14:52:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    50,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:52:50Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    50,
                    0,
                    293,
                    0
                ],
                "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Inertial Odometry for Cycling Based on Mixture of Experts\n  Algorithm"
                },
                "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%."
                },
                "authors": [
                    {
                        "name": "Hao Qiao"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xiaoyao Yu"
                    },
                    {
                        "name": "Jian kuang"
                    },
                    {
                        "name": "Xiaoji Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoji Niu"
                },
                "author": "Xiaoji Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12081v2",
                "updated": "2025-10-20T14:52:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    28,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-16T15:31:14Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    15,
                    31,
                    14,
                    5,
                    228,
                    0
                ],
                "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion\n  Language Models"
                },
                "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input. All the resources are available at\nhttps://walkermitty.github.io/VimoRAG/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input. All the resources are available at\nhttps://walkermitty.github.io/VimoRAG/"
                },
                "authors": [
                    {
                        "name": "Haidong Xu"
                    },
                    {
                        "name": "Guangwei Xu"
                    },
                    {
                        "name": "Zhedong Zheng"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Ruijie Guo"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Min zhang"
                    },
                    {
                        "name": "Hao Fei"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fei"
                },
                "author": "Hao Fei",
                "arxiv_comment": "Accepted by NeurIPS 2025; Project Page:\n  https://walkermitty.github.io/VimoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12814v2",
                "updated": "2025-10-20T14:52:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    7,
                    0,
                    293,
                    0
                ],
                "published": "2025-05-19T07:45:09Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    7,
                    45,
                    9,
                    0,
                    139,
                    0
                ],
                "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control\n  for Advanced Role-Playing LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control\n  for Advanced Role-Playing LLMs"
                },
                "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."
                },
                "authors": [
                    {
                        "name": "Xilong Cheng"
                    },
                    {
                        "name": "Yunxiao Qin"
                    },
                    {
                        "name": "Yuting Tan"
                    },
                    {
                        "name": "Zhengnan Li"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Hongjiang Xiao"
                    },
                    {
                        "name": "Yuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Zhang"
                },
                "author": "Yuan Zhang",
                "arxiv_comment": "Pre-MIT Press publication version, has been accepted by TACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17603v1",
                "updated": "2025-10-20T14:51:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    51,
                    14,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:51:14Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    51,
                    14,
                    0,
                    293,
                    0
                ],
                "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D\n  Modeling"
                },
                "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications."
                },
                "authors": [
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Zuoou Li"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "NeurIPS 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11168v3",
                "updated": "2025-10-20T14:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    49,
                    14,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-12T04:07:11Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    4,
                    7,
                    11,
                    3,
                    163,
                    0
                ],
                "title": "WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture\n  Recognition"
                },
                "summary": "Human-machine interaction, particularly in prosthetic and robotic control,\nhas seen progress with gesture recognition via surface electromyographic (sEMG)\nsignals.However, classifying similar gestures that produce nearly identical\nmuscle signals remains a challenge, often reducing classification accuracy.\nTraditional deep learning models for sEMG gesture recognition are large and\ncomputationally expensive, limiting their deployment on resource-constrained\nembedded systems. In this work, we propose WaveFormer, a lightweight\ntransformer-based architecture tailored for sEMG gesture recognition. Our model\nintegrates time-domain and frequency-domain features through a novel learnable\nwavelet transform, enhancing feature extraction. In particular, the WaveletConv\nmodule, a multi-level wavelet decomposition layer with depthwise separable\nconvolution, ensures both efficiency and compactness. With just 3.1 million\nparameters, WaveFormer achieves 95% classification accuracy on the EPN612\ndataset, outperforming larger models. Furthermore, when profiled on a laptop\nequipped with an Intel CPU, INT8 quantization achieves real-time deployment\nwith a 6.75 ms inference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-machine interaction, particularly in prosthetic and robotic control,\nhas seen progress with gesture recognition via surface electromyographic (sEMG)\nsignals.However, classifying similar gestures that produce nearly identical\nmuscle signals remains a challenge, often reducing classification accuracy.\nTraditional deep learning models for sEMG gesture recognition are large and\ncomputationally expensive, limiting their deployment on resource-constrained\nembedded systems. In this work, we propose WaveFormer, a lightweight\ntransformer-based architecture tailored for sEMG gesture recognition. Our model\nintegrates time-domain and frequency-domain features through a novel learnable\nwavelet transform, enhancing feature extraction. In particular, the WaveletConv\nmodule, a multi-level wavelet decomposition layer with depthwise separable\nconvolution, ensures both efficiency and compactness. With just 3.1 million\nparameters, WaveFormer achieves 95% classification accuracy on the EPN612\ndataset, outperforming larger models. Furthermore, when profiled on a laptop\nequipped with an Intel CPU, INT8 quantization achieves real-time deployment\nwith a 6.75 ms inference latency."
                },
                "authors": [
                    {
                        "name": "Yanlong Chen"
                    },
                    {
                        "name": "Mattia Orlandi"
                    },
                    {
                        "name": "Pierangelo Maria Rapa"
                    },
                    {
                        "name": "Simone Benatti"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Yawei Li"
                    }
                ],
                "author_detail": {
                    "name": "Yawei Li"
                },
                "author": "Yawei Li",
                "arxiv_comment": "6 pages, 3 figures, accepted to IEEE EMBS Conference on Neural\n  Engineering (NER) 2025. Code and data are available at\n  https://github.com/ForeverBlue816/WaveFormer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17593v1",
                "updated": "2025-10-20T14:43:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    43,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:43:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    43,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "Paradoxical increase of capacity due to spurious overlaps in attractor\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paradoxical increase of capacity due to spurious overlaps in attractor\n  networks"
                },
                "summary": "In Hopfield-type associative memory models, memories are stored in the\nconnectivity matrix and can be retrieved subsequently thanks to the collective\ndynamics of the network. In these models, the retrieval of a particular memory\ncan be hampered by overlaps between the network state and other memories,\ntermed spurious overlaps since these overlaps collectively introduce noise in\nthe retrieval process. In classic models, spurious overlaps increase the\nvariance of synaptic inputs but do not affect the mean. We show here that in\nmodels equipped with a learning rule inferred from neurobiological data,\nspurious overlaps collectively reduce the mean synaptic inputs to neurons, and\nthat this mean reduction causes in turn an increase in storage capacity through\na sparsening of network activity. Our paper demonstrates a link between a\nspecific feature of experimentally inferred plasticity rules and network\nstorage capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Hopfield-type associative memory models, memories are stored in the\nconnectivity matrix and can be retrieved subsequently thanks to the collective\ndynamics of the network. In these models, the retrieval of a particular memory\ncan be hampered by overlaps between the network state and other memories,\ntermed spurious overlaps since these overlaps collectively introduce noise in\nthe retrieval process. In classic models, spurious overlaps increase the\nvariance of synaptic inputs but do not affect the mean. We show here that in\nmodels equipped with a learning rule inferred from neurobiological data,\nspurious overlaps collectively reduce the mean synaptic inputs to neurons, and\nthat this mean reduction causes in turn an increase in storage capacity through\na sparsening of network activity. Our paper demonstrates a link between a\nspecific feature of experimentally inferred plasticity rules and network\nstorage capacity."
                },
                "authors": [
                    {
                        "name": "Marco Benedetti"
                    },
                    {
                        "name": "Nicolas Brunel"
                    },
                    {
                        "name": "Enzo Marinari"
                    },
                    {
                        "name": "Ulises Pereira Obilinovic"
                    }
                ],
                "author_detail": {
                    "name": "Ulises Pereira Obilinovic"
                },
                "author": "Ulises Pereira Obilinovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17590v1",
                "updated": "2025-10-20T14:40:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    40,
                    26,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:40:26Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    40,
                    26,
                    0,
                    293,
                    0
                ],
                "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with\n  Web-Grounded Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with\n  Web-Grounded Reasoning"
                },
                "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce."
                },
                "authors": [
                    {
                        "name": "Mir Nafis Sharear Shopnil"
                    },
                    {
                        "name": "Sharad Duwal"
                    },
                    {
                        "name": "Abhishek Tyagi"
                    },
                    {
                        "name": "Adiba Mahbub Proma"
                    }
                ],
                "author_detail": {
                    "name": "Adiba Mahbub Proma"
                },
                "author": "Adiba Mahbub Proma",
                "arxiv_comment": "16 pages, 3 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17586v2",
                "updated": "2025-10-21T05:15:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    5,
                    15,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T14:35:19Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    35,
                    19,
                    0,
                    293,
                    0
                ],
                "title": "DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework"
                },
                "summary": "Large language models (LLMs) have advanced Text-to-SQL, yet existing\nsolutions still fall short of system-level reliability. The limitation is not\nmerely in individual modules - e.g., schema linking, reasoning, and\nverification - but more critically in the lack of structured orchestration that\nenforces correctness across the entire workflow. This gap motivates a paradigm\nshift: treating Text-to-SQL not as free-form language generation but as a\nsoftware-engineering problem that demands structured, verifiable orchestration.\nWe present DeepEye-SQL, a software-engineering-inspired framework that reframes\nText-to-SQL as the development of a small software program, executed through a\nverifiable process guided by the Software Development Life Cycle (SDLC).\nDeepEye-SQL integrates four synergistic stages: it grounds ambiguous user\nintent through semantic value retrieval and robust schema linking; enhances\nfault tolerance with N-version SQL generation using diverse reasoning\nparadigms; ensures deterministic verification via a tool-chain of unit tests\nand targeted LLM-guided revision; and introduces confidence-aware selection\nthat clusters execution results to estimate confidence and then takes a\nhigh-confidence shortcut or runs unbalanced pairwise adjudication in\nlow-confidence cases, yielding a calibrated, quality-gated output. This\nSDLC-aligned workflow transforms ad hoc query generation into a disciplined\nengineering process. Using ~30B open-source LLMs without any fine-tuning,\nDeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on\nSpider-Test, outperforming state-of-the-art solutions. This highlights that\nprincipled orchestration, rather than LLM scaling alone, is key to achieving\nsystem-level reliability in Text-to-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced Text-to-SQL, yet existing\nsolutions still fall short of system-level reliability. The limitation is not\nmerely in individual modules - e.g., schema linking, reasoning, and\nverification - but more critically in the lack of structured orchestration that\nenforces correctness across the entire workflow. This gap motivates a paradigm\nshift: treating Text-to-SQL not as free-form language generation but as a\nsoftware-engineering problem that demands structured, verifiable orchestration.\nWe present DeepEye-SQL, a software-engineering-inspired framework that reframes\nText-to-SQL as the development of a small software program, executed through a\nverifiable process guided by the Software Development Life Cycle (SDLC).\nDeepEye-SQL integrates four synergistic stages: it grounds ambiguous user\nintent through semantic value retrieval and robust schema linking; enhances\nfault tolerance with N-version SQL generation using diverse reasoning\nparadigms; ensures deterministic verification via a tool-chain of unit tests\nand targeted LLM-guided revision; and introduces confidence-aware selection\nthat clusters execution results to estimate confidence and then takes a\nhigh-confidence shortcut or runs unbalanced pairwise adjudication in\nlow-confidence cases, yielding a calibrated, quality-gated output. This\nSDLC-aligned workflow transforms ad hoc query generation into a disciplined\nengineering process. Using ~30B open-source LLMs without any fine-tuning,\nDeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on\nSpider-Test, outperforming state-of-the-art solutions. This highlights that\nprincipled orchestration, rather than LLM scaling alone, is key to achieving\nsystem-level reliability in Text-to-SQL."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Zhujun Xue"
                    },
                    {
                        "name": "Yinan Mei"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17582v1",
                "updated": "2025-10-20T14:31:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    31,
                    43,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:31:43Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    31,
                    43,
                    0,
                    293,
                    0
                ],
                "title": "Non-interference analysis of bounded labeled Petri nets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-interference analysis of bounded labeled Petri nets"
                },
                "summary": "This paper focuses on a fundamental problem on information security of\nbounded labeled Petri nets: non-interference analysis. As in hierarchical\ncontrol, we assume that a system is observed by users at different levels,\nnamely high-level users and low-level users. The output events produced by the\nfiring of transitions are also partitioned into high-level output events and\nlow-level output events. In general, high-level users can observe the\noccurrence of all the output events, while low-level users can only observe the\noccurrence of low-level output events. A system is said to be non-interferent\nif low-level users cannot infer the firing of transitions labeled with\nhigh-level output events by looking at low-level outputs. In this paper, we\nstudy a particular non-interference property, namely strong non-deterministic\nnon-interference (SNNI), using a special automaton called SNNI Verifier, and\npropose a necessary and sufficient condition for SNNI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on a fundamental problem on information security of\nbounded labeled Petri nets: non-interference analysis. As in hierarchical\ncontrol, we assume that a system is observed by users at different levels,\nnamely high-level users and low-level users. The output events produced by the\nfiring of transitions are also partitioned into high-level output events and\nlow-level output events. In general, high-level users can observe the\noccurrence of all the output events, while low-level users can only observe the\noccurrence of low-level output events. A system is said to be non-interferent\nif low-level users cannot infer the firing of transitions labeled with\nhigh-level output events by looking at low-level outputs. In this paper, we\nstudy a particular non-interference property, namely strong non-deterministic\nnon-interference (SNNI), using a special automaton called SNNI Verifier, and\npropose a necessary and sufficient condition for SNNI."
                },
                "authors": [
                    {
                        "name": "Ning Ran"
                    },
                    {
                        "name": "Zhengguang Wu"
                    },
                    {
                        "name": "Shaokang Zhang"
                    },
                    {
                        "name": "Zhou He"
                    },
                    {
                        "name": "Carla Seatzu"
                    }
                ],
                "author_detail": {
                    "name": "Carla Seatzu"
                },
                "author": "Carla Seatzu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01847v2",
                "updated": "2025-10-20T14:30:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    30,
                    46,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-03T17:02:55Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    17,
                    2,
                    55,
                    6,
                    215,
                    0
                ],
                "title": "Test-Time Training for Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Training for Speech Enhancement"
                },
                "summary": "This paper introduces a novel application of Test-Time Training (TTT) for\nSpeech Enhancement, addressing the challenges posed by unpredictable noise\nconditions and domain shifts. This method combines a main speech enhancement\ntask with a self-supervised auxiliary task in a Y-shaped architecture. The\nmodel dynamically adapts to new domains during inference time by optimizing the\nproposed self-supervised tasks like noise-augmented signal reconstruction or\nmasked spectrogram prediction, bypassing the need for labeled data. We further\nintroduce various TTT strategies offering a trade-off between adaptation and\nefficiency. Evaluations across synthetic and real-world datasets show\nconsistent improvements across speech quality metrics, outperforming the\nbaseline model. This work highlights the effectiveness of TTT in speech\nenhancement, providing insights for future research in adaptive and robust\nspeech processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel application of Test-Time Training (TTT) for\nSpeech Enhancement, addressing the challenges posed by unpredictable noise\nconditions and domain shifts. This method combines a main speech enhancement\ntask with a self-supervised auxiliary task in a Y-shaped architecture. The\nmodel dynamically adapts to new domains during inference time by optimizing the\nproposed self-supervised tasks like noise-augmented signal reconstruction or\nmasked spectrogram prediction, bypassing the need for labeled data. We further\nintroduce various TTT strategies offering a trade-off between adaptation and\nefficiency. Evaluations across synthetic and real-world datasets show\nconsistent improvements across speech quality metrics, outperforming the\nbaseline model. This work highlights the effectiveness of TTT in speech\nenhancement, providing insights for future research in adaptive and robust\nspeech processing."
                },
                "authors": [
                    {
                        "name": "Avishkar Behera"
                    },
                    {
                        "name": "Riya Ann Easow"
                    },
                    {
                        "name": "Venkatesh Parvathala"
                    },
                    {
                        "name": "K. Sri Rama Murty"
                    }
                ],
                "author_detail": {
                    "name": "K. Sri Rama Murty"
                },
                "author": "K. Sri Rama Murty",
                "arxiv_doi": "10.21437/Interspeech.2025-2725",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21437/Interspeech.2025-2725",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of Interspeech 2025",
                "arxiv_journal_ref": "Proceedings of Interspeech 2025, pp. 2375-2379",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00432v2",
                "updated": "2025-10-20T14:27:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    27,
                    9,
                    0,
                    293,
                    0
                ],
                "published": "2025-07-01T05:23:05Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    23,
                    5,
                    1,
                    182,
                    0
                ],
                "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning"
                },
                "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models."
                },
                "authors": [
                    {
                        "name": "Maggie Huan"
                    },
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Tuney Zheng"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Minxin Du"
                    },
                    {
                        "name": "Radha Poovendran"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17576v1",
                "updated": "2025-10-20T14:24:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    24,
                    39,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:24:39Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    24,
                    39,
                    0,
                    293,
                    0
                ],
                "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot\n  Disassembly: Demonstration on EV Batteries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot\n  Disassembly: Demonstration on EV Batteries"
                },
                "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort."
                },
                "authors": [
                    {
                        "name": "Cansu Erdogan"
                    },
                    {
                        "name": "Cesar Alan Contreras"
                    },
                    {
                        "name": "Alireza Rastegarpanah"
                    },
                    {
                        "name": "Manolis Chiou"
                    },
                    {
                        "name": "Rustam Stolkin"
                    }
                ],
                "author_detail": {
                    "name": "Rustam Stolkin"
                },
                "author": "Rustam Stolkin",
                "arxiv_comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17575v1",
                "updated": "2025-10-20T14:22:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    22,
                    57,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:22:57Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    22,
                    57,
                    0,
                    293,
                    0
                ],
                "title": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support"
                },
                "summary": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research."
                },
                "authors": [
                    {
                        "name": "Ash Sharma"
                    },
                    {
                        "name": "Karen Cochrane"
                    },
                    {
                        "name": "James R. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "James R. Wallace"
                },
                "author": "James R. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17568v1",
                "updated": "2025-10-20T14:17:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    17,
                    16,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:17:16Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    17,
                    16,
                    0,
                    293,
                    0
                ],
                "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception"
                },
                "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction."
                },
                "authors": [
                    {
                        "name": "Kaichen Zhou"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Grace Chen"
                    },
                    {
                        "name": "Xinhai Chang"
                    },
                    {
                        "name": "Gaspard Beaudouin"
                    },
                    {
                        "name": "Fangneng Zhan"
                    },
                    {
                        "name": "Paul Pu Liang"
                    },
                    {
                        "name": "Mengyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyu Wang"
                },
                "author": "Mengyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17566v1",
                "updated": "2025-10-20T14:13:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    13,
                    26,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:13:26Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    13,
                    26,
                    0,
                    293,
                    0
                ],
                "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for\n  End-to-End Weakly-Supervised Road Crack Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WP-CrackNet: A Collaborative Adversarial Learning Framework for\n  End-to-End Weakly-Supervised Road Crack Detection"
                },
                "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/."
                },
                "authors": [
                    {
                        "name": "Nachuan Ma"
                    },
                    {
                        "name": "Zhengfei Song"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Xiaoyu Tang"
                    },
                    {
                        "name": "Chengxi Zhang"
                    },
                    {
                        "name": "Rui Fan"
                    },
                    {
                        "name": "Lihua Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Xie"
                },
                "author": "Lihua Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17565v1",
                "updated": "2025-10-20T14:13:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    13,
                    19,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:13:19Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    13,
                    19,
                    0,
                    293,
                    0
                ],
                "title": "MEGADES: MEGARA galaxy disc evolution survey. Ionised gas diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEGADES: MEGARA galaxy disc evolution survey. Ionised gas diagnosis"
                },
                "summary": "We present the ionised gas properties and metallicity gradients of 43\ngalaxies observed by the MEGADES survey. Using the MEGARA (Multi-Espectrografo\nen GTC de Alta Resolucion para Astronomia) instrument, our data combine\nrelatively high spectral (R ~ 6000) and spatial (0.62 arcsec) resolution to\nstudy the ionised gas through classic BPT diagnostics in the [N II] and [S II]\nvariants. We examine how these diagrams vary with radius and with the velocity\ndispersion of the H-alpha line, and we propose a new diagnostic based on the\nratio between the velocity dispersions of the [N II] 6584 and H-alpha lines to\ndisentangle the relative roles of AGN, shocks and H II regions. Many regions,\nregardless of galactocentric distance, show shock-like emission, inferred from\ntheir line ratios (high [N II]6584/H-alpha and intermediate [O III]5007/H-beta)\nand their location between H II and AGN regimes. This improved selection of H\nII-like regions enables robust oxygen abundance estimates using the N2\nindicator. Most galaxies show negligible metallicity gradients, especially\nlow-abundance (<8.37 dex) fast rotators, with an average slope of 0.005 dex\nRe^-1 and a dispersion of 0.42 dex Re^-1. Above 8.37 dex, fast rotators display\nmildly negative gradients (mean -0.68 dex Re^-1, dispersion 0.93). For the full\nMEGADES sample, the mean gradient is -0.03 dex Re^-1 with a dispersion of 0.77.\nWe discuss the implications for the influence of galactic winds on abundance\ngradients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the ionised gas properties and metallicity gradients of 43\ngalaxies observed by the MEGADES survey. Using the MEGARA (Multi-Espectrografo\nen GTC de Alta Resolucion para Astronomia) instrument, our data combine\nrelatively high spectral (R ~ 6000) and spatial (0.62 arcsec) resolution to\nstudy the ionised gas through classic BPT diagnostics in the [N II] and [S II]\nvariants. We examine how these diagrams vary with radius and with the velocity\ndispersion of the H-alpha line, and we propose a new diagnostic based on the\nratio between the velocity dispersions of the [N II] 6584 and H-alpha lines to\ndisentangle the relative roles of AGN, shocks and H II regions. Many regions,\nregardless of galactocentric distance, show shock-like emission, inferred from\ntheir line ratios (high [N II]6584/H-alpha and intermediate [O III]5007/H-beta)\nand their location between H II and AGN regimes. This improved selection of H\nII-like regions enables robust oxygen abundance estimates using the N2\nindicator. Most galaxies show negligible metallicity gradients, especially\nlow-abundance (<8.37 dex) fast rotators, with an average slope of 0.005 dex\nRe^-1 and a dispersion of 0.42 dex Re^-1. Above 8.37 dex, fast rotators display\nmildly negative gradients (mean -0.68 dex Re^-1, dispersion 0.93). For the full\nMEGADES sample, the mean gradient is -0.03 dex Re^-1 with a dispersion of 0.77.\nWe discuss the implications for the influence of galactic winds on abundance\ngradients."
                },
                "authors": [
                    {
                        "name": "M. Chamorro-Cazorla"
                    },
                    {
                        "name": "A. Gil de Paz"
                    },
                    {
                        "name": "A. Castillo-Morales"
                    },
                    {
                        "name": "A. Camps-Fariña"
                    },
                    {
                        "name": "J. Gallego"
                    },
                    {
                        "name": "E. Carrasco"
                    },
                    {
                        "name": "J. Iglesias-Páramo"
                    },
                    {
                        "name": "R. Cedazo"
                    },
                    {
                        "name": "M. L. García-Vargas"
                    },
                    {
                        "name": "S. Pascual"
                    },
                    {
                        "name": "N. Cardiel"
                    },
                    {
                        "name": "A. Pérez-Calpena"
                    },
                    {
                        "name": "P. Gómez-Álvarez"
                    },
                    {
                        "name": "I. Martínez-Delgado"
                    },
                    {
                        "name": "C. Catalán-Torrecilla"
                    },
                    {
                        "name": "J. Zamorano"
                    }
                ],
                "author_detail": {
                    "name": "J. Zamorano"
                },
                "author": "J. Zamorano",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17561v1",
                "updated": "2025-10-20T14:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    8,
                    58,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    8,
                    58,
                    0,
                    293,
                    0
                ],
                "title": "Spectral Thresholds in Correlated Spiked Models and Fundamental Limits\n  of Partial Least Squares",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Thresholds in Correlated Spiked Models and Fundamental Limits\n  of Partial Least Squares"
                },
                "summary": "We provide a rigorous random matrix theory analysis of spiked\ncross-covariance models where the signals across two high-dimensional data\nchannels are partially aligned. These models are motivated by multi-modal\nlearning and form the standard generative setting underlying Partial Least\nSquares (PLS), a widely used yet theoretically underdeveloped method. We show\nthat the leading singular values of the sample cross-covariance matrix undergo\na Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the\nprecise thresholds for the emergence of informative components. Our results\nyield the first sharp asymptotic description of the signal recovery\ncapabilities of PLS in this setting, revealing a fundamental performance gap\nbetween PLS and the Bayes-optimal estimator. In particular, we identify the SNR\nand correlation regimes where PLS fails to recover any signal, despite\ndetectability being possible in principle. These findings clarify the\ntheoretical limits of PLS and provide guidance for the design of reliable\nmulti-modal inference methods in high dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a rigorous random matrix theory analysis of spiked\ncross-covariance models where the signals across two high-dimensional data\nchannels are partially aligned. These models are motivated by multi-modal\nlearning and form the standard generative setting underlying Partial Least\nSquares (PLS), a widely used yet theoretically underdeveloped method. We show\nthat the leading singular values of the sample cross-covariance matrix undergo\na Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the\nprecise thresholds for the emergence of informative components. Our results\nyield the first sharp asymptotic description of the signal recovery\ncapabilities of PLS in this setting, revealing a fundamental performance gap\nbetween PLS and the Bayes-optimal estimator. In particular, we identify the SNR\nand correlation regimes where PLS fails to recover any signal, despite\ndetectability being possible in principle. These findings clarify the\ntheoretical limits of PLS and provide guidance for the design of reliable\nmulti-modal inference methods in high dimensions."
                },
                "authors": [
                    {
                        "name": "Pierre Mergny"
                    },
                    {
                        "name": "Lenka Zdeborová"
                    }
                ],
                "author_detail": {
                    "name": "Lenka Zdeborová"
                },
                "author": "Lenka Zdeborová",
                "arxiv_comment": "24 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17555v1",
                "updated": "2025-10-20T14:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    2,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:02:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    2,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "Language Confusion Gate: Language-Aware Decoding Through Model\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Confusion Gate: Language-Aware Decoding Through Model\n  Self-Distillation"
                },
                "summary": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate."
                },
                "authors": [
                    {
                        "name": "Collin Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17553v1",
                "updated": "2025-10-20T14:01:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    1,
                    13,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:01:13Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    1,
                    13,
                    0,
                    293,
                    0
                ],
                "title": "Relaxing the Assumption of Strongly Non-Informative Linkage Error in\n  Secondary Regression Analysis of Linked Files",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxing the Assumption of Strongly Non-Informative Linkage Error in\n  Secondary Regression Analysis of Linked Files"
                },
                "summary": "Data analysis of files that are a result of linking records from multiple\nsources are often affected by linkage errors. Records may be linked\nincorrectly, or their links may be missed. In consequence, it is essential that\nsuch errors are taken into account to ensure valid post-linkage inference.\nHere, we propose an extension to a general framework for regression with linked\ncovariates and responses based on a two-component mixture model, which was\ndeveloped in prior work. This framework addresses the challenging case of\nsecondary analysis in which only the linked data is available and information\nabout the record linkage process is limited. The extension considered herein\nrelaxes the assumption of strongly non-informative linkage in the framework\naccording to which linkage does not depend on the covariates used in the\nanalysis, which may be limiting in practice. The effectiveness of the proposed\nextension is investigated by simulations and a case study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data analysis of files that are a result of linking records from multiple\nsources are often affected by linkage errors. Records may be linked\nincorrectly, or their links may be missed. In consequence, it is essential that\nsuch errors are taken into account to ensure valid post-linkage inference.\nHere, we propose an extension to a general framework for regression with linked\ncovariates and responses based on a two-component mixture model, which was\ndeveloped in prior work. This framework addresses the challenging case of\nsecondary analysis in which only the linked data is available and information\nabout the record linkage process is limited. The extension considered herein\nrelaxes the assumption of strongly non-informative linkage in the framework\naccording to which linkage does not depend on the covariates used in the\nanalysis, which may be limiting in practice. The effectiveness of the proposed\nextension is investigated by simulations and a case study."
                },
                "authors": [
                    {
                        "name": "Priyanjali Bukke"
                    },
                    {
                        "name": "Martin Slawski"
                    }
                ],
                "author_detail": {
                    "name": "Martin Slawski"
                },
                "author": "Martin Slawski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17543v1",
                "updated": "2025-10-20T13:52:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    52,
                    58,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:52:58Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    52,
                    58,
                    0,
                    293,
                    0
                ],
                "title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment"
                },
                "summary": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17535v1",
                "updated": "2025-10-20T13:39:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    39,
                    48,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:39:48Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    39,
                    48,
                    0,
                    293,
                    0
                ],
                "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How role-play shapes relevance judgment in zero-shot LLM rankers"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications."
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Catherine Chen"
                    },
                    {
                        "name": "Panagiotis Eustratiadis"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12024v3",
                "updated": "2025-10-21T07:03:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    7,
                    3,
                    45,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-21T07:42:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    42,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "FlexQuant: A Flexible and Efficient Dynamic Precision Switching\n  Framework for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQuant: A Flexible and Efficient Dynamic Precision Switching\n  Framework for LLM Quantization"
                },
                "summary": "The rapid advancement of large language models (LLMs) has exacerbated the\nmemory bottleneck due to the widening gap between model parameter scaling and\nhardware capabilities. While post-training quantization techniques effectively\nreduce memory overhead, existing methods predominantly rely on static\nquantization strategies, which struggle to adapt to dynamic workloads. To\naddress this, we propose FlexQuant, a dynamic precision-switching framework\nthat optimizes the trade-off between inference speed and accuracy. Leveraging\nmodel perplexity entropy and Kullback-Leibler divergence, FlexQuant enables\nfine-grained, layer-wise mixed-precision quantization and dynamically adjusts\nbit-widths during each token generation. FlexQuant provides a comprehensive\nanalysis of quantization strategies, introduces a precision requirement model\nfor optimal switching, and implements efficient fine-grained precision\nmanagement. Evaluations demonstrate that FlexQuant achieves a 1.3x end-to-end\nspeedup across diverse language tasks with negligible accuracy loss introduced.\nThis framework offers a flexible and adaptive solution for efficient LLM\ndeployment. Code is released at https://github.com/ZongwuWang/FlexQuant.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has exacerbated the\nmemory bottleneck due to the widening gap between model parameter scaling and\nhardware capabilities. While post-training quantization techniques effectively\nreduce memory overhead, existing methods predominantly rely on static\nquantization strategies, which struggle to adapt to dynamic workloads. To\naddress this, we propose FlexQuant, a dynamic precision-switching framework\nthat optimizes the trade-off between inference speed and accuracy. Leveraging\nmodel perplexity entropy and Kullback-Leibler divergence, FlexQuant enables\nfine-grained, layer-wise mixed-precision quantization and dynamically adjusts\nbit-widths during each token generation. FlexQuant provides a comprehensive\nanalysis of quantization strategies, introduces a precision requirement model\nfor optimal switching, and implements efficient fine-grained precision\nmanagement. Evaluations demonstrate that FlexQuant achieves a 1.3x end-to-end\nspeedup across diverse language tasks with negligible accuracy loss introduced.\nThis framework offers a flexible and adaptive solution for efficient LLM\ndeployment. Code is released at https://github.com/ZongwuWang/FlexQuant.git."
                },
                "authors": [
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "JinHong Xia"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Shouren Zhao"
                    },
                    {
                        "name": "Jinjin Li"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "10 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17534v1",
                "updated": "2025-10-20T13:37:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    37,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:37:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    37,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based\n  Guidance"
                },
                "summary": "Today's young people are facing increasing psychological stress due to\nvarious social issues. Traditional stress management tools often rely on static\nscripts or passive content, which are ineffective in alleviating stress. NieNie\naddresses this gap by combining rhythm biofeedback with real-time psychological\nguidance through a large language model (LLM), offering an interactive, tactile\nresponse. The system is specifically designed for young people experiencing\nemotional stress, collecting physiological signals such as heart rate\nvariability and generating adaptive squeeze-release rhythms via soft, tactile\ndevices. Utilising LLM, the system provides timely squeezing rhythms and\npsychologically guided feedback prompts, offering personalised rhythm games\nwhile reinforcing stress restructuring. Unlike traditional mental health apps,\nNieNie places users within an embodied interactive loop, leveraging tactile\ninteraction, biofeedback, and adaptive language support to create an immersive\nstress regulation experience. This study demonstrates how embodied systems can\nconnect bodily actions with mental health in everyday contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's young people are facing increasing psychological stress due to\nvarious social issues. Traditional stress management tools often rely on static\nscripts or passive content, which are ineffective in alleviating stress. NieNie\naddresses this gap by combining rhythm biofeedback with real-time psychological\nguidance through a large language model (LLM), offering an interactive, tactile\nresponse. The system is specifically designed for young people experiencing\nemotional stress, collecting physiological signals such as heart rate\nvariability and generating adaptive squeeze-release rhythms via soft, tactile\ndevices. Utilising LLM, the system provides timely squeezing rhythms and\npsychologically guided feedback prompts, offering personalised rhythm games\nwhile reinforcing stress restructuring. Unlike traditional mental health apps,\nNieNie places users within an embodied interactive loop, leveraging tactile\ninteraction, biofeedback, and adaptive language support to create an immersive\nstress regulation experience. This study demonstrates how embodied systems can\nconnect bodily actions with mental health in everyday contexts."
                },
                "authors": [
                    {
                        "name": "Yichen Yu"
                    },
                    {
                        "name": "Qiaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoran Wang"
                },
                "author": "Qiaoran Wang",
                "arxiv_doi": "10.1145/3714394.3750586",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3714394.3750586",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.17534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17532v1",
                "updated": "2025-10-20T13:35:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    35,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:35:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    35,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and\n  Interpretable Survival Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and\n  Interpretable Survival Prediction"
                },
                "summary": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology."
                },
                "authors": [
                    {
                        "name": "Raghu Vamshi Hemadri"
                    },
                    {
                        "name": "Geetha Krishna Guruju"
                    },
                    {
                        "name": "Kristi Topollai"
                    },
                    {
                        "name": "Anna Ewa Choromanska"
                    }
                ],
                "author_detail": {
                    "name": "Anna Ewa Choromanska"
                },
                "author": "Anna Ewa Choromanska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00161v2",
                "updated": "2025-10-20T13:29:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    29,
                    29,
                    0,
                    293,
                    0
                ],
                "published": "2025-07-31T21:04:12Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    4,
                    12,
                    3,
                    212,
                    0
                ],
                "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch the Weights: Unsupervised monitoring and control of fine-tuned\n  LLMs"
                },
                "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch."
                },
                "authors": [
                    {
                        "name": "Ziqian Zhong"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Raghunathan"
                },
                "author": "Aditi Raghunathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17519v1",
                "updated": "2025-10-20T13:20:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    20,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:20:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    20,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models"
                },
                "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}."
                },
                "authors": [
                    {
                        "name": "Yongshun Zhang"
                    },
                    {
                        "name": "Zhongyi Fan"
                    },
                    {
                        "name": "Yonghang Zhang"
                    },
                    {
                        "name": "Zhangzikang Li"
                    },
                    {
                        "name": "Weifeng Chen"
                    },
                    {
                        "name": "Zhongwei Feng"
                    },
                    {
                        "name": "Chaoyue Wang"
                    },
                    {
                        "name": "Peng Hou"
                    },
                    {
                        "name": "Anxiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Anxiang Zeng"
                },
                "author": "Anxiang Zeng",
                "arxiv_comment": "Technical Report; Project Page:\n  \\href{https://github.com/Shopee-MUG/MUG-V}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17516v1",
                "updated": "2025-10-20T13:14:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    14,
                    38,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    14,
                    38,
                    0,
                    293,
                    0
                ],
                "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors"
                },
                "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators."
                },
                "authors": [
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Paul Röttger"
                    }
                ],
                "author_detail": {
                    "name": "Paul Röttger"
                },
                "author": "Paul Röttger",
                "arxiv_comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17509v1",
                "updated": "2025-10-20T13:05:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    5,
                    22,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:05:22Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    5,
                    22,
                    0,
                    293,
                    0
                ],
                "title": "Annotation-Efficient Universal Honesty Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotation-Efficient Universal Honesty Alignment"
                },
                "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Jingtong Wu"
                    },
                    {
                        "name": "Zengxin Han"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17501v1",
                "updated": "2025-10-20T12:54:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:54:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"
                },
                "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization."
                },
                "authors": [
                    {
                        "name": "Yuanli Wu"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Yue Du"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12289v2",
                "updated": "2025-10-20T12:53:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    53,
                    46,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-14T08:49:40Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    49,
                    40,
                    1,
                    287,
                    0
                ],
                "title": "Nonparametric Identification and Estimation of Spatial Treatment Effect\n  Boundaries: Evidence from 42 Million Pollution Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Identification and Estimation of Spatial Treatment Effect\n  Boundaries: Evidence from 42 Million Pollution Observations"
                },
                "summary": "This paper develops a nonparametric framework for identifying and estimating\nspatial boundaries of treatment effects in settings with geographic spillovers.\nWhile atmospheric dispersion theory predicts exponential decay of pollution\nunder idealized assumptions, these assumptions -- steady winds, homogeneous\natmospheres, flat terrain -- are systematically violated in practice. I\nestablish nonparametric identification of spatial boundaries under weak\nsmoothness and monotonicity conditions, propose a kernel-based estimator with\ndata-driven bandwidth selection, and derive asymptotic theory for inference.\nUsing 42 million satellite observations of NO$_2$ concentrations near coal\nplants (2019-2021), I find that nonparametric kernel regression reduces\nprediction errors by 1.0 percentage point on average compared to parametric\nexponential decay assumptions, with largest improvements at policy-relevant\ndistances: 2.8 percentage points at 10 km (near-source impacts) and 3.7\npercentage points at 100 km (long-range transport). Parametric methods\nsystematically underestimate near-source concentrations while overestimating\nlong-range decay. The COVID-19 pandemic provides a natural experiment\nvalidating the framework's temporal sensitivity: NO$_2$ concentrations dropped\n4.6\\% in 2020, then recovered 5.7\\% in 2021. These results demonstrate that\nflexible, data-driven spatial methods substantially outperform restrictive\nparametric assumptions in environmental policy applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a nonparametric framework for identifying and estimating\nspatial boundaries of treatment effects in settings with geographic spillovers.\nWhile atmospheric dispersion theory predicts exponential decay of pollution\nunder idealized assumptions, these assumptions -- steady winds, homogeneous\natmospheres, flat terrain -- are systematically violated in practice. I\nestablish nonparametric identification of spatial boundaries under weak\nsmoothness and monotonicity conditions, propose a kernel-based estimator with\ndata-driven bandwidth selection, and derive asymptotic theory for inference.\nUsing 42 million satellite observations of NO$_2$ concentrations near coal\nplants (2019-2021), I find that nonparametric kernel regression reduces\nprediction errors by 1.0 percentage point on average compared to parametric\nexponential decay assumptions, with largest improvements at policy-relevant\ndistances: 2.8 percentage points at 10 km (near-source impacts) and 3.7\npercentage points at 100 km (long-range transport). Parametric methods\nsystematically underestimate near-source concentrations while overestimating\nlong-range decay. The COVID-19 pandemic provides a natural experiment\nvalidating the framework's temporal sensitivity: NO$_2$ concentrations dropped\n4.6\\% in 2020, then recovered 5.7\\% in 2021. These results demonstrate that\nflexible, data-driven spatial methods substantially outperform restrictive\nparametric assumptions in environmental policy applications."
                },
                "authors": [
                    {
                        "name": "Tatsuru Kikuchi"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuru Kikuchi"
                },
                "author": "Tatsuru Kikuchi",
                "arxiv_comment": "53 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17496v1",
                "updated": "2025-10-20T12:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    51,
                    13,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:51:13Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    51,
                    13,
                    0,
                    293,
                    0
                ],
                "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models"
                },
                "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."
                },
                "authors": [
                    {
                        "name": "Giacomo Camposampiero"
                    },
                    {
                        "name": "Michael Hersche"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Abbas Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rahimi"
                },
                "author": "Abbas Rahimi",
                "arxiv_comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17491v1",
                "updated": "2025-10-20T12:46:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:46:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "title": "Empowering Real-World: A Survey on the Technology, Practice, and\n  Evaluation of LLM-driven Industry Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Real-World: A Survey on the Technology, Practice, and\n  Evaluation of LLM-driven Industry Agents"
                },
                "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents."
                },
                "authors": [
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Liang Yue"
                    },
                    {
                        "name": "Jinxin Fan"
                    },
                    {
                        "name": "Caishen Zhou"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Mingming Zhao"
                    },
                    {
                        "name": "Shixiong Kai"
                    },
                    {
                        "name": "Kaiyang Guo"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Wenjing Cun"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17489v1",
                "updated": "2025-10-20T12:41:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    41,
                    44,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:41:44Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    41,
                    44,
                    0,
                    293,
                    0
                ],
                "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured\n  Hierarchical Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured\n  Hierarchical Representation Learning"
                },
                "summary": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree."
                },
                "authors": [
                    {
                        "name": "Yongxin He"
                    },
                    {
                        "name": "Shan Zhang"
                    },
                    {
                        "name": "Yixuan Cao"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "To appear in NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00783v2",
                "updated": "2025-10-20T12:31:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    31,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-01T02:20:45Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    2,
                    20,
                    45,
                    6,
                    152,
                    0
                ],
                "title": "KG-TRACES: Enhancing Large Language Models with Knowledge\n  Graph-constrained Trajectory Reasoning and Attribution Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-TRACES: Enhancing Large Language Models with Knowledge\n  Graph-constrained Trajectory Reasoning and Attribution Supervision"
                },
                "summary": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES."
                },
                "authors": [
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17483v1",
                "updated": "2025-10-20T12:27:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    27,
                    55,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:27:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    27,
                    55,
                    0,
                    293,
                    0
                ],
                "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs."
                },
                "authors": [
                    {
                        "name": "Zheyue Tan"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Yueqing Zhuang"
                    },
                    {
                        "name": "Yadong Li"
                    },
                    {
                        "name": "Guowei Niu"
                    },
                    {
                        "name": "Cheng Qin"
                    },
                    {
                        "name": "Zhuyu Yao"
                    },
                    {
                        "name": "Congyi Liu"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Boxun Li"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17478v1",
                "updated": "2025-10-20T12:22:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    22,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:22:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    22,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "Towards geological inference with process-based and deep generative\n  modeling, part 2: inversion of fluvial deposits and latent-space\n  disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards geological inference with process-based and deep generative\n  modeling, part 2: inversion of fluvial deposits and latent-space\n  disentanglement"
                },
                "summary": "High costs and uncertainties make subsurface decision-making challenging, as\nacquiring new data is rarely scalable. Embedding geological knowledge directly\ninto predictive models offers a valuable alternative. A joint approach enables\njust that: process-based models that mimic geological processes can help train\ngenerative models that make predictions more efficiently. This study explores\nwhether a generative adversarial network (GAN) - a type of deep-learning\nalgorithm for generative modeling - trained to produce fluvial deposits can be\ninverted to match well and seismic data. Four inversion approaches applied to\nthree test samples with 4, 8, and 20 wells struggled to match these well data,\nespecially as the well number increased or as the test sample diverged from the\ntraining data. The key bottleneck lies in the GAN's latent representation: it\nis entangled, so samples with similar sedimentological features are not\nnecessarily close in the latent space. Label conditioning or latent\noverparameterization can partially disentangle the latent space during\ntraining, although not yet sufficiently for a successful inversion. Fine-tuning\nthe GAN to restructure the latent space locally reduces mismatches to\nacceptable levels for all test cases, with and without seismic data. But this\napproach depends on an initial, partially successful inversion step, which\ninfluences the quality and diversity of the final samples. Overall, GANs can\nalready handle the tasks required for their integration into geomodeling\nworkflows. We still need to further assess their robustness, and how to best\nleverage them in support of geological interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High costs and uncertainties make subsurface decision-making challenging, as\nacquiring new data is rarely scalable. Embedding geological knowledge directly\ninto predictive models offers a valuable alternative. A joint approach enables\njust that: process-based models that mimic geological processes can help train\ngenerative models that make predictions more efficiently. This study explores\nwhether a generative adversarial network (GAN) - a type of deep-learning\nalgorithm for generative modeling - trained to produce fluvial deposits can be\ninverted to match well and seismic data. Four inversion approaches applied to\nthree test samples with 4, 8, and 20 wells struggled to match these well data,\nespecially as the well number increased or as the test sample diverged from the\ntraining data. The key bottleneck lies in the GAN's latent representation: it\nis entangled, so samples with similar sedimentological features are not\nnecessarily close in the latent space. Label conditioning or latent\noverparameterization can partially disentangle the latent space during\ntraining, although not yet sufficiently for a successful inversion. Fine-tuning\nthe GAN to restructure the latent space locally reduces mismatches to\nacceptable levels for all test cases, with and without seismic data. But this\napproach depends on an initial, partially successful inversion step, which\ninfluences the quality and diversity of the final samples. Overall, GANs can\nalready handle the tasks required for their integration into geomodeling\nworkflows. We still need to further assess their robustness, and how to best\nleverage them in support of geological interpretation."
                },
                "authors": [
                    {
                        "name": "Guillaume Rongier"
                    },
                    {
                        "name": "Luk Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Luk Peeters"
                },
                "author": "Luk Peeters",
                "arxiv_comment": "52 pages, 42 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.6.3; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17476v1",
                "updated": "2025-10-20T12:19:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    19,
                    8,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:19:08Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    19,
                    8,
                    0,
                    293,
                    0
                ],
                "title": "Disparities in Multilingual LLM-Based Healthcare Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disparities in Multilingual LLM-Based Healthcare Q&A"
                },
                "summary": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Ipek Baris Schlicht"
                    },
                    {
                        "name": "Burcu Sayin"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Frederik M. Labonté"
                    },
                    {
                        "name": "Cesare Barbera"
                    },
                    {
                        "name": "Marco Viviani"
                    },
                    {
                        "name": "Paolo Rosso"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00921v2",
                "updated": "2025-10-20T12:17:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    17,
                    41,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-31T16:06:12Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    6,
                    12,
                    6,
                    243,
                    0
                ],
                "title": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling"
                },
                "summary": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining in-context learning (ICL) from\ndemonstrations with supervised fine-tuning. SIFT considerably outperforms both\nICL and decoder-as-encoder fine-tuning baselines on a range of standard SL\ntasks. We further find that although long context hinders the performance of\ngenerative SL in both ICL and SIFT, this deficiency can be mitigated by\nremoving the instruction, as instructions are shown to be largely unnecessary\nfor achieving strong SL performance with SIFT. Our findings highlight strengths\nand limitations of SL with LLMs, underscoring the importance of a\nresponse-based generative task formulation for effective SL performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining in-context learning (ICL) from\ndemonstrations with supervised fine-tuning. SIFT considerably outperforms both\nICL and decoder-as-encoder fine-tuning baselines on a range of standard SL\ntasks. We further find that although long context hinders the performance of\ngenerative SL in both ICL and SIFT, this deficiency can be mitigated by\nremoving the instruction, as instructions are shown to be largely unnecessary\nfor achieving strong SL performance with SIFT. Our findings highlight strengths\nand limitations of SL with LLMs, underscoring the importance of a\nresponse-based generative task formulation for effective SL performance."
                },
                "authors": [
                    {
                        "name": "David Dukić"
                    },
                    {
                        "name": "Goran Glavaš"
                    },
                    {
                        "name": "Jan Šnajder"
                    }
                ],
                "author_detail": {
                    "name": "Jan Šnajder"
                },
                "author": "Jan Šnajder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17472v1",
                "updated": "2025-10-20T12:14:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    14,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:14:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    14,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time\n  Training for Reliable Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Self-Consistency: Statistical Guarantees and Test-Time\n  Training for Reliable Reasoning in LLMs"
                },
                "summary": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs."
                },
                "authors": [
                    {
                        "name": "Paula Cordero-Encinar"
                    },
                    {
                        "name": "Andrew B. Duncan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew B. Duncan"
                },
                "author": "Andrew B. Duncan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01879v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01879v4",
                "updated": "2025-10-20T12:05:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    5,
                    19,
                    0,
                    293,
                    0
                ],
                "published": "2025-02-26T17:26:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    26,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,\n  And Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,\n  And Vision"
                },
                "summary": "This work proposes an industry-level omni-modal large language model (LLM)\npipeline that integrates auditory, visual, and linguistic modalities to\novercome challenges such as limited tri-modal datasets, high computational\ncosts, and complex feature alignments. Our pipeline consists of three main\ncomponents: First, a modular framework enabling flexible configuration of\nvarious encoder-LLM-decoder architectures. Second, a lightweight training\nstrategy that pre-trains audio-language alignment on the state-of-the-art\nvision-language model Qwen2.5-VL, thus avoiding the costly pre-training of\nvision-specific modalities. Third, an audio synthesis pipeline that generates\nhigh-quality audio-text data from diverse real-world scenarios, supporting\napplications such as Automatic Speech Recognition and Speech-to-Speech chat. To\nthis end, we introduce an industry-level omni-modal LLM, Nexus. Extensive\nexperiments validate the efficacy of our pipeline, yielding the following key\nfindings:(1) In the visual understanding task, Nexus exhibits superior\nperformance compared with its backbone model - Qwen2.5-VL-7B, validating the\nefficiency of our training strategy. (2) Within the English Spoken\nQuestion-Answering task, the model achieves better accuracy than the\nsame-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In\nour real-world ASR testset, Nexus achieves outstanding performance, indicating\nits robustness in real scenarios. (4) In the Speech-to-Text Translation task,\nour model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task,\nbased on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is\ncomparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth\nanalysis of tri-modal alignment reveals that incorporating the audio modality\nenhances representational alignment between vision and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes an industry-level omni-modal large language model (LLM)\npipeline that integrates auditory, visual, and linguistic modalities to\novercome challenges such as limited tri-modal datasets, high computational\ncosts, and complex feature alignments. Our pipeline consists of three main\ncomponents: First, a modular framework enabling flexible configuration of\nvarious encoder-LLM-decoder architectures. Second, a lightweight training\nstrategy that pre-trains audio-language alignment on the state-of-the-art\nvision-language model Qwen2.5-VL, thus avoiding the costly pre-training of\nvision-specific modalities. Third, an audio synthesis pipeline that generates\nhigh-quality audio-text data from diverse real-world scenarios, supporting\napplications such as Automatic Speech Recognition and Speech-to-Speech chat. To\nthis end, we introduce an industry-level omni-modal LLM, Nexus. Extensive\nexperiments validate the efficacy of our pipeline, yielding the following key\nfindings:(1) In the visual understanding task, Nexus exhibits superior\nperformance compared with its backbone model - Qwen2.5-VL-7B, validating the\nefficiency of our training strategy. (2) Within the English Spoken\nQuestion-Answering task, the model achieves better accuracy than the\nsame-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In\nour real-world ASR testset, Nexus achieves outstanding performance, indicating\nits robustness in real scenarios. (4) In the Speech-to-Text Translation task,\nour model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task,\nbased on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is\ncomparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth\nanalysis of tri-modal alignment reveals that incorporating the audio modality\nenhances representational alignment between vision and language."
                },
                "authors": [
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Yingji Zhang"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Weijie Zhang"
                    },
                    {
                        "name": "Chenggong Gong"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Ziliang Gan"
                    },
                    {
                        "name": "Ziao Wang"
                    },
                    {
                        "name": "Haipang Wu"
                    },
                    {
                        "name": "Ji Liu"
                    },
                    {
                        "name": "André Freitas"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Rongjuncheng Zhang"
                    },
                    {
                        "name": "Yong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yong Dai"
                },
                "author": "Yong Dai",
                "arxiv_comment": "Project: https://github.com/HiThink-Research/NEXUS-O",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01879v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01879v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.04203v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.04203v3",
                "updated": "2025-10-20T12:04:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    4,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2023-03-07T19:47:01Z",
                "published_parsed": [
                    2023,
                    3,
                    7,
                    19,
                    47,
                    1,
                    1,
                    66,
                    0
                ],
                "title": "What is Memory? A Homological Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is Memory? A Homological Perspective"
                },
                "summary": "We introduce the delta-homology model of memory, a unified framework in which\nrecall, learning, and prediction emerge from cycle closure, the completion of\ntopologically constrained trajectories within the brain's latent manifold. A\nDirac-like memory trace corresponds to a nontrivial homology generator,\nrepresenting a sparse, irreducible attractor that reactivates only when\ninference trajectories close upon themselves. In this view, memory is not a\nstatic attractor landscape but a topological process of recurrence, where\nstructure arises through the stabilization of closed loops. Building on this\nprinciple, we represent spike-timing dynamics as spatiotemporal complexes, in\nwhich temporally consistent transitions among neurons form chain complexes\nsupporting persistent activation cycles. These cycles are organized into cell\nposets, compact causal representations that encode overlapping and\ncompositional memory traces. Within this construction, learning and recall\ncorrespond to cycle closure under contextual modulation: inference trajectories\nstabilize into nontrivial homology classes when both local synchrony (context)\nand global recurrence (content) are satisfied. We formalize this mechanism\nthrough the Context-Content Uncertainty Principle (CCUP), which states that\ncognition minimizes joint uncertainty between a high-entropy context variable\nand a low-entropy content variable. Synchronization acts as a context filter\nselecting coherent subnetworks, while recurrence acts as a content filter\nvalidating nontrivial cycles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the delta-homology model of memory, a unified framework in which\nrecall, learning, and prediction emerge from cycle closure, the completion of\ntopologically constrained trajectories within the brain's latent manifold. A\nDirac-like memory trace corresponds to a nontrivial homology generator,\nrepresenting a sparse, irreducible attractor that reactivates only when\ninference trajectories close upon themselves. In this view, memory is not a\nstatic attractor landscape but a topological process of recurrence, where\nstructure arises through the stabilization of closed loops. Building on this\nprinciple, we represent spike-timing dynamics as spatiotemporal complexes, in\nwhich temporally consistent transitions among neurons form chain complexes\nsupporting persistent activation cycles. These cycles are organized into cell\nposets, compact causal representations that encode overlapping and\ncompositional memory traces. Within this construction, learning and recall\ncorrespond to cycle closure under contextual modulation: inference trajectories\nstabilize into nontrivial homology classes when both local synchrony (context)\nand global recurrence (content) are satisfied. We formalize this mechanism\nthrough the Context-Content Uncertainty Principle (CCUP), which states that\ncognition minimizes joint uncertainty between a high-entropy context variable\nand a low-entropy content variable. Synchronization acts as a context filter\nselecting coherent subnetworks, while recurrence acts as a content filter\nvalidating nontrivial cycles."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.04203v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.04203v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15188v2",
                "updated": "2025-10-20T12:03:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    3,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-16T23:14:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    23,
                    14,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph\n  Anomaly Detection and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph\n  Anomaly Detection and LLMs"
                },
                "summary": "Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade\ndetection in system-level audit logs. Provenance graphs model these logs as\nconnected entities and events, revealing relationships that are missed by\nlinear log representations. Existing systems apply anomaly detection to these\ngraphs but often suffer from high false positive rates and coarse-grained\nalerts. Their reliance on node attributes like file paths or IPs leads to\nspurious correlations, reducing detection robustness and reliability. To fully\nunderstand an attack's progression and impact, security analysts need systems\nthat can generate accurate, human-like narratives of the entire attack. To\naddress these challenges, we introduce OCR-APT, a system for APT detection and\nreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks\n(GNNs) for subgraph anomaly detection, learning behavior patterns around nodes\nrather than fragile attributes such as file paths or IPs. This approach leads\nto a more robust anomaly detection. It then iterates over detected subgraphs\nusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.\nEach stage is validated before proceeding, reducing hallucinations and ensuring\nan interpretable final report. Our evaluations on the DARPA TC3, OpTC, and\nNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both\ndetection accuracy and alert interpretability. Moreover, OCR-APT reconstructs\nhuman-like reports that comprehensively capture the attack story.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade\ndetection in system-level audit logs. Provenance graphs model these logs as\nconnected entities and events, revealing relationships that are missed by\nlinear log representations. Existing systems apply anomaly detection to these\ngraphs but often suffer from high false positive rates and coarse-grained\nalerts. Their reliance on node attributes like file paths or IPs leads to\nspurious correlations, reducing detection robustness and reliability. To fully\nunderstand an attack's progression and impact, security analysts need systems\nthat can generate accurate, human-like narratives of the entire attack. To\naddress these challenges, we introduce OCR-APT, a system for APT detection and\nreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks\n(GNNs) for subgraph anomaly detection, learning behavior patterns around nodes\nrather than fragile attributes such as file paths or IPs. This approach leads\nto a more robust anomaly detection. It then iterates over detected subgraphs\nusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.\nEach stage is validated before proceeding, reducing hallucinations and ensuring\nan interpretable final report. Our evaluations on the DARPA TC3, OpTC, and\nNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both\ndetection accuracy and alert interpretability. Moreover, OCR-APT reconstructs\nhuman-like reports that comprehensively capture the attack story."
                },
                "authors": [
                    {
                        "name": "Ahmed Aly"
                    },
                    {
                        "name": "Essam Mansour"
                    },
                    {
                        "name": "Amr Youssef"
                    }
                ],
                "author_detail": {
                    "name": "Amr Youssef"
                },
                "arxiv_affiliation": "Concordia University",
                "author": "Amr Youssef",
                "arxiv_comment": "This is the authors' extended version of the paper accepted for\n  publication at the ACM SIGSAC Conference on Computer and Communications\n  Security (CCS 2025). The final published version is available at\n  https://doi.org/10.1145/3719027.3765219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13472v2",
                "updated": "2025-10-20T12:00:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    0,
                    10,
                    0,
                    293,
                    0
                ],
                "published": "2025-04-18T05:26:32Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    26,
                    32,
                    4,
                    108,
                    0
                ],
                "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities. However,\nthey generally evaluate the generated code based on static prompts, and tend to\nfail for complex code scenarios which typically involve multiple requirements\nand require more contextual information. In addition, these approaches lack\nfine-grained evaluation for complex code, resulting in limited explainability.\nTo mitigate the limitations, we propose CodeVisionary, the first agent-based\nevaluation framework for complex code generation. CodeVisionary consists of two\nstages: (1) Requirement-guided multi-dimensional context distillation stage and\n(2) Fine-grained scoring and summarization stage. A comprehensive evaluation\nreport is also generated for enhanced explainability. For validation, we\nconstruct a new benchmark consisting of 363 samples spanning 37 coding\nscenarios and 23 programming languages. Extensive experiments demonstrate that\nCodeVisionary achieves the best performance among three baselines for\nevaluating complex code generation, outperforming the best baseline with\naverage improvements of 0.217, 0.163, and 0.141 in Pearson, Spearman, and\nKendall-Tau coefficients, respectively. The resources of CodeVisionary are\navailable at https://github.com/Eshe0922/CodeVisionary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities. However,\nthey generally evaluate the generated code based on static prompts, and tend to\nfail for complex code scenarios which typically involve multiple requirements\nand require more contextual information. In addition, these approaches lack\nfine-grained evaluation for complex code, resulting in limited explainability.\nTo mitigate the limitations, we propose CodeVisionary, the first agent-based\nevaluation framework for complex code generation. CodeVisionary consists of two\nstages: (1) Requirement-guided multi-dimensional context distillation stage and\n(2) Fine-grained scoring and summarization stage. A comprehensive evaluation\nreport is also generated for enhanced explainability. For validation, we\nconstruct a new benchmark consisting of 363 samples spanning 37 coding\nscenarios and 23 programming languages. Extensive experiments demonstrate that\nCodeVisionary achieves the best performance among three baselines for\nevaluating complex code generation, outperforming the best baseline with\naverage improvements of 0.217, 0.163, and 0.141 in Pearson, Spearman, and\nKendall-Tau coefficients, respectively. The resources of CodeVisionary are\navailable at https://github.com/Eshe0922/CodeVisionary."
                },
                "authors": [
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03313v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03313v3",
                "updated": "2025-10-20T11:58:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    58,
                    5,
                    0,
                    293,
                    0
                ],
                "published": "2025-03-05T09:45:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models"
                },
                "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."
                },
                "authors": [
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03313v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17460v1",
                "updated": "2025-10-20T11:49:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    49,
                    26,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T11:49:26Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    49,
                    26,
                    0,
                    293,
                    0
                ],
                "title": "Evaluating Large Language Models on Urdu Idiom Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models on Urdu Idiom Translation"
                },
                "summary": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu."
                },
                "authors": [
                    {
                        "name": "Muhammad Farmal Khan"
                    },
                    {
                        "name": "Mousumi Akter"
                    }
                ],
                "author_detail": {
                    "name": "Mousumi Akter"
                },
                "author": "Mousumi Akter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17459v1",
                "updated": "2025-10-20T11:46:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T11:46:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "title": "Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural\n  Network"
                },
                "summary": "In this work, we propose a new flow-matching Markov chain Monte Carlo\n(FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary\nsystems, especially for those only one exoplanet is involved. Compared to\ntraditional methods that rely on random sampling within the Bayesian framework,\nour approach first leverages flow matching posterior estimation (FMPE) to\nefficiently constrain the prior range of physical parameters, and then employs\nMCMC to accurately infer the posterior distribution. For example, in the\norbital parameter inference of beta Pictoris b, our model achieved a\nsubstantial speed-up while maintaining comparable accuracy-running 77.8 times\nfaster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested\nsampling. Moreover, our FM-MCMC method also attained the highest average\nlog-likelihood among all approaches, demonstrating its superior sampling\nefficiency and accuracy. This highlights the scalability and efficiency of our\napproach, making it well-suited for processing the massive datasets expected\nfrom future exoplanet surveys. Beyond astrophysics, our methodology establishes\na versatile paradigm for synergizing deep generative models with traditional\nsampling, which can be adopted to tackle complex inference problems in other\nfields, such as cosmology, biomedical imaging, and particle physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a new flow-matching Markov chain Monte Carlo\n(FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary\nsystems, especially for those only one exoplanet is involved. Compared to\ntraditional methods that rely on random sampling within the Bayesian framework,\nour approach first leverages flow matching posterior estimation (FMPE) to\nefficiently constrain the prior range of physical parameters, and then employs\nMCMC to accurately infer the posterior distribution. For example, in the\norbital parameter inference of beta Pictoris b, our model achieved a\nsubstantial speed-up while maintaining comparable accuracy-running 77.8 times\nfaster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested\nsampling. Moreover, our FM-MCMC method also attained the highest average\nlog-likelihood among all approaches, demonstrating its superior sampling\nefficiency and accuracy. This highlights the scalability and efficiency of our\napproach, making it well-suited for processing the massive datasets expected\nfrom future exoplanet surveys. Beyond astrophysics, our methodology establishes\na versatile paradigm for synergizing deep generative models with traditional\nsampling, which can be adopted to tackle complex inference problems in other\nfields, such as cosmology, biomedical imaging, and particle physics."
                },
                "authors": [
                    {
                        "name": "Bo Liang"
                    },
                    {
                        "name": "Hanlin Song"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Yuxiang Xu"
                    },
                    {
                        "name": "Zihao Xiao"
                    },
                    {
                        "name": "Manjia Liang"
                    },
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "Wei-Liang Qian"
                    },
                    {
                        "name": "Li-e Qiang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Ziren Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ziren Luo"
                },
                "author": "Ziren Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12571v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12571v4",
                "updated": "2025-10-20T11:43:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    43,
                    41,
                    0,
                    293,
                    0
                ],
                "published": "2024-08-22T17:39:26Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    39,
                    26,
                    3,
                    235,
                    0
                ],
                "title": "Deep-learning-based continuous attacks on quantum key distribution\n  protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-learning-based continuous attacks on quantum key distribution\n  protocols"
                },
                "summary": "The most important characteristic of a Quantum Key Distribution (QKD)\nprotocol is its security against third-party attacks, and the potential\ncountermeasures available. While new types of attacks are regularly developed\nin the literature, they rarely involve the use of weak continuous measurement\nand more specifically machine learning to infer the qubit states. In this\npaper, we design a new individual attack scheme called\n\\textit{Deep-learning-based continuous attack} (DLCA) that exploits continuous\nmeasurement together with the powerful pattern recognition capacities of deep\nrecurrent neural networks. As a minimal model, we present its performances when\napplied in the case of the BB84 protocol with intrinsic noise in the\ncommunication channel. Our results suggest that our attack's performances lie\nbetween the ones of standard intercept-and-resend attacks and of the optimal\nindividual attack, namely the phase-covariant quantum cloner. Our attack scheme\ndemonstrates deep-learning-enhanced quantum state tomography applied to QKD,\nand could be generalized in many different ways, notably in the cases of\nquantum hacking attacks targeting implementation vulnerabilities that could\ncompromise the security of QKD protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most important characteristic of a Quantum Key Distribution (QKD)\nprotocol is its security against third-party attacks, and the potential\ncountermeasures available. While new types of attacks are regularly developed\nin the literature, they rarely involve the use of weak continuous measurement\nand more specifically machine learning to infer the qubit states. In this\npaper, we design a new individual attack scheme called\n\\textit{Deep-learning-based continuous attack} (DLCA) that exploits continuous\nmeasurement together with the powerful pattern recognition capacities of deep\nrecurrent neural networks. As a minimal model, we present its performances when\napplied in the case of the BB84 protocol with intrinsic noise in the\ncommunication channel. Our results suggest that our attack's performances lie\nbetween the ones of standard intercept-and-resend attacks and of the optimal\nindividual attack, namely the phase-covariant quantum cloner. Our attack scheme\ndemonstrates deep-learning-enhanced quantum state tomography applied to QKD,\nand could be generalized in many different ways, notably in the cases of\nquantum hacking attacks targeting implementation vulnerabilities that could\ncompromise the security of QKD protocols."
                },
                "authors": [
                    {
                        "name": "Théo Lejeune"
                    },
                    {
                        "name": "François Damanet"
                    }
                ],
                "author_detail": {
                    "name": "François Damanet"
                },
                "author": "François Damanet",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12571v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12571v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17458v1",
                "updated": "2025-10-20T11:42:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    42,
                    17,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T11:42:17Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    42,
                    17,
                    0,
                    293,
                    0
                ],
                "title": "Explainable AI for microseismic event detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI for microseismic event detection"
                },
                "summary": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors."
                },
                "authors": [
                    {
                        "name": "Ayrat Abdullin"
                    },
                    {
                        "name": "Denis Anikiev"
                    },
                    {
                        "name": "Umair bin Waheed"
                    }
                ],
                "author_detail": {
                    "name": "Umair bin Waheed"
                },
                "author": "Umair bin Waheed",
                "arxiv_comment": "Submitted to Artificial Intelligence in Geosciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21974v3",
                "updated": "2025-10-20T11:36:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    36,
                    6,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-27T07:32:16Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    32,
                    16,
                    4,
                    178,
                    0
                ],
                "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism"
                },
                "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation."
                },
                "authors": [
                    {
                        "name": "Simon Münker"
                    },
                    {
                        "name": "Nils Schwager"
                    },
                    {
                        "name": "Achim Rettinger"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettinger"
                },
                "author": "Achim Rettinger",
                "arxiv_comment": "11 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17450v1",
                "updated": "2025-10-20T11:35:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    35,
                    46,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T11:35:46Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    35,
                    46,
                    0,
                    293,
                    0
                ],
                "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance\n  Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance\n  Missions"
                },
                "summary": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects."
                },
                "authors": [
                    {
                        "name": "Johan Schubert"
                    },
                    {
                        "name": "Farzad Kamrani"
                    },
                    {
                        "name": "Tove Gustavi"
                    }
                ],
                "author_detail": {
                    "name": "Tove Gustavi"
                },
                "author": "Tove Gustavi",
                "arxiv_comment": "Presented at the 6th International Workshop on Active Inference,\n  15-17 October 2025, Montreal, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17449v2",
                "updated": "2025-10-21T02:17:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    2,
                    17,
                    12,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T11:35:09Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    35,
                    9,
                    0,
                    293,
                    0
                ],
                "title": "Observable spins in gravitational waves from compact binary mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observable spins in gravitational waves from compact binary mergers"
                },
                "summary": "We investigate the measurability of effective inspiral spin in the detectable\ncompact binary mergers using gravitational-wave observations. Measurements from\nthe latest gravitational-wave transient catalog do not rule out the existence\nof binary systems with non-zero effective spins. However, we observe an\napparent correlation between the inferred effective inspiral spin and the\nloudness of the gravitational-wave events- loud events typically have\nclose-to-zero effective spins whereas fainter events tend to be inferred with\nrelatively arbitrary effective spins. Through simulations, we demonstrate that\nnon-negligible effective spins can be systematically inferred from non-spinning\nsystems at small signal strengths. These two observations support the\npossibility that the effective spin magnitudes in the observable compact\nbinaries are generally small. Future detections can have potential impact on\nthe understanding of their population and other astrophysical inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the measurability of effective inspiral spin in the detectable\ncompact binary mergers using gravitational-wave observations. Measurements from\nthe latest gravitational-wave transient catalog do not rule out the existence\nof binary systems with non-zero effective spins. However, we observe an\napparent correlation between the inferred effective inspiral spin and the\nloudness of the gravitational-wave events- loud events typically have\nclose-to-zero effective spins whereas fainter events tend to be inferred with\nrelatively arbitrary effective spins. Through simulations, we demonstrate that\nnon-negligible effective spins can be systematically inferred from non-spinning\nsystems at small signal strengths. These two observations support the\npossibility that the effective spin magnitudes in the observable compact\nbinaries are generally small. Future detections can have potential impact on\nthe understanding of their population and other astrophysical inferences."
                },
                "authors": [
                    {
                        "name": "Souradeep Pal"
                    }
                ],
                "author_detail": {
                    "name": "Souradeep Pal"
                },
                "author": "Souradeep Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.17802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17802v1",
                "updated": "2025-10-20T17:59:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    59,
                    25,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:59:25Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    59,
                    25,
                    0,
                    293,
                    0
                ],
                "title": "Unbiased Gradient Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased Gradient Low-Rank Projection"
                },
                "summary": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yuxing Liu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17800v1",
                "updated": "2025-10-20T17:58:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    58,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:58:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    58,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glyph: Scaling Context Windows via Visual-Text Compression"
                },
                "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yusen Liu"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Yulin Fei"
                    },
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17797v1",
                "updated": "2025-10-20T17:55:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    55,
                    11,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:55:11Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    55,
                    11,
                    0,
                    293,
                    0
                ],
                "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics"
                },
                "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Roshan Ram"
                    },
                    {
                        "name": "Zixiang Chen"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Frank Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Weiran Yao"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Yao"
                },
                "author": "Weiran Yao",
                "arxiv_comment": "Technical report; 13 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17795v1",
                "updated": "2025-10-20T17:53:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    53,
                    23,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:53:23Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    53,
                    23,
                    0,
                    293,
                    0
                ],
                "title": "Executable Knowledge Graphs for Replicating AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executable Knowledge Graphs for Replicating AI Research"
                },
                "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG."
                },
                "authors": [
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Zhuoyun Yu"
                    },
                    {
                        "name": "Xuehai Wang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10815v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10815v3",
                "updated": "2025-10-20T17:46:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    46,
                    57,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-12T21:42:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    21,
                    42,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems"
                },
                "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities."
                },
                "authors": [
                    {
                        "name": "Meiru Zhang"
                    },
                    {
                        "name": "Philipp Borchert"
                    },
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    }
                ],
                "author_detail": {
                    "name": "Gerasimos Lampouras"
                },
                "author": "Gerasimos Lampouras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10815v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10815v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17764v1",
                "updated": "2025-10-20T17:22:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    22,
                    32,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:22:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    22,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from\n  Benchmarks to Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from\n  Benchmarks to Applications"
                },
                "summary": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use."
                },
                "authors": [
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Zhaonan Li"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Weiyu Chen"
                    },
                    {
                        "name": "Shijie Lu"
                    },
                    {
                        "name": "Yuxi Huang"
                    },
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Phu Tran"
                    },
                    {
                        "name": "Ji-Eun Irene Yum"
                    },
                    {
                        "name": "Muhammad Ali Khan"
                    },
                    {
                        "name": "Muhammad Umar Afzal"
                    },
                    {
                        "name": "Irbaz Bin Riaz"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "author": "Ben Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18196v3",
                "updated": "2025-10-20T17:16:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    16,
                    38,
                    0,
                    293,
                    0
                ],
                "published": "2024-12-24T06:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    6,
                    5,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by\n  Pseudo Gradient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by\n  Pseudo Gradient"
                },
                "summary": "While automatic prompt generation methods have recently received significant\nattention, their robustness remains poorly understood. In this paper, we\nintroduce PertBench, a comprehensive benchmark dataset that includes a wide\nrange of input perturbations, designed to systematically evaluate the\nrobustness of current auto-prompting techniques. Our analysis reveals\nsubstantial vulnerabilities in existing prompt generation strategies, where\neven minor modifications to the prompt can lead to significant differences in\nmodel output. To address this issue, we propose PGO, a gradient-free prompt\ngeneration framework that leverages perturbation types as pseudo-gradient\nsignals to guide LLMs in producing more robust prompts. In contrast to existing\nmethods that assess prompt quality only on clean, well-structured inputs, our\napproach explicitly emphasizes robustness under noisy and perturbed conditions.\nExtensive experiments across diverse tasks and multiple LLMs show PGO\nconsistently outperforms previous methods in maintaining performance under\ninput perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automatic prompt generation methods have recently received significant\nattention, their robustness remains poorly understood. In this paper, we\nintroduce PertBench, a comprehensive benchmark dataset that includes a wide\nrange of input perturbations, designed to systematically evaluate the\nrobustness of current auto-prompting techniques. Our analysis reveals\nsubstantial vulnerabilities in existing prompt generation strategies, where\neven minor modifications to the prompt can lead to significant differences in\nmodel output. To address this issue, we propose PGO, a gradient-free prompt\ngeneration framework that leverages perturbation types as pseudo-gradient\nsignals to guide LLMs in producing more robust prompts. In contrast to existing\nmethods that assess prompt quality only on clean, well-structured inputs, our\napproach explicitly emphasizes robustness under noisy and perturbed conditions.\nExtensive experiments across diverse tasks and multiple LLMs show PGO\nconsistently outperforms previous methods in maintaining performance under\ninput perturbations."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17748v1",
                "updated": "2025-10-20T17:02:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:02:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    2,
                    40,
                    0,
                    293,
                    0
                ],
                "title": "This is Going to Sound Crazy, But What If We Used Large Language Models\n  to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We\n  Will Find Better Configurations More Quickly Than Retraining From Scratch!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is Going to Sound Crazy, But What If We Used Large Language Models\n  to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We\n  Will Find Better Configurations More Quickly Than Retraining From Scratch!"
                },
                "summary": "Tuning database management systems (DBMSs) is challenging due to trillions of\npossible configurations and evolving workloads. Recent advances in tuning have\nled to breakthroughs in optimizing over the possible configurations. However,\ndue to their design and inability to leverage query-level historical insights,\nexisting automated tuners struggle to adapt and re-optimize the DBMS when the\nenvironment changes (e.g., workload drift, schema transfer).\n  This paper presents the Booster framework that assists existing tuners in\nadapting to environment changes (e.g., drift, cross-schema transfer). Booster\nstructures historical artifacts into query-configuration contexts, prompts\nlarge language models (LLMs) to suggest configurations for each query based on\nrelevant contexts, and then composes the query-level suggestions into a\nholistic configuration with beam search. With multiple OLAP workloads, we\nevaluate Booster's ability to assist different state-of-the-art tuners (e.g.,\ncost-/machine learning-/LLM-based) in adapting to environment changes. By\ncomposing recommendations derived from query-level insights, Booster assists\ntuners in discovering configurations that are up to 74% better and in up to\n4.7x less time than the alternative approach of continuing to tune from\nhistorical configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning database management systems (DBMSs) is challenging due to trillions of\npossible configurations and evolving workloads. Recent advances in tuning have\nled to breakthroughs in optimizing over the possible configurations. However,\ndue to their design and inability to leverage query-level historical insights,\nexisting automated tuners struggle to adapt and re-optimize the DBMS when the\nenvironment changes (e.g., workload drift, schema transfer).\n  This paper presents the Booster framework that assists existing tuners in\nadapting to environment changes (e.g., drift, cross-schema transfer). Booster\nstructures historical artifacts into query-configuration contexts, prompts\nlarge language models (LLMs) to suggest configurations for each query based on\nrelevant contexts, and then composes the query-level suggestions into a\nholistic configuration with beam search. With multiple OLAP workloads, we\nevaluate Booster's ability to assist different state-of-the-art tuners (e.g.,\ncost-/machine learning-/LLM-based) in adapting to environment changes. By\ncomposing recommendations derived from query-level insights, Booster assists\ntuners in discovering configurations that are up to 74% better and in up to\n4.7x less time than the alternative approach of continuing to tune from\nhistorical configurations."
                },
                "authors": [
                    {
                        "name": "William Zhang"
                    },
                    {
                        "name": "Wan Shen Lim"
                    },
                    {
                        "name": "Andrew Pavlo"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Pavlo"
                },
                "author": "Andrew Pavlo",
                "arxiv_comment": "Accepted to SIGMOD2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17739v1",
                "updated": "2025-10-20T16:50:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    50,
                    3,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:50:03Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    50,
                    3,
                    0,
                    293,
                    0
                ],
                "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation\n  for Visual Place Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Multi-Condition Representation Modelling via Matrix Factorisation\n  for Visual Place Recognition"
                },
                "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight."
                },
                "authors": [
                    {
                        "name": "Timur Ismagilov"
                    },
                    {
                        "name": "Shakaiba Majeed"
                    },
                    {
                        "name": "Michael Milford"
                    },
                    {
                        "name": "Tan Viet Tuyen Nguyen"
                    },
                    {
                        "name": "Sarvapali D. Ramchurn"
                    },
                    {
                        "name": "Shoaib Ehsan"
                    }
                ],
                "author_detail": {
                    "name": "Shoaib Ehsan"
                },
                "author": "Shoaib Ehsan",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17727v2",
                "updated": "2025-10-21T05:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    5,
                    22,
                    16,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T16:43:06Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    43,
                    6,
                    0,
                    293,
                    0
                ],
                "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Fine-Grained Operating Points for Black-Box LLMs"
                },
                "summary": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs."
                },
                "authors": [
                    {
                        "name": "Ege Beyazit"
                    },
                    {
                        "name": "KL Navaneet"
                    },
                    {
                        "name": "Prashant Mathur"
                    },
                    {
                        "name": "Roi Blanco"
                    },
                    {
                        "name": "Vidit Bansal"
                    },
                    {
                        "name": "Karim Bouyarmane"
                    }
                ],
                "author_detail": {
                    "name": "Karim Bouyarmane"
                },
                "author": "Karim Bouyarmane",
                "arxiv_comment": "Under review at ICLR 2026. 36 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17726v1",
                "updated": "2025-10-20T16:42:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    49,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:42:49Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    49,
                    0,
                    293,
                    0
                ],
                "title": "Rethinking Search: A Study of University Students' Perspectives on Using\n  LLMs and Traditional Search Engines in Academic Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Search: A Study of University Students' Perspectives on Using\n  LLMs and Traditional Search Engines in Academic Problem Solving"
                },
                "summary": "With the increasing integration of Artificial Intelligence (AI) in academic\nproblem solving, university students frequently alternate between traditional\nsearch engines like Google and large language models (LLMs) for information\nretrieval. This study explores students' perceptions of both tools, emphasizing\nusability, efficiency, and their integration into academic workflows. Employing\na mixed-methods approach, we surveyed 109 students from diverse disciplines and\nconducted in-depth interviews with 12 participants. Quantitative analyses,\nincluding ANOVA and chi-square tests, were used to assess differences in\nefficiency, satisfaction, and tool preference. Qualitative insights revealed\nthat students commonly switch between GPT and Google: using Google for\ncredible, multi-source information and GPT for summarization, explanation, and\ndrafting. While neither tool proved sufficient on its own, there was a strong\ndemand for a hybrid solution. In response, we developed a prototype, a chatbot\nembedded within the search interface, that combines GPT's conversational\ncapabilities with Google's reliability to enhance academic research and reduce\ncognitive load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing integration of Artificial Intelligence (AI) in academic\nproblem solving, university students frequently alternate between traditional\nsearch engines like Google and large language models (LLMs) for information\nretrieval. This study explores students' perceptions of both tools, emphasizing\nusability, efficiency, and their integration into academic workflows. Employing\na mixed-methods approach, we surveyed 109 students from diverse disciplines and\nconducted in-depth interviews with 12 participants. Quantitative analyses,\nincluding ANOVA and chi-square tests, were used to assess differences in\nefficiency, satisfaction, and tool preference. Qualitative insights revealed\nthat students commonly switch between GPT and Google: using Google for\ncredible, multi-source information and GPT for summarization, explanation, and\ndrafting. While neither tool proved sufficient on its own, there was a strong\ndemand for a hybrid solution. In response, we developed a prototype, a chatbot\nembedded within the search interface, that combines GPT's conversational\ncapabilities with Google's reliability to enhance academic research and reduce\ncognitive load."
                },
                "authors": [
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Md. Sadman Haque"
                    },
                    {
                        "name": "Zobaer Ibn Razzaque"
                    },
                    {
                        "name": "Robiul Awoul Robin"
                    },
                    {
                        "name": "Sabila Nawshin"
                    }
                ],
                "author_detail": {
                    "name": "Sabila Nawshin"
                },
                "author": "Sabila Nawshin",
                "arxiv_comment": "Acctepted at the EMNLP 2025 HCI+NLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17725v1",
                "updated": "2025-10-20T16:42:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    30,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:42:30Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    42,
                    30,
                    0,
                    293,
                    0
                ],
                "title": "AcademicEval: Live Long-Context LLM Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcademicEval: Live Long-Context LLM Benchmark"
                },
                "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval"
                },
                "authors": [
                    {
                        "name": "Haozhen Zhang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "arxiv_comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17722v1",
                "updated": "2025-10-20T16:38:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    38,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:38:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    38,
                    40,
                    0,
                    293,
                    0
                ],
                "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues"
                },
                "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research."
                },
                "authors": [
                    {
                        "name": "Yaning Pan"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Yongqian Wen"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Guohui Zhang"
                    },
                    {
                        "name": "Haoxuan Hu"
                    },
                    {
                        "name": "Zhiyu Pan"
                    },
                    {
                        "name": "Yibing Huang"
                    },
                    {
                        "name": "Zhidong Gan"
                    },
                    {
                        "name": "Yonghong Lin"
                    },
                    {
                        "name": "An Ping"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu",
                "arxiv_comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17720v1",
                "updated": "2025-10-20T16:36:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    36,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:36:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    36,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity\n  Recognition"
                },
                "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power."
                },
                "authors": [
                    {
                        "name": "Nanda Kumar Rengarajan"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Chun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Wang"
                },
                "author": "Chun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05605v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05605v5",
                "updated": "2025-10-20T16:35:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    35,
                    34,
                    0,
                    293,
                    0
                ],
                "published": "2025-02-08T15:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    15,
                    21,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "Evolving LLMs' Self-Refinement Capability via Iterative Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving LLMs' Self-Refinement Capability via Iterative Preference\n  Optimization"
                },
                "summary": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinyu Cui"
                    },
                    {
                        "name": "Xuanfa Jin"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05605v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05605v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17715v1",
                "updated": "2025-10-20T16:29:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    29,
                    53,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:29:53Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    29,
                    53,
                    0,
                    293,
                    0
                ],
                "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QueST: Incentivizing LLMs to Generate Difficult Problems"
                },
                "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17705v1",
                "updated": "2025-10-20T16:19:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    19,
                    27,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:19:27Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    19,
                    27,
                    0,
                    293,
                    0
                ],
                "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM."
                },
                "authors": [
                    {
                        "name": "Dayan Pan"
                    },
                    {
                        "name": "Zhaoyang Fu"
                    },
                    {
                        "name": "Jingyuan Wang"
                    },
                    {
                        "name": "Xiao Han"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_doi": "10.1145/3746252.3761289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.17705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM' 25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17700v1",
                "updated": "2025-10-20T16:15:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    15,
                    3,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:15:03Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    15,
                    3,
                    0,
                    293,
                    0
                ],
                "title": "Elastic ViTs from Pretrained Models without Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic ViTs from Pretrained Models without Retraining"
                },
                "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/"
                },
                "authors": [
                    {
                        "name": "Walter Simoncini"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17698v1",
                "updated": "2025-10-20T16:11:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    11,
                    34,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T16:11:34Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    11,
                    34,
                    0,
                    293,
                    0
                ],
                "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM\n  Educational Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mining Effective Pedagogical Strategies from Learner-LLM\n  Educational Dialogues"
                },
                "summary": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies."
                },
                "authors": [
                    {
                        "name": "Liqun He"
                    },
                    {
                        "name": "Manolis Mavrikis"
                    },
                    {
                        "name": "Mutlu Cukurova"
                    }
                ],
                "author_detail": {
                    "name": "Mutlu Cukurova"
                },
                "author": "Mutlu Cukurova",
                "arxiv_doi": "10.1007/978-3-031-99261-2_42",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99261-2_42",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.17698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12943v2",
                "updated": "2025-10-20T15:55:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    55,
                    28,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-14T19:42:24Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    19,
                    42,
                    24,
                    1,
                    287,
                    0
                ],
                "title": "The Curious Case of Curiosity across Human Cultures and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curious Case of Curiosity across Human Cultures and LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have expanded their role in\nhuman interaction, yet curiosity -- a central driver of inquiry -- remains\nunderexplored in these systems, particularly across cultural contexts. In this\nwork, we investigate cultural variation in curiosity using Yahoo! Answers, a\nreal-world multi-country dataset spanning diverse topics. We introduce CUEST\n(CUriosity Evaluation across SocieTies), an evaluation framework that measures\nhuman-model alignment in curiosity through linguistic (style), topic preference\n(content) analysis and grounding insights in social science constructs. Across\nopen- and closed-source models, we find that LLMs flatten cross-cultural\ndiversity, aligning more closely with how curiosity is expressed in Western\ncountries. We then explore fine-tuning strategies to induce curiosity in LLMs,\nnarrowing the human-model alignment gap by up to 50%. Finally, we demonstrate\nthe practical value of curiosity for LLM adaptability across cultures, showing\nits importance for future NLP research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have expanded their role in\nhuman interaction, yet curiosity -- a central driver of inquiry -- remains\nunderexplored in these systems, particularly across cultural contexts. In this\nwork, we investigate cultural variation in curiosity using Yahoo! Answers, a\nreal-world multi-country dataset spanning diverse topics. We introduce CUEST\n(CUriosity Evaluation across SocieTies), an evaluation framework that measures\nhuman-model alignment in curiosity through linguistic (style), topic preference\n(content) analysis and grounding insights in social science constructs. Across\nopen- and closed-source models, we find that LLMs flatten cross-cultural\ndiversity, aligning more closely with how curiosity is expressed in Western\ncountries. We then explore fine-tuning strategies to induce curiosity in LLMs,\nnarrowing the human-model alignment gap by up to 50%. Finally, we demonstrate\nthe practical value of curiosity for LLM adaptability across cultures, showing\nits importance for future NLP research."
                },
                "authors": [
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "arxiv_comment": "Preprint (Paper under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15763v2",
                "updated": "2025-10-20T15:53:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    53,
                    26,
                    0,
                    293,
                    0
                ],
                "published": "2025-01-27T04:16:42Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    16,
                    42,
                    0,
                    27,
                    0
                ],
                "title": "NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose\n  Estimation"
                },
                "summary": "The widespread application of 3D human pose estimation (HPE) is limited by\nresource-constrained edge devices, requiring more efficient models. A key\napproach to enhancing efficiency involves designing networks based on the\nstructural characteristics of input data. However, effectively utilizing the\nstructural priors in human skeletal inputs remains challenging. To address\nthis, we leverage both explicit and implicit spatio-temporal priors of the\nhuman body through innovative model design and a pre-training proxy task.\nFirst, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE\nnetwork with stacked Hierarchical Mixers to capture explicit features.\nSpecifically, the spatial Hierarchical Mixer efficiently learns the human\nphysical topology across multiple semantic levels, while the temporal\nHierarchical Mixer with discrete cosine transform and low-pass filtering\ncaptures local instantaneous movements and global action coherence. Moreover,\nEfficient Temporal-Spatial Tokenization (ETST) is introduced to enhance\nspatio-temporal interaction and reduce computational complexity significantly.\nSecond, PoseCLR is proposed as a general pre-training method based on\ncontrastive learning for 3D HPE, aimed at extracting implicit representations\nof human topology. By aligning 2D poses from diverse viewpoints in the proxy\ntask, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing\nthe high-dimensional features of the human body, leading to further performance\nimprovements. Extensive experiments verify that NanoHTNet with PoseCLR\noutperforms other state-of-the-art methods in efficiency, making it ideal for\ndeployment on edge devices like the Jetson Nano. Code and models are available\nat https://github.com/vefalun/NanoHTNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread application of 3D human pose estimation (HPE) is limited by\nresource-constrained edge devices, requiring more efficient models. A key\napproach to enhancing efficiency involves designing networks based on the\nstructural characteristics of input data. However, effectively utilizing the\nstructural priors in human skeletal inputs remains challenging. To address\nthis, we leverage both explicit and implicit spatio-temporal priors of the\nhuman body through innovative model design and a pre-training proxy task.\nFirst, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE\nnetwork with stacked Hierarchical Mixers to capture explicit features.\nSpecifically, the spatial Hierarchical Mixer efficiently learns the human\nphysical topology across multiple semantic levels, while the temporal\nHierarchical Mixer with discrete cosine transform and low-pass filtering\ncaptures local instantaneous movements and global action coherence. Moreover,\nEfficient Temporal-Spatial Tokenization (ETST) is introduced to enhance\nspatio-temporal interaction and reduce computational complexity significantly.\nSecond, PoseCLR is proposed as a general pre-training method based on\ncontrastive learning for 3D HPE, aimed at extracting implicit representations\nof human topology. By aligning 2D poses from diverse viewpoints in the proxy\ntask, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing\nthe high-dimensional features of the human body, leading to further performance\nimprovements. Extensive experiments verify that NanoHTNet with PoseCLR\noutperforms other state-of-the-art methods in efficiency, making it ideal for\ndeployment on edge devices like the Jetson Nano. Code and models are available\nat https://github.com/vefalun/NanoHTNet."
                },
                "authors": [
                    {
                        "name": "Jialun Cai"
                    },
                    {
                        "name": "Mengyuan Liu"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Shuheng Zhou"
                    },
                    {
                        "name": "Wenhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Li"
                },
                "author": "Wenhao Li",
                "arxiv_doi": "10.1109/TIP.2025.3608662.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIP.2025.3608662.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by TIP 2025, Open Sourced",
                "arxiv_journal_ref": "IEEE Transactions on Image Processing, vol. 34, pp. 6655-6668,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17671v1",
                "updated": "2025-10-20T15:41:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    41,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:41:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    41,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LILO: Bayesian Optimization with Interactive Natural Language Feedback"
                },
                "summary": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes."
                },
                "authors": [
                    {
                        "name": "Katarzyna Kobalczyk"
                    },
                    {
                        "name": "Zhiyuan Jerry Lin"
                    },
                    {
                        "name": "Benjamin Letham"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Maximilian Balandat"
                    },
                    {
                        "name": "Eytan Bakshy"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Bakshy"
                },
                "author": "Eytan Bakshy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17659v2",
                "updated": "2025-10-21T03:38:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    38,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T15:32:11Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    32,
                    11,
                    0,
                    293,
                    0
                ],
                "title": "Field-Trial Quantum Key Distribution with Qubit-Based Frame\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Field-Trial Quantum Key Distribution with Qubit-Based Frame\n  Synchronization"
                },
                "summary": "Quantum key distribution (QKD) is a cryptographic technique that uses quantum\nmechanical principles to enable secure key exchange. Practical deployment of\nQKD requires robust, cost-effective systems that can operate in challenging\nfield environments. A major challenge is achieving reliable clock\nsynchronization without adding hardware complexity. Conventional approaches\noften use separate classical light signals, which increase costs and introduce\nnoise that degrades quantum channel performance. To address this limitation, we\ndemonstrate a QKD system incorporating a recently proposed qubit-based\ndistributed frame synchronization method, deployed over a metropolitan fiber\nnetwork in Nanning, China. Using the polarization-encoded one-decoy-state BB84\nprotocol and the recently proposed qubit-based distributed frame\nsynchronization method, our system achieves synchronization directly from the\nquantum signal, eliminating the need for dedicated synchronization hardware.\nFurthermore, to counteract dynamic polarization disturbances in urban fibers,\nthe system integrates qubit-based polarization feedback control, enabling\nreal-time polarization compensation through an automated polarization\ncontroller using data recovered from the qubit-based synchronization signals.\nDuring 12 hours of continuous operation, the system maintained a low average\nquantum bit error rate (QBER) of 1.12/%, achieving a secure key rate of 26.6\nkbit/s under 18 dB channel loss. Even under a high channel loss of 40 dB, a\nfinite-key secure rate of 115 bit/s was achieved. This study represents the\nfirst successful long-term validation of a frame-synchronization based QKD\nscheme in a real urban environment, demonstrating exceptional stability and\nhigh-loss tolerance, and offering an alternative for building practical,\nscalable, and cost-efficient quantum-secure communication networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum key distribution (QKD) is a cryptographic technique that uses quantum\nmechanical principles to enable secure key exchange. Practical deployment of\nQKD requires robust, cost-effective systems that can operate in challenging\nfield environments. A major challenge is achieving reliable clock\nsynchronization without adding hardware complexity. Conventional approaches\noften use separate classical light signals, which increase costs and introduce\nnoise that degrades quantum channel performance. To address this limitation, we\ndemonstrate a QKD system incorporating a recently proposed qubit-based\ndistributed frame synchronization method, deployed over a metropolitan fiber\nnetwork in Nanning, China. Using the polarization-encoded one-decoy-state BB84\nprotocol and the recently proposed qubit-based distributed frame\nsynchronization method, our system achieves synchronization directly from the\nquantum signal, eliminating the need for dedicated synchronization hardware.\nFurthermore, to counteract dynamic polarization disturbances in urban fibers,\nthe system integrates qubit-based polarization feedback control, enabling\nreal-time polarization compensation through an automated polarization\ncontroller using data recovered from the qubit-based synchronization signals.\nDuring 12 hours of continuous operation, the system maintained a low average\nquantum bit error rate (QBER) of 1.12/%, achieving a secure key rate of 26.6\nkbit/s under 18 dB channel loss. Even under a high channel loss of 40 dB, a\nfinite-key secure rate of 115 bit/s was achieved. This study represents the\nfirst successful long-term validation of a frame-synchronization based QKD\nscheme in a real urban environment, demonstrating exceptional stability and\nhigh-loss tolerance, and offering an alternative for building practical,\nscalable, and cost-efficient quantum-secure communication networks."
                },
                "authors": [
                    {
                        "name": "Rui Guan"
                    },
                    {
                        "name": "Jingchun Yu"
                    },
                    {
                        "name": "Zhaoyun Li"
                    },
                    {
                        "name": "Hongbo Xie"
                    },
                    {
                        "name": "Yuxing Wei"
                    },
                    {
                        "name": "Sen Li"
                    },
                    {
                        "name": "Jing Wen"
                    },
                    {
                        "name": "Xiaodong Liang"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Kejin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Kejin Wei"
                },
                "author": "Kejin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17652v1",
                "updated": "2025-10-20T15:27:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    27,
                    53,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:27:53Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    27,
                    53,
                    0,
                    293,
                    0
                ],
                "title": "Qomhra: A Bilingual Irish-English Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qomhra: A Bilingual Irish-English Large Language Model"
                },
                "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality."
                },
                "authors": [
                    {
                        "name": "Joseph McInerney"
                    }
                ],
                "author_detail": {
                    "name": "Joseph McInerney"
                },
                "author": "Joseph McInerney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17651v1",
                "updated": "2025-10-20T15:26:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    26,
                    43,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:26:43Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    26,
                    43,
                    0,
                    293,
                    0
                ],
                "title": "Frugal Federated Learning for Violence Detection: A Comparison of\n  LoRA-Tuned VLMs and Personalized CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frugal Federated Learning for Violence Detection: A Comparison of\n  LoRA-Tuned VLMs and Personalized CNNs"
                },
                "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems."
                },
                "authors": [
                    {
                        "name": "Sébastien Thuau"
                    },
                    {
                        "name": "Siba Haidar"
                    },
                    {
                        "name": "Ayush Bajracharya"
                    },
                    {
                        "name": "Rachid Chelouah"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Chelouah"
                },
                "author": "Rachid Chelouah",
                "arxiv_comment": "7 pages, 1 figure, FLTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17650v1",
                "updated": "2025-10-20T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    26,
                    38,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:26:38Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    26,
                    38,
                    0,
                    293,
                    0
                ],
                "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data\n  Augmentation for Robust Lung Ultrasound Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data\n  Augmentation for Robust Lung Ultrasound Classification"
                },
                "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging."
                },
                "authors": [
                    {
                        "name": "Athanasios Angelakis"
                    },
                    {
                        "name": "Amne Mousa"
                    },
                    {
                        "name": "Micah L. A. Heldeweg"
                    },
                    {
                        "name": "Laurens A. Biesheuvel"
                    },
                    {
                        "name": "Mark A. Haaksma"
                    },
                    {
                        "name": "Jasper M. Smit"
                    },
                    {
                        "name": "Pieter R. Tuinman"
                    },
                    {
                        "name": "Paul W. G. Elbers"
                    }
                ],
                "author_detail": {
                    "name": "Paul W. G. Elbers"
                },
                "author": "Paul W. G. Elbers",
                "arxiv_comment": "14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine\n  Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),\n  eess.IV (Image and Video Processing). Code available at:\n  https://github.com/Bluesman79/ZACH-ViT Installation: pip install zachvit\n  Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17642v1",
                "updated": "2025-10-20T15:21:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    21,
                    46,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:21:46Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    21,
                    46,
                    0,
                    293,
                    0
                ],
                "title": "Quantum Federated Learning: Architectural Elements and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Federated Learning: Architectural Elements and Future Directions"
                },
                "summary": "Federated learning (FL) focuses on collaborative model training without the\nneed to move the private data silos to a central server. Despite its several\nbenefits, the classical FL is plagued with several limitations, such as high\ncomputational power required for model training(which is critical for\nlow-resource clients), privacy risks, large update traffic, and non-IID\nheterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated\nLearning (QFL), which introduces quantum computation, that addresses multiple\nchallenges of classical FL and offers rapid computing capability while keeping\nthe classical orchestration intact. Firstly, we motivate QFL with a concrete\npresentation on pain points of classical FL, followed by a discussion on a\ngeneral architecture of QFL frameworks specifying the roles of client and\nserver, communication primitives and the quantum model placement. We classify\nthe existing QFL systems based on four criteria - quantum architecture (pure\nQFL, hybrid QFL), data processing method (quantum data encoding, quantum\nfeature mapping, and quantum feature selection & dimensionality reduction),\nnetwork topology (centralized, hierarchial, decentralized), and quantum\nsecurity mechanisms (quantum key distribution, quantum homomorphic encryption,\nquantum differential privacy, blind quantum computing). We then describe\napplications of QFL in healthcare, vehicular networks, wireless networks, and\nnetwork security, clearly highlighting where QFL improves communication\nefficiency, security, and performance compared to classical FL. We close with\nmultiple challenges and future works in QFL, including extension of QFL beyond\nclassification tasks, adversarial attacks, realistic hardware deployment,\nquantum communication protocols deployment, aggregation of different quantum\nmodels, and quantum split learning as an alternative to QFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) focuses on collaborative model training without the\nneed to move the private data silos to a central server. Despite its several\nbenefits, the classical FL is plagued with several limitations, such as high\ncomputational power required for model training(which is critical for\nlow-resource clients), privacy risks, large update traffic, and non-IID\nheterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated\nLearning (QFL), which introduces quantum computation, that addresses multiple\nchallenges of classical FL and offers rapid computing capability while keeping\nthe classical orchestration intact. Firstly, we motivate QFL with a concrete\npresentation on pain points of classical FL, followed by a discussion on a\ngeneral architecture of QFL frameworks specifying the roles of client and\nserver, communication primitives and the quantum model placement. We classify\nthe existing QFL systems based on four criteria - quantum architecture (pure\nQFL, hybrid QFL), data processing method (quantum data encoding, quantum\nfeature mapping, and quantum feature selection & dimensionality reduction),\nnetwork topology (centralized, hierarchial, decentralized), and quantum\nsecurity mechanisms (quantum key distribution, quantum homomorphic encryption,\nquantum differential privacy, blind quantum computing). We then describe\napplications of QFL in healthcare, vehicular networks, wireless networks, and\nnetwork security, clearly highlighting where QFL improves communication\nefficiency, security, and performance compared to classical FL. We close with\nmultiple challenges and future works in QFL, including extension of QFL beyond\nclassification tasks, adversarial attacks, realistic hardware deployment,\nquantum communication protocols deployment, aggregation of different quantum\nmodels, and quantum split learning as an alternative to QFL."
                },
                "authors": [
                    {
                        "name": "Siva Sai"
                    },
                    {
                        "name": "Abhishek Sawaika"
                    },
                    {
                        "name": "Prabhjot Singh"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "28 PAGES, 11 figures, introductory review article (book chapter), to\n  be published in a book with springer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; A.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17638v1",
                "updated": "2025-10-20T15:20:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    20,
                    5,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:20:05Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    20,
                    5,
                    0,
                    293,
                    0
                ],
                "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet\n  Arena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet\n  Arena"
                },
                "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears."
                },
                "authors": [
                    {
                        "name": "Qingchuan Yang"
                    },
                    {
                        "name": "Simon Mahns"
                    },
                    {
                        "name": "Sida Li"
                    },
                    {
                        "name": "Anri Gu"
                    },
                    {
                        "name": "Jibang Wu"
                    },
                    {
                        "name": "Haifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Xu"
                },
                "author": "Haifeng Xu",
                "arxiv_comment": "https://www.prophetarena.co/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17633v1",
                "updated": "2025-10-20T15:14:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    14,
                    25,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:14:25Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    14,
                    25,
                    0,
                    293,
                    0
                ],
                "title": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated\n  Refusal Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated\n  Refusal Steering"
                },
                "summary": "Large Audio-Language Models (LALMs) are becoming essential as a powerful\nmultimodal backbone for real-world applications. However, recent studies show\nthat audio inputs can more easily elicit harmful responses than text, exposing\nnew risks toward deployment. While safety alignment has made initial advances\nin LLMs and Large Vision-Language Models (LVLMs), we find that vanilla\nadaptation of these approaches to LALMs faces two key limitations: 1) LLM-based\nsteering fails under audio input due to the large distributional gap between\nactivations, and 2) prompt-based defenses induce over-refusals on benign-speech\nqueries. To address these challenges, we propose Safe-Ablated Refusal Steering\n(SARSteer), the first inference-time defense framework for LALMs. Specifically,\nSARSteer leverages text-derived refusal steering to enforce rejection without\nmanipulating audio inputs and introduces decomposed safe-space ablation to\nmitigate over-refusal. Extensive experiments demonstrate that SARSteer\nsignificantly improves harmful-query refusal while preserving benign responses,\nestablishing a principled step toward safety alignment in LALMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) are becoming essential as a powerful\nmultimodal backbone for real-world applications. However, recent studies show\nthat audio inputs can more easily elicit harmful responses than text, exposing\nnew risks toward deployment. While safety alignment has made initial advances\nin LLMs and Large Vision-Language Models (LVLMs), we find that vanilla\nadaptation of these approaches to LALMs faces two key limitations: 1) LLM-based\nsteering fails under audio input due to the large distributional gap between\nactivations, and 2) prompt-based defenses induce over-refusals on benign-speech\nqueries. To address these challenges, we propose Safe-Ablated Refusal Steering\n(SARSteer), the first inference-time defense framework for LALMs. Specifically,\nSARSteer leverages text-derived refusal steering to enforce rejection without\nmanipulating audio inputs and introduces decomposed safe-space ablation to\nmitigate over-refusal. Extensive experiments demonstrate that SARSteer\nsignificantly improves harmful-query refusal while preserving benign responses,\nestablishing a principled step toward safety alignment in LALMs."
                },
                "authors": [
                    {
                        "name": "Weilin Lin"
                    },
                    {
                        "name": "Jianze Li"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Li Liu"
                    }
                ],
                "author_detail": {
                    "name": "Li Liu"
                },
                "author": "Li Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17614v1",
                "updated": "2025-10-20T15:00:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    0,
                    2,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T15:00:02Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    15,
                    0,
                    2,
                    0,
                    293,
                    0
                ],
                "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and\n  Reward-Trend Guided Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and\n  Reward-Trend Guided Adaptive Exploration"
                },
                "summary": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection."
                },
                "authors": [
                    {
                        "name": "Praphul Singh"
                    },
                    {
                        "name": "Corey Barrett"
                    },
                    {
                        "name": "Sumana Srivasta"
                    },
                    {
                        "name": "Irfan Bulu"
                    },
                    {
                        "name": "Sri Gadde"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    }
                ],
                "author_detail": {
                    "name": "Krishnaram Kenthapadi"
                },
                "author": "Krishnaram Kenthapadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01611v3",
                "updated": "2025-10-20T14:59:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    59,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-02T02:49:06Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    2,
                    49,
                    6,
                    3,
                    275,
                    0
                ],
                "title": "PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: \\textit{Can LLMs be effectively applied to\npsychological counseling?} To determine whether an LLM can effectively take on\nthe role of a psychological counselor, the first step is to assess whether it\nmeets the qualifications required for such a role, namely the ability to pass\nthe U.S. National Counselor Certification Exam (NCE). This is because, just as\na human counselor must pass a certification exam to practice, an LLM must\ndemonstrate sufficient psychological knowledge to meet the standards required\nfor such a role. To address this, we introduce PsychCounsel-Bench, a benchmark\ngrounded in U.S.national counselor examinations, a licensure test for\nprofessional counselors that requires about 70\\% accuracy to pass.\nPsychCounsel-Bench comprises approximately 2,252 carefully curated\nsingle-choice questions, crafted to require deep understanding and broad enough\nto cover various sub-disciplines of psychology. This benchmark provides a\ncomprehensive assessment of an LLM's ability to function as a counselor. Our\nevaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and\nGemma3-27B achieve well above the passing threshold, while smaller open-source\nmodels (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results\nsuggest that only frontier LLMs are currently capable of meeting counseling\nexam standards, highlighting both the promise and the challenges of developing\npsychology-oriented LLMs. We release the proposed dataset for public use:\nhttps://github.com/cloversjtu/PsychCounsel-Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: \\textit{Can LLMs be effectively applied to\npsychological counseling?} To determine whether an LLM can effectively take on\nthe role of a psychological counselor, the first step is to assess whether it\nmeets the qualifications required for such a role, namely the ability to pass\nthe U.S. National Counselor Certification Exam (NCE). This is because, just as\na human counselor must pass a certification exam to practice, an LLM must\ndemonstrate sufficient psychological knowledge to meet the standards required\nfor such a role. To address this, we introduce PsychCounsel-Bench, a benchmark\ngrounded in U.S.national counselor examinations, a licensure test for\nprofessional counselors that requires about 70\\% accuracy to pass.\nPsychCounsel-Bench comprises approximately 2,252 carefully curated\nsingle-choice questions, crafted to require deep understanding and broad enough\nto cover various sub-disciplines of psychology. This benchmark provides a\ncomprehensive assessment of an LLM's ability to function as a counselor. Our\nevaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and\nGemma3-27B achieve well above the passing threshold, while smaller open-source\nmodels (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results\nsuggest that only frontier LLMs are currently capable of meeting counseling\nexam standards, highlighting both the promise and the challenges of developing\npsychology-oriented LLMs. We release the proposed dataset for public use:\nhttps://github.com/cloversjtu/PsychCounsel-Bench"
                },
                "authors": [
                    {
                        "name": "Min Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Min Zeng"
                },
                "author": "Min Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17611v1",
                "updated": "2025-10-20T14:57:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    57,
                    52,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:57:52Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    57,
                    52,
                    0,
                    293,
                    0
                ],
                "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum\n  Unsupervised Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum\n  Unsupervised Anomaly Detection"
                },
                "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications."
                },
                "authors": [
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Lei Fan"
                    },
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Weihang Zhang"
                    },
                    {
                        "name": "Wenbing Zhu"
                    },
                    {
                        "name": "Hong Yan"
                    },
                    {
                        "name": "Fang Chen"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Hongen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Hongen Liao"
                },
                "author": "Hongen Liao",
                "arxiv_comment": "Extended version of CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17604v1",
                "updated": "2025-10-20T14:52:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    50,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:52:50Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    50,
                    0,
                    293,
                    0
                ],
                "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Inertial Odometry for Cycling Based on Mixture of Experts\n  Algorithm"
                },
                "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%."
                },
                "authors": [
                    {
                        "name": "Hao Qiao"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xiaoyao Yu"
                    },
                    {
                        "name": "Jian kuang"
                    },
                    {
                        "name": "Xiaoji Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoji Niu"
                },
                "author": "Xiaoji Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12081v2",
                "updated": "2025-10-20T14:52:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    28,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-16T15:31:14Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    15,
                    31,
                    14,
                    5,
                    228,
                    0
                ],
                "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion\n  Language Models"
                },
                "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input. All the resources are available at\nhttps://walkermitty.github.io/VimoRAG/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input. All the resources are available at\nhttps://walkermitty.github.io/VimoRAG/"
                },
                "authors": [
                    {
                        "name": "Haidong Xu"
                    },
                    {
                        "name": "Guangwei Xu"
                    },
                    {
                        "name": "Zhedong Zheng"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Ruijie Guo"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Min zhang"
                    },
                    {
                        "name": "Hao Fei"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fei"
                },
                "author": "Hao Fei",
                "arxiv_comment": "Accepted by NeurIPS 2025; Project Page:\n  https://walkermitty.github.io/VimoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12814v2",
                "updated": "2025-10-20T14:52:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    52,
                    7,
                    0,
                    293,
                    0
                ],
                "published": "2025-05-19T07:45:09Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    7,
                    45,
                    9,
                    0,
                    139,
                    0
                ],
                "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control\n  for Advanced Role-Playing LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control\n  for Advanced Role-Playing LLMs"
                },
                "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."
                },
                "authors": [
                    {
                        "name": "Xilong Cheng"
                    },
                    {
                        "name": "Yunxiao Qin"
                    },
                    {
                        "name": "Yuting Tan"
                    },
                    {
                        "name": "Zhengnan Li"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Hongjiang Xiao"
                    },
                    {
                        "name": "Yuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Zhang"
                },
                "author": "Yuan Zhang",
                "arxiv_comment": "Pre-MIT Press publication version, has been accepted by TACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17603v1",
                "updated": "2025-10-20T14:51:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    51,
                    14,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:51:14Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    51,
                    14,
                    0,
                    293,
                    0
                ],
                "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D\n  Modeling"
                },
                "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications."
                },
                "authors": [
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Zuoou Li"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "NeurIPS 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11168v3",
                "updated": "2025-10-20T14:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    49,
                    14,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-12T04:07:11Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    4,
                    7,
                    11,
                    3,
                    163,
                    0
                ],
                "title": "WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture\n  Recognition"
                },
                "summary": "Human-machine interaction, particularly in prosthetic and robotic control,\nhas seen progress with gesture recognition via surface electromyographic (sEMG)\nsignals.However, classifying similar gestures that produce nearly identical\nmuscle signals remains a challenge, often reducing classification accuracy.\nTraditional deep learning models for sEMG gesture recognition are large and\ncomputationally expensive, limiting their deployment on resource-constrained\nembedded systems. In this work, we propose WaveFormer, a lightweight\ntransformer-based architecture tailored for sEMG gesture recognition. Our model\nintegrates time-domain and frequency-domain features through a novel learnable\nwavelet transform, enhancing feature extraction. In particular, the WaveletConv\nmodule, a multi-level wavelet decomposition layer with depthwise separable\nconvolution, ensures both efficiency and compactness. With just 3.1 million\nparameters, WaveFormer achieves 95% classification accuracy on the EPN612\ndataset, outperforming larger models. Furthermore, when profiled on a laptop\nequipped with an Intel CPU, INT8 quantization achieves real-time deployment\nwith a 6.75 ms inference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-machine interaction, particularly in prosthetic and robotic control,\nhas seen progress with gesture recognition via surface electromyographic (sEMG)\nsignals.However, classifying similar gestures that produce nearly identical\nmuscle signals remains a challenge, often reducing classification accuracy.\nTraditional deep learning models for sEMG gesture recognition are large and\ncomputationally expensive, limiting their deployment on resource-constrained\nembedded systems. In this work, we propose WaveFormer, a lightweight\ntransformer-based architecture tailored for sEMG gesture recognition. Our model\nintegrates time-domain and frequency-domain features through a novel learnable\nwavelet transform, enhancing feature extraction. In particular, the WaveletConv\nmodule, a multi-level wavelet decomposition layer with depthwise separable\nconvolution, ensures both efficiency and compactness. With just 3.1 million\nparameters, WaveFormer achieves 95% classification accuracy on the EPN612\ndataset, outperforming larger models. Furthermore, when profiled on a laptop\nequipped with an Intel CPU, INT8 quantization achieves real-time deployment\nwith a 6.75 ms inference latency."
                },
                "authors": [
                    {
                        "name": "Yanlong Chen"
                    },
                    {
                        "name": "Mattia Orlandi"
                    },
                    {
                        "name": "Pierangelo Maria Rapa"
                    },
                    {
                        "name": "Simone Benatti"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Yawei Li"
                    }
                ],
                "author_detail": {
                    "name": "Yawei Li"
                },
                "author": "Yawei Li",
                "arxiv_comment": "6 pages, 3 figures, accepted to IEEE EMBS Conference on Neural\n  Engineering (NER) 2025. Code and data are available at\n  https://github.com/ForeverBlue816/WaveFormer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17586v2",
                "updated": "2025-10-21T05:15:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    5,
                    15,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T14:35:19Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    35,
                    19,
                    0,
                    293,
                    0
                ],
                "title": "DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework"
                },
                "summary": "Large language models (LLMs) have advanced Text-to-SQL, yet existing\nsolutions still fall short of system-level reliability. The limitation is not\nmerely in individual modules - e.g., schema linking, reasoning, and\nverification - but more critically in the lack of structured orchestration that\nenforces correctness across the entire workflow. This gap motivates a paradigm\nshift: treating Text-to-SQL not as free-form language generation but as a\nsoftware-engineering problem that demands structured, verifiable orchestration.\nWe present DeepEye-SQL, a software-engineering-inspired framework that reframes\nText-to-SQL as the development of a small software program, executed through a\nverifiable process guided by the Software Development Life Cycle (SDLC).\nDeepEye-SQL integrates four synergistic stages: it grounds ambiguous user\nintent through semantic value retrieval and robust schema linking; enhances\nfault tolerance with N-version SQL generation using diverse reasoning\nparadigms; ensures deterministic verification via a tool-chain of unit tests\nand targeted LLM-guided revision; and introduces confidence-aware selection\nthat clusters execution results to estimate confidence and then takes a\nhigh-confidence shortcut or runs unbalanced pairwise adjudication in\nlow-confidence cases, yielding a calibrated, quality-gated output. This\nSDLC-aligned workflow transforms ad hoc query generation into a disciplined\nengineering process. Using ~30B open-source LLMs without any fine-tuning,\nDeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on\nSpider-Test, outperforming state-of-the-art solutions. This highlights that\nprincipled orchestration, rather than LLM scaling alone, is key to achieving\nsystem-level reliability in Text-to-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced Text-to-SQL, yet existing\nsolutions still fall short of system-level reliability. The limitation is not\nmerely in individual modules - e.g., schema linking, reasoning, and\nverification - but more critically in the lack of structured orchestration that\nenforces correctness across the entire workflow. This gap motivates a paradigm\nshift: treating Text-to-SQL not as free-form language generation but as a\nsoftware-engineering problem that demands structured, verifiable orchestration.\nWe present DeepEye-SQL, a software-engineering-inspired framework that reframes\nText-to-SQL as the development of a small software program, executed through a\nverifiable process guided by the Software Development Life Cycle (SDLC).\nDeepEye-SQL integrates four synergistic stages: it grounds ambiguous user\nintent through semantic value retrieval and robust schema linking; enhances\nfault tolerance with N-version SQL generation using diverse reasoning\nparadigms; ensures deterministic verification via a tool-chain of unit tests\nand targeted LLM-guided revision; and introduces confidence-aware selection\nthat clusters execution results to estimate confidence and then takes a\nhigh-confidence shortcut or runs unbalanced pairwise adjudication in\nlow-confidence cases, yielding a calibrated, quality-gated output. This\nSDLC-aligned workflow transforms ad hoc query generation into a disciplined\nengineering process. Using ~30B open-source LLMs without any fine-tuning,\nDeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on\nSpider-Test, outperforming state-of-the-art solutions. This highlights that\nprincipled orchestration, rather than LLM scaling alone, is key to achieving\nsystem-level reliability in Text-to-SQL."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Zhujun Xue"
                    },
                    {
                        "name": "Yinan Mei"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00432v2",
                "updated": "2025-10-20T14:27:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    27,
                    9,
                    0,
                    293,
                    0
                ],
                "published": "2025-07-01T05:23:05Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    23,
                    5,
                    1,
                    182,
                    0
                ],
                "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning"
                },
                "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models."
                },
                "authors": [
                    {
                        "name": "Maggie Huan"
                    },
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Tuney Zheng"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Minxin Du"
                    },
                    {
                        "name": "Radha Poovendran"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17576v1",
                "updated": "2025-10-20T14:24:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    24,
                    39,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:24:39Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    24,
                    39,
                    0,
                    293,
                    0
                ],
                "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot\n  Disassembly: Demonstration on EV Batteries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot\n  Disassembly: Demonstration on EV Batteries"
                },
                "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort."
                },
                "authors": [
                    {
                        "name": "Cansu Erdogan"
                    },
                    {
                        "name": "Cesar Alan Contreras"
                    },
                    {
                        "name": "Alireza Rastegarpanah"
                    },
                    {
                        "name": "Manolis Chiou"
                    },
                    {
                        "name": "Rustam Stolkin"
                    }
                ],
                "author_detail": {
                    "name": "Rustam Stolkin"
                },
                "author": "Rustam Stolkin",
                "arxiv_comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17575v1",
                "updated": "2025-10-20T14:22:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    22,
                    57,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:22:57Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    22,
                    57,
                    0,
                    293,
                    0
                ],
                "title": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support"
                },
                "summary": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research."
                },
                "authors": [
                    {
                        "name": "Ash Sharma"
                    },
                    {
                        "name": "Karen Cochrane"
                    },
                    {
                        "name": "James R. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "James R. Wallace"
                },
                "author": "James R. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17555v1",
                "updated": "2025-10-20T14:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    2,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T14:02:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    2,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "Language Confusion Gate: Language-Aware Decoding Through Model\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Confusion Gate: Language-Aware Decoding Through Model\n  Self-Distillation"
                },
                "summary": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate."
                },
                "authors": [
                    {
                        "name": "Collin Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Chenhan Yuan"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17535v1",
                "updated": "2025-10-20T13:39:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    39,
                    48,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:39:48Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    39,
                    48,
                    0,
                    293,
                    0
                ],
                "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How role-play shapes relevance judgment in zero-shot LLM rankers"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications."
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Catherine Chen"
                    },
                    {
                        "name": "Panagiotis Eustratiadis"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12024v3",
                "updated": "2025-10-21T07:03:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    7,
                    3,
                    45,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-21T07:42:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    42,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "FlexQuant: A Flexible and Efficient Dynamic Precision Switching\n  Framework for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQuant: A Flexible and Efficient Dynamic Precision Switching\n  Framework for LLM Quantization"
                },
                "summary": "The rapid advancement of large language models (LLMs) has exacerbated the\nmemory bottleneck due to the widening gap between model parameter scaling and\nhardware capabilities. While post-training quantization techniques effectively\nreduce memory overhead, existing methods predominantly rely on static\nquantization strategies, which struggle to adapt to dynamic workloads. To\naddress this, we propose FlexQuant, a dynamic precision-switching framework\nthat optimizes the trade-off between inference speed and accuracy. Leveraging\nmodel perplexity entropy and Kullback-Leibler divergence, FlexQuant enables\nfine-grained, layer-wise mixed-precision quantization and dynamically adjusts\nbit-widths during each token generation. FlexQuant provides a comprehensive\nanalysis of quantization strategies, introduces a precision requirement model\nfor optimal switching, and implements efficient fine-grained precision\nmanagement. Evaluations demonstrate that FlexQuant achieves a 1.3x end-to-end\nspeedup across diverse language tasks with negligible accuracy loss introduced.\nThis framework offers a flexible and adaptive solution for efficient LLM\ndeployment. Code is released at https://github.com/ZongwuWang/FlexQuant.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has exacerbated the\nmemory bottleneck due to the widening gap between model parameter scaling and\nhardware capabilities. While post-training quantization techniques effectively\nreduce memory overhead, existing methods predominantly rely on static\nquantization strategies, which struggle to adapt to dynamic workloads. To\naddress this, we propose FlexQuant, a dynamic precision-switching framework\nthat optimizes the trade-off between inference speed and accuracy. Leveraging\nmodel perplexity entropy and Kullback-Leibler divergence, FlexQuant enables\nfine-grained, layer-wise mixed-precision quantization and dynamically adjusts\nbit-widths during each token generation. FlexQuant provides a comprehensive\nanalysis of quantization strategies, introduces a precision requirement model\nfor optimal switching, and implements efficient fine-grained precision\nmanagement. Evaluations demonstrate that FlexQuant achieves a 1.3x end-to-end\nspeedup across diverse language tasks with negligible accuracy loss introduced.\nThis framework offers a flexible and adaptive solution for efficient LLM\ndeployment. Code is released at https://github.com/ZongwuWang/FlexQuant.git."
                },
                "authors": [
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "JinHong Xia"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Shouren Zhao"
                    },
                    {
                        "name": "Jinjin Li"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "10 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17534v1",
                "updated": "2025-10-20T13:37:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    37,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:37:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    37,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based\n  Guidance"
                },
                "summary": "Today's young people are facing increasing psychological stress due to\nvarious social issues. Traditional stress management tools often rely on static\nscripts or passive content, which are ineffective in alleviating stress. NieNie\naddresses this gap by combining rhythm biofeedback with real-time psychological\nguidance through a large language model (LLM), offering an interactive, tactile\nresponse. The system is specifically designed for young people experiencing\nemotional stress, collecting physiological signals such as heart rate\nvariability and generating adaptive squeeze-release rhythms via soft, tactile\ndevices. Utilising LLM, the system provides timely squeezing rhythms and\npsychologically guided feedback prompts, offering personalised rhythm games\nwhile reinforcing stress restructuring. Unlike traditional mental health apps,\nNieNie places users within an embodied interactive loop, leveraging tactile\ninteraction, biofeedback, and adaptive language support to create an immersive\nstress regulation experience. This study demonstrates how embodied systems can\nconnect bodily actions with mental health in everyday contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's young people are facing increasing psychological stress due to\nvarious social issues. Traditional stress management tools often rely on static\nscripts or passive content, which are ineffective in alleviating stress. NieNie\naddresses this gap by combining rhythm biofeedback with real-time psychological\nguidance through a large language model (LLM), offering an interactive, tactile\nresponse. The system is specifically designed for young people experiencing\nemotional stress, collecting physiological signals such as heart rate\nvariability and generating adaptive squeeze-release rhythms via soft, tactile\ndevices. Utilising LLM, the system provides timely squeezing rhythms and\npsychologically guided feedback prompts, offering personalised rhythm games\nwhile reinforcing stress restructuring. Unlike traditional mental health apps,\nNieNie places users within an embodied interactive loop, leveraging tactile\ninteraction, biofeedback, and adaptive language support to create an immersive\nstress regulation experience. This study demonstrates how embodied systems can\nconnect bodily actions with mental health in everyday contexts."
                },
                "authors": [
                    {
                        "name": "Yichen Yu"
                    },
                    {
                        "name": "Qiaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoran Wang"
                },
                "author": "Qiaoran Wang",
                "arxiv_doi": "10.1145/3714394.3750586",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3714394.3750586",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.17534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17532v1",
                "updated": "2025-10-20T13:35:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    35,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:35:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    35,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and\n  Interpretable Survival Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and\n  Interpretable Survival Prediction"
                },
                "summary": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology."
                },
                "authors": [
                    {
                        "name": "Raghu Vamshi Hemadri"
                    },
                    {
                        "name": "Geetha Krishna Guruju"
                    },
                    {
                        "name": "Kristi Topollai"
                    },
                    {
                        "name": "Anna Ewa Choromanska"
                    }
                ],
                "author_detail": {
                    "name": "Anna Ewa Choromanska"
                },
                "author": "Anna Ewa Choromanska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00161v2",
                "updated": "2025-10-20T13:29:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    29,
                    29,
                    0,
                    293,
                    0
                ],
                "published": "2025-07-31T21:04:12Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    4,
                    12,
                    3,
                    212,
                    0
                ],
                "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch the Weights: Unsupervised monitoring and control of fine-tuned\n  LLMs"
                },
                "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch."
                },
                "authors": [
                    {
                        "name": "Ziqian Zhong"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Raghunathan"
                },
                "author": "Aditi Raghunathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17521v1",
                "updated": "2025-10-20T13:21:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    21,
                    9,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:21:09Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    21,
                    9,
                    0,
                    293,
                    0
                ],
                "title": "Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense\n  CTFs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense\n  CTFs"
                },
                "summary": "We empirically evaluate whether AI systems are more effective at attacking or\ndefending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution\nframework, we deployed autonomous agents in 23 Attack/Defense CTF\nbattlegrounds. Statistical analysis reveals defensive agents achieve 54.3%\nunconstrained patching success versus 28.3% offensive initial access\n(p=0.0193), but this advantage disappears under operational constraints: when\ndefense requires maintaining availability (23.9%) and preventing all intrusions\n(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy\nanalysis suggests potential patterns in vulnerability exploitation, though\nlimited sample sizes preclude definitive conclusions. This study provides the\nfirst controlled empirical evidence challenging claims of AI attacker\nadvantage, demonstrating that defensive effectiveness critically depends on\nsuccess criteria, a nuance absent from conceptual analyses but essential for\ndeployment. These findings underscore the urgency for defenders to adopt\nopen-source Cybersecurity AI frameworks to maintain security equilibrium\nagainst accelerating offensive automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We empirically evaluate whether AI systems are more effective at attacking or\ndefending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution\nframework, we deployed autonomous agents in 23 Attack/Defense CTF\nbattlegrounds. Statistical analysis reveals defensive agents achieve 54.3%\nunconstrained patching success versus 28.3% offensive initial access\n(p=0.0193), but this advantage disappears under operational constraints: when\ndefense requires maintaining availability (23.9%) and preventing all intrusions\n(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy\nanalysis suggests potential patterns in vulnerability exploitation, though\nlimited sample sizes preclude definitive conclusions. This study provides the\nfirst controlled empirical evidence challenging claims of AI attacker\nadvantage, demonstrating that defensive effectiveness critically depends on\nsuccess criteria, a nuance absent from conceptual analyses but essential for\ndeployment. These findings underscore the urgency for defenders to adopt\nopen-source Cybersecurity AI frameworks to maintain security equilibrium\nagainst accelerating offensive automation."
                },
                "authors": [
                    {
                        "name": "Francesco Balassone"
                    },
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Stefan Rass"
                    },
                    {
                        "name": "Martin Pinzger"
                    },
                    {
                        "name": "Gaetano Perrone"
                    },
                    {
                        "name": "Simon Pietro Romano"
                    },
                    {
                        "name": "Peter Schartner"
                    }
                ],
                "author_detail": {
                    "name": "Peter Schartner"
                },
                "author": "Peter Schartner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17516v1",
                "updated": "2025-10-20T13:14:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    14,
                    38,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    14,
                    38,
                    0,
                    293,
                    0
                ],
                "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors"
                },
                "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators."
                },
                "authors": [
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Paul Röttger"
                    }
                ],
                "author_detail": {
                    "name": "Paul Röttger"
                },
                "author": "Paul Röttger",
                "arxiv_comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17509v1",
                "updated": "2025-10-20T13:05:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    5,
                    22,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T13:05:22Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    5,
                    22,
                    0,
                    293,
                    0
                ],
                "title": "Annotation-Efficient Universal Honesty Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotation-Efficient Universal Honesty Alignment"
                },
                "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Jingtong Wu"
                    },
                    {
                        "name": "Zengxin Han"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17501v1",
                "updated": "2025-10-20T12:54:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:54:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"
                },
                "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization."
                },
                "authors": [
                    {
                        "name": "Yuanli Wu"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Yue Du"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17496v1",
                "updated": "2025-10-20T12:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    51,
                    13,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:51:13Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    51,
                    13,
                    0,
                    293,
                    0
                ],
                "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models"
                },
                "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."
                },
                "authors": [
                    {
                        "name": "Giacomo Camposampiero"
                    },
                    {
                        "name": "Michael Hersche"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Abbas Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rahimi"
                },
                "author": "Abbas Rahimi",
                "arxiv_comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17491v1",
                "updated": "2025-10-20T12:46:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:46:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "title": "Empowering Real-World: A Survey on the Technology, Practice, and\n  Evaluation of LLM-driven Industry Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Real-World: A Survey on the Technology, Practice, and\n  Evaluation of LLM-driven Industry Agents"
                },
                "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents."
                },
                "authors": [
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Liang Yue"
                    },
                    {
                        "name": "Jinxin Fan"
                    },
                    {
                        "name": "Caishen Zhou"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Mingming Zhao"
                    },
                    {
                        "name": "Shixiong Kai"
                    },
                    {
                        "name": "Kaiyang Guo"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Wenjing Cun"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17489v1",
                "updated": "2025-10-20T12:41:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    41,
                    44,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:41:44Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    41,
                    44,
                    0,
                    293,
                    0
                ],
                "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured\n  Hierarchical Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured\n  Hierarchical Representation Learning"
                },
                "summary": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree."
                },
                "authors": [
                    {
                        "name": "Yongxin He"
                    },
                    {
                        "name": "Shan Zhang"
                    },
                    {
                        "name": "Yixuan Cao"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "To appear in NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00783v2",
                "updated": "2025-10-20T12:31:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    31,
                    56,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-01T02:20:45Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    2,
                    20,
                    45,
                    6,
                    152,
                    0
                ],
                "title": "KG-TRACES: Enhancing Large Language Models with Knowledge\n  Graph-constrained Trajectory Reasoning and Attribution Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-TRACES: Enhancing Large Language Models with Knowledge\n  Graph-constrained Trajectory Reasoning and Attribution Supervision"
                },
                "summary": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES."
                },
                "authors": [
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Botian Shi"
                    }
                ],
                "author_detail": {
                    "name": "Botian Shi"
                },
                "author": "Botian Shi",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17483v1",
                "updated": "2025-10-20T12:27:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    27,
                    55,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:27:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    27,
                    55,
                    0,
                    293,
                    0
                ],
                "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs."
                },
                "authors": [
                    {
                        "name": "Zheyue Tan"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Yueqing Zhuang"
                    },
                    {
                        "name": "Yadong Li"
                    },
                    {
                        "name": "Guowei Niu"
                    },
                    {
                        "name": "Cheng Qin"
                    },
                    {
                        "name": "Zhuyu Yao"
                    },
                    {
                        "name": "Congyi Liu"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Boxun Li"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17476v1",
                "updated": "2025-10-20T12:19:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    19,
                    8,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:19:08Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    19,
                    8,
                    0,
                    293,
                    0
                ],
                "title": "Disparities in Multilingual LLM-Based Healthcare Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disparities in Multilingual LLM-Based Healthcare Q&A"
                },
                "summary": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Ipek Baris Schlicht"
                    },
                    {
                        "name": "Burcu Sayin"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Frederik M. Labonté"
                    },
                    {
                        "name": "Cesare Barbera"
                    },
                    {
                        "name": "Marco Viviani"
                    },
                    {
                        "name": "Paolo Rosso"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00921v2",
                "updated": "2025-10-20T12:17:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    17,
                    41,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-31T16:06:12Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    6,
                    12,
                    6,
                    243,
                    0
                ],
                "title": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling"
                },
                "summary": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining in-context learning (ICL) from\ndemonstrations with supervised fine-tuning. SIFT considerably outperforms both\nICL and decoder-as-encoder fine-tuning baselines on a range of standard SL\ntasks. We further find that although long context hinders the performance of\ngenerative SL in both ICL and SIFT, this deficiency can be mitigated by\nremoving the instruction, as instructions are shown to be largely unnecessary\nfor achieving strong SL performance with SIFT. Our findings highlight strengths\nand limitations of SL with LLMs, underscoring the importance of a\nresponse-based generative task formulation for effective SL performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining in-context learning (ICL) from\ndemonstrations with supervised fine-tuning. SIFT considerably outperforms both\nICL and decoder-as-encoder fine-tuning baselines on a range of standard SL\ntasks. We further find that although long context hinders the performance of\ngenerative SL in both ICL and SIFT, this deficiency can be mitigated by\nremoving the instruction, as instructions are shown to be largely unnecessary\nfor achieving strong SL performance with SIFT. Our findings highlight strengths\nand limitations of SL with LLMs, underscoring the importance of a\nresponse-based generative task formulation for effective SL performance."
                },
                "authors": [
                    {
                        "name": "David Dukić"
                    },
                    {
                        "name": "Goran Glavaš"
                    },
                    {
                        "name": "Jan Šnajder"
                    }
                ],
                "author_detail": {
                    "name": "Jan Šnajder"
                },
                "author": "Jan Šnajder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17472v1",
                "updated": "2025-10-20T12:14:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    14,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T12:14:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    14,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time\n  Training for Reliable Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Self-Consistency: Statistical Guarantees and Test-Time\n  Training for Reliable Reasoning in LLMs"
                },
                "summary": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs."
                },
                "authors": [
                    {
                        "name": "Paula Cordero-Encinar"
                    },
                    {
                        "name": "Andrew B. Duncan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew B. Duncan"
                },
                "author": "Andrew B. Duncan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01879v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01879v4",
                "updated": "2025-10-20T12:05:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    5,
                    19,
                    0,
                    293,
                    0
                ],
                "published": "2025-02-26T17:26:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    26,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,\n  And Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio,\n  And Vision"
                },
                "summary": "This work proposes an industry-level omni-modal large language model (LLM)\npipeline that integrates auditory, visual, and linguistic modalities to\novercome challenges such as limited tri-modal datasets, high computational\ncosts, and complex feature alignments. Our pipeline consists of three main\ncomponents: First, a modular framework enabling flexible configuration of\nvarious encoder-LLM-decoder architectures. Second, a lightweight training\nstrategy that pre-trains audio-language alignment on the state-of-the-art\nvision-language model Qwen2.5-VL, thus avoiding the costly pre-training of\nvision-specific modalities. Third, an audio synthesis pipeline that generates\nhigh-quality audio-text data from diverse real-world scenarios, supporting\napplications such as Automatic Speech Recognition and Speech-to-Speech chat. To\nthis end, we introduce an industry-level omni-modal LLM, Nexus. Extensive\nexperiments validate the efficacy of our pipeline, yielding the following key\nfindings:(1) In the visual understanding task, Nexus exhibits superior\nperformance compared with its backbone model - Qwen2.5-VL-7B, validating the\nefficiency of our training strategy. (2) Within the English Spoken\nQuestion-Answering task, the model achieves better accuracy than the\nsame-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In\nour real-world ASR testset, Nexus achieves outstanding performance, indicating\nits robustness in real scenarios. (4) In the Speech-to-Text Translation task,\nour model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task,\nbased on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is\ncomparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth\nanalysis of tri-modal alignment reveals that incorporating the audio modality\nenhances representational alignment between vision and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes an industry-level omni-modal large language model (LLM)\npipeline that integrates auditory, visual, and linguistic modalities to\novercome challenges such as limited tri-modal datasets, high computational\ncosts, and complex feature alignments. Our pipeline consists of three main\ncomponents: First, a modular framework enabling flexible configuration of\nvarious encoder-LLM-decoder architectures. Second, a lightweight training\nstrategy that pre-trains audio-language alignment on the state-of-the-art\nvision-language model Qwen2.5-VL, thus avoiding the costly pre-training of\nvision-specific modalities. Third, an audio synthesis pipeline that generates\nhigh-quality audio-text data from diverse real-world scenarios, supporting\napplications such as Automatic Speech Recognition and Speech-to-Speech chat. To\nthis end, we introduce an industry-level omni-modal LLM, Nexus. Extensive\nexperiments validate the efficacy of our pipeline, yielding the following key\nfindings:(1) In the visual understanding task, Nexus exhibits superior\nperformance compared with its backbone model - Qwen2.5-VL-7B, validating the\nefficiency of our training strategy. (2) Within the English Spoken\nQuestion-Answering task, the model achieves better accuracy than the\nsame-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In\nour real-world ASR testset, Nexus achieves outstanding performance, indicating\nits robustness in real scenarios. (4) In the Speech-to-Text Translation task,\nour model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task,\nbased on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is\ncomparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth\nanalysis of tri-modal alignment reveals that incorporating the audio modality\nenhances representational alignment between vision and language."
                },
                "authors": [
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Yingji Zhang"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Weijie Zhang"
                    },
                    {
                        "name": "Chenggong Gong"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Ziliang Gan"
                    },
                    {
                        "name": "Ziao Wang"
                    },
                    {
                        "name": "Haipang Wu"
                    },
                    {
                        "name": "Ji Liu"
                    },
                    {
                        "name": "André Freitas"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Rongjuncheng Zhang"
                    },
                    {
                        "name": "Yong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yong Dai"
                },
                "author": "Yong Dai",
                "arxiv_comment": "Project: https://github.com/HiThink-Research/NEXUS-O",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01879v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01879v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15188v2",
                "updated": "2025-10-20T12:03:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    3,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-16T23:14:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    23,
                    14,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph\n  Anomaly Detection and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph\n  Anomaly Detection and LLMs"
                },
                "summary": "Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade\ndetection in system-level audit logs. Provenance graphs model these logs as\nconnected entities and events, revealing relationships that are missed by\nlinear log representations. Existing systems apply anomaly detection to these\ngraphs but often suffer from high false positive rates and coarse-grained\nalerts. Their reliance on node attributes like file paths or IPs leads to\nspurious correlations, reducing detection robustness and reliability. To fully\nunderstand an attack's progression and impact, security analysts need systems\nthat can generate accurate, human-like narratives of the entire attack. To\naddress these challenges, we introduce OCR-APT, a system for APT detection and\nreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks\n(GNNs) for subgraph anomaly detection, learning behavior patterns around nodes\nrather than fragile attributes such as file paths or IPs. This approach leads\nto a more robust anomaly detection. It then iterates over detected subgraphs\nusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.\nEach stage is validated before proceeding, reducing hallucinations and ensuring\nan interpretable final report. Our evaluations on the DARPA TC3, OpTC, and\nNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both\ndetection accuracy and alert interpretability. Moreover, OCR-APT reconstructs\nhuman-like reports that comprehensively capture the attack story.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade\ndetection in system-level audit logs. Provenance graphs model these logs as\nconnected entities and events, revealing relationships that are missed by\nlinear log representations. Existing systems apply anomaly detection to these\ngraphs but often suffer from high false positive rates and coarse-grained\nalerts. Their reliance on node attributes like file paths or IPs leads to\nspurious correlations, reducing detection robustness and reliability. To fully\nunderstand an attack's progression and impact, security analysts need systems\nthat can generate accurate, human-like narratives of the entire attack. To\naddress these challenges, we introduce OCR-APT, a system for APT detection and\nreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks\n(GNNs) for subgraph anomaly detection, learning behavior patterns around nodes\nrather than fragile attributes such as file paths or IPs. This approach leads\nto a more robust anomaly detection. It then iterates over detected subgraphs\nusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.\nEach stage is validated before proceeding, reducing hallucinations and ensuring\nan interpretable final report. Our evaluations on the DARPA TC3, OpTC, and\nNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both\ndetection accuracy and alert interpretability. Moreover, OCR-APT reconstructs\nhuman-like reports that comprehensively capture the attack story."
                },
                "authors": [
                    {
                        "name": "Ahmed Aly"
                    },
                    {
                        "name": "Essam Mansour"
                    },
                    {
                        "name": "Amr Youssef"
                    }
                ],
                "author_detail": {
                    "name": "Amr Youssef"
                },
                "arxiv_affiliation": "Concordia University",
                "author": "Amr Youssef",
                "arxiv_comment": "This is the authors' extended version of the paper accepted for\n  publication at the ACM SIGSAC Conference on Computer and Communications\n  Security (CCS 2025). The final published version is available at\n  https://doi.org/10.1145/3719027.3765219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13472v2",
                "updated": "2025-10-20T12:00:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    0,
                    10,
                    0,
                    293,
                    0
                ],
                "published": "2025-04-18T05:26:32Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    26,
                    32,
                    4,
                    108,
                    0
                ],
                "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeVisionary: An Agent-based Framework for Evaluating Large Language\n  Models in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities. However,\nthey generally evaluate the generated code based on static prompts, and tend to\nfail for complex code scenarios which typically involve multiple requirements\nand require more contextual information. In addition, these approaches lack\nfine-grained evaluation for complex code, resulting in limited explainability.\nTo mitigate the limitations, we propose CodeVisionary, the first agent-based\nevaluation framework for complex code generation. CodeVisionary consists of two\nstages: (1) Requirement-guided multi-dimensional context distillation stage and\n(2) Fine-grained scoring and summarization stage. A comprehensive evaluation\nreport is also generated for enhanced explainability. For validation, we\nconstruct a new benchmark consisting of 363 samples spanning 37 coding\nscenarios and 23 programming languages. Extensive experiments demonstrate that\nCodeVisionary achieves the best performance among three baselines for\nevaluating complex code generation, outperforming the best baseline with\naverage improvements of 0.217, 0.163, and 0.141 in Pearson, Spearman, and\nKendall-Tau coefficients, respectively. The resources of CodeVisionary are\navailable at https://github.com/Eshe0922/CodeVisionary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities. However,\nthey generally evaluate the generated code based on static prompts, and tend to\nfail for complex code scenarios which typically involve multiple requirements\nand require more contextual information. In addition, these approaches lack\nfine-grained evaluation for complex code, resulting in limited explainability.\nTo mitigate the limitations, we propose CodeVisionary, the first agent-based\nevaluation framework for complex code generation. CodeVisionary consists of two\nstages: (1) Requirement-guided multi-dimensional context distillation stage and\n(2) Fine-grained scoring and summarization stage. A comprehensive evaluation\nreport is also generated for enhanced explainability. For validation, we\nconstruct a new benchmark consisting of 363 samples spanning 37 coding\nscenarios and 23 programming languages. Extensive experiments demonstrate that\nCodeVisionary achieves the best performance among three baselines for\nevaluating complex code generation, outperforming the best baseline with\naverage improvements of 0.217, 0.163, and 0.141 in Pearson, Spearman, and\nKendall-Tau coefficients, respectively. The resources of CodeVisionary are\navailable at https://github.com/Eshe0922/CodeVisionary."
                },
                "authors": [
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03313v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03313v3",
                "updated": "2025-10-20T11:58:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    58,
                    5,
                    0,
                    293,
                    0
                ],
                "published": "2025-03-05T09:45:22Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    45,
                    22,
                    2,
                    64,
                    0
                ],
                "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph\n  Foundation Models"
                },
                "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."
                },
                "authors": [
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03313v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17462v1",
                "updated": "2025-10-20T11:56:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    56,
                    2,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T11:56:02Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    56,
                    2,
                    0,
                    293,
                    0
                ],
                "title": "ORIX: Orchestration of RIS with xApps for Smart Wireless Factory\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORIX: Orchestration of RIS with xApps for Smart Wireless Factory\n  Environments"
                },
                "summary": "The vision of a smart wireless factory (SWF) demands highly flexible,\nlow-latency, and reliable connectivity that goes beyond conventional wireless\nsolutions. Reconfigurable intelligent surface (RIS)-empowered communications,\nwhen integrated with the open radio access network (O-RAN) architectures, have\nemerged as a promising enabler to meet these challenging requirements. This\narticle introduces the methodology for the orchestration of RIS with xApps\n(ORIX), bringing the RIS technology into the O-RAN ecosystem through xApp-based\ncontrol for SWF environments. ORIX features three key components: an\nO-RAN-compliant RIS service model for dynamic configuration, an RIS channel\nsimulator that supports 3GPP indoor factory models with multiple industrial\nscenarios, and practical RIS optimization strategies with finite-resolution\ncontrol. Together, these elements provide a realistic end-to-end emulation\nplatform for evaluating RIS placement, control, and performance in SWF\nenvironments prior to deployment. The presented case study demonstrates how\nORIX enables the evaluation of achievable performance gains, exploration of\ntrade-offs among key RIS design parameters, and identification of deployment\nstrategies that balance system performance with practical implementation\nconstraints. By bridging theoretical advances with industrial feasibility, ORIX\nlays the groundwork for RIS-assisted O-RAN networks to power next-generation\nwireless communication in industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision of a smart wireless factory (SWF) demands highly flexible,\nlow-latency, and reliable connectivity that goes beyond conventional wireless\nsolutions. Reconfigurable intelligent surface (RIS)-empowered communications,\nwhen integrated with the open radio access network (O-RAN) architectures, have\nemerged as a promising enabler to meet these challenging requirements. This\narticle introduces the methodology for the orchestration of RIS with xApps\n(ORIX), bringing the RIS technology into the O-RAN ecosystem through xApp-based\ncontrol for SWF environments. ORIX features three key components: an\nO-RAN-compliant RIS service model for dynamic configuration, an RIS channel\nsimulator that supports 3GPP indoor factory models with multiple industrial\nscenarios, and practical RIS optimization strategies with finite-resolution\ncontrol. Together, these elements provide a realistic end-to-end emulation\nplatform for evaluating RIS placement, control, and performance in SWF\nenvironments prior to deployment. The presented case study demonstrates how\nORIX enables the evaluation of achievable performance gains, exploration of\ntrade-offs among key RIS design parameters, and identification of deployment\nstrategies that balance system performance with practical implementation\nconstraints. By bridging theoretical advances with industrial feasibility, ORIX\nlays the groundwork for RIS-assisted O-RAN networks to power next-generation\nwireless communication in industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Sefa Kayraklik"
                    },
                    {
                        "name": "Ali Fuat Sahin"
                    },
                    {
                        "name": "Onur Salan"
                    },
                    {
                        "name": "Recep A. Tasci"
                    },
                    {
                        "name": "Recep Vural"
                    },
                    {
                        "name": "Yusuf Islam Tek"
                    },
                    {
                        "name": "Ertugrul Basar"
                    },
                    {
                        "name": "Ibrahim Hokelek"
                    },
                    {
                        "name": "Ali Gorcin"
                    },
                    {
                        "name": "Karim Boutiba"
                    },
                    {
                        "name": "Adlen Ksentini"
                    }
                ],
                "author_detail": {
                    "name": "Adlen Ksentini"
                },
                "author": "Adlen Ksentini",
                "arxiv_comment": "Submitted in IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17460v1",
                "updated": "2025-10-20T11:49:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    49,
                    26,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T11:49:26Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    49,
                    26,
                    0,
                    293,
                    0
                ],
                "title": "Evaluating Large Language Models on Urdu Idiom Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models on Urdu Idiom Translation"
                },
                "summary": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu."
                },
                "authors": [
                    {
                        "name": "Muhammad Farmal Khan"
                    },
                    {
                        "name": "Mousumi Akter"
                    }
                ],
                "author_detail": {
                    "name": "Mousumi Akter"
                },
                "author": "Mousumi Akter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21974v3",
                "updated": "2025-10-20T11:36:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    36,
                    6,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-27T07:32:16Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    7,
                    32,
                    16,
                    4,
                    178,
                    0
                ],
                "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism"
                },
                "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation."
                },
                "authors": [
                    {
                        "name": "Simon Münker"
                    },
                    {
                        "name": "Nils Schwager"
                    },
                    {
                        "name": "Achim Rettinger"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettinger"
                },
                "author": "Achim Rettinger",
                "arxiv_comment": "11 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02298v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02298v4",
                "updated": "2025-10-20T11:32:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    32,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-04T11:06:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers."
                },
                "authors": [
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02298v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02298v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17415v1",
                "updated": "2025-10-20T10:57:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    57,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:57:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    57,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional\n  Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BenCao: An Instruction-Tuned Large Language Model for Traditional\n  Chinese Medicine"
                },
                "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Jiacheng Xie"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Hanyao Zhang"
                    },
                    {
                        "name": "Lening Zhao"
                    },
                    {
                        "name": "Jiaxuan He"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Xiaoting Tang"
                    },
                    {
                        "name": "Guanghui An"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17403v1",
                "updated": "2025-10-20T10:43:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    43,
                    51,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:43:51Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    43,
                    51,
                    0,
                    293,
                    0
                ],
                "title": "Process Automation Architecture Using RFID for Transparent Voting\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Automation Architecture Using RFID for Transparent Voting\n  Systems"
                },
                "summary": "This paper presents the development of a process automation architecture\nleveraging Radio Frequency Identification (RFID) technology for secure,\ntransparent and efficient voting systems. The proposed architecture automates\nthe voting workflow through RFID-enabled voter identification, encrypted vote\ncasting, and secure data transmission. Each eligible voter receives a smart\nRFID card containing a uniquely encrypted identifier, which is verified using\nan RC522 reader interfaced with a microcontroller. Upon successful\nverification, the voter interacts with a touchscreen interface to cast a vote,\nwhich is then encrypted using AES-128 and securely stored on a local SD card or\ntransmitted via GSM to a central server. A tamper-proof monitoring mechanism\nrecords each session with time-stamped digital signatures, ensuring\nauditability and data integrity. The architecture is designed to function in\nboth online and offline modes, with an automated batch synchronization\nmechanism that updates vote records once network connectivity is restored.\nSystem testing in simulated environments confirmed 100% voter authentication\naccuracy, minimized latency (average voting time of 11.5 seconds), and\nrobustness against cloning, double voting, and data interception. The\nintegration of real-time monitoring and secure process control modules enables\nelectoral authorities to automate data logging, detect anomalies, and validate\nsystem integrity dynamically. This work demonstrates a scalable,\nautomation-driven solution for voting infrastructure, offering enhanced\ntransparency, resilience, and deployment flexibility, especially in\nenvironments where digital transformation of electoral processes is critically\nneeded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development of a process automation architecture\nleveraging Radio Frequency Identification (RFID) technology for secure,\ntransparent and efficient voting systems. The proposed architecture automates\nthe voting workflow through RFID-enabled voter identification, encrypted vote\ncasting, and secure data transmission. Each eligible voter receives a smart\nRFID card containing a uniquely encrypted identifier, which is verified using\nan RC522 reader interfaced with a microcontroller. Upon successful\nverification, the voter interacts with a touchscreen interface to cast a vote,\nwhich is then encrypted using AES-128 and securely stored on a local SD card or\ntransmitted via GSM to a central server. A tamper-proof monitoring mechanism\nrecords each session with time-stamped digital signatures, ensuring\nauditability and data integrity. The architecture is designed to function in\nboth online and offline modes, with an automated batch synchronization\nmechanism that updates vote records once network connectivity is restored.\nSystem testing in simulated environments confirmed 100% voter authentication\naccuracy, minimized latency (average voting time of 11.5 seconds), and\nrobustness against cloning, double voting, and data interception. The\nintegration of real-time monitoring and secure process control modules enables\nelectoral authorities to automate data logging, detect anomalies, and validate\nsystem integrity dynamically. This work demonstrates a scalable,\nautomation-driven solution for voting infrastructure, offering enhanced\ntransparency, resilience, and deployment flexibility, especially in\nenvironments where digital transformation of electoral processes is critically\nneeded."
                },
                "authors": [
                    {
                        "name": "Stella N. Arinze"
                    },
                    {
                        "name": "Patrick U. Okafor"
                    },
                    {
                        "name": "Onyekachi M. Egwuagu"
                    },
                    {
                        "name": "Augustine O. Nwajana"
                    }
                ],
                "author_detail": {
                    "name": "Augustine O. Nwajana"
                },
                "author": "Augustine O. Nwajana",
                "arxiv_comment": "7 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17402v1",
                "updated": "2025-10-20T10:43:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    43,
                    33,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:43:33Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    43,
                    33,
                    0,
                    293,
                    0
                ],
                "title": "Leveraging Group Relative Policy Optimization to Advance Large Language\n  Models in Traditional Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Group Relative Policy Optimization to Advance Large Language\n  Models in Traditional Chinese Medicine"
                },
                "summary": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Jiacheng Xie"
                    },
                    {
                        "name": "Shuai Zeng"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Xiaoting Tang"
                    },
                    {
                        "name": "Guanghui An"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17393v1",
                "updated": "2025-10-20T10:34:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    34,
                    1,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:34:01Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    34,
                    1,
                    0,
                    293,
                    0
                ],
                "title": "3S-Trader: A Multi-LLM Framework for Adaptive Stock Scoring, Strategy,\n  and Selection in Portfolio Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3S-Trader: A Multi-LLM Framework for Adaptive Stock Scoring, Strategy,\n  and Selection in Portfolio Optimization"
                },
                "summary": "Large Language Models (LLMs) have recently gained popularity in stock trading\nfor their ability to process multimodal financial data. However, most existing\nmethods focus on single-stock trading and lack the capacity to reason over\nmultiple candidates for portfolio construction. Moreover, they typically lack\nthe flexibility to revise their strategies in response to market shifts,\nlimiting their adaptability in real-world trading. To address these challenges,\nwe propose 3S-Trader, a training-free framework that incorporates scoring,\nstrategy, and selection modules for stock portfolio construction. The scoring\nmodule summarizes each stock's recent signals into a concise report covering\nmultiple scoring dimensions, enabling efficient comparison across candidates.\nThe strategy module analyzes historical strategies and overall market\nconditions to iteratively generate an optimized selection strategy. Based on\nthis strategy, the selection module identifies and assembles a portfolio by\nchoosing stocks with higher scores in relevant dimensions. We evaluate our\nframework across four distinct stock universes, including the Dow Jones\nIndustrial Average (DJIA) constituents and three sector-specific stock sets.\nCompared with existing multi-LLM frameworks and time-series-based baselines,\n3S-Trader achieves the highest accumulated return of 131.83% on DJIA\nconstituents with a Sharpe ratio of 0.31 and Calmar ratio of 11.84, while also\ndelivering consistently strong results across other sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently gained popularity in stock trading\nfor their ability to process multimodal financial data. However, most existing\nmethods focus on single-stock trading and lack the capacity to reason over\nmultiple candidates for portfolio construction. Moreover, they typically lack\nthe flexibility to revise their strategies in response to market shifts,\nlimiting their adaptability in real-world trading. To address these challenges,\nwe propose 3S-Trader, a training-free framework that incorporates scoring,\nstrategy, and selection modules for stock portfolio construction. The scoring\nmodule summarizes each stock's recent signals into a concise report covering\nmultiple scoring dimensions, enabling efficient comparison across candidates.\nThe strategy module analyzes historical strategies and overall market\nconditions to iteratively generate an optimized selection strategy. Based on\nthis strategy, the selection module identifies and assembles a portfolio by\nchoosing stocks with higher scores in relevant dimensions. We evaluate our\nframework across four distinct stock universes, including the Dow Jones\nIndustrial Average (DJIA) constituents and three sector-specific stock sets.\nCompared with existing multi-LLM frameworks and time-series-based baselines,\n3S-Trader achieves the highest accumulated return of 131.83% on DJIA\nconstituents with a Sharpe ratio of 0.31 and Calmar ratio of 11.84, while also\ndelivering consistently strong results across other sectors."
                },
                "authors": [
                    {
                        "name": "Kefan Chen"
                    },
                    {
                        "name": "Hussain Ahmad"
                    },
                    {
                        "name": "Diksha Goel"
                    },
                    {
                        "name": "Claudia Szabo"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Szabo"
                },
                "author": "Claudia Szabo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17389v1",
                "updated": "2025-10-20T10:30:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    30,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:30:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    30,
                    40,
                    0,
                    293,
                    0
                ],
                "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level\n  Adaptability in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level\n  Adaptability in LLMs"
                },
                "summary": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt."
                },
                "authors": [
                    {
                        "name": "Numaan Naeem"
                    },
                    {
                        "name": "Abdellah El Mekki"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Abdul-Mageed"
                },
                "author": "Muhammad Abdul-Mageed",
                "arxiv_comment": "28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17388v1",
                "updated": "2025-10-20T10:26:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    26,
                    26,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:26:26Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    26,
                    26,
                    0,
                    293,
                    0
                ],
                "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple,\n  Self-Contained Directives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple,\n  Self-Contained Directives"
                },
                "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following."
                },
                "authors": [
                    {
                        "name": "Henry Lim"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "arxiv_comment": "11 pages, 1 figure, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17385v1",
                "updated": "2025-10-20T10:22:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    22,
                    1,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:22:01Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    22,
                    1,
                    0,
                    293,
                    0
                ],
                "title": "TabR1: Taming GRPO for tabular reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabR1: Taming GRPO for tabular reasoning LLMs"
                },
                "summary": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B)."
                },
                "authors": [
                    {
                        "name": "Pengxiang Cai"
                    },
                    {
                        "name": "Zihao Gao"
                    },
                    {
                        "name": "Jintai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jintai Chen"
                },
                "author": "Jintai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18065v2",
                "updated": "2025-10-20T10:19:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    19,
                    21,
                    0,
                    293,
                    0
                ],
                "published": "2025-03-23T13:18:17Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    13,
                    18,
                    17,
                    6,
                    82,
                    0
                ],
                "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation\n  Models for Augmenting Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation\n  Models for Augmenting Vision-Language Navigation"
                },
                "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction\npairs can be obtained in both simulator-free and labor-saving manners to\npromote generalization. Specifically, we first introduce Object-Enriched\nObservation Rewriting, where we combine Vision-Language Models (VLMs) and Large\nLanguage Models (LLMs) to derive rewritten object-enriched scene descriptions,\nenabling observation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction\npairs can be obtained in both simulator-free and labor-saving manners to\npromote generalization. Specifically, we first introduce Object-Enriched\nObservation Rewriting, where we combine Vision-Language Models (VLMs) and Large\nLanguage Models (LLMs) to derive rewritten object-enriched scene descriptions,\nenabling observation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM."
                },
                "authors": [
                    {
                        "name": "Ziming Wei"
                    },
                    {
                        "name": "Bingqian Lin"
                    },
                    {
                        "name": "Yunshuang Nie"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Shikui Ma"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17376v1",
                "updated": "2025-10-20T10:11:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    11,
                    34,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:11:34Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    11,
                    34,
                    0,
                    293,
                    0
                ],
                "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent"
                },
                "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution."
                },
                "authors": [
                    {
                        "name": "Yongmin Li"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "to be published in ICSE 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01234v2",
                "updated": "2025-10-20T10:09:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    9,
                    5,
                    0,
                    293,
                    0
                ],
                "published": "2025-04-01T22:48:22Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    22,
                    48,
                    22,
                    1,
                    91,
                    0
                ],
                "title": "First Field-Trial Demonstration of L4 Autonomous Optical Network for\n  Distributed AI Training Communication: An LLM-Powered Multi-AI-Agent Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Field-Trial Demonstration of L4 Autonomous Optical Network for\n  Distributed AI Training Communication: An LLM-Powered Multi-AI-Agent Solution"
                },
                "summary": "We demonstrate the first cross-domain cross-layer level-4 autonomous optical\nnetwork via a multi-AI-agent system. Field trials show ~98% task completion\nrate across the distributed AI training lifecycle-3.2x higher than single\nagents using state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate the first cross-domain cross-layer level-4 autonomous optical\nnetwork via a multi-AI-agent system. Field trials show ~98% task completion\nrate across the distributed AI training lifecycle-3.2x higher than single\nagents using state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Qizhi Qiu"
                    },
                    {
                        "name": "Xiaomin Liu"
                    },
                    {
                        "name": "Dianxuan Fu"
                    },
                    {
                        "name": "Xingyu Liu"
                    },
                    {
                        "name": "Leyan Fei"
                    },
                    {
                        "name": "Yuming Cheng"
                    },
                    {
                        "name": "Lilin Yi"
                    },
                    {
                        "name": "Weisheng Hu"
                    },
                    {
                        "name": "Qunbi Zhuge"
                    }
                ],
                "author_detail": {
                    "name": "Qunbi Zhuge"
                },
                "author": "Qunbi Zhuge",
                "arxiv_comment": "Accepted by 51st European Conference on Optical Communication (ECOC\n  2025), paper W.02.01.177",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17372v1",
                "updated": "2025-10-20T10:08:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    8,
                    53,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:08:53Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    8,
                    53,
                    0,
                    293,
                    0
                ],
                "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition\n  Performance without Privacy Compromise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition\n  Performance without Privacy Compromise"
                },
                "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research."
                },
                "authors": [
                    {
                        "name": "Paweł Borsukiewicz"
                    },
                    {
                        "name": "Fadi Boutros"
                    },
                    {
                        "name": "Iyiola E. Olatunji"
                    },
                    {
                        "name": "Charles Beumier"
                    },
                    {
                        "name": "Wendkûuni C. Ouedraogo"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    }
                ],
                "author_detail": {
                    "name": "Tegawendé F. Bissyandé"
                },
                "author": "Tegawendé F. Bissyandé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17369v1",
                "updated": "2025-10-20T10:06:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    6,
                    39,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:06:39Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    6,
                    39,
                    0,
                    293,
                    0
                ],
                "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on\n  Soft Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on\n  Soft Robots"
                },
                "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments."
                },
                "authors": [
                    {
                        "name": "Haochen Su"
                    },
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Francesco Stella"
                    },
                    {
                        "name": "Andrea Peirone"
                    },
                    {
                        "name": "Kai Junge"
                    },
                    {
                        "name": "Josie Hughes"
                    }
                ],
                "author_detail": {
                    "name": "Josie Hughes"
                },
                "author": "Josie Hughes",
                "arxiv_comment": "Accepted by NeurIPS 2025 SpaVLE workshop. 4 pages, 2 figures(in main\n  paper, excluding references and supplements)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17364v1",
                "updated": "2025-10-20T10:04:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    4,
                    49,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:04:49Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    4,
                    49,
                    0,
                    293,
                    0
                ],
                "title": "Recurrent Attention-based Token Selection for Efficient Streaming\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Attention-based Token Selection for Efficient Streaming\n  Video-LLMs"
                },
                "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Vaggelis Dorovatas"
                    },
                    {
                        "name": "Soroush Seifi"
                    },
                    {
                        "name": "Gunshi Gupta"
                    },
                    {
                        "name": "Rahaf Aljundi"
                    }
                ],
                "author_detail": {
                    "name": "Rahaf Aljundi"
                },
                "author": "Rahaf Aljundi",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17363v1",
                "updated": "2025-10-20T10:03:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    3,
                    31,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T10:03:31Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    10,
                    3,
                    31,
                    0,
                    293,
                    0
                ],
                "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task\n  Attention for Monocular Spatial Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task\n  Attention for Monocular Spatial Perception"
                },
                "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks."
                },
                "authors": [
                    {
                        "name": "U. V. B. L Udugama"
                    },
                    {
                        "name": "George Vosselman"
                    },
                    {
                        "name": "Francesco Nex"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Nex"
                },
                "author": "Francesco Nex",
                "arxiv_comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). 8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17358v1",
                "updated": "2025-10-20T09:58:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    58,
                    34,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T09:58:34Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    58,
                    34,
                    0,
                    293,
                    0
                ],
                "title": "Localist LLMs with Recruitment Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localist LLMs with Recruitment Learning"
                },
                "summary": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovations are (1) a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining, (2) an\ninformation-theoretic recruitment mechanism that adaptively allocates semantic\nblocks as needed, eliminating the requirement for complete domain knowledge at\ninitialization, and (3) a hierarchical recruitment framework that extends\ncapacity allocation to entire specialized LLMs, enabling multi-granularity\narchitectural adaptation. This is achieved through group sparsity penalties on\nattention mechanisms, information-theoretic anchor design, dynamic rule\ninjection, and principled recruitment criteria based on penalized likelihood\nwith explicit units. We provide rigorous mathematical results establishing\nexplicit threshold conditions under which attention provably concentrates on\nsemantically relevant blocks at stationary points, with exact bounds on\nattention entropy and pointer fidelity. The hierarchical recruitment mechanism\nprovides convergence guarantees at both the block level (fine-grained,\nwithin-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the\nsystem discovers semantic partitions that balance model complexity against data\nencoding efficiency. This framework enables practitioners to continuously\ninterpolate between interpretable and high-performance modes while adapting\narchitectural capacity at multiple granularities, supporting applications in\nregulated domains requiring both transparency and capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovations are (1) a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining, (2) an\ninformation-theoretic recruitment mechanism that adaptively allocates semantic\nblocks as needed, eliminating the requirement for complete domain knowledge at\ninitialization, and (3) a hierarchical recruitment framework that extends\ncapacity allocation to entire specialized LLMs, enabling multi-granularity\narchitectural adaptation. This is achieved through group sparsity penalties on\nattention mechanisms, information-theoretic anchor design, dynamic rule\ninjection, and principled recruitment criteria based on penalized likelihood\nwith explicit units. We provide rigorous mathematical results establishing\nexplicit threshold conditions under which attention provably concentrates on\nsemantically relevant blocks at stationary points, with exact bounds on\nattention entropy and pointer fidelity. The hierarchical recruitment mechanism\nprovides convergence guarantees at both the block level (fine-grained,\nwithin-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the\nsystem discovers semantic partitions that balance model complexity against data\nencoding efficiency. This framework enables practitioners to continuously\ninterpolate between interpretable and high-performance modes while adapting\narchitectural capacity at multiple granularities, supporting applications in\nregulated domains requiring both transparency and capability."
                },
                "authors": [
                    {
                        "name": "Joachim Diederich"
                    }
                ],
                "author_detail": {
                    "name": "Joachim Diederich"
                },
                "author": "Joachim Diederich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17354v1",
                "updated": "2025-10-20T09:56:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    56,
                    43,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T09:56:43Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    56,
                    43,
                    0,
                    293,
                    0
                ],
                "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks."
                },
                "authors": [
                    {
                        "name": "Chenghao Zhang"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "This work is in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19360v2",
                "updated": "2025-10-20T09:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    45,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-09-18T15:06:46Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    6,
                    46,
                    3,
                    261,
                    0
                ],
                "title": "Semantic Representation Attack against Aligned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Representation Attack against Aligned Large Language Models"
                },
                "summary": "Large Language Models (LLMs) increasingly employ alignment techniques to\nprevent harmful outputs. Despite these safeguards, attackers can circumvent\nthem by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure,\nhere is...'', suffering from limited convergence, unnatural prompts, and high\ncomputational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that\nfundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the\nsemantic representation space comprising diverse responses with equivalent\nharmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and\nprompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to\nefficiently generate semantically coherent and concise adversarial prompts by\nmaintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and\ndemonstrate that our method achieves unprecedented attack success rates\n(89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while\nmaintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our\nSemantic Representation Attack.\n  The code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly employ alignment techniques to\nprevent harmful outputs. Despite these safeguards, attackers can circumvent\nthem by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure,\nhere is...'', suffering from limited convergence, unnatural prompts, and high\ncomputational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that\nfundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the\nsemantic representation space comprising diverse responses with equivalent\nharmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and\nprompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to\nefficiently generate semantically coherent and concise adversarial prompts by\nmaintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and\ndemonstrate that our method achieves unprecedented attack success rates\n(89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while\nmaintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our\nSemantic Representation Attack.\n  The code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jiawei Lian"
                    },
                    {
                        "name": "Jianhong Pan"
                    },
                    {
                        "name": "Lefan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Shaohui Mei"
                    },
                    {
                        "name": "Lap-Pui Chau"
                    }
                ],
                "author_detail": {
                    "name": "Lap-Pui Chau"
                },
                "author": "Lap-Pui Chau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17346v1",
                "updated": "2025-10-20T09:43:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    43,
                    39,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T09:43:39Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    43,
                    39,
                    0,
                    293,
                    0
                ],
                "title": "TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart\n  Sound Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart\n  Sound Segmentation"
                },
                "summary": "Deep learning approaches for heart-sound (PCG) segmentation built on\ntime--frequency features can be accurate but often rely on large expert-labeled\ndatasets, limiting robustness and deployment. We present TopSeg, a topological\nrepresentation-centric framework that encodes PCG dynamics with multi-scale\ntopological features and decodes them using a lightweight temporal\nconvolutional network (TCN) with an order- and duration-constrained inference\nstep. To evaluate data efficiency and generalization, we train exclusively on\nPhysioNet 2016 dataset with subject-level subsampling and perform external\nvalidation on CirCor dataset. Under matched-capacity decoders, the topological\nfeatures consistently outperform spectrogram and envelope inputs, with the\nlargest margins at low data budgets; as a full system, TopSeg surpasses\nrepresentative end-to-end baselines trained on their native inputs under the\nsame budgets while remaining competitive at full data. Ablations at 10%\ntraining confirm that all scales contribute and that combining H_0 and H_1\nyields more reliable S1/S2 localization and boundary stability. These results\nindicate that topology-aware representations provide a strong inductive bias\nfor data-efficient, cross-dataset PCG segmentation, supporting practical use\nwhen labeled data are limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning approaches for heart-sound (PCG) segmentation built on\ntime--frequency features can be accurate but often rely on large expert-labeled\ndatasets, limiting robustness and deployment. We present TopSeg, a topological\nrepresentation-centric framework that encodes PCG dynamics with multi-scale\ntopological features and decodes them using a lightweight temporal\nconvolutional network (TCN) with an order- and duration-constrained inference\nstep. To evaluate data efficiency and generalization, we train exclusively on\nPhysioNet 2016 dataset with subject-level subsampling and perform external\nvalidation on CirCor dataset. Under matched-capacity decoders, the topological\nfeatures consistently outperform spectrogram and envelope inputs, with the\nlargest margins at low data budgets; as a full system, TopSeg surpasses\nrepresentative end-to-end baselines trained on their native inputs under the\nsame budgets while remaining competitive at full data. Ablations at 10%\ntraining confirm that all scales contribute and that combining H_0 and H_1\nyields more reliable S1/S2 localization and boundary stability. These results\nindicate that topology-aware representations provide a strong inductive bias\nfor data-efficient, cross-dataset PCG segmentation, supporting practical use\nwhen labeled data are limited."
                },
                "authors": [
                    {
                        "name": "Peihong Zhang"
                    },
                    {
                        "name": "Zhixin Li"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Rui Sang"
                    },
                    {
                        "name": "Yiqiang Cai"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Shengchen Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengchen Li"
                },
                "author": "Shengchen Li",
                "arxiv_comment": "Paper has submitted to ICASSP2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15862v3",
                "updated": "2025-10-21T08:23:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    23,
                    52,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-17T17:53:06Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    17,
                    53,
                    6,
                    4,
                    290,
                    0
                ],
                "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold"
                },
                "summary": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under Apache 2.0 license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under Apache 2.0 license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS."
                },
                "authors": [
                    {
                        "name": "Yi Wan"
                    },
                    {
                        "name": "Jiuqi Wang"
                    },
                    {
                        "name": "Liam Li"
                    },
                    {
                        "name": "Jinsong Liu"
                    },
                    {
                        "name": "Ruihao Zhu"
                    },
                    {
                        "name": "Zheqing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zheqing Zhu"
                },
                "author": "Zheqing Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04162v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04162v4",
                "updated": "2025-10-20T09:37:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    37,
                    16,
                    0,
                    293,
                    0
                ],
                "published": "2025-03-06T07:25:19Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    7,
                    25,
                    19,
                    3,
                    65,
                    0
                ],
                "title": "SRA-CL: Semantic Retrieval Augmented Contrastive Learning for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRA-CL: Semantic Retrieval Augmented Contrastive Learning for Sequential\n  Recommendation"
                },
                "summary": "Contrastive learning has shown effectiveness in improving sequential\nrecommendation models. However, existing methods still face challenges in\ngenerating high-quality contrastive pairs: they either rely on random\nperturbations that corrupt user preference patterns or depend on sparse\ncollaborative data that generates unreliable contrastive pairs. Furthermore,\nexisting approaches typically require predefined selection rules that impose\nstrong assumptions, limiting the model's ability to autonomously learn optimal\ncontrastive pairs. To address these limitations, we propose a novel approach\nnamed Semantic Retrieval Augmented Contrastive Learning (SRA-CL). SRA-CL\nleverages the semantic understanding and reasoning capabilities of LLMs to\ngenerate expressive embeddings that capture both user preferences and item\ncharacteristics. These semantic embeddings enable the construction of candidate\npools for inter-user and intra-user contrastive learning through semantic-based\nretrieval. To further enhance the quality of the contrastive samples, we\nintroduce a learnable sample synthesizer that optimizes the contrastive sample\ngeneration process during model training. SRA-CL adopts a plug-and-play design,\nenabling seamless integration with existing sequential recommendation\narchitectures. Extensive experiments on four public datasets demonstrate the\neffectiveness and model-agnostic nature of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive learning has shown effectiveness in improving sequential\nrecommendation models. However, existing methods still face challenges in\ngenerating high-quality contrastive pairs: they either rely on random\nperturbations that corrupt user preference patterns or depend on sparse\ncollaborative data that generates unreliable contrastive pairs. Furthermore,\nexisting approaches typically require predefined selection rules that impose\nstrong assumptions, limiting the model's ability to autonomously learn optimal\ncontrastive pairs. To address these limitations, we propose a novel approach\nnamed Semantic Retrieval Augmented Contrastive Learning (SRA-CL). SRA-CL\nleverages the semantic understanding and reasoning capabilities of LLMs to\ngenerate expressive embeddings that capture both user preferences and item\ncharacteristics. These semantic embeddings enable the construction of candidate\npools for inter-user and intra-user contrastive learning through semantic-based\nretrieval. To further enhance the quality of the contrastive samples, we\nintroduce a learnable sample synthesizer that optimizes the contrastive sample\ngeneration process during model training. SRA-CL adopts a plug-and-play design,\nenabling seamless integration with existing sequential recommendation\narchitectures. Extensive experiments on four public datasets demonstrate the\neffectiveness and model-agnostic nature of our approach."
                },
                "authors": [
                    {
                        "name": "Ziqiang Cui"
                    },
                    {
                        "name": "Yunpeng Weng"
                    },
                    {
                        "name": "Xing Tang"
                    },
                    {
                        "name": "Xiaokun Zhang"
                    },
                    {
                        "name": "Shiwei Li"
                    },
                    {
                        "name": "Peiyang Liu"
                    },
                    {
                        "name": "Bowei He"
                    },
                    {
                        "name": "Dugang Liu"
                    },
                    {
                        "name": "Weihong Luo"
                    },
                    {
                        "name": "Xiuqiang He"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code is available at:\n  https://github.com/ziqiangcui/SRA-CL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04162v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04162v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16293v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16293v4",
                "updated": "2025-10-20T09:35:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    35,
                    27,
                    0,
                    293,
                    0
                ],
                "published": "2025-09-19T15:08:33Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    8,
                    33,
                    4,
                    262,
                    0
                ],
                "title": "Robust LLM Training Infrastructure at ByteDance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Training Infrastructure at ByteDance"
                },
                "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform and achieves 97% ETTR for a three-month training job on 9,600 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform and achieves 97% ETTR for a three-month training job on 9,600 GPUs."
                },
                "authors": [
                    {
                        "name": "Borui Wan"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Zuquan Song"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Houmin Wei"
                    },
                    {
                        "name": "Chenyuan Wang"
                    },
                    {
                        "name": "Weiqiang Lou"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Mofan Zhang"
                    },
                    {
                        "name": "Kaihua Jiang"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Xiaoyun Zhi"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Zhe Nan"
                    },
                    {
                        "name": "Zhuolin Zheng"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Qinlong Wang"
                    },
                    {
                        "name": "Huan Yu"
                    },
                    {
                        "name": "Jinxin Chi"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Zixian Du"
                    },
                    {
                        "name": "Sida Zhao"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Jingzhe Tang"
                    },
                    {
                        "name": "Zherui Liu"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16293v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16293v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17335v2",
                "updated": "2025-10-21T07:12:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    7,
                    12,
                    7,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T09:27:24Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    27,
                    24,
                    0,
                    293,
                    0
                ],
                "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular\n  Materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular\n  Materials"
                },
                "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks."
                },
                "authors": [
                    {
                        "name": "Xintong Yang"
                    },
                    {
                        "name": "Minglun Wei"
                    },
                    {
                        "name": "Ze Ji"
                    },
                    {
                        "name": "Yu-Kun Lai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Kun Lai"
                },
                "author": "Yu-Kun Lai",
                "arxiv_comment": "Accepted as a regular paper by the IEEE Transactions on Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17330v1",
                "updated": "2025-10-20T09:23:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    23,
                    29,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T09:23:29Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    23,
                    29,
                    0,
                    293,
                    0
                ],
                "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License\n  Plate Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CharDiff: A Diffusion Model with Character-Level Guidance for License\n  Plate Image Restoration"
                },
                "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Gyuhwan Park"
                    },
                    {
                        "name": "Kihyun Na"
                    },
                    {
                        "name": "Injung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Injung Kim"
                },
                "author": "Injung Kim",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17314v1",
                "updated": "2025-10-20T09:01:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    1,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T09:01:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    9,
                    1,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward\n  Modeling"
                },
                "summary": "Reward models are essential for aligning Large Language Models (LLMs) with\nhuman values, yet their development is hampered by costly preference datasets\nand poor interpretability. While recent rubric-based approaches offer\ntransparency, they often lack systematic quality control and optimization,\ncreating a trade-off between scalability and reliability. We address these\nlimitations with a novel, training-free framework built on a key assumption:\n\\textit{evaluation rubrics underlying human preferences exhibit significant\ngeneralization ability across diverse queries}, a property that enables\nremarkable data efficiency. Our two-stage approach first infers high-quality,\nquery-specific rubrics using a validation-guided\n\\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these\ngranular rubrics into a compact, non-redundant core set by maximizing an\n\\textbf{information-theoretic coding rate}. The final output is an\ninterpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments\ndemonstrate the framework's exceptional data efficiency and performance.\nCritically, using just 70 preference pairs (1.5\\% of the source data), our\nmethod also empowers smaller models like Qwen3-8B to outperform specialized,\nfully-trained counterparts. This work pioneers a scalable, interpretable, and\ndata-efficient path for reward modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models are essential for aligning Large Language Models (LLMs) with\nhuman values, yet their development is hampered by costly preference datasets\nand poor interpretability. While recent rubric-based approaches offer\ntransparency, they often lack systematic quality control and optimization,\ncreating a trade-off between scalability and reliability. We address these\nlimitations with a novel, training-free framework built on a key assumption:\n\\textit{evaluation rubrics underlying human preferences exhibit significant\ngeneralization ability across diverse queries}, a property that enables\nremarkable data efficiency. Our two-stage approach first infers high-quality,\nquery-specific rubrics using a validation-guided\n\\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these\ngranular rubrics into a compact, non-redundant core set by maximizing an\n\\textbf{information-theoretic coding rate}. The final output is an\ninterpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments\ndemonstrate the framework's exceptional data efficiency and performance.\nCritically, using just 70 preference pairs (1.5\\% of the source data), our\nmethod also empowers smaller models like Qwen3-8B to outperform specialized,\nfully-trained counterparts. This work pioneers a scalable, interpretable, and\ndata-efficient path for reward modeling."
                },
                "authors": [
                    {
                        "name": "Lipeng Xie"
                    },
                    {
                        "name": "Sen Huang"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Anni Zou"
                    },
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Dingchao Ren"
                    },
                    {
                        "name": "Kezun Zhang"
                    },
                    {
                        "name": "Haoyuan Hu"
                    },
                    {
                        "name": "Boyin Liu"
                    },
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09211v2",
                "updated": "2025-10-20T08:57:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    57,
                    20,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-10T09:45:35Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    45,
                    35,
                    4,
                    283,
                    0
                ],
                "title": "DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought\n  Correction"
                },
                "summary": "When performing reasoning tasks with user-specific requirements, such as\nstrict output formats, large language models (LLMs) often prioritize reasoning\nover adherence to detailed instructions. Fine-tuning LLMs on supervised\ndatasets to address this is impractical due to high computational costs and\nlimited parameter access. To tackle this, we propose DICE, a lightweight\nframework that guides small language models (SLMs) to refine LLMs' outputs\nthrough chain-of-thought (CoT) correction. DICE decouples the process by first\nprompting LLMs to generate natural language responses, then using trained SLMs\nto analyze and refine these outputs to meet structured output specifications.\nThis framework preserves LLMs' broad knowledge and reasoning capabilities while\nensuring the outputs conform to user demands. Specifically, DICE first\nconstructs structured CoT adaptation datasets via a two-stage method and\nsubsequently applies a dual-tuning strategy to fine-tune SLMs for generating\nstructured outputs in an analyze-then-answer pattern. Experiments demonstrate\nthat DICE improves the average format accuracy and content correctness of LLM\noutputs by 35.4\\% and 29.4\\%, respectively, achieving state-of-the-art (SOTA)\nperformance over other competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When performing reasoning tasks with user-specific requirements, such as\nstrict output formats, large language models (LLMs) often prioritize reasoning\nover adherence to detailed instructions. Fine-tuning LLMs on supervised\ndatasets to address this is impractical due to high computational costs and\nlimited parameter access. To tackle this, we propose DICE, a lightweight\nframework that guides small language models (SLMs) to refine LLMs' outputs\nthrough chain-of-thought (CoT) correction. DICE decouples the process by first\nprompting LLMs to generate natural language responses, then using trained SLMs\nto analyze and refine these outputs to meet structured output specifications.\nThis framework preserves LLMs' broad knowledge and reasoning capabilities while\nensuring the outputs conform to user demands. Specifically, DICE first\nconstructs structured CoT adaptation datasets via a two-stage method and\nsubsequently applies a dual-tuning strategy to fine-tune SLMs for generating\nstructured outputs in an analyze-then-answer pattern. Experiments demonstrate\nthat DICE improves the average format accuracy and content correctness of LLM\noutputs by 35.4\\% and 29.4\\%, respectively, achieving state-of-the-art (SOTA)\nperformance over other competitive baselines."
                },
                "authors": [
                    {
                        "name": "Yiqi Li"
                    },
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "This paper was accepted to the EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17311v1",
                "updated": "2025-10-20T08:54:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    54,
                    31,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T08:54:31Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    54,
                    31,
                    0,
                    293,
                    0
                ],
                "title": "The Hidden Dangers of Public Serverless Repositories: An Empirical\n  Security Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Dangers of Public Serverless Repositories: An Empirical\n  Security Assessment"
                },
                "summary": "Serverless computing has rapidly emerged as a prominent cloud paradigm,\nenabling developers to focus solely on application logic without the burden of\nmanaging servers or underlying infrastructure. Public serverless repositories\nhave become key to accelerating the development of serverless applications.\nHowever, their growing popularity makes them attractive targets for\nadversaries. Despite this, the security posture of these repositories remains\nlargely unexplored, exposing developers and organizations to potential risks.\nIn this paper, we present the first comprehensive analysis of the security\nlandscape of serverless components hosted in public repositories. We analyse\n2,758 serverless components from five widely used public repositories popular\namong developers and enterprises, and 125,936 Infrastructure as Code (IaC)\ntemplates across three widely used IaC frameworks. Our analysis reveals\nsystemic vulnerabilities including outdated software packages, misuse of\nsensitive parameters, exploitable deployment configurations, susceptibility to\ntypo-squatting attacks and opportunities to embed malicious behaviour within\ncompressed serverless components. Finally, we provide practical recommendations\nto mitigate these threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has rapidly emerged as a prominent cloud paradigm,\nenabling developers to focus solely on application logic without the burden of\nmanaging servers or underlying infrastructure. Public serverless repositories\nhave become key to accelerating the development of serverless applications.\nHowever, their growing popularity makes them attractive targets for\nadversaries. Despite this, the security posture of these repositories remains\nlargely unexplored, exposing developers and organizations to potential risks.\nIn this paper, we present the first comprehensive analysis of the security\nlandscape of serverless components hosted in public repositories. We analyse\n2,758 serverless components from five widely used public repositories popular\namong developers and enterprises, and 125,936 Infrastructure as Code (IaC)\ntemplates across three widely used IaC frameworks. Our analysis reveals\nsystemic vulnerabilities including outdated software packages, misuse of\nsensitive parameters, exploitable deployment configurations, susceptibility to\ntypo-squatting attacks and opportunities to embed malicious behaviour within\ncompressed serverless components. Finally, we provide practical recommendations\nto mitigate these threats."
                },
                "authors": [
                    {
                        "name": "Eduard Marin"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Alessio Pavoni"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Roberto Di Pietro"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Di Pietro"
                },
                "author": "Roberto Di Pietro",
                "arxiv_comment": "Accepted at ESORICS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02124v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02124v5",
                "updated": "2025-10-20T08:48:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    48,
                    15,
                    0,
                    293,
                    0
                ],
                "published": "2025-08-04T07:05:15Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    7,
                    5,
                    15,
                    0,
                    216,
                    0
                ],
                "title": "Trainable Dynamic Mask Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable Dynamic Mask Sparse Attention"
                },
                "summary": "The increasing demand for long-context modeling in large language models\n(LLMs) is bottlenecked by the quadratic complexity of the standard\nself-attention mechanism. The community has proposed sparse attention to\nmitigate this issue. However, position-aware sparse attention methods rely on\nstatic sparse structures that lack adaptability to diverse query contexts,\nwhile content-aware sparse attention methods depend on heuristic key-value\nselection, hindering full differentiability. We introduce a trainable dynamic\nmask sparse attention mechanism, a method that merges the advantages of both\nposition-aware and content-aware approaches. Dynamic Mask Attention (DMA)\nachieves this through three key innovations: First, it leverages value vector\nrepresentations to generate content-aware dynamic masks, enabling the model to\nadaptively identify and attend to critical information. Second, it computes\nposition-aware sparse weights in a hardware-friendly manner, efficiently\nskipping unnecessary computational regions. Finally, we demonstrate that the\nintroduced dynamic mask and sparse weights do not obstruct gradients,\nsupporting end-to-end training. We have validated the performance of DMA\nthrough comprehensive experiments. A large body of experimental evidence shows\nthat DMA consistently holds a Pareto advantage over state-of-the-art sparse\nattention baselines in tasks including scaling laws, multi-query associative\nrecall, standard benchmarks, and needle in a haystack tests, while also\ndelivering up to a 10x overall speedup. These results highlight its ability to\neffectively balance model efficiency with long-context modeling capabilities.\nOur computational kernel code is now open-source at\nhttps://github.com/SmallDoges/flash-dmattn to encourage further research and\napplication by the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for long-context modeling in large language models\n(LLMs) is bottlenecked by the quadratic complexity of the standard\nself-attention mechanism. The community has proposed sparse attention to\nmitigate this issue. However, position-aware sparse attention methods rely on\nstatic sparse structures that lack adaptability to diverse query contexts,\nwhile content-aware sparse attention methods depend on heuristic key-value\nselection, hindering full differentiability. We introduce a trainable dynamic\nmask sparse attention mechanism, a method that merges the advantages of both\nposition-aware and content-aware approaches. Dynamic Mask Attention (DMA)\nachieves this through three key innovations: First, it leverages value vector\nrepresentations to generate content-aware dynamic masks, enabling the model to\nadaptively identify and attend to critical information. Second, it computes\nposition-aware sparse weights in a hardware-friendly manner, efficiently\nskipping unnecessary computational regions. Finally, we demonstrate that the\nintroduced dynamic mask and sparse weights do not obstruct gradients,\nsupporting end-to-end training. We have validated the performance of DMA\nthrough comprehensive experiments. A large body of experimental evidence shows\nthat DMA consistently holds a Pareto advantage over state-of-the-art sparse\nattention baselines in tasks including scaling laws, multi-query associative\nrecall, standard benchmarks, and needle in a haystack tests, while also\ndelivering up to a 10x overall speedup. These results highlight its ability to\neffectively balance model efficiency with long-context modeling capabilities.\nOur computational kernel code is now open-source at\nhttps://github.com/SmallDoges/flash-dmattn to encourage further research and\napplication by the community."
                },
                "authors": [
                    {
                        "name": "Jingze Shi"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Yiran Peng"
                    },
                    {
                        "name": "Bingheng Wu"
                    },
                    {
                        "name": "Liangdong Wang"
                    },
                    {
                        "name": "Guang Liu"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02124v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02124v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15707v2",
                "updated": "2025-10-20T08:29:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    29,
                    40,
                    0,
                    293,
                    0
                ],
                "published": "2025-05-30T09:05:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    9,
                    5,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every Rollout Counts: Optimal Resource Allocation for Efficient\n  Test-Time Scaling"
                },
                "summary": "Test-Time Scaling (TTS) improves the performance of Large Language Models\n(LLMs) by using additional inference-time computation to explore multiple\nreasoning paths through search. Yet how to allocate a fixed rollout budget most\neffectively during search remains underexplored, often resulting in inefficient\nuse of compute at test time. To bridge this gap, we formulate test-time search\nas a resource allocation problem and derive the optimal allocation strategy\nthat maximizes the probability of obtaining a correct solution under a fixed\nrollout budget. Within this formulation, we reveal a core limitation of\nexisting search methods: solution-level allocation tends to favor reasoning\ndirections with more candidates, leading to theoretically suboptimal and\ninefficient use of compute. To address this, we propose Direction-Oriented\nResource Allocation (DORA), a provably optimal method that mitigates this bias\nby decoupling direction quality from candidate count and allocating resources\nat the direction level. To demonstrate DORA's effectiveness, we conduct\nextensive experiments on challenging mathematical reasoning benchmarks\nincluding MATH500, AIME2024, and AIME2025. The empirical results show that DORA\nconsistently outperforms strong baselines with comparable computational cost,\nachieving state-of-the-art accuracy. We hope our findings contribute to a\nbroader understanding of optimal TTS for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) improves the performance of Large Language Models\n(LLMs) by using additional inference-time computation to explore multiple\nreasoning paths through search. Yet how to allocate a fixed rollout budget most\neffectively during search remains underexplored, often resulting in inefficient\nuse of compute at test time. To bridge this gap, we formulate test-time search\nas a resource allocation problem and derive the optimal allocation strategy\nthat maximizes the probability of obtaining a correct solution under a fixed\nrollout budget. Within this formulation, we reveal a core limitation of\nexisting search methods: solution-level allocation tends to favor reasoning\ndirections with more candidates, leading to theoretically suboptimal and\ninefficient use of compute. To address this, we propose Direction-Oriented\nResource Allocation (DORA), a provably optimal method that mitigates this bias\nby decoupling direction quality from candidate count and allocating resources\nat the direction level. To demonstrate DORA's effectiveness, we conduct\nextensive experiments on challenging mathematical reasoning benchmarks\nincluding MATH500, AIME2024, and AIME2025. The empirical results show that DORA\nconsistently outperforms strong baselines with comparable computational cost,\nachieving state-of-the-art accuracy. We hope our findings contribute to a\nbroader understanding of optimal TTS for LLMs."
                },
                "authors": [
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "Accepted at NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17281v1",
                "updated": "2025-10-20T08:16:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    16,
                    12,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T08:16:12Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    16,
                    12,
                    0,
                    293,
                    0
                ],
                "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems"
                },
                "summary": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms."
                },
                "authors": [
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yichen Tang"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Jianming Long"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17276v1",
                "updated": "2025-10-20T08:02:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    2,
                    51,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T08:02:51Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    8,
                    2,
                    51,
                    0,
                    293,
                    0
                ],
                "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking and Fixing Defenses Against Control-Flow Hijacking in\n  Multi-Agent Systems"
                },
                "summary": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation."
                },
                "authors": [
                    {
                        "name": "Rishi Jha"
                    },
                    {
                        "name": "Harold Triedman"
                    },
                    {
                        "name": "Justin Wagle"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04559v2",
                "updated": "2025-10-20T07:48:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    48,
                    22,
                    0,
                    293,
                    0
                ],
                "published": "2025-06-05T02:28:07Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    2,
                    28,
                    7,
                    3,
                    156,
                    0
                ],
                "title": "Reasoning-Aligned Perception Decoupling for Scalable Multi-modal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Aligned Perception Decoupling for Scalable Multi-modal\n  Reasoning"
                },
                "summary": "Recent breakthroughs in reasoning language models have significantly advanced\ntext-based reasoning. On the other hand, Multi-modal Large Language Models\n(MLLMs) still lag behind, hindered by their outdated internal LLMs. Upgrading\nthese is often prohibitively expensive, as it requires complete vision-language\nalignment retraining which is costly. To address this issue, we introduce\nPerception-Reasoning Decoupling, which modularizes the MLLM's reasoning\ncomponent and makes it easily replaceable. This approach redefines the MLLM's\nrole to convert multi-modal inputs into detailed textual outputs that can be\nprocessed by any powerful, external, text-only LLM reasoners. To align the\nMLLM's perceptual output with the final reasoning task, we propose a novel\nreinforcement learning algorithm called Visual Perception Optimization (VPO).\nVPO rewards the MLLM based on the correctness of answers generated by the\nexternal reasoner to produce faithful and query-relevant captions. Together,\nthis decoupling pipeline and VPO form our Reasoning-Aligned PerceptIon\nDecoupling (RAPID) approach. Empirical results show that RAPID achieves\nsignificant performance gains on multi-modal reasoning benchmarks. Crucially,\nRAPID enables a novel inference-time scaling paradigm: Once trained with VPO,\nthe MLLM can be paired with any state-of-the-art LLM reasoner for consistent\nperformance improvement without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in reasoning language models have significantly advanced\ntext-based reasoning. On the other hand, Multi-modal Large Language Models\n(MLLMs) still lag behind, hindered by their outdated internal LLMs. Upgrading\nthese is often prohibitively expensive, as it requires complete vision-language\nalignment retraining which is costly. To address this issue, we introduce\nPerception-Reasoning Decoupling, which modularizes the MLLM's reasoning\ncomponent and makes it easily replaceable. This approach redefines the MLLM's\nrole to convert multi-modal inputs into detailed textual outputs that can be\nprocessed by any powerful, external, text-only LLM reasoners. To align the\nMLLM's perceptual output with the final reasoning task, we propose a novel\nreinforcement learning algorithm called Visual Perception Optimization (VPO).\nVPO rewards the MLLM based on the correctness of answers generated by the\nexternal reasoner to produce faithful and query-relevant captions. Together,\nthis decoupling pipeline and VPO form our Reasoning-Aligned PerceptIon\nDecoupling (RAPID) approach. Empirical results show that RAPID achieves\nsignificant performance gains on multi-modal reasoning benchmarks. Crucially,\nRAPID enables a novel inference-time scaling paradigm: Once trained with VPO,\nthe MLLM can be paired with any state-of-the-art LLM reasoner for consistent\nperformance improvement without retraining."
                },
                "authors": [
                    {
                        "name": "Yunhao Gou"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Zhili Liu"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "James T. Kwok"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19557v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19557v4",
                "updated": "2025-10-20T07:44:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    44,
                    55,
                    0,
                    293,
                    0
                ],
                "published": "2024-04-30T13:39:26Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    13,
                    39,
                    26,
                    1,
                    121,
                    0
                ],
                "title": "Neural Dynamic Data Valuation: A Stochastic Optimal Control Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Dynamic Data Valuation: A Stochastic Optimal Control Approach"
                },
                "summary": "Data valuation has become a cornerstone of the modern data economy, where\ndatasets function as tradable intellectual assets that drive decision-making,\nmodel training, and market transactions. Despite substantial progress, existing\nvaluation methods remain limited by high computational cost, weak fairness\nguarantees, and poor interpretability, which hinder their deployment in\nlarge-scale, high-stakes applications. This paper introduces Neural Dynamic\nData Valuation (NDDV), a new framework that formulates data valuation as a\nstochastic optimal control problem to capture the dynamic evolution of data\nutility over time. Unlike static combinatorial approaches, NDDV models data\ninteractions through continuous trajectories that reflect both individual and\ncollective learning dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data valuation has become a cornerstone of the modern data economy, where\ndatasets function as tradable intellectual assets that drive decision-making,\nmodel training, and market transactions. Despite substantial progress, existing\nvaluation methods remain limited by high computational cost, weak fairness\nguarantees, and poor interpretability, which hinder their deployment in\nlarge-scale, high-stakes applications. This paper introduces Neural Dynamic\nData Valuation (NDDV), a new framework that formulates data valuation as a\nstochastic optimal control problem to capture the dynamic evolution of data\nutility over time. Unlike static combinatorial approaches, NDDV models data\ninteractions through continuous trajectories that reflect both individual and\ncollective learning dynamics."
                },
                "authors": [
                    {
                        "name": "Zhangyong Liang"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Pengfei Zhang"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19557v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19557v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17256v1",
                "updated": "2025-10-20T07:43:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    43,
                    53,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:43:53Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    43,
                    53,
                    0,
                    293,
                    0
                ],
                "title": "Explainability of Large Language Models: Opportunities and Challenges\n  toward Generating Trustworthy Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability of Large Language Models: Opportunities and Challenges\n  toward Generating Trustworthy Explanations"
                },
                "summary": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations."
                },
                "authors": [
                    {
                        "name": "Shahin Atakishiyev"
                    },
                    {
                        "name": "Housam K. B. Babiker"
                    },
                    {
                        "name": "Jiayi Dai"
                    },
                    {
                        "name": "Nawshad Farruque"
                    },
                    {
                        "name": "Teruaki Hayashi"
                    },
                    {
                        "name": "Nafisa Sadaf Hriti"
                    },
                    {
                        "name": "Md Abed Rahman"
                    },
                    {
                        "name": "Iain Smith"
                    },
                    {
                        "name": "Mi-Young Kim"
                    },
                    {
                        "name": "Osmar R. Zaïane"
                    },
                    {
                        "name": "Randy Goebel"
                    }
                ],
                "author_detail": {
                    "name": "Randy Goebel"
                },
                "author": "Randy Goebel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]