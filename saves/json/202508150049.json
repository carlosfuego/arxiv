[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v1",
                "updated": "2025-08-13T13:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining."
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v4",
                "updated": "2025-08-11T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    16,
                    52,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09208v1",
                "updated": "2025-08-10T14:05:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "published": "2025-08-10T14:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge"
                },
                "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."
                },
                "authors": [
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvre Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09192v1",
                "updated": "2025-08-08T04:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T04:51:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing"
                },
                "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiachun Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jrn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06526v1",
                "updated": "2025-08-02T03:50:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T03:50:14Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "title": "PiKV: KV Cache Management System for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiKV: KV Cache Management System for Mixture of Experts"
                },
                "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Xuhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Wang"
                },
                "author": "Xuhong Wang",
                "arxiv_comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09982v1",
                "updated": "2025-08-13T17:55:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    31,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:55:31Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    31,
                    2,
                    225,
                    0
                ],
                "title": "2D bilayer electron-hole superfluidity with unequal and anisotropic\n  masses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D bilayer electron-hole superfluidity with unequal and anisotropic\n  masses"
                },
                "summary": "We investigate the stability of electron-hole superfluidity in\ntwo-dimensional bilayers with unequal and anisotropic effective masses. Using a\nzero-temperature, self-consistent Hartree-Fock approach, we study two\nexperimentally relevant deviations from the ideal equal-mass isotropic case:\n(i) isotropic but unequal conduction and valence band masses ($m_c^* \\neq\nm_v^*$), and (ii) equal average masses with orthogonal in-plane anisotropies\n$(m_{c,x}^*, m^*_{c,y}) = (m_1^*, m_2^*)$ and $(m^*_{v,x}, m^*_{v,y}) = (m_2^*,\nm_1^*)$. For both scenarios, we compute the order parameter and analyze the\nBEC-BCS crossover as a function of layer separation and mass ratio. We find\nthat both mass imbalance and mass anisotropy reduce the pairing strength and\nsuppress the inferred critical temperature $T_c$ by breaking perfect Fermi\nsurface nesting, and shift the BEC-BCS crossover. Despite these effects,\nsuperfluidity remains robust across the full range of densities and interlayer\nseparations considered, with no transition to an unpaired plasma state in the\nabsence of screening. Our results provide a baseline for understanding the\ninterplay of mass mismatch and anisotropy in current and emerging bilayer\nplatforms, including van der Waals heterostructures and anisotropic\ntwo-dimensional semiconductors. Our work also establishes that Fermi surface\nnesting is not a key ingredient for the bilayer superfluidity, which is always\nthe ground state for all electron-hole bilayers although the resultant $T_c$\ndepends on the parameter details and may very well be unmeasurably low for\nlarge interlayer separations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the stability of electron-hole superfluidity in\ntwo-dimensional bilayers with unequal and anisotropic effective masses. Using a\nzero-temperature, self-consistent Hartree-Fock approach, we study two\nexperimentally relevant deviations from the ideal equal-mass isotropic case:\n(i) isotropic but unequal conduction and valence band masses ($m_c^* \\neq\nm_v^*$), and (ii) equal average masses with orthogonal in-plane anisotropies\n$(m_{c,x}^*, m^*_{c,y}) = (m_1^*, m_2^*)$ and $(m^*_{v,x}, m^*_{v,y}) = (m_2^*,\nm_1^*)$. For both scenarios, we compute the order parameter and analyze the\nBEC-BCS crossover as a function of layer separation and mass ratio. We find\nthat both mass imbalance and mass anisotropy reduce the pairing strength and\nsuppress the inferred critical temperature $T_c$ by breaking perfect Fermi\nsurface nesting, and shift the BEC-BCS crossover. Despite these effects,\nsuperfluidity remains robust across the full range of densities and interlayer\nseparations considered, with no transition to an unpaired plasma state in the\nabsence of screening. Our results provide a baseline for understanding the\ninterplay of mass mismatch and anisotropy in current and emerging bilayer\nplatforms, including van der Waals heterostructures and anisotropic\ntwo-dimensional semiconductors. Our work also establishes that Fermi surface\nnesting is not a key ingredient for the bilayer superfluidity, which is always\nthe ground state for all electron-hole bilayers although the resultant $T_c$\ndepends on the parameter details and may very well be unmeasurably low for\nlarge interlayer separations."
                },
                "authors": [
                    {
                        "name": "Jihang Zhu"
                    },
                    {
                        "name": "Sankar Das Sarma"
                    }
                ],
                "author_detail": {
                    "name": "Sankar Das Sarma"
                },
                "author": "Sankar Das Sarma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11511v2",
                "updated": "2025-08-13T17:53:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    53,
                    18,
                    2,
                    225,
                    0
                ],
                "published": "2024-07-16T08:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    49,
                    35,
                    1,
                    198,
                    0
                ],
                "title": "Multi-Step Reasoning with Large Language Models, a Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning with Large Language Models, a Survey"
                },
                "summary": "Language models with billions of parameters exhibit in-context learning\nabilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks.\n  The research on LLM reasoning abilities started with the question whether\nLLMs can solve grade school math word problems, and has expanded to other tasks\nin the past few years. This paper reviews the field of multi-step reasoning\nwith LLMs. We propose a taxonomy that identifies different ways to generate,\nevaluate, and control multi-step reasoning. We provide an in-depth coverage of\ncore approaches and open problems, and we propose a research agenda for the\nnear future.\n  We find that multi-step reasoning approaches have progressed beyond math word\nproblems, and can now successfully solve challenges in logic, combinatorial\ngames, and robotics, sometimes by first generating code that is then executed\nby external tools. Many studies in multi-step methods are using reinforcement\nlearning for finetuning, external optimization loops, in context reinforcement\nlearning, and self-reflection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models with billions of parameters exhibit in-context learning\nabilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks.\n  The research on LLM reasoning abilities started with the question whether\nLLMs can solve grade school math word problems, and has expanded to other tasks\nin the past few years. This paper reviews the field of multi-step reasoning\nwith LLMs. We propose a taxonomy that identifies different ways to generate,\nevaluate, and control multi-step reasoning. We provide an in-depth coverage of\ncore approaches and open problems, and we propose a research agenda for the\nnear future.\n  We find that multi-step reasoning approaches have progressed beyond math word\nproblems, and can now successfully solve challenges in logic, combinatorial\ngames, and robotics, sometimes by first generating code that is then executed\nby external tools. Many studies in multi-step methods are using reinforcement\nlearning for finetuning, external optimization loops, in context reinforcement\nlearning, and self-reflection."
                },
                "authors": [
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Annie Wong"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Joost Broekens"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Back"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Back"
                },
                "author": "Thomas Back",
                "arxiv_comment": "revised version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18923v2",
                "updated": "2025-08-13T17:44:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    44,
                    18,
                    2,
                    225,
                    0
                ],
                "published": "2025-03-24T17:46:09Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    46,
                    9,
                    0,
                    83,
                    0
                ],
                "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models"
                },
                "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in videos remains a critical unsolved challenge. To address this gap,\nwe introduce Video SimpleQA, the first comprehensive benchmark tailored for\nfactuality evaluation in video contexts. Our work differs from existing video\nbenchmarks through the following key features: 1) Knowledge required: demanding\nintegration of external knowledge beyond the video's explicit narrative; 2)\nMulti-hop fact-seeking question: Each question involves multiple explicit facts\nand requires strict factual grounding without hypothetical or subjective\ninferences. We also include per-hop single-fact-based sub-QAs alongside final\nQAs to enable fine-grained, stepby-step evaluation; 3) Short-form definitive\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat with minimal scoring variance; 4) Temporal grounded required: Requiring\nanswers to rely on one or more temporal segments in videos, rather than single\nframes. We extensively evaluate 33 state-of-the-art LVLMs and summarize key\nfindings as follows: 1) Current LVLMs exhibit notable deficiencies in factual\nadherence, with the best-performing model o3 merely achieving an F-score of\n66.3%; 2) Most LVLMs are overconfident in what they generate, with self-stated\nconfidence exceeding actual accuracy; 3) Retrieval-augmented generation\ndemonstrates consistent improvements at the cost of additional inference time\noverhead; 4) Multi-hop QA demonstrates substantially degraded performance\ncompared to single-hop sub-QAs, with first-hop object or event recognition\nemerging as the primary bottleneck. We position Video SimpleQA as the\ncornerstone benchmark for video factuality assessment, aiming to steer LVLM\ndevelopment toward verifiable grounding in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in videos remains a critical unsolved challenge. To address this gap,\nwe introduce Video SimpleQA, the first comprehensive benchmark tailored for\nfactuality evaluation in video contexts. Our work differs from existing video\nbenchmarks through the following key features: 1) Knowledge required: demanding\nintegration of external knowledge beyond the video's explicit narrative; 2)\nMulti-hop fact-seeking question: Each question involves multiple explicit facts\nand requires strict factual grounding without hypothetical or subjective\ninferences. We also include per-hop single-fact-based sub-QAs alongside final\nQAs to enable fine-grained, stepby-step evaluation; 3) Short-form definitive\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat with minimal scoring variance; 4) Temporal grounded required: Requiring\nanswers to rely on one or more temporal segments in videos, rather than single\nframes. We extensively evaluate 33 state-of-the-art LVLMs and summarize key\nfindings as follows: 1) Current LVLMs exhibit notable deficiencies in factual\nadherence, with the best-performing model o3 merely achieving an F-score of\n66.3%; 2) Most LVLMs are overconfident in what they generate, with self-stated\nconfidence exceeding actual accuracy; 3) Retrieval-augmented generation\ndemonstrates consistent improvements at the cost of additional inference time\noverhead; 4) Multi-hop QA demonstrates substantially degraded performance\ncompared to single-hop sub-QAs, with first-hop object or event recognition\nemerging as the primary bottleneck. We position Video SimpleQA as the\ncornerstone benchmark for video factuality assessment, aiming to steer LVLM\ndevelopment toward verifiable grounding in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Yingyao Wang"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haoze Zhao"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Wangbo Yu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09971v1",
                "updated": "2025-08-13T17:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Vision-driven River Following of UAV via Safe Reinforcement Learning\n  using Semantic Dynamics Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-driven River Following of UAV via Safe Reinforcement Learning\n  using Semantic Dynamics Model"
                },
                "summary": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is\ncritical for applications such as rescue, surveillance, and environmental\nmonitoring, particularly in dense riverine environments where GPS signals are\nunreliable. We formalize river following as a coverage control problem in which\nthe reward function is submodular, yielding diminishing returns as more unique\nriver segments are visited, thereby framing the task as a Submodular Markov\nDecision Process. First, we introduce Marginal Gain Advantage Estimation, which\nrefines the reward advantage function by using a sliding window baseline\ncomputed from historical episodic returns, thus aligning the advantage\nestimation with the agent's evolving recognition of action value in\nnon-Markovian settings. Second, we develop a Semantic Dynamics Model based on\npatchified water semantic masks that provides more interpretable and\ndata-efficient short-term prediction of future observations compared to latent\nvision dynamics models. Third, we present the Constrained Actor Dynamics\nEstimator architecture, which integrates the actor, the cost estimator, and SDM\nfor cost advantage estimation to form a model-based SafeRL framework capable of\nsolving partially observable Constrained Submodular Markov Decision Processes.\nSimulation results demonstrate that MGAE achieves faster convergence and\nsuperior performance over traditional critic-based methods like Generalized\nAdvantage Estimation. SDM provides more accurate short-term state predictions\nthat enable the cost estimator to better predict potential violations. Overall,\nCADE effectively integrates safety regulation into model-based RL, with the\nLagrangian approach achieving the soft balance of reward and safety during\ntraining, while the safety layer enhances performance during inference by hard\naction overlay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is\ncritical for applications such as rescue, surveillance, and environmental\nmonitoring, particularly in dense riverine environments where GPS signals are\nunreliable. We formalize river following as a coverage control problem in which\nthe reward function is submodular, yielding diminishing returns as more unique\nriver segments are visited, thereby framing the task as a Submodular Markov\nDecision Process. First, we introduce Marginal Gain Advantage Estimation, which\nrefines the reward advantage function by using a sliding window baseline\ncomputed from historical episodic returns, thus aligning the advantage\nestimation with the agent's evolving recognition of action value in\nnon-Markovian settings. Second, we develop a Semantic Dynamics Model based on\npatchified water semantic masks that provides more interpretable and\ndata-efficient short-term prediction of future observations compared to latent\nvision dynamics models. Third, we present the Constrained Actor Dynamics\nEstimator architecture, which integrates the actor, the cost estimator, and SDM\nfor cost advantage estimation to form a model-based SafeRL framework capable of\nsolving partially observable Constrained Submodular Markov Decision Processes.\nSimulation results demonstrate that MGAE achieves faster convergence and\nsuperior performance over traditional critic-based methods like Generalized\nAdvantage Estimation. SDM provides more accurate short-term state predictions\nthat enable the cost estimator to better predict potential violations. Overall,\nCADE effectively integrates safety regulation into model-based RL, with the\nLagrangian approach achieving the soft balance of reward and safety during\ntraining, while the safety layer enhances performance during inference by hard\naction overlay."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Nina Mahmoudian"
                    }
                ],
                "author_detail": {
                    "name": "Nina Mahmoudian"
                },
                "author": "Nina Mahmoudian",
                "arxiv_comment": "Submitted to Robotics and Autonomous Systems (RAS) journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09968v1",
                "updated": "2025-08-13T17:33:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    33,
                    37,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:33:37Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    33,
                    37,
                    2,
                    225,
                    0
                ],
                "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"
                },
                "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise"
                },
                "authors": [
                    {
                        "name": "Luca Eyring"
                    },
                    {
                        "name": "Shyamgopal Karthik"
                    },
                    {
                        "name": "Alexey Dosovitskiy"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Project page: https://noisehypernetworks.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09965v1",
                "updated": "2025-08-13T17:32:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    32,
                    14,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:32:14Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    32,
                    14,
                    2,
                    225,
                    0
                ],
                "title": "GW231123: a Possible Primordial Black Hole Origin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GW231123: a Possible Primordial Black Hole Origin"
                },
                "summary": "GW231123, the heaviest binary black hole merger detected by the\nLIGO-Virgo-KAGRA collaboration to date, lies in the pair-instability mass gap\nand exhibits unusually high component spins. In this letter, we show that both\nmerging black holes may have a primordial origin with smaller initial masses.\nThe observed masses and, crucially, the spins of GW231123 are naturally\naccommodated within the most vanilla primordial black hole framework, once\ncosmological accretion is taken into account. Interestingly, the parameter\nspace needed to explain the inferred GW231123 rate is at the edge of the\nexclusion region from Xray and CMB observations, suggesting that this\ninterpretation can be either confirmed or ruled out. The upcoming O5 observing\nrun by the collaboration should detect ${\\cal O}(20)$ similar events, testing\ntheir mass-spin correlation, while next-generation detectors would be capable\nof observing high redshift events, as predicted in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GW231123, the heaviest binary black hole merger detected by the\nLIGO-Virgo-KAGRA collaboration to date, lies in the pair-instability mass gap\nand exhibits unusually high component spins. In this letter, we show that both\nmerging black holes may have a primordial origin with smaller initial masses.\nThe observed masses and, crucially, the spins of GW231123 are naturally\naccommodated within the most vanilla primordial black hole framework, once\ncosmological accretion is taken into account. Interestingly, the parameter\nspace needed to explain the inferred GW231123 rate is at the edge of the\nexclusion region from Xray and CMB observations, suggesting that this\ninterpretation can be either confirmed or ruled out. The upcoming O5 observing\nrun by the collaboration should detect ${\\cal O}(20)$ similar events, testing\ntheir mass-spin correlation, while next-generation detectors would be capable\nof observing high redshift events, as predicted in this scenario."
                },
                "authors": [
                    {
                        "name": "Valerio De Luca"
                    },
                    {
                        "name": "Gabriele Franciolini"
                    },
                    {
                        "name": "Antonio Riotto"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Riotto"
                },
                "author": "Antonio Riotto",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11260v2",
                "updated": "2025-08-13T17:29:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    29,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2024-04-17T11:11:23Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    11,
                    23,
                    2,
                    108,
                    0
                ],
                "title": "WISDOM Project -- XXVI. Cross-checking supermassive black hole mass\n  estimates from ALMA CO gas kinematics and SINFONI stellar kinematics in the\n  galaxy NGC 4751",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISDOM Project -- XXVI. Cross-checking supermassive black hole mass\n  estimates from ALMA CO gas kinematics and SINFONI stellar kinematics in the\n  galaxy NGC 4751"
                },
                "summary": "We present high angular resolution ($0.19''$ or $\\approx24$ pc) ALMA\nobservations of the $^{12}$CO(3-2) line emission of the galaxy NGC 4751. The\ndata provide evidence for the presence of a central SMBH. Assuming a constant\nmass-to-light ratio ($M/L$), we infer a SMBH mass\n$M_\\text{BH}=3.43^{+0.45}_{-0.44}[\\text{stat},3\\sigma]^{+0.22}_{-0.64}[\\text{sys}]\\times10^9$\nM$_\\odot$ and a F160W filter stellar\n$M/L_{F160W}=2.68\\pm0.11[\\text{stat},3\\sigma]^{+0.10}_{-0.80}[\\text{sys}]$\nM$_\\odot$/L$_{\\odot,\\text{F160W}}$, where the first uncertainties are\nstatistical and the second systematic. Assuming a linearly spatially-varying\n$M/L$, we infer\n$M_\\text{BH}=2.79^{+0.75}_{-0.57}[\\text{stat},3\\sigma]^{+0.75}_{-0.45}[\\text{syst}]\\times10^9$\nM$_\\odot$ and\n$(M/L_\\text{F160W})/(\\text{M}_\\odot/\\text{L}_{\\odot,\\text{F160W}})=3.07^{+0.27}_{-0.35}[\\text{stat},3\\sigma]^{+0.08}_{-1.14}[\\text{sys}]-0.09^{+0.08}_{-0.06}[\\text{stat},3\\sigma]^{+0.08}_{-0.01}[\\text{sys}](R/\\text{arcsec})$,\nwhere $R$ is the galactocentric radius. We also present SMBH mass estimates\nusing the Jeans Anisotropic Modelling (JAM) method and Very Large Telescope\nSpectrograph for INtegral Field Observations in the Near Infrared (SINFONI)\nstellar kinematics. Assuming a cylindrically-aligned velocity ellipsoid\n(JAM$_\\text{cyl}$) we infer $M_\\text{BH}=(2.52\\pm 0.36)\\times10^9$ M$_\\odot$,\nwhile assuming a spherically-aligned velocity ellipsoid (JAM$_\\text{sph}$) we\ninfer $M_\\text{BH}=(3.24\\pm0.87)\\times10^9$ M$_\\odot$. The SMBH mass assuming a\nconstant $M/L$ is statistically consistent with that of JAM$_\\text{sph}$,\nwhereas the mass assuming a linearly-varying $M/L$ is consistent with both\nJAM$_\\text{cyl}$ and JAM$_\\text{sph}$ (within the uncertainties). Our derived\nmasses are larger than (and inconsistent with) one previous stellar dynamical\nmeasurement using the Schwarzschild orbit-superposition method and the same\nSINFONI kinematics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present high angular resolution ($0.19''$ or $\\approx24$ pc) ALMA\nobservations of the $^{12}$CO(3-2) line emission of the galaxy NGC 4751. The\ndata provide evidence for the presence of a central SMBH. Assuming a constant\nmass-to-light ratio ($M/L$), we infer a SMBH mass\n$M_\\text{BH}=3.43^{+0.45}_{-0.44}[\\text{stat},3\\sigma]^{+0.22}_{-0.64}[\\text{sys}]\\times10^9$\nM$_\\odot$ and a F160W filter stellar\n$M/L_{F160W}=2.68\\pm0.11[\\text{stat},3\\sigma]^{+0.10}_{-0.80}[\\text{sys}]$\nM$_\\odot$/L$_{\\odot,\\text{F160W}}$, where the first uncertainties are\nstatistical and the second systematic. Assuming a linearly spatially-varying\n$M/L$, we infer\n$M_\\text{BH}=2.79^{+0.75}_{-0.57}[\\text{stat},3\\sigma]^{+0.75}_{-0.45}[\\text{syst}]\\times10^9$\nM$_\\odot$ and\n$(M/L_\\text{F160W})/(\\text{M}_\\odot/\\text{L}_{\\odot,\\text{F160W}})=3.07^{+0.27}_{-0.35}[\\text{stat},3\\sigma]^{+0.08}_{-1.14}[\\text{sys}]-0.09^{+0.08}_{-0.06}[\\text{stat},3\\sigma]^{+0.08}_{-0.01}[\\text{sys}](R/\\text{arcsec})$,\nwhere $R$ is the galactocentric radius. We also present SMBH mass estimates\nusing the Jeans Anisotropic Modelling (JAM) method and Very Large Telescope\nSpectrograph for INtegral Field Observations in the Near Infrared (SINFONI)\nstellar kinematics. Assuming a cylindrically-aligned velocity ellipsoid\n(JAM$_\\text{cyl}$) we infer $M_\\text{BH}=(2.52\\pm 0.36)\\times10^9$ M$_\\odot$,\nwhile assuming a spherically-aligned velocity ellipsoid (JAM$_\\text{sph}$) we\ninfer $M_\\text{BH}=(3.24\\pm0.87)\\times10^9$ M$_\\odot$. The SMBH mass assuming a\nconstant $M/L$ is statistically consistent with that of JAM$_\\text{sph}$,\nwhereas the mass assuming a linearly-varying $M/L$ is consistent with both\nJAM$_\\text{cyl}$ and JAM$_\\text{sph}$ (within the uncertainties). Our derived\nmasses are larger than (and inconsistent with) one previous stellar dynamical\nmeasurement using the Schwarzschild orbit-superposition method and the same\nSINFONI kinematics."
                },
                "authors": [
                    {
                        "name": "Pandora Dominiak"
                    },
                    {
                        "name": "Michele Cappellari"
                    },
                    {
                        "name": "Martin Bureau"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Marc Sarzi"
                    },
                    {
                        "name": "Ilaria Ruffa"
                    },
                    {
                        "name": "Satoru Iguchi"
                    },
                    {
                        "name": "Thomas G. Williams"
                    },
                    {
                        "name": "Hengyue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hengyue Zhang"
                },
                "author": "Hengyue Zhang",
                "arxiv_comment": "20 pages, 15 figures, accepted by MNRAS on 13 August 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06399v2",
                "updated": "2025-08-13T17:20:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    20,
                    50,
                    2,
                    225,
                    0
                ],
                "published": "2025-01-11T01:12:23Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    1,
                    12,
                    23,
                    5,
                    11,
                    0
                ],
                "title": "GenAI Confessions: Black-box Membership Inference for Generative Image\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI Confessions: Black-box Membership Inference for Generative Image\n  Models"
                },
                "summary": "From a simple text prompt, generative-AI image models can create stunningly\nrealistic and creative images bounded, it seems, by only our imagination. These\nmodels have achieved this remarkable feat thanks, in part, to the ingestion of\nbillions of images collected from nearly every corner of the internet. Many\ncreators have understandably expressed concern over how their intellectual\nproperty has been ingested without their permission or a mechanism to opt out\nof training. As a result, questions of fair use and copyright infringement have\nquickly emerged. We describe a method that allows us to determine if a model\nwas trained on a specific image or set of images. This method is\ncomputationally efficient and assumes no explicit knowledge of the model\narchitecture or weights (so-called black-box membership inference). We\nanticipate that this method will be crucial for auditing existing models and,\nlooking ahead, ensuring the fairer development and deployment of generative AI\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From a simple text prompt, generative-AI image models can create stunningly\nrealistic and creative images bounded, it seems, by only our imagination. These\nmodels have achieved this remarkable feat thanks, in part, to the ingestion of\nbillions of images collected from nearly every corner of the internet. Many\ncreators have understandably expressed concern over how their intellectual\nproperty has been ingested without their permission or a mechanism to opt out\nof training. As a result, questions of fair use and copyright infringement have\nquickly emerged. We describe a method that allows us to determine if a model\nwas trained on a specific image or set of images. This method is\ncomputationally efficient and assumes no explicit knowledge of the model\narchitecture or weights (so-called black-box membership inference). We\nanticipate that this method will be crucial for auditing existing models and,\nlooking ahead, ensuring the fairer development and deployment of generative AI\nmodels."
                },
                "authors": [
                    {
                        "name": "Matyas Bohacek"
                    },
                    {
                        "name": "Hany Farid"
                    }
                ],
                "author_detail": {
                    "name": "Hany Farid"
                },
                "author": "Hany Farid",
                "arxiv_comment": "https://genai-confessions.github.io",
                "arxiv_journal_ref": "ICCV-W 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09958v1",
                "updated": "2025-08-13T17:19:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    19,
                    41,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:19:41Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    19,
                    41,
                    2,
                    225,
                    0
                ],
                "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks"
                },
                "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Eddie Zhang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "arxiv_comment": "Submitted to AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03611v2",
                "updated": "2025-08-13T17:17:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    17,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-05T16:27:10Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    27,
                    10,
                    1,
                    217,
                    0
                ],
                "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and\n  Predictive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block: Balancing Load in LLM Serving with Context, Knowledge and\n  Predictive Scheduling"
                },
                "summary": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "12 pages, 8 figures excluding appendix. V1: Fix some typos and\n  grammar issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09956v2",
                "updated": "2025-08-14T01:29:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    1,
                    29,
                    55,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T17:17:17Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    17,
                    17,
                    2,
                    225,
                    0
                ],
                "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering"
                },
                "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology."
                },
                "authors": [
                    {
                        "name": "Fares Antaki"
                    },
                    {
                        "name": "David Mikhail"
                    },
                    {
                        "name": "Daniel Milad"
                    },
                    {
                        "name": "Danny A Mammo"
                    },
                    {
                        "name": "Sumit Sharma"
                    },
                    {
                        "name": "Sunil K Srivastava"
                    },
                    {
                        "name": "Bing Yu Chen"
                    },
                    {
                        "name": "Samir Touma"
                    },
                    {
                        "name": "Mertcan Sevgi"
                    },
                    {
                        "name": "Jonathan El-Khoury"
                    },
                    {
                        "name": "Pearse A Keane"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yih Chung Tham"
                    },
                    {
                        "name": "Renaud Duval"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Duval"
                },
                "author": "Renaud Duval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09951v1",
                "updated": "2025-08-13T17:11:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    11,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:11:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    11,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Tight correlation of star formation with [Ci] and CO lines across cosmic\n  time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight correlation of star formation with [Ci] and CO lines across cosmic\n  time"
                },
                "summary": "Context. Cold molecular gas tracers, such as CI and CO lines, have been\nwidely used to infer specific characteristics of the ISM and to derive\nstar-formation relations among galaxies. Aims. However, there is still a lack\nof systematic studies of the star-formation scaling relation of CO and [CI]\nlines across cosmic time on multiple physical scales. Methods. We used\nobservations of the ground state transitions of [CI], CO, and [CII], for 885\nsources collected from the literature, to infer possible correlations between\nline luminosities of $\\rm L^{'}_{[CI](1-0)}, \\rm L^{'}_{CO(1-0)}$, and $\\rm\nL^{'}_{[CII]}$ with star formation rates (SFR). With linear regression, we fit\nthe relations between SFR and molecular mass derived from CO, CI, and CII\nlines. Results. The relation between [CI] and CO-based total molecular masses\nis weakly superlinear. Nevertheless, they can be calibrated against each other.\nFor $\\rm \\alpha_{CO} = 0.8$ and $4.0\\ \\rm\n{M}_{\\odot}\\,({K}\\,{km}\\,{s}^{-1}\\,{pc}^2)^{-1}$ we derive $\\alpha_{\\rm [CI]} =\n3.9$ and $\\sim$$17\\ \\rm {M}_{\\odot}\\,({K}\\,{km}\\,{s}^{-1}\\,{pc}^2)^{-1}$ ,\nrespectively. Using the \\emph{lmfit} package, we derived relation slopes of\nSFR--$\\rm L^{'}_{[CI](1-0)}$, SFR--$\\rm L^{'}_{CO(1-0)}$, and SFR--$\\rm\nL^{'}_{[CII](1-0)}$ to be $\\rm \\beta$ = 1.06 $\\pm$ 0.02, 1.24 $\\pm$ 0.02, and\n0.74 $\\pm$ 0.02, respectively. With a Bayesian-inference \\emph{linmix} method,\nwe find consistent results. Conclusions. Our relations for [CI](1-0) and\nCO(1-0) indicate that they trace similar molecular gas contents, across\ndifferent redshifts and different types of galaxies. This suggests that these\ncorrelations do not have strong evolution with cosmic time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Cold molecular gas tracers, such as CI and CO lines, have been\nwidely used to infer specific characteristics of the ISM and to derive\nstar-formation relations among galaxies. Aims. However, there is still a lack\nof systematic studies of the star-formation scaling relation of CO and [CI]\nlines across cosmic time on multiple physical scales. Methods. We used\nobservations of the ground state transitions of [CI], CO, and [CII], for 885\nsources collected from the literature, to infer possible correlations between\nline luminosities of $\\rm L^{'}_{[CI](1-0)}, \\rm L^{'}_{CO(1-0)}$, and $\\rm\nL^{'}_{[CII]}$ with star formation rates (SFR). With linear regression, we fit\nthe relations between SFR and molecular mass derived from CO, CI, and CII\nlines. Results. The relation between [CI] and CO-based total molecular masses\nis weakly superlinear. Nevertheless, they can be calibrated against each other.\nFor $\\rm \\alpha_{CO} = 0.8$ and $4.0\\ \\rm\n{M}_{\\odot}\\,({K}\\,{km}\\,{s}^{-1}\\,{pc}^2)^{-1}$ we derive $\\alpha_{\\rm [CI]} =\n3.9$ and $\\sim$$17\\ \\rm {M}_{\\odot}\\,({K}\\,{km}\\,{s}^{-1}\\,{pc}^2)^{-1}$ ,\nrespectively. Using the \\emph{lmfit} package, we derived relation slopes of\nSFR--$\\rm L^{'}_{[CI](1-0)}$, SFR--$\\rm L^{'}_{CO(1-0)}$, and SFR--$\\rm\nL^{'}_{[CII](1-0)}$ to be $\\rm \\beta$ = 1.06 $\\pm$ 0.02, 1.24 $\\pm$ 0.02, and\n0.74 $\\pm$ 0.02, respectively. With a Bayesian-inference \\emph{linmix} method,\nwe find consistent results. Conclusions. Our relations for [CI](1-0) and\nCO(1-0) indicate that they trace similar molecular gas contents, across\ndifferent redshifts and different types of galaxies. This suggests that these\ncorrelations do not have strong evolution with cosmic time."
                },
                "authors": [
                    {
                        "name": "Theodoros Topkaras"
                    },
                    {
                        "name": "Thomas G. Bisbas"
                    },
                    {
                        "name": "Zhi-Yu Zhang"
                    },
                    {
                        "name": "V. Ossenkopf-Okada"
                    }
                ],
                "author_detail": {
                    "name": "V. Ossenkopf-Okada"
                },
                "author": "V. Ossenkopf-Okada",
                "arxiv_comment": "18 pages, 5 figure, Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09950v1",
                "updated": "2025-08-13T17:11:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    11,
                    20,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:11:20Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    11,
                    20,
                    2,
                    225,
                    0
                ],
                "title": "PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement\n  Learning for Legged Robots in Crawl Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement\n  Learning for Legged Robots in Crawl Spaces"
                },
                "summary": "The legged locomotion in spatially constrained structures (called crawl\nspaces) is challenging. In crawl spaces, current exteroceptive locomotion\nlearning methods are limited by large noises and errors of the sensors in\npossible low visibility conditions, and current proprioceptive locomotion\nlearning methods are difficult in traversing crawl spaces because only ground\nfeatures are inferred. In this study, a point cloud supervised proprioceptive\nlocomotion reinforcement learning method for legged robots in crawl spaces is\nproposed. A state estimation network is designed to estimate the robot's\nsurrounding ground and spatial features as well as the robot's collision states\nusing historical proprioceptive sensor data. The point cloud is represented in\npolar coordinate frame and a point cloud processing method is proposed to\nefficiently extract the ground and spatial features that are used to supervise\nthe state estimation network learning. Comprehensive reward functions that\nguide the robot to traverse through crawl spaces after collisions are designed.\nExperiments demonstrate that, compared to existing methods, our method exhibits\nmore agile locomotion in crawl spaces. This study enhances the ability of\nlegged robots to traverse spatially constrained environments without requiring\nexteroceptive sensors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The legged locomotion in spatially constrained structures (called crawl\nspaces) is challenging. In crawl spaces, current exteroceptive locomotion\nlearning methods are limited by large noises and errors of the sensors in\npossible low visibility conditions, and current proprioceptive locomotion\nlearning methods are difficult in traversing crawl spaces because only ground\nfeatures are inferred. In this study, a point cloud supervised proprioceptive\nlocomotion reinforcement learning method for legged robots in crawl spaces is\nproposed. A state estimation network is designed to estimate the robot's\nsurrounding ground and spatial features as well as the robot's collision states\nusing historical proprioceptive sensor data. The point cloud is represented in\npolar coordinate frame and a point cloud processing method is proposed to\nefficiently extract the ground and spatial features that are used to supervise\nthe state estimation network learning. Comprehensive reward functions that\nguide the robot to traverse through crawl spaces after collisions are designed.\nExperiments demonstrate that, compared to existing methods, our method exhibits\nmore agile locomotion in crawl spaces. This study enhances the ability of\nlegged robots to traverse spatially constrained environments without requiring\nexteroceptive sensors."
                },
                "authors": [
                    {
                        "name": "Bida Ma"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Chenkun Qi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yule Mo"
                    },
                    {
                        "name": "Jinkai Wang"
                    },
                    {
                        "name": "Chunpeng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chunpeng Lu"
                },
                "author": "Chunpeng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09949v1",
                "updated": "2025-08-13T17:08:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    8,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:08:22Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    8,
                    22,
                    2,
                    225,
                    0
                ],
                "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning"
                },
                "summary": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance."
                },
                "authors": [
                    {
                        "name": "Trevine Oorloff"
                    },
                    {
                        "name": "Vishwanath Sindagi"
                    },
                    {
                        "name": "Wele Gedara Chaminda Bandara"
                    },
                    {
                        "name": "Ali Shafahi"
                    },
                    {
                        "name": "Amin Ghiasi"
                    },
                    {
                        "name": "Charan Prakash"
                    },
                    {
                        "name": "Reza Ardekani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Ardekani"
                },
                "author": "Reza Ardekani",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09945v1",
                "updated": "2025-08-13T17:00:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    0,
                    44,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:00:44Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    0,
                    44,
                    2,
                    225,
                    0
                ],
                "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models"
                },
                "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets."
                },
                "authors": [
                    {
                        "name": "Lingjie Jiang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09943v1",
                "updated": "2025-08-13T16:57:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    57,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    57,
                    49,
                    2,
                    225,
                    0
                ],
                "title": "AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using\n  Diffusion Models"
                },
                "summary": "Low-dose CT (LDCT) protocols reduce radiation exposure but increase image\nnoise, compromising diagnostic confidence. Diffusion-based generative models\nhave shown promise for LDCT denoising by learning image priors and performing\niterative refinement. In this work, we introduce AST-n, an accelerated\ninference framework that initiates reverse diffusion from intermediate noise\nlevels, and integrate high-order ODE solvers within conditioned models to\nfurther reduce sampling steps. We evaluate two acceleration paradigms--AST-n\nsampling and standard scheduling with high-order solvers -- on the Low Dose CT\nGrand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %\nof standard dose. Conditioned models using only 25 steps (AST-25) achieve peak\nsignal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)\nabove 0.95, closely matching standard baselines while cutting inference time\nfrom ~16 seg to under 1 seg per slice. Unconditional sampling suffers\nsubstantial quality loss, underscoring the necessity of conditioning. We also\nassess DDIM inversion, which yields marginal PSNR gains at the cost of doubling\ninference time, limiting its clinical practicality. Our results demonstrate\nthat AST-n with high-order samplers enables rapid LDCT reconstruction without\nsignificant loss of image fidelity, advancing the feasibility of\ndiffusion-based methods in clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-dose CT (LDCT) protocols reduce radiation exposure but increase image\nnoise, compromising diagnostic confidence. Diffusion-based generative models\nhave shown promise for LDCT denoising by learning image priors and performing\niterative refinement. In this work, we introduce AST-n, an accelerated\ninference framework that initiates reverse diffusion from intermediate noise\nlevels, and integrate high-order ODE solvers within conditioned models to\nfurther reduce sampling steps. We evaluate two acceleration paradigms--AST-n\nsampling and standard scheduling with high-order solvers -- on the Low Dose CT\nGrand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %\nof standard dose. Conditioned models using only 25 steps (AST-25) achieve peak\nsignal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)\nabove 0.95, closely matching standard baselines while cutting inference time\nfrom ~16 seg to under 1 seg per slice. Unconditional sampling suffers\nsubstantial quality loss, underscoring the necessity of conditioning. We also\nassess DDIM inversion, which yields marginal PSNR gains at the cost of doubling\ninference time, limiting its clinical practicality. Our results demonstrate\nthat AST-n with high-order samplers enables rapid LDCT reconstruction without\nsignificant loss of image fidelity, advancing the feasibility of\ndiffusion-based methods in clinical workflows."
                },
                "authors": [
                    {
                        "name": "Toms de la Sotta"
                    },
                    {
                        "name": "Jos M. Saavedra"
                    },
                    {
                        "name": "Hctor Henrquez"
                    },
                    {
                        "name": "Violeta Chang"
                    },
                    {
                        "name": "Aline Xavier"
                    }
                ],
                "author_detail": {
                    "name": "Aline Xavier"
                },
                "author": "Aline Xavier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09937v1",
                "updated": "2025-08-13T16:42:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    42,
                    1,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:42:01Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    42,
                    1,
                    2,
                    225,
                    0
                ],
                "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions."
                },
                "authors": [
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Momin Abbas"
                    },
                    {
                        "name": "Maysa Malfiza Garcia de Macedo"
                    },
                    {
                        "name": "Marcelo Carpinette Grave"
                    },
                    {
                        "name": "Luan Soares de Souza"
                    },
                    {
                        "name": "Tiago Machado"
                    },
                    {
                        "name": "Rogerio A de Paula"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Heloisa Caroline de Souza Pereira Candello"
                    },
                    {
                        "name": "Rebecka Nordenlow"
                    },
                    {
                        "name": "Aminat Adebiyi"
                    }
                ],
                "author_detail": {
                    "name": "Aminat Adebiyi"
                },
                "author": "Aminat Adebiyi",
                "arxiv_doi": "10.48550/arXiv.2508.09937",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2508.09937",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In submission",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21817v3",
                "updated": "2025-08-14T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    16,
                    52,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-29T13:51:46Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    13,
                    51,
                    46,
                    1,
                    210,
                    0
                ],
                "title": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on\n  Vulnerability Datasets Detect Top 25 CWE Weaknesses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on\n  Vulnerability Datasets Detect Top 25 CWE Weaknesses?"
                },
                "summary": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from the same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 33% when evaluated on independent data, with some performing close to\nrandom guessing. To address these limitations, we present a three-part\nsolution. First, we introduce a manually curated test dataset, BenchVul,\ncovering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a\nhigh-quality training dataset, TitanVul, comprising 38,863 functions by\naggregating seven public sources and applying deduplication and validation\nusing a novel multi-agent LLM framework. Third, we propose a Realistic\nVulnerability Generation (RVG) framework, which synthesizes context-aware\nvulnerability examples for underrepresented but critical CWE types through\nsimulated development workflows. Our evaluation shows the strengths of each\ncomponent in closing the generalization gap. First, BenchVul shows the\nlimitations of self-testing: models trained on existing datasets, such as\nBigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to\n0.519 and from 0.713 to 0.607). Second, training models on TitanVul\ndemonstrates improved generalization, with model performance increasing from\n0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul.\nThird, supplementing TitanVul with RVG-generated data yields further gains,\nincreasing model performance by 14.0% to 0.874.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from the same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 33% when evaluated on independent data, with some performing close to\nrandom guessing. To address these limitations, we present a three-part\nsolution. First, we introduce a manually curated test dataset, BenchVul,\ncovering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a\nhigh-quality training dataset, TitanVul, comprising 38,863 functions by\naggregating seven public sources and applying deduplication and validation\nusing a novel multi-agent LLM framework. Third, we propose a Realistic\nVulnerability Generation (RVG) framework, which synthesizes context-aware\nvulnerability examples for underrepresented but critical CWE types through\nsimulated development workflows. Our evaluation shows the strengths of each\ncomponent in closing the generalization gap. First, BenchVul shows the\nlimitations of self-testing: models trained on existing datasets, such as\nBigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to\n0.519 and from 0.713 to 0.607). Second, training models on TitanVul\ndemonstrates improved generalization, with model performance increasing from\n0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul.\nThird, supplementing TitanVul with RVG-generated data yields further gains,\nincreasing model performance by 14.0% to 0.874."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ngoc Tan Bui"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Chengran Yang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Jinfeng Jiang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Chiok Yew Ho"
                    },
                    {
                        "name": "Jie Tan"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Yide Yin"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09932v2",
                "updated": "2025-08-14T13:25:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    25,
                    18,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T16:33:02Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    33,
                    2,
                    2,
                    225,
                    0
                ],
                "title": "Mathematical Computation and Reasoning Errors by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Computation and Reasoning Errors by Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Edith Aurora Graf"
                    }
                ],
                "author_detail": {
                    "name": "Edith Aurora Graf"
                },
                "author": "Edith Aurora Graf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16725v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16725v3",
                "updated": "2025-08-13T16:31:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    31,
                    15,
                    2,
                    225,
                    0
                ],
                "published": "2024-02-26T16:47:44Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    47,
                    44,
                    0,
                    57,
                    0
                ],
                "title": "Inference on the proportion of variance explained in principal component\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on the proportion of variance explained in principal component\n  analysis"
                },
                "summary": "Principal component analysis (PCA) is a longstanding and well-studied\napproach for dimension reduction. It rests upon the assumption that the\nunderlying signal in the data has low rank, and thus can be well-summarized\nusing a small number of dimensions. The output of PCA is typically represented\nusing a scree plot, which displays the proportion of variance explained (PVE)\nby each principal component. While the PVE is extensively reported in routine\ndata analyses, to the best of our knowledge the notion of inference on the PVE\nremains unexplored.\n  In this paper, we consider inference on the PVE. We first introduce a new\npopulation quantity for the PVE with respect to an unknown matrix mean.\nCritically, our interest lies in the PVE of the sample principal components (as\nopposed to unobserved population principal components); thus, the population\nPVE that we introduce is defined conditional on the sample singular vectors. We\nshow that it is possible to conduct inference, in the sense of confidence\nintervals, p-values, and point estimates, on this population quantity.\nFurthermore, we can conduct valid inference on the PVE of a subset of the\nprincipal components, even when the subset is selected using a data-driven\napproach such as the elbow rule. We demonstrate the proposed approach in\nsimulation and in an application to a gene expression dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal component analysis (PCA) is a longstanding and well-studied\napproach for dimension reduction. It rests upon the assumption that the\nunderlying signal in the data has low rank, and thus can be well-summarized\nusing a small number of dimensions. The output of PCA is typically represented\nusing a scree plot, which displays the proportion of variance explained (PVE)\nby each principal component. While the PVE is extensively reported in routine\ndata analyses, to the best of our knowledge the notion of inference on the PVE\nremains unexplored.\n  In this paper, we consider inference on the PVE. We first introduce a new\npopulation quantity for the PVE with respect to an unknown matrix mean.\nCritically, our interest lies in the PVE of the sample principal components (as\nopposed to unobserved population principal components); thus, the population\nPVE that we introduce is defined conditional on the sample singular vectors. We\nshow that it is possible to conduct inference, in the sense of confidence\nintervals, p-values, and point estimates, on this population quantity.\nFurthermore, we can conduct valid inference on the PVE of a subset of the\nprincipal components, even when the subset is selected using a data-driven\napproach such as the elbow rule. We demonstrate the proposed approach in\nsimulation and in an application to a gene expression dataset."
                },
                "authors": [
                    {
                        "name": "Ronan Perry"
                    },
                    {
                        "name": "Snigdha Panigrahi"
                    },
                    {
                        "name": "Jacob Bien"
                    },
                    {
                        "name": "Daniela Witten"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Witten"
                },
                "author": "Daniela Witten",
                "arxiv_doi": "10.1080/01621459.2025.2538895",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/01621459.2025.2538895",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.16725v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16725v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear: Journal of the American Statistical Association",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09926v1",
                "updated": "2025-08-13T16:24:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    24,
                    15,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:24:15Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    24,
                    15,
                    2,
                    225,
                    0
                ],
                "title": "Towards Comprehensive Cellular Characterisation of H&E slides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Comprehensive Cellular Characterisation of H&E slides"
                },
                "summary": "Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/."
                },
                "authors": [
                    {
                        "name": "Benjamin Adjadj"
                    },
                    {
                        "name": "Pierre-Antoine Bannier"
                    },
                    {
                        "name": "Guillaume Horent"
                    },
                    {
                        "name": "Sebastien Mandela"
                    },
                    {
                        "name": "Aurore Lyon"
                    },
                    {
                        "name": "Kathryn Schutte"
                    },
                    {
                        "name": "Ulysse Marteau"
                    },
                    {
                        "name": "Valentin Gaury"
                    },
                    {
                        "name": "Laura Dumont"
                    },
                    {
                        "name": "Thomas Mathieu"
                    },
                    {
                        "name": "Reda Belbahri"
                    },
                    {
                        "name": "Benot Schmauch"
                    },
                    {
                        "name": "Eric Durand"
                    },
                    {
                        "name": "Katharina Von Loga"
                    },
                    {
                        "name": "Lucie Gillet"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Gillet"
                },
                "arxiv_affiliation": "Owkin",
                "author": "Lucie Gillet",
                "arxiv_comment": "33 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09911v1",
                "updated": "2025-08-13T16:07:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    7,
                    45,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:07:45Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    7,
                    45,
                    2,
                    225,
                    0
                ],
                "title": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous\n  Deliberation on Perspectivist Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous\n  Deliberation on Perspectivist Data"
                },
                "summary": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets."
                },
                "authors": [
                    {
                        "name": "Malik Khadar"
                    },
                    {
                        "name": "Daniel Runningen"
                    },
                    {
                        "name": "Julia Tang"
                    },
                    {
                        "name": "Stevie Chancellor"
                    },
                    {
                        "name": "Harmanpreet Kaur"
                    }
                ],
                "author_detail": {
                    "name": "Harmanpreet Kaur"
                },
                "author": "Harmanpreet Kaur",
                "arxiv_doi": "10.1145/3757707",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757707",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear at CSCW 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09904v1",
                "updated": "2025-08-13T16:02:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    2,
                    55,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:02:55Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    2,
                    55,
                    2,
                    225,
                    0
                ],
                "title": "Beyond Nave Prompting: Strategies for Improved Zero-shot\n  Context-aided Forecasting with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Nave Prompting: Strategies for Improved Zero-shot\n  Context-aided Forecasting with LLMs"
                },
                "summary": "Forecasting in real-world settings requires models to integrate not only\nhistorical data but also relevant contextual information, often available in\ntextual form. While recent work has shown that large language models (LLMs) can\nbe effective context-aided forecasters via na\\\"ive direct prompting, their full\npotential remains underexplored. We address this gap with 4 strategies,\nproviding new insights into the zero-shot capabilities of LLMs in this setting.\nReDP improves interpretability by eliciting explicit reasoning traces, allowing\nus to assess the model's reasoning over the context independently from its\nforecast accuracy. CorDP leverages LLMs solely to refine existing forecasts\nwith context, enhancing their applicability in real-world forecasting\npipelines. IC-DP proposes embedding historical examples of context-aided\nforecasting tasks in the prompt, substantially improving accuracy even for the\nlargest models. Finally, RouteDP optimizes resource efficiency by using LLMs to\nestimate task difficulty, and routing the most challenging tasks to larger\nmodels. Evaluated on different kinds of context-aided forecasting tasks from\nthe CiK benchmark, our strategies demonstrate distinct benefits over na\\\"ive\nprompting across LLMs of different sizes and families. These results open the\ndoor to further simple yet effective improvements in LLM-based context-aided\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting in real-world settings requires models to integrate not only\nhistorical data but also relevant contextual information, often available in\ntextual form. While recent work has shown that large language models (LLMs) can\nbe effective context-aided forecasters via na\\\"ive direct prompting, their full\npotential remains underexplored. We address this gap with 4 strategies,\nproviding new insights into the zero-shot capabilities of LLMs in this setting.\nReDP improves interpretability by eliciting explicit reasoning traces, allowing\nus to assess the model's reasoning over the context independently from its\nforecast accuracy. CorDP leverages LLMs solely to refine existing forecasts\nwith context, enhancing their applicability in real-world forecasting\npipelines. IC-DP proposes embedding historical examples of context-aided\nforecasting tasks in the prompt, substantially improving accuracy even for the\nlargest models. Finally, RouteDP optimizes resource efficiency by using LLMs to\nestimate task difficulty, and routing the most challenging tasks to larger\nmodels. Evaluated on different kinds of context-aided forecasting tasks from\nthe CiK benchmark, our strategies demonstrate distinct benefits over na\\\"ive\nprompting across LLMs of different sizes and families. These results open the\ndoor to further simple yet effective improvements in LLM-based context-aided\nforecasting."
                },
                "authors": [
                    {
                        "name": "Arjun Ashok"
                    },
                    {
                        "name": "Andrew Robert Williams"
                    },
                    {
                        "name": "Vincent Zhihao Zheng"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Alexandre Drouin"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Drouin"
                },
                "author": "Alexandre Drouin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09897v1",
                "updated": "2025-08-13T15:56:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    56,
                    16,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:56:16Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    56,
                    16,
                    2,
                    225,
                    0
                ],
                "title": "Finetuning Large Language Model as an Effective Symbolic Regressor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning Large Language Model as an Effective Symbolic Regressor"
                },
                "summary": "Deriving governing equations from observational data, known as Symbolic\nRegression (SR), is a cornerstone of scientific discovery. Large Language\nModels (LLMs) have shown promise in this task by leveraging their vast\ncross-disciplinary scientific knowledge. However, existing LLM-based methods\nprimarily rely on direct inference or prompt engineering, often requiring\nexcessive inference iterations to converge on correct formulas or failing to\ntreating complex equation targets. These limitations in effectiveness and\ngeneralization stem from an inherent tension between pre-trained LLMs'\nproficiency in approximate reasoning and the high-precision demands of SR\ntasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR\ncapability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning\nremains a critical barrier. We thus introduce SymbArena, specifically\nengineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse\nequations formulated as corpora of 1.83 billion tokens for LLM utilization,\nenabling effective training and inference. Further, SymbArena proposes a\nheuristics metric to precisely quantify form-level consistency, going beyond\nexisting SR numerical-oriented evaluation strategies. With this benchmark, we\nexplore mainstream LLM fine-tuning techniques for SR tasks and establish\nSymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental\nresults validate SymbolicChat as the first LLM to exceed traditional numerical\nmethods in both numerical precision and symbolic form accuracy, outperforming\nthe second-best LLM baseline with improvements of 2-fold gains in R2 score and\n8.37% in form-level consistency score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving governing equations from observational data, known as Symbolic\nRegression (SR), is a cornerstone of scientific discovery. Large Language\nModels (LLMs) have shown promise in this task by leveraging their vast\ncross-disciplinary scientific knowledge. However, existing LLM-based methods\nprimarily rely on direct inference or prompt engineering, often requiring\nexcessive inference iterations to converge on correct formulas or failing to\ntreating complex equation targets. These limitations in effectiveness and\ngeneralization stem from an inherent tension between pre-trained LLMs'\nproficiency in approximate reasoning and the high-precision demands of SR\ntasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR\ncapability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning\nremains a critical barrier. We thus introduce SymbArena, specifically\nengineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse\nequations formulated as corpora of 1.83 billion tokens for LLM utilization,\nenabling effective training and inference. Further, SymbArena proposes a\nheuristics metric to precisely quantify form-level consistency, going beyond\nexisting SR numerical-oriented evaluation strategies. With this benchmark, we\nexplore mainstream LLM fine-tuning techniques for SR tasks and establish\nSymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental\nresults validate SymbolicChat as the first LLM to exceed traditional numerical\nmethods in both numerical precision and symbolic form accuracy, outperforming\nthe second-best LLM baseline with improvements of 2-fold gains in R2 score and\n8.37% in form-level consistency score."
                },
                "authors": [
                    {
                        "name": "Yingfan Hua"
                    },
                    {
                        "name": "Ruikun Li"
                    },
                    {
                        "name": "Jun Yao"
                    },
                    {
                        "name": "Guohang Zhuang"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08604v2",
                "updated": "2025-08-13T15:55:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    55,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T03:37:16Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    37,
                    16,
                    1,
                    224,
                    0
                ],
                "title": "Transferable Model-agnostic Vision-Language Model Adaptation for\n  Efficient Weak-to-Strong Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Model-agnostic Vision-Language Model Adaptation for\n  Efficient Weak-to-Strong Generalization"
                },
                "summary": "Vision-Language Models (VLMs) have been widely used in various visual\nrecognition tasks due to their remarkable generalization capabilities. As these\nmodels grow in size and complexity, fine-tuning becomes costly, emphasizing the\nneed to reuse adaptation knowledge from 'weaker' models to efficiently enhance\n'stronger' ones. However, existing adaptation transfer methods exhibit limited\ntransferability across models due to their model-specific design and high\ncomputational demands. To tackle this, we propose Transferable Model-agnostic\nadapter (TransMiter), a light-weight adapter that improves vision-language\nmodels 'without backpropagation'. TransMiter captures the knowledge gap between\npre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained,\nthis knowledge can be seamlessly transferred across different models without\nthe need for backpropagation. Moreover, TransMiter consists of only a few\nlayers, inducing a negligible additional inference cost. Notably, supplementing\nthe process with a few labeled data further yields additional performance gain,\noften surpassing a fine-tuned stronger model, with a marginal training cost.\nExperimental results and analyses demonstrate that TransMiter effectively and\nefficiently transfers adaptation knowledge while preserving generalization\nabilities across VLMs of different sizes and architectures in visual\nrecognition tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have been widely used in various visual\nrecognition tasks due to their remarkable generalization capabilities. As these\nmodels grow in size and complexity, fine-tuning becomes costly, emphasizing the\nneed to reuse adaptation knowledge from 'weaker' models to efficiently enhance\n'stronger' ones. However, existing adaptation transfer methods exhibit limited\ntransferability across models due to their model-specific design and high\ncomputational demands. To tackle this, we propose Transferable Model-agnostic\nadapter (TransMiter), a light-weight adapter that improves vision-language\nmodels 'without backpropagation'. TransMiter captures the knowledge gap between\npre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained,\nthis knowledge can be seamlessly transferred across different models without\nthe need for backpropagation. Moreover, TransMiter consists of only a few\nlayers, inducing a negligible additional inference cost. Notably, supplementing\nthe process with a few labeled data further yields additional performance gain,\noften surpassing a fine-tuned stronger model, with a marginal training cost.\nExperimental results and analyses demonstrate that TransMiter effectively and\nefficiently transfers adaptation knowledge while preserving generalization\nabilities across VLMs of different sizes and architectures in visual\nrecognition tasks."
                },
                "authors": [
                    {
                        "name": "Jihwan Park"
                    },
                    {
                        "name": "Taehoon song"
                    },
                    {
                        "name": "Sanghyeok Lee"
                    },
                    {
                        "name": "Miso Choi"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09893v1",
                "updated": "2025-08-13T15:51:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    51,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:51:05Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    51,
                    5,
                    2,
                    225,
                    0
                ],
                "title": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"
                },
                "summary": "Regulatory compliance question answering (QA) requires precise, verifiable\ninformation, and domain-specific expertise, posing challenges for Large\nLanguage Models (LLMs). In this work, we present a novel multi-agent framework\nthat integrates a Knowledge Graph (KG) of Regulatory triplets with\nRetrieval-Augmented Generation (RAG) to address these demands. First, agents\nbuild and maintain an ontology-free KG by extracting subject--predicate--object\n(SPO) triplets from regulatory documents and systematically cleaning,\nnormalizing, deduplicating, and updating them. Second, these triplets are\nembedded and stored along with their corresponding textual sections and\nmetadata in a single enriched vector database, allowing for both graph-based\nreasoning and efficient information retrieval. Third, an orchestrated agent\npipeline leverages triplet-level retrieval for question answering, ensuring\nhigh semantic alignment between user queries and the factual\n\"who-did-what-to-whom\" core captured by the graph. Our hybrid system\noutperforms conventional methods in complex regulatory queries, ensuring\nfactual correctness with embedded triplets, enabling traceability through a\nunified vector database, and enhancing understanding through subgraph\nvisualization, providing a robust foundation for compliance-driven and broader\naudit-focused applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regulatory compliance question answering (QA) requires precise, verifiable\ninformation, and domain-specific expertise, posing challenges for Large\nLanguage Models (LLMs). In this work, we present a novel multi-agent framework\nthat integrates a Knowledge Graph (KG) of Regulatory triplets with\nRetrieval-Augmented Generation (RAG) to address these demands. First, agents\nbuild and maintain an ontology-free KG by extracting subject--predicate--object\n(SPO) triplets from regulatory documents and systematically cleaning,\nnormalizing, deduplicating, and updating them. Second, these triplets are\nembedded and stored along with their corresponding textual sections and\nmetadata in a single enriched vector database, allowing for both graph-based\nreasoning and efficient information retrieval. Third, an orchestrated agent\npipeline leverages triplet-level retrieval for question answering, ensuring\nhigh semantic alignment between user queries and the factual\n\"who-did-what-to-whom\" core captured by the graph. Our hybrid system\noutperforms conventional methods in complex regulatory queries, ensuring\nfactual correctness with embedded triplets, enabling traceability through a\nunified vector database, and enhancing understanding through subgraph\nvisualization, providing a robust foundation for compliance-driven and broader\naudit-focused applications."
                },
                "authors": [
                    {
                        "name": "Bhavik Agarwal"
                    },
                    {
                        "name": "Hemant Sunil Jomraj"
                    },
                    {
                        "name": "Simone Kaplunov"
                    },
                    {
                        "name": "Jack Krolick"
                    },
                    {
                        "name": "Viktoria Rojkova"
                    }
                ],
                "author_detail": {
                    "name": "Viktoria Rojkova"
                },
                "author": "Viktoria Rojkova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23077v3",
                "updated": "2025-08-13T15:48:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    48,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2025-03-29T13:27:46Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    13,
                    27,
                    46,
                    5,
                    88,
                    0
                ],
                "title": "Efficient Inference for Large Reasoning Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Large Reasoning Models: A Survey"
                },
                "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in solving complex tasks. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. The overview structure of\nthis paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from reasoning scenarios, object functions, and performance \\&\nefficiency aspects. Besides, we present open challenges in this field,\nincluding human-centric controllable reasoning, trade-off between\ninterpretability and efficiency of reasoning, ensuring the safety of efficient\nreasoning, and broader applications of efficient reasoning. In addition, we\nhighlight key insights for enhancing LRMs' inference efficiency via techniques\nsuch as model merging, new architectures, and agent routers. We hope this work\nserves as a valuable guide, helping researchers overcome challenges in this\nvibrant field. A collection of efficient reasoning methods for LRMs (papers and\ncodes) is provided at this link:\nhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in solving complex tasks. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. The overview structure of\nthis paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from reasoning scenarios, object functions, and performance \\&\nefficiency aspects. Besides, we present open challenges in this field,\nincluding human-centric controllable reasoning, trade-off between\ninterpretability and efficiency of reasoning, ensuring the safety of efficient\nreasoning, and broader applications of efficient reasoning. In addition, we\nhighlight key insights for enhancing LRMs' inference efficiency via techniques\nsuch as model merging, new architectures, and agent routers. We hope this work\nserves as a valuable guide, helping researchers overcome challenges in this\nvibrant field. A collection of efficient reasoning methods for LRMs (papers and\ncodes) is provided at this link:\nhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs."
                },
                "authors": [
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Ruihan Gong"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Stan Z. Li"
                    },
                    {
                        "name": "Keqin Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqin Li"
                },
                "author": "Keqin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20427v2",
                "updated": "2025-08-13T15:47:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    47,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-03-26T10:59:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    59,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "Localizing entropy production along non-equilibrium trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing entropy production along non-equilibrium trajectories"
                },
                "summary": "An important open problem in nonequilibrium thermodynamics is the\nquantification and spatiotemporal localization of entropy production in complex\nnanoscale processes from experimental data. Here we address this issue through\na data-driven approach that combines the recently developed short-time\nthermodynamic uncertainty relation based inference scheme with machine learning\ntechniques. Our approach leverages the flexible function representation\nprovided by deep neural networks to achieve accurate reconstruction of\nhigh-dimensional, potentially time-dependent dissipative force fields as well\nas the localization of entropy production in both space and time along\nnonequilibrium trajectories. We demonstrate the versatility of the framework\nthrough applications to diverse systems of fundamental interest and\nexperimental significance, where it successfully addresses distinct challenges\nin localizing entropy production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important open problem in nonequilibrium thermodynamics is the\nquantification and spatiotemporal localization of entropy production in complex\nnanoscale processes from experimental data. Here we address this issue through\na data-driven approach that combines the recently developed short-time\nthermodynamic uncertainty relation based inference scheme with machine learning\ntechniques. Our approach leverages the flexible function representation\nprovided by deep neural networks to achieve accurate reconstruction of\nhigh-dimensional, potentially time-dependent dissipative force fields as well\nas the localization of entropy production in both space and time along\nnonequilibrium trajectories. We demonstrate the versatility of the framework\nthrough applications to diverse systems of fundamental interest and\nexperimental significance, where it successfully addresses distinct challenges\nin localizing entropy production."
                },
                "authors": [
                    {
                        "name": "Biswajit Das"
                    },
                    {
                        "name": "Sreekanth K Manikandan"
                    }
                ],
                "author_detail": {
                    "name": "Sreekanth K Manikandan"
                },
                "author": "Sreekanth K Manikandan",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09889v1",
                "updated": "2025-08-13T15:46:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    46,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:46:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    46,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving"
                },
                "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems."
                },
                "authors": [
                    {
                        "name": "Zhitian Xie"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Chengyue Yu"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09482v2",
                "updated": "2025-08-13T15:33:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    33,
                    40,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-12T17:32:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    32,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Inference under Staggered Adoption: Case Study of the Affordable Care\n  Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference under Staggered Adoption: Case Study of the Affordable Care\n  Act"
                },
                "summary": "Panel data consists of a collection of $N$ units that are observed over $T$\nunits of time. A policy or treatment is subject to staggered adoption if\ndifferent units take on treatment at different times and remains treated (or\nnever at all). Assessing the effectiveness of such a policy requires estimating\nthe treatment effect, corresponding to the difference between outcomes for\ntreated versus untreated units. We develop inference procedures that build upon\na computationally efficient matrix estimator for treatment effects in panel\ndata. Our routines return confidence intervals (CIs) both for individual\ntreatment effects, as well as for more general bilinear functionals of\ntreatment effects, with prescribed coverage guarantees. We apply these\ninferential methods to analyze the effectiveness of Medicaid expansion portion\nof the Affordable Care Act. Based on our analysis, Medicaid expansion has led\nto substantial reductions in uninsurance rates, has reduced infant mortality\nrates, and has had no significant effects on healthcare expenditures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panel data consists of a collection of $N$ units that are observed over $T$\nunits of time. A policy or treatment is subject to staggered adoption if\ndifferent units take on treatment at different times and remains treated (or\nnever at all). Assessing the effectiveness of such a policy requires estimating\nthe treatment effect, corresponding to the difference between outcomes for\ntreated versus untreated units. We develop inference procedures that build upon\na computationally efficient matrix estimator for treatment effects in panel\ndata. Our routines return confidence intervals (CIs) both for individual\ntreatment effects, as well as for more general bilinear functionals of\ntreatment effects, with prescribed coverage guarantees. We apply these\ninferential methods to analyze the effectiveness of Medicaid expansion portion\nof the Affordable Care Act. Based on our analysis, Medicaid expansion has led\nto substantial reductions in uninsurance rates, has reduced infant mortality\nrates, and has had no significant effects on healthcare expenditures."
                },
                "authors": [
                    {
                        "name": "Eric Xia"
                    },
                    {
                        "name": "Yuling Yan"
                    },
                    {
                        "name": "Martin J. Wainwright"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Wainwright"
                },
                "author": "Martin J. Wainwright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15336v2",
                "updated": "2025-08-13T15:32:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    32,
                    29,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-21T18:00:01Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    0,
                    1,
                    0,
                    111,
                    0
                ],
                "title": "On DESI's DR2 exclusion of $$CDM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On DESI's DR2 exclusion of $$CDM"
                },
                "summary": "The DESI collaboration, combining their Baryon Acoustic Oscillation (BAO)\ndata with cosmic microwave background (CMB) anisotropy and supernovae data,\nhave found significant indication against the $\\Lambda$CDM cosmology. This can\nalso be interpreted as the significance of the detection of the $w_a$ parameter\nthat measures variation of the dark energy equation of state. DESI's DR2\narticle quotes exclusion of $\\Lambda$CDM for combinations of BAO and CMB data\nwith each of three different and overlapping supernovae compilations (at\n2.8-sigma for Pantheon+, 3.8-sigma for Union3, and 4.2-sigma for DESY5). We\nshow that one can neither choose amongst nor average over these three different\nsignificances. We demonstrate how a principled statistical combination yields a\ncombined exclusion significance of 3.1-sigma. Further we argue that, faced with\nthese competing significances, the most secure inference from the DESI DR2\nresults is the 3.1-sigma level exclusion of $\\Lambda$CDM obtained from\ncombining DESI+CMB alone, omitting supernovae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DESI collaboration, combining their Baryon Acoustic Oscillation (BAO)\ndata with cosmic microwave background (CMB) anisotropy and supernovae data,\nhave found significant indication against the $\\Lambda$CDM cosmology. This can\nalso be interpreted as the significance of the detection of the $w_a$ parameter\nthat measures variation of the dark energy equation of state. DESI's DR2\narticle quotes exclusion of $\\Lambda$CDM for combinations of BAO and CMB data\nwith each of three different and overlapping supernovae compilations (at\n2.8-sigma for Pantheon+, 3.8-sigma for Union3, and 4.2-sigma for DESY5). We\nshow that one can neither choose amongst nor average over these three different\nsignificances. We demonstrate how a principled statistical combination yields a\ncombined exclusion significance of 3.1-sigma. Further we argue that, faced with\nthese competing significances, the most secure inference from the DESI DR2\nresults is the 3.1-sigma level exclusion of $\\Lambda$CDM obtained from\ncombining DESI+CMB alone, omitting supernovae."
                },
                "authors": [
                    {
                        "name": "Marina Corts"
                    },
                    {
                        "name": "Andrew R Liddle"
                    }
                ],
                "author_detail": {
                    "name": "Andrew R Liddle"
                },
                "author": "Andrew R Liddle",
                "arxiv_comment": "6 pages with two figures, minor updates and additional references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09883v1",
                "updated": "2025-08-13T15:32:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    32,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:32:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    32,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Beyond Scaling Law: A Data-Efficient Distillation Framework for\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scaling Law: A Data-Efficient Distillation Framework for\n  Reasoning"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable reasoning capabilities in\ntasks such as algorithmic coding and mathematical problem-solving. Recent\nmethods have improved reasoning through expanded corpus and multistage training\ncombining reinforcement learning and supervised fine-tuning. Although some\nmethods suggest that small but targeted dataset can incentivize reasoning via\nonly distillation, a reasoning scaling laws is still taking shape, increasing\ncomputational costs. To address this, we propose a data-efficient distillation\nframework (DED) that optimizes the Pareto frontier of reasoning distillation.\nInspired by the on-policy learning and diverse roll-out strategies of\nreinforcement learning, the key idea of our approach is threefold: (1) We\nidentify that benchmark scores alone do not determine an effective teacher\nmodel. Through comprehensive comparisons of leading reasoning LLMs, we develop\na method to select an optimal teacher model. (2) While scaling distillation can\nenhance reasoning, it often degrades out-of-domain performance. A carefully\ncurated, smaller corpus achieves a balanced trade-off between in-domain and\nout-of-domain capabilities. (3) Diverse reasoning trajectories encourage the\nstudent model to develop robust reasoning skills. We validate our method\nthrough evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and\ncode generation (LiveCodeBench), achieving state-of-the-art results with only\n0.8k carefully curated examples, bypassing the need for extensive scaling. Our\nsystematic analysis demonstrates that DED outperforms existing methods by\nconsidering factors beyond superficial hardness, token length, or teacher model\ncapability. This work offers a practical and efficient pathway to advanced\nreasoning while preserving general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable reasoning capabilities in\ntasks such as algorithmic coding and mathematical problem-solving. Recent\nmethods have improved reasoning through expanded corpus and multistage training\ncombining reinforcement learning and supervised fine-tuning. Although some\nmethods suggest that small but targeted dataset can incentivize reasoning via\nonly distillation, a reasoning scaling laws is still taking shape, increasing\ncomputational costs. To address this, we propose a data-efficient distillation\nframework (DED) that optimizes the Pareto frontier of reasoning distillation.\nInspired by the on-policy learning and diverse roll-out strategies of\nreinforcement learning, the key idea of our approach is threefold: (1) We\nidentify that benchmark scores alone do not determine an effective teacher\nmodel. Through comprehensive comparisons of leading reasoning LLMs, we develop\na method to select an optimal teacher model. (2) While scaling distillation can\nenhance reasoning, it often degrades out-of-domain performance. A carefully\ncurated, smaller corpus achieves a balanced trade-off between in-domain and\nout-of-domain capabilities. (3) Diverse reasoning trajectories encourage the\nstudent model to develop robust reasoning skills. We validate our method\nthrough evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and\ncode generation (LiveCodeBench), achieving state-of-the-art results with only\n0.8k carefully curated examples, bypassing the need for extensive scaling. Our\nsystematic analysis demonstrates that DED outperforms existing methods by\nconsidering factors beyond superficial hardness, token length, or teacher model\ncapability. This work offers a practical and efficient pathway to advanced\nreasoning while preserving general capabilities."
                },
                "authors": [
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Xiaoguang Jiang"
                    },
                    {
                        "name": "Huiyang Li"
                    },
                    {
                        "name": "Jucai Zhai"
                    },
                    {
                        "name": "Dengfeng Liu"
                    },
                    {
                        "name": "Qiaobo Hao"
                    },
                    {
                        "name": "Huang Liu"
                    },
                    {
                        "name": "Zhiguo Yang"
                    },
                    {
                        "name": "Ji Xie"
                    },
                    {
                        "name": "Ninglun Gu"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Kailai Zhang"
                    },
                    {
                        "name": "Yelun Bao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15288v2",
                "updated": "2025-08-13T15:28:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    28,
                    59,
                    2,
                    225,
                    0
                ],
                "published": "2024-01-27T04:02:52Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    4,
                    2,
                    52,
                    5,
                    27,
                    0
                ],
                "title": "STAC: Leveraging Spatio-Temporal Data Associations For Efficient\n  Cross-Camera Streaming and Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAC: Leveraging Spatio-Temporal Data Associations For Efficient\n  Cross-Camera Streaming and Analytics"
                },
                "summary": "In IoT based distributed network of cameras, real-time multi-camera video\nanalytics is challenged by high bandwidth demands and redundant visual data,\ncreating a fundamental tension where reducing data saves network overhead but\ncan degrade model performance, and vice versa. We present STAC, a cross-cameras\nsurveillance system that leverages spatio-temporal associations for efficient\nobject tracking under constrained network conditions. STAC integrates\nmulti-resolution feature learning, ensuring robustness under variable networked\nsystem level optimizations such as frame filtering, FFmpeg-based compression,\nand Region-of-Interest (RoI) masking, to eliminate redundant content across\ndistributed video streams while preserving downstream model accuracy for object\nidentification and tracking. Evaluated on NVIDIA's AICity Challenge dataset,\nSTAC achieves a 76\\% improvement in tracking accuracy and an 8.6x reduction in\ninference latency over a standard multi-object multi-camera tracking baseline\n(using YOLOv4 and DeepSORT). Furthermore, 29\\% of redundant frames are\nfiltered, significantly reducing data volume without compromising inference\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In IoT based distributed network of cameras, real-time multi-camera video\nanalytics is challenged by high bandwidth demands and redundant visual data,\ncreating a fundamental tension where reducing data saves network overhead but\ncan degrade model performance, and vice versa. We present STAC, a cross-cameras\nsurveillance system that leverages spatio-temporal associations for efficient\nobject tracking under constrained network conditions. STAC integrates\nmulti-resolution feature learning, ensuring robustness under variable networked\nsystem level optimizations such as frame filtering, FFmpeg-based compression,\nand Region-of-Interest (RoI) masking, to eliminate redundant content across\ndistributed video streams while preserving downstream model accuracy for object\nidentification and tracking. Evaluated on NVIDIA's AICity Challenge dataset,\nSTAC achieves a 76\\% improvement in tracking accuracy and an 8.6x reduction in\ninference latency over a standard multi-object multi-camera tracking baseline\n(using YOLOv4 and DeepSORT). Furthermore, 29\\% of redundant frames are\nfiltered, significantly reducing data volume without compromising inference\nquality."
                },
                "authors": [
                    {
                        "name": "Ragini Gupta"
                    },
                    {
                        "name": "Lingzhi Zhao"
                    },
                    {
                        "name": "Jiaxi Li"
                    },
                    {
                        "name": "Volodymyr Vakhniuk"
                    },
                    {
                        "name": "Claudiu Danilov"
                    },
                    {
                        "name": "Josh Eckhardt"
                    },
                    {
                        "name": "Keyshla Bernard"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.2; I.4.0; C.2.2; C.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11249v2",
                "updated": "2025-08-13T15:27:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    27,
                    23,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-15T14:46:25Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    46,
                    25,
                    1,
                    105,
                    0
                ],
                "title": "Cryo-em images are intrinsically low dimensional",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryo-em images are intrinsically low dimensional"
                },
                "summary": "Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry."
                },
                "authors": [
                    {
                        "name": "Luke Evans"
                    },
                    {
                        "name": "Octavian-Vlad Murad"
                    },
                    {
                        "name": "Lars Dingeldein"
                    },
                    {
                        "name": "Pilar Cossio"
                    },
                    {
                        "name": "Roberto Covino"
                    },
                    {
                        "name": "Marina Meila"
                    }
                ],
                "author_detail": {
                    "name": "Marina Meila"
                },
                "author": "Marina Meila",
                "arxiv_doi": "10.1103/txrb-fw3z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/txrb-fw3z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.11249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08291v2",
                "updated": "2025-08-13T15:19:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    19,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-05T16:39:54Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    39,
                    54,
                    1,
                    217,
                    0
                ],
                "title": "Probabilistic Emissivity Retrieval from Hyperspectral Data via\n  Physics-Guided Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Emissivity Retrieval from Hyperspectral Data via\n  Physics-Guided Variational Inference"
                },
                "summary": "Recent research has proven neural networks to be a powerful tool for\nperforming hyperspectral imaging (HSI) target identification. However, many\ndeep learning frameworks deliver a single material class prediction and operate\non a per-pixel basis; such approaches are limited in their interpretability and\nrestricted to predicting materials that are accessible in available training\nlibraries. In this work, we present an inverse modeling approach in the form of\na physics-conditioned generative model.A probabilistic latent-variable model\nlearns the underlying distribution of HSI radiance measurements and produces\nthe conditional distribution of the emissivity spectrum. Moreover, estimates of\nthe HSI scene's atmosphere and background are used as a physically relevant\nconditioning mechanism to contextualize a given radiance measurement during the\nencoding and decoding processes. Furthermore, we employ an in-the-loop\naugmentation scheme and physics-based loss criteria to avoid bias towards a\npredefined training material set and to encourage the model to learn physically\nconsistent inverse mappings. Monte-Carlo sampling of the model's conditioned\nposterior delivers a sought emissivity distribution and allows for\ninterpretable uncertainty quantification. Moreover, a distribution-based\nmaterial matching scheme is presented to return a set of likely material\nmatches for an inferred emissivity distribution. Hence, we present a strategy\nto incorporate contextual information about a given HSI scene, capture the\npossible variation of underlying material spectra, and provide interpretable\nprobability measures of a candidate material accounting for given\nremotely-sensed radiance measurement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has proven neural networks to be a powerful tool for\nperforming hyperspectral imaging (HSI) target identification. However, many\ndeep learning frameworks deliver a single material class prediction and operate\non a per-pixel basis; such approaches are limited in their interpretability and\nrestricted to predicting materials that are accessible in available training\nlibraries. In this work, we present an inverse modeling approach in the form of\na physics-conditioned generative model.A probabilistic latent-variable model\nlearns the underlying distribution of HSI radiance measurements and produces\nthe conditional distribution of the emissivity spectrum. Moreover, estimates of\nthe HSI scene's atmosphere and background are used as a physically relevant\nconditioning mechanism to contextualize a given radiance measurement during the\nencoding and decoding processes. Furthermore, we employ an in-the-loop\naugmentation scheme and physics-based loss criteria to avoid bias towards a\npredefined training material set and to encourage the model to learn physically\nconsistent inverse mappings. Monte-Carlo sampling of the model's conditioned\nposterior delivers a sought emissivity distribution and allows for\ninterpretable uncertainty quantification. Moreover, a distribution-based\nmaterial matching scheme is presented to return a set of likely material\nmatches for an inferred emissivity distribution. Hence, we present a strategy\nto incorporate contextual information about a given HSI scene, capture the\npossible variation of underlying material spectra, and provide interpretable\nprobability measures of a candidate material accounting for given\nremotely-sensed radiance measurement."
                },
                "authors": [
                    {
                        "name": "Joshua R. Tempelman"
                    },
                    {
                        "name": "Kevin Mitchell"
                    },
                    {
                        "name": "Adam J. Wachtor"
                    },
                    {
                        "name": "Eric B. Flynn"
                    }
                ],
                "author_detail": {
                    "name": "Eric B. Flynn"
                },
                "author": "Eric B. Flynn",
                "arxiv_comment": "14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11421v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11421v3",
                "updated": "2025-08-13T15:18:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    18,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-13T02:39:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    39,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems"
                },
                "summary": "With the rapid growth of Internet services, recommendation systems play a\ncentral role in delivering personalized content. Faced with massive user\nrequests and complex model architectures, the key challenge for real-time\nrecommendation systems is how to reduce inference latency and increase system\nthroughput without sacrificing recommendation quality. This paper addresses the\nhigh computational cost and resource bottlenecks of deep learning models in\nreal-time settings by proposing a combined set of modeling- and system-level\nacceleration and optimization strategies. At the model level, we dramatically\nreduce parameter counts and compute requirements through lightweight network\ndesign, structured pruning, and weight quantization. At the system level, we\nintegrate multiple heterogeneous compute platforms and high-performance\ninference libraries, and we design elastic inference scheduling and\nload-balancing mechanisms based on real-time load characteristics. Experiments\nshow that, while maintaining the original recommendation accuracy, our methods\ncut latency to less than 30% of the baseline and more than double system\nthroughput, offering a practical solution for deploying large-scale online\nrecommendation services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Internet services, recommendation systems play a\ncentral role in delivering personalized content. Faced with massive user\nrequests and complex model architectures, the key challenge for real-time\nrecommendation systems is how to reduce inference latency and increase system\nthroughput without sacrificing recommendation quality. This paper addresses the\nhigh computational cost and resource bottlenecks of deep learning models in\nreal-time settings by proposing a combined set of modeling- and system-level\nacceleration and optimization strategies. At the model level, we dramatically\nreduce parameter counts and compute requirements through lightweight network\ndesign, structured pruning, and weight quantization. At the system level, we\nintegrate multiple heterogeneous compute platforms and high-performance\ninference libraries, and we design elastic inference scheduling and\nload-balancing mechanisms based on real-time load characteristics. Experiments\nshow that, while maintaining the original recommendation accuracy, our methods\ncut latency to less than 30% of the baseline and more than double system\nthroughput, offering a practical solution for deploying large-scale online\nrecommendation services."
                },
                "authors": [
                    {
                        "name": "Junli Shao"
                    },
                    {
                        "name": "Jing Dong"
                    },
                    {
                        "name": "Dingzhou Wang"
                    },
                    {
                        "name": "Kowei Shih"
                    },
                    {
                        "name": "Dannier Li"
                    },
                    {
                        "name": "Chengrui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chengrui Zhou"
                },
                "author": "Chengrui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11421v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11421v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09874v1",
                "updated": "2025-08-13T15:16:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    16,
                    29,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:16:29Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    16,
                    29,
                    2,
                    225,
                    0
                ],
                "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain."
                },
                "authors": [
                    {
                        "name": "Jiaqi Cao"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Rubin Wei"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12697v2",
                "updated": "2025-08-13T15:13:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    13,
                    33,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-15T02:54:25Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    2,
                    54,
                    25,
                    6,
                    166,
                    0
                ],
                "title": "MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small\n  Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small\n  Object Detection"
                },
                "summary": "Small object detection in UAV imagery is crucial for applications such as\nsearch-and-rescue, traffic monitoring, and environmental surveillance, but it\nis hampered by tiny object size, low signal-to-noise ratios, and limited\nfeature extraction. Existing multi-scale fusion methods help, but add\ncomputational burden and blur fine details, making small object detection in\ncluttered scenes difficult. To overcome these challenges, we propose the\nMulti-scale Global-detail Feature Integration Strategy (MGDFIS), a unified\nfusion framework that tightly couples global context with local detail to boost\ndetection performance while maintaining efficiency. MGDFIS comprises three\nsynergistic modules: the FusionLock-TSS Attention Module, which marries\ntoken-statistics self-attention with DynamicTanh normalization to highlight\nspectral and spatial cues at minimal cost; the Global-detail Integration\nModule, which fuses multi-scale context via directional convolution and\nparallel attention while preserving subtle shape and texture variations; and\nthe Dynamic Pixel Attention Module, which generates pixel-wise weighting maps\nto rebalance uneven foreground and background distributions and sharpen\nresponses to true object regions. Extensive experiments on the VisDrone\nbenchmark demonstrate that MGDFIS consistently outperforms state-of-the-art\nmethods across diverse backbone architectures and detection frameworks,\nachieving superior precision and recall with low inference time. By striking an\noptimal balance between accuracy and resource usage, MGDFIS provides a\npractical solution for small-object detection on resource-constrained UAV\nplatforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small object detection in UAV imagery is crucial for applications such as\nsearch-and-rescue, traffic monitoring, and environmental surveillance, but it\nis hampered by tiny object size, low signal-to-noise ratios, and limited\nfeature extraction. Existing multi-scale fusion methods help, but add\ncomputational burden and blur fine details, making small object detection in\ncluttered scenes difficult. To overcome these challenges, we propose the\nMulti-scale Global-detail Feature Integration Strategy (MGDFIS), a unified\nfusion framework that tightly couples global context with local detail to boost\ndetection performance while maintaining efficiency. MGDFIS comprises three\nsynergistic modules: the FusionLock-TSS Attention Module, which marries\ntoken-statistics self-attention with DynamicTanh normalization to highlight\nspectral and spatial cues at minimal cost; the Global-detail Integration\nModule, which fuses multi-scale context via directional convolution and\nparallel attention while preserving subtle shape and texture variations; and\nthe Dynamic Pixel Attention Module, which generates pixel-wise weighting maps\nto rebalance uneven foreground and background distributions and sharpen\nresponses to true object regions. Extensive experiments on the VisDrone\nbenchmark demonstrate that MGDFIS consistently outperforms state-of-the-art\nmethods across diverse backbone architectures and detection frameworks,\nachieving superior precision and recall with low inference time. By striking an\noptimal balance between accuracy and resource usage, MGDFIS provides a\npractical solution for small-object detection on resource-constrained UAV\nplatforms."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Xuecheng Bai"
                    },
                    {
                        "name": "Boyu Hu"
                    },
                    {
                        "name": "Chuanzhi Xu"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Vera Chung"
                    },
                    {
                        "name": "Tingxue Li"
                    },
                    {
                        "name": "Xiaoming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Chen"
                },
                "author": "Xiaoming Chen",
                "arxiv_comment": "9 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09871v1",
                "updated": "2025-08-13T15:09:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    9,
                    45,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:09:45Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    9,
                    45,
                    2,
                    225,
                    0
                ],
                "title": "Inference of germinal center evolutionary dynamics via simulation-based\n  deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of germinal center evolutionary dynamics via simulation-based\n  deep learning"
                },
                "summary": "B cells and the antibodies they produce are vital to health and survival,\nmotivating research on the details of the mutational and evolutionary processes\nin the germinal centers (GC) from which mature B cells arise. It is known that\nB cells with higher affinity for their cognate antigen (Ag) will, on average,\ntend to have more offspring. However the exact form of this relationship\nbetween affinity and fecundity, which we call the ``affinity-fitness response\nfunction'', is not known. Here we use deep learning and simulation-based\ninference to learn this function from a unique experiment that replays a\nparticular combination of GC conditions many times. All code is freely\navailable at https://github.com/matsengrp/gcdyn, while datasets and inference\nresults can be found at https://doi.org/10.5281/zenodo.15022130.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B cells and the antibodies they produce are vital to health and survival,\nmotivating research on the details of the mutational and evolutionary processes\nin the germinal centers (GC) from which mature B cells arise. It is known that\nB cells with higher affinity for their cognate antigen (Ag) will, on average,\ntend to have more offspring. However the exact form of this relationship\nbetween affinity and fecundity, which we call the ``affinity-fitness response\nfunction'', is not known. Here we use deep learning and simulation-based\ninference to learn this function from a unique experiment that replays a\nparticular combination of GC conditions many times. All code is freely\navailable at https://github.com/matsengrp/gcdyn, while datasets and inference\nresults can be found at https://doi.org/10.5281/zenodo.15022130."
                },
                "authors": [
                    {
                        "name": "Duncan K Ralph"
                    },
                    {
                        "name": "Athanasios G Bakis"
                    },
                    {
                        "name": "Jared Galloway"
                    },
                    {
                        "name": "Ashni A Vora"
                    },
                    {
                        "name": "Tatsuya Araki"
                    },
                    {
                        "name": "Gabriel D Victora"
                    },
                    {
                        "name": "Yun S Song"
                    },
                    {
                        "name": "William S DeWitt"
                    },
                    {
                        "name": "Frederick A Matsen IV"
                    }
                ],
                "author_detail": {
                    "name": "Frederick A Matsen IV"
                },
                "author": "Frederick A Matsen IV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05603v2",
                "updated": "2025-08-13T15:01:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    1,
                    1,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-05T21:33:31Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    33,
                    31,
                    3,
                    156,
                    0
                ],
                "title": "Bayesian inference of neutron star crust properties using an ab\n  initio-benchmarked meta-model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of neutron star crust properties using an ab\n  initio-benchmarked meta-model"
                },
                "summary": "Accurate modeling of the neutron star crust is essential for interpreting\nmultimessenger observations and constraining the nuclear equation of state\n(EoS). However, standard phenomenological EoS models often rely on heuristic\nextrapolations in the low-density regime, which are inconsistent with\nmicroscopic predictions. In this work, we refine a unified meta-modeling\nframework for the EoS by incorporating low-density corrections based on energy\ndensity functionals constrained by ab initio neutron-matter calculations. Using\nBayesian inference to combine information from astrophysical observations,\nnuclear theory, and experiments, we assess the impact of these corrections on\nkey crustal properties, including the crust-core transition density and\npressure, crustal composition, and moment of inertia. The improved model\nreduces uncertainties in the inner crust and emphasizes the importance of\nlow-density physics in EoS modeling, highlighting the value of integrating both\ntheoretical and observational constraints across densities to robustly describe\nthe EoS. Moreover, the adopted approach can be readily applied to any existing\nEoS model to provide a solid framework for interpreting upcoming high-precision\nmultimessenger data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate modeling of the neutron star crust is essential for interpreting\nmultimessenger observations and constraining the nuclear equation of state\n(EoS). However, standard phenomenological EoS models often rely on heuristic\nextrapolations in the low-density regime, which are inconsistent with\nmicroscopic predictions. In this work, we refine a unified meta-modeling\nframework for the EoS by incorporating low-density corrections based on energy\ndensity functionals constrained by ab initio neutron-matter calculations. Using\nBayesian inference to combine information from astrophysical observations,\nnuclear theory, and experiments, we assess the impact of these corrections on\nkey crustal properties, including the crust-core transition density and\npressure, crustal composition, and moment of inertia. The improved model\nreduces uncertainties in the inner crust and emphasizes the importance of\nlow-density physics in EoS modeling, highlighting the value of integrating both\ntheoretical and observational constraints across densities to robustly describe\nthe EoS. Moreover, the adopted approach can be readily applied to any existing\nEoS model to provide a solid framework for interpreting upcoming high-precision\nmultimessenger data."
                },
                "authors": [
                    {
                        "name": "S. Burrello"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "M. Antonelli"
                    },
                    {
                        "name": "M. Colonna"
                    },
                    {
                        "name": "A. Fantina"
                    }
                ],
                "author_detail": {
                    "name": "A. Fantina"
                },
                "author": "A. Fantina",
                "arxiv_doi": "10.1103/74qx-8ym8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/74qx-8ym8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06950v3",
                "updated": "2025-08-13T14:59:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    59,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-09T11:56:59Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    56,
                    59,
                    5,
                    221,
                    0
                ],
                "title": "Large Language Models Do Not Simulate Human Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Do Not Simulate Human Psychology"
                },
                "summary": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application."
                },
                "authors": [
                    {
                        "name": "Sarah Schrder"
                    },
                    {
                        "name": "Thekla Morgenroth"
                    },
                    {
                        "name": "Ulrike Kuhl"
                    },
                    {
                        "name": "Valerie Vaquet"
                    },
                    {
                        "name": "Benjamin Paaen"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Paaen"
                },
                "author": "Benjamin Paaen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09857v1",
                "updated": "2025-08-13T14:49:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    49,
                    54,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:49:54Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    49,
                    54,
                    2,
                    225,
                    0
                ],
                "title": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video\n  VAE Train Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video\n  VAE Train Better"
                },
                "summary": "Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection."
                },
                "authors": [
                    {
                        "name": "Yupeng Zhou"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Ziheng Ouyang"
                    },
                    {
                        "name": "Yuming Chen"
                    },
                    {
                        "name": "Ruoyi Du"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Qibin Hou"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Hou"
                },
                "author": "Qibin Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13205v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13205v5",
                "updated": "2025-08-13T14:34:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    34,
                    4,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-16T08:09:32Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    9,
                    32,
                    0,
                    167,
                    0
                ],
                "title": "Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based\n  Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based\n  Mobile Agents"
                },
                "summary": "Mobile agents powered by vision-language models (VLMs) are increasingly\nadopted for tasks such as UI automation and camera-based assistance. These\nagents are typically fine-tuned using small-scale, user-collected data, making\nthem susceptible to stealthy training-time threats. This work introduces VIBMA,\nthe first clean-text backdoor attack targeting VLM-based mobile agents. The\nattack injects malicious behaviors into the model by modifying only the visual\ninput while preserving textual prompts and instructions, achieving stealth\nthrough the complete absence of textual anomalies. Once the agent is fine-tuned\non this poisoned data, adding a predefined visual pattern (trigger) at\ninference time activates the attacker-specified behavior (backdoor). Our attack\naligns the training gradients of poisoned samples with those of an\nattacker-specified target instance, effectively embedding backdoor-specific\nfeatures into the poisoned data. To ensure the robustness and stealthiness of\nthe attack, we design three trigger variants that better resemble real-world\nscenarios: static patches, dynamic motion patterns, and low-opacity blended\ncontent. Extensive experiments on six Android applications and three\nmobile-compatible VLMs demonstrate that our attack achieves high success rates\n(ASR up to 94.67%) while preserving clean-task behavior (FSR up to 95.85%). We\nfurther conduct ablation studies to understand how key design factors impact\nattack reliability and stealth. These findings is the first to reveal the\nsecurity vulnerabilities of mobile agents and their susceptibility to backdoor\ninjection, underscoring the need for robust defenses in mobile agent adaptation\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile agents powered by vision-language models (VLMs) are increasingly\nadopted for tasks such as UI automation and camera-based assistance. These\nagents are typically fine-tuned using small-scale, user-collected data, making\nthem susceptible to stealthy training-time threats. This work introduces VIBMA,\nthe first clean-text backdoor attack targeting VLM-based mobile agents. The\nattack injects malicious behaviors into the model by modifying only the visual\ninput while preserving textual prompts and instructions, achieving stealth\nthrough the complete absence of textual anomalies. Once the agent is fine-tuned\non this poisoned data, adding a predefined visual pattern (trigger) at\ninference time activates the attacker-specified behavior (backdoor). Our attack\naligns the training gradients of poisoned samples with those of an\nattacker-specified target instance, effectively embedding backdoor-specific\nfeatures into the poisoned data. To ensure the robustness and stealthiness of\nthe attack, we design three trigger variants that better resemble real-world\nscenarios: static patches, dynamic motion patterns, and low-opacity blended\ncontent. Extensive experiments on six Android applications and three\nmobile-compatible VLMs demonstrate that our attack achieves high success rates\n(ASR up to 94.67%) while preserving clean-task behavior (FSR up to 95.85%). We\nfurther conduct ablation studies to understand how key design factors impact\nattack reliability and stealth. These findings is the first to reveal the\nsecurity vulnerabilities of mobile agents and their susceptibility to backdoor\ninjection, underscoring the need for robust defenses in mobile agent adaptation\npipelines."
                },
                "authors": [
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Yuliang Lu"
                    },
                    {
                        "name": "Xitong Gao"
                    },
                    {
                        "name": "Ee-Chien Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ee-Chien Chang"
                },
                "author": "Ee-Chien Chang",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13205v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13205v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09848v2",
                "updated": "2025-08-14T02:08:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    2,
                    8,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T14:28:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    28,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts"
                },
                "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Tsz Ting Chung"
                    },
                    {
                        "name": "Chulun Zhou"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Haoshu Lu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09846v1",
                "updated": "2025-08-13T14:26:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    26,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:26:57Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    26,
                    57,
                    2,
                    225,
                    0
                ],
                "title": "Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter\n  Estimation for Wheeled Humanoid Locomanipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter\n  Estimation for Wheeled Humanoid Locomanipulation"
                },
                "summary": "This paper presents an object-aware whole-body bilateral teleoperation\nframework for wheeled humanoid loco-manipulation. This framework combines\nwhole-body bilateral teleoperation with an online multi-stage object inertial\nparameter estimation module, which is the core technical contribution of this\nwork. The multi-stage process sequentially integrates a vision-based object\nsize estimator, an initial parameter guess generated by a large vision-language\nmodel (VLM), and a decoupled hierarchical sampling strategy. The visual size\nestimate and VLM prior offer a strong initial guess of the object's inertial\nparameters, significantly reducing the search space for sampling-based\nrefinement and improving the overall estimation speed. A hierarchical strategy\nfirst estimates mass and center of mass, then infers inertia from object size\nto ensure physically feasible parameters, while a decoupled multi-hypothesis\nscheme enhances robustness to VLM prior errors. Our estimator operates in\nparallel with high-fidelity simulation and hardware, enabling real-time online\nupdates. The estimated parameters are then used to update the wheeled\nhumanoid's equilibrium point, allowing the operator to focus more on locomotion\nand manipulation. This integration improves the haptic force feedback for\ndynamic synchronization, enabling more dynamic whole-body teleoperation. By\ncompensating for object dynamics using the estimated parameters, the framework\nalso improves manipulation tracking while preserving compliant behavior. We\nvalidate the system on a customized wheeled humanoid with a robotic gripper and\nhuman-machine interface, demonstrating real-time execution of lifting,\ndelivering, and releasing tasks with a payload weighing approximately one-third\nof the robot's body weight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an object-aware whole-body bilateral teleoperation\nframework for wheeled humanoid loco-manipulation. This framework combines\nwhole-body bilateral teleoperation with an online multi-stage object inertial\nparameter estimation module, which is the core technical contribution of this\nwork. The multi-stage process sequentially integrates a vision-based object\nsize estimator, an initial parameter guess generated by a large vision-language\nmodel (VLM), and a decoupled hierarchical sampling strategy. The visual size\nestimate and VLM prior offer a strong initial guess of the object's inertial\nparameters, significantly reducing the search space for sampling-based\nrefinement and improving the overall estimation speed. A hierarchical strategy\nfirst estimates mass and center of mass, then infers inertia from object size\nto ensure physically feasible parameters, while a decoupled multi-hypothesis\nscheme enhances robustness to VLM prior errors. Our estimator operates in\nparallel with high-fidelity simulation and hardware, enabling real-time online\nupdates. The estimated parameters are then used to update the wheeled\nhumanoid's equilibrium point, allowing the operator to focus more on locomotion\nand manipulation. This integration improves the haptic force feedback for\ndynamic synchronization, enabling more dynamic whole-body teleoperation. By\ncompensating for object dynamics using the estimated parameters, the framework\nalso improves manipulation tracking while preserving compliant behavior. We\nvalidate the system on a customized wheeled humanoid with a robotic gripper and\nhuman-machine interface, demonstrating real-time execution of lifting,\ndelivering, and releasing tasks with a payload weighing approximately one-third\nof the robot's body weight."
                },
                "authors": [
                    {
                        "name": "Donghoon Baek"
                    },
                    {
                        "name": "Amartya Purushottam"
                    },
                    {
                        "name": "Jason J. Choi"
                    },
                    {
                        "name": "Joao Ramos"
                    }
                ],
                "author_detail": {
                    "name": "Joao Ramos"
                },
                "author": "Joao Ramos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09836v1",
                "updated": "2025-08-13T14:16:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    16,
                    42,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:16:42Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    16,
                    42,
                    2,
                    225,
                    0
                ],
                "title": "Embodied Tactile Perception of Soft Objects Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Tactile Perception of Soft Objects Properties"
                },
                "summary": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics."
                },
                "authors": [
                    {
                        "name": "Anirvan Dutta"
                    },
                    {
                        "name": "Alexis WM Devillard"
                    },
                    {
                        "name": "Zhihuan Zhang"
                    },
                    {
                        "name": "Xiaoxiao Cheng"
                    },
                    {
                        "name": "Etienne Burdet"
                    }
                ],
                "author_detail": {
                    "name": "Etienne Burdet"
                },
                "author": "Etienne Burdet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09834v1",
                "updated": "2025-08-13T14:13:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    13,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:13:46Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    13,
                    46,
                    2,
                    225,
                    0
                ],
                "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiaoyu Mo"
                    },
                    {
                        "name": "Daizong Liu"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Wenliang Chen"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08840v2",
                "updated": "2025-08-13T14:09:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    9,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-12T00:37:14Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    0,
                    37,
                    14,
                    3,
                    347,
                    0
                ],
                "title": "The Causal Effect of the Two-For-One Strategy in the National Basketball\n  Association",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Causal Effect of the Two-For-One Strategy in the National Basketball\n  Association"
                },
                "summary": "This study evaluates the effectiveness of the two-for-one strategy in\nbasketball by applying a causal inference framework to play-by-play data from\nthe 2018-19 and 2021-22 National Basketball Association regular seasons.\nIncorporating factors such as player lineup, betting odds, and player ratings,\nwe compute the average treatment effect and find that the two-for-one strategy\nhas a positive impact on game outcomes, suggesting it can benefit teams when\nemployed effectively. Additionally, we investigate potential heterogeneity in\nthe strategy's effectiveness using the causal forest framework, with tests\nindicating no significant variation across different contexts. These findings\noffer valuable insights into the tactical advantages of the two-for-one\nstrategy in professional basketball.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the effectiveness of the two-for-one strategy in\nbasketball by applying a causal inference framework to play-by-play data from\nthe 2018-19 and 2021-22 National Basketball Association regular seasons.\nIncorporating factors such as player lineup, betting odds, and player ratings,\nwe compute the average treatment effect and find that the two-for-one strategy\nhas a positive impact on game outcomes, suggesting it can benefit teams when\nemployed effectively. Additionally, we investigate potential heterogeneity in\nthe strategy's effectiveness using the causal forest framework, with tests\nindicating no significant variation across different contexts. These findings\noffer valuable insights into the tactical advantages of the two-for-one\nstrategy in professional basketball."
                },
                "authors": [
                    {
                        "name": "Prateek Sasan"
                    },
                    {
                        "name": "Daryl Swartzentruber"
                    }
                ],
                "author_detail": {
                    "name": "Daryl Swartzentruber"
                },
                "author": "Daryl Swartzentruber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09832v1",
                "updated": "2025-08-13T14:07:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    7,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:07:05Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    7,
                    5,
                    2,
                    225,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models in Fine-Grained Review\n  Comment Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models in Fine-Grained Review\n  Comment Classification"
                },
                "summary": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process."
                },
                "authors": [
                    {
                        "name": "Linh Nguyen"
                    },
                    {
                        "name": "Chunhua Liu"
                    },
                    {
                        "name": "Hong Yi Lin"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    }
                ],
                "author_detail": {
                    "name": "Patanamon Thongtanunam"
                },
                "author": "Patanamon Thongtanunam",
                "arxiv_comment": "Accepted at 2025 IEEE International Conference on Source Code\n  Analysis & Manipulation (SCAM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09823v1",
                "updated": "2025-08-13T13:55:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    55,
                    43,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:55:43Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    55,
                    43,
                    2,
                    225,
                    0
                ],
                "title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in\n  Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in\n  Medical Imaging"
                },
                "summary": "KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at\n\\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at\n\\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}."
                },
                "authors": [
                    {
                        "name": "Valentin Boussot"
                    },
                    {
                        "name": "Jean-Louis Dillenseger"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Louis Dillenseger"
                },
                "author": "Jean-Louis Dillenseger",
                "arxiv_comment": "https://github.com/vboussot/KonfAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09820v1",
                "updated": "2025-08-13T13:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    44,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    44,
                    2,
                    225,
                    0
                ],
                "title": "Provable In-Context Vector Arithmetic via Retrieving Task Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable In-Context Vector Arithmetic via Retrieving Task Concepts"
                },
                "summary": "In-context learning (ICL) has garnered significant attention for its ability\nto grasp functions/tasks from demonstrations. Recent studies suggest the\npresence of a latent task/function vector in LLMs during ICL. Merullo et al.\n(2024) showed that LLMs leverage this vector alongside the residual stream for\nWord2Vec-like vector arithmetic, solving factual-recall ICL tasks.\nAdditionally, recent work empirically highlighted the key role of\nQuestion-Answer data in enhancing factual-recall capabilities. Despite these\ninsights, a theoretical explanation remains elusive. To move one step forward,\nwe propose a theoretical framework building on empirically grounded\nhierarchical concept modeling. We develop an optimization theory, showing how\nnonlinear residual transformers trained via gradient descent on cross-entropy\nloss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss\nconvergence and show the strong generalization, including robustness to concept\nrecombination and distribution shifts. These results elucidate the advantages\nof transformers over static embedding predecessors. Empirical simulations\ncorroborate our theoretical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has garnered significant attention for its ability\nto grasp functions/tasks from demonstrations. Recent studies suggest the\npresence of a latent task/function vector in LLMs during ICL. Merullo et al.\n(2024) showed that LLMs leverage this vector alongside the residual stream for\nWord2Vec-like vector arithmetic, solving factual-recall ICL tasks.\nAdditionally, recent work empirically highlighted the key role of\nQuestion-Answer data in enhancing factual-recall capabilities. Despite these\ninsights, a theoretical explanation remains elusive. To move one step forward,\nwe propose a theoretical framework building on empirically grounded\nhierarchical concept modeling. We develop an optimization theory, showing how\nnonlinear residual transformers trained via gradient descent on cross-entropy\nloss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss\nconvergence and show the strong generalization, including robustness to concept\nrecombination and distribution shifts. These results elucidate the advantages\nof transformers over static embedding predecessors. Empirical simulations\ncorroborate our theoretical insights."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    },
                    {
                        "name": "Taiji Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Taiji Suzuki"
                },
                "author": "Taiji Suzuki",
                "arxiv_comment": "Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09818v1",
                "updated": "2025-08-13T13:54:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    16,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:16Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    16,
                    2,
                    225,
                    0
                ],
                "title": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior\n  Understanding from Motion and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior\n  Understanding from Motion and Video"
                },
                "summary": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation."
                },
                "authors": [
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md Yeasin Rahat"
                    },
                    {
                        "name": "Nafiz Fahad"
                    },
                    {
                        "name": "Abir Ahmed"
                    },
                    {
                        "name": "Liew Tze Hui"
                    }
                ],
                "author_detail": {
                    "name": "Liew Tze Hui"
                },
                "author": "Liew Tze Hui",
                "arxiv_comment": "Accepted in ICCVDM '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09815v1",
                "updated": "2025-08-13T13:47:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    47,
                    55,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:47:55Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    47,
                    55,
                    2,
                    225,
                    0
                ],
                "title": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights\n  from Multi-Agent Security Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights\n  from Multi-Agent Security Research"
                },
                "summary": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments."
                },
                "authors": [
                    {
                        "name": "Klaudia Krawiecka"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14368v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14368v5",
                "updated": "2025-08-13T13:44:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    44,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-18T22:04:56Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    4,
                    56,
                    2,
                    353,
                    0
                ],
                "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation"
                },
                "summary": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Francis Ferraro"
                    }
                ],
                "author_detail": {
                    "name": "Francis Ferraro"
                },
                "author": "Francis Ferraro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14368v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14368v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03012v2",
                "updated": "2025-08-13T13:42:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    42,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-01-06T13:37:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    37,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Analyzing Finetuning Representation Shift for Multimodal LLMs Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Finetuning Representation Shift for Multimodal LLMs Steering"
                },
                "summary": "Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in\nunderstanding multimodal inputs. However, understanding and interpreting the\nbehavior of such complex models is a challenging task, not to mention the\ndynamic shifts that may occur during fine-tuning, or due to covariate shift\nbetween datasets. In this work, we apply concept-level analysis towards MLLM\nunderstanding. More specifically, we propose to map hidden states to\ninterpretable visual and textual concepts. This enables us to more efficiently\ncompare certain semantic dynamics, such as the shift from an original and\nfine-tuned model, revealing concept alteration and potential biases that may\noccur during fine-tuning. We also demonstrate the use of shift vectors to\ncapture these concepts changes. These shift vectors allow us to recover\nfine-tuned concepts by applying simple, computationally inexpensive additive\nconcept shifts in the original model. Finally, our findings also have direct\napplications for MLLM steering, which can be used for model debiasing as well\nas enforcing safety in MLLM output. All in all, we propose a novel,\ntraining-free, ready-to-use framework for MLLM behavior interpretability and\ncontrol. Our implementation is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in\nunderstanding multimodal inputs. However, understanding and interpreting the\nbehavior of such complex models is a challenging task, not to mention the\ndynamic shifts that may occur during fine-tuning, or due to covariate shift\nbetween datasets. In this work, we apply concept-level analysis towards MLLM\nunderstanding. More specifically, we propose to map hidden states to\ninterpretable visual and textual concepts. This enables us to more efficiently\ncompare certain semantic dynamics, such as the shift from an original and\nfine-tuned model, revealing concept alteration and potential biases that may\noccur during fine-tuning. We also demonstrate the use of shift vectors to\ncapture these concepts changes. These shift vectors allow us to recover\nfine-tuned concepts by applying simple, computationally inexpensive additive\nconcept shifts in the original model. Finally, our findings also have direct\napplications for MLLM steering, which can be used for model debiasing as well\nas enforcing safety in MLLM output. All in all, we propose a novel,\ntraining-free, ready-to-use framework for MLLM behavior interpretability and\ncontrol. Our implementation is publicly available."
                },
                "authors": [
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "ICCV 2025. The first three authors contributed equally. Project page\n  and code: https://pegah-\n  kh.github.io/projects/lmm-finetuning-analysis-and-steering/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15619v2",
                "updated": "2025-08-13T13:36:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    36,
                    14,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-18T16:51:21Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    51,
                    21,
                    2,
                    169,
                    0
                ],
                "title": "Further Evidence for a Direct-Collapse Origin of the Supermassive Black\n  Hole at the Center of the Infinity Galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Further Evidence for a Direct-Collapse Origin of the Supermassive Black\n  Hole at the Center of the Infinity Galaxy"
                },
                "summary": "The z=1.14 $\\infty$ galaxy consists of two ringed nuclei with an active\nsupermassive black hole (SMBH) in between them. The system is likely the result\nof a nearly face-on collision between two disk galaxies with massive bulges. In\nvan Dokkum et al. (2025) we suggested that the SMBH may have formed from\nshocked and compressed gas at the collision site, in a runaway gravitational\ncollapse. Here we test this hypothesis using newly obtained JWST NIRSpec IFU\nobservations. We first confirm that the system has a cloud of gas in between\nthe nuclei that is photo-ionized by an AGN-like object near its center. Next,\nwe constrain the origin of the SMBH from its radial velocity. If it formed in\nthe cloud its velocity should be similar to the surrounding gas, whereas it\nwould be offset if the SMBH had escaped from one of the nuclei or were\nassociated with a faint galaxy. We find that the radial velocity of the SMBH is\nwithin $\\sim 50$ km/s of that of the surrounding gas, as expected if the SMBH\nformed within the cloud. Unexpectedly, we find that both nuclei have active\nSMBHs as well, as inferred from very broad H$\\alpha$ emission with FWHM $\\sim\n3000$ km/s. This rules out scenarios where the central SMBH was ejected from\none of the nuclei in a gravitational recoil. Taken together, these results\nstrengthen the hypothesis that the object at the center of the $\\infty$ galaxy\nis a newly formed SMBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The z=1.14 $\\infty$ galaxy consists of two ringed nuclei with an active\nsupermassive black hole (SMBH) in between them. The system is likely the result\nof a nearly face-on collision between two disk galaxies with massive bulges. In\nvan Dokkum et al. (2025) we suggested that the SMBH may have formed from\nshocked and compressed gas at the collision site, in a runaway gravitational\ncollapse. Here we test this hypothesis using newly obtained JWST NIRSpec IFU\nobservations. We first confirm that the system has a cloud of gas in between\nthe nuclei that is photo-ionized by an AGN-like object near its center. Next,\nwe constrain the origin of the SMBH from its radial velocity. If it formed in\nthe cloud its velocity should be similar to the surrounding gas, whereas it\nwould be offset if the SMBH had escaped from one of the nuclei or were\nassociated with a faint galaxy. We find that the radial velocity of the SMBH is\nwithin $\\sim 50$ km/s of that of the surrounding gas, as expected if the SMBH\nformed within the cloud. Unexpectedly, we find that both nuclei have active\nSMBHs as well, as inferred from very broad H$\\alpha$ emission with FWHM $\\sim\n3000$ km/s. This rules out scenarios where the central SMBH was ejected from\none of the nuclei in a gravitational recoil. Taken together, these results\nstrengthen the hypothesis that the object at the center of the $\\infty$ galaxy\nis a newly formed SMBH."
                },
                "authors": [
                    {
                        "name": "Pieter van Dokkum"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Connor Jennings"
                    },
                    {
                        "name": "Imad Pasha"
                    },
                    {
                        "name": "Josephine F. W. Baggen"
                    }
                ],
                "author_detail": {
                    "name": "Josephine F. W. Baggen"
                },
                "author": "Josephine F. W. Baggen",
                "arxiv_comment": "Accepted for publication in ApJ Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09802v1",
                "updated": "2025-08-13T13:34:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    34,
                    39,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:34:39Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    34,
                    39,
                    2,
                    225,
                    0
                ],
                "title": "MUJICA: Reforming SISR Models for PBR Material Super-Resolution via\n  Cross-Map Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUJICA: Reforming SISR Models for PBR Material Super-Resolution via\n  Cross-Map Attention"
                },
                "summary": "Physically Based Rendering (PBR) materials are typically characterized by\nmultiple 2D texture maps such as basecolor, normal, metallic, and roughness\nwhich encode spatially-varying bi-directional reflectance distribution function\n(SVBRDF) parameters to model surface reflectance properties and microfacet\ninteractions. Upscaling SVBRDF material is valuable for modern 3D graphics\napplications. However, existing Single Image Super-Resolution (SISR) methods\nstruggle with cross-map inconsistency, inadequate modeling of modality-specific\nfeatures, and limited generalization due to data distribution shifts. In this\nwork, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention\n(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based\nSISR models for PBR material super-resolution. MUJICA is seamlessly attached\nafter the pre-trained and frozen SISR backbone. It leverages cross-map\nattention to fuse features while preserving remarkable reconstruction ability\nof the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and\nHMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map\nconsistency. Experiments demonstrate that MUJICA enables efficient training\neven with limited resources and delivers state-of-the-art performance on PBR\nmaterial datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physically Based Rendering (PBR) materials are typically characterized by\nmultiple 2D texture maps such as basecolor, normal, metallic, and roughness\nwhich encode spatially-varying bi-directional reflectance distribution function\n(SVBRDF) parameters to model surface reflectance properties and microfacet\ninteractions. Upscaling SVBRDF material is valuable for modern 3D graphics\napplications. However, existing Single Image Super-Resolution (SISR) methods\nstruggle with cross-map inconsistency, inadequate modeling of modality-specific\nfeatures, and limited generalization due to data distribution shifts. In this\nwork, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention\n(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based\nSISR models for PBR material super-resolution. MUJICA is seamlessly attached\nafter the pre-trained and frozen SISR backbone. It leverages cross-map\nattention to fuse features while preserving remarkable reconstruction ability\nof the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and\nHMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map\nconsistency. Experiments demonstrate that MUJICA enables efficient training\neven with limited resources and delivers state-of-the-art performance on PBR\nmaterial datasets."
                },
                "authors": [
                    {
                        "name": "Xin Du"
                    },
                    {
                        "name": "Maoyuan Xu"
                    },
                    {
                        "name": "Zhi Ying"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Ying"
                },
                "author": "Zhi Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11790v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11790v4",
                "updated": "2025-08-13T13:29:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    29,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2025-01-20T23:41:22Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    41,
                    22,
                    0,
                    20,
                    0
                ],
                "title": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables\n  Questions"
                },
                "summary": "Recent studies have raised significant concerns regarding the reliability of\ncurrent mathematics benchmarks, highlighting issues such as simplistic design\nand potential data contamination. Consequently, developing a reliable benchmark\nthat effectively evaluates large language models' (LLMs) genuine capabilities\nin mathematical reasoning remains a critical challenge. To address these\nconcerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking\nLLMs with Random Variables in mathematical reasoning. Specifically, we build\nquestion-generating functions to produce random variable questions (RVQs),\nwhose background content mirrors original benchmark problems, but with\nrandomized variable combinations, rendering them \"unseen\" to LLMs. Models must\ncompletely understand the inherent question pattern to correctly answer RVQs\nwith diverse variable combinations. Thus, an LLM's genuine reasoning capability\nis reflected through its accuracy and robustness on RV-Bench. We conducted\nextensive experiments on over 30 representative LLMs across more than 1,000\nRVQs. Our findings propose that LLMs exhibit a proficiency imbalance between\nencountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals\nthat proficiency generalization across similar mathematical reasoning tasks is\nlimited, but we verified it can still be effectively elicited through test-time\nscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have raised significant concerns regarding the reliability of\ncurrent mathematics benchmarks, highlighting issues such as simplistic design\nand potential data contamination. Consequently, developing a reliable benchmark\nthat effectively evaluates large language models' (LLMs) genuine capabilities\nin mathematical reasoning remains a critical challenge. To address these\nconcerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking\nLLMs with Random Variables in mathematical reasoning. Specifically, we build\nquestion-generating functions to produce random variable questions (RVQs),\nwhose background content mirrors original benchmark problems, but with\nrandomized variable combinations, rendering them \"unseen\" to LLMs. Models must\ncompletely understand the inherent question pattern to correctly answer RVQs\nwith diverse variable combinations. Thus, an LLM's genuine reasoning capability\nis reflected through its accuracy and robustness on RV-Bench. We conducted\nextensive experiments on over 30 representative LLMs across more than 1,000\nRVQs. Our findings propose that LLMs exhibit a proficiency imbalance between\nencountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals\nthat proficiency generalization across similar mathematical reasoning tasks is\nlimited, but we verified it can still be effectively elicited through test-time\nscaling."
                },
                "authors": [
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Su Dong"
                    },
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Yujing Zhang"
                    },
                    {
                        "name": "Zhu Wang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11790v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11790v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v5",
                "updated": "2025-08-13T13:27:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    27,
                    26,
                    2,
                    225,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08712v2",
                "updated": "2025-08-13T13:24:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    24,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T07:56:04Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    7,
                    56,
                    4,
                    1,
                    224,
                    0
                ],
                "title": "A Survey on Parallel Text Generation: From Parallel Decoding to\n  Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Parallel Text Generation: From Parallel Decoding to\n  Diffusion Language Models"
                },
                "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation."
                },
                "authors": [
                    {
                        "name": "Lingzhe Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Chiming Duan"
                    },
                    {
                        "name": "Minghua He"
                    },
                    {
                        "name": "Leyi Pan"
                    },
                    {
                        "name": "Pei Xiao"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Aiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Aiwei Liu"
                },
                "author": "Aiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09791v1",
                "updated": "2025-08-13T13:22:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    22,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:22:49Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    22,
                    49,
                    2,
                    225,
                    0
                ],
                "title": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration\n  Recommendations"
                },
                "summary": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses."
                },
                "authors": [
                    {
                        "name": "Junxiao Han"
                    },
                    {
                        "name": "Yarong Wang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00554v2",
                "updated": "2025-08-13T13:17:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    17,
                    6,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-01T11:48:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism"
                },
                "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multi-agent systems and traditional quantitative investment methods\nacross diverse evaluation metrics. ContestTrade is open-sourced on GitHub at\nhttps://github.com/FinStep-AI/ContestTrade.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multi-agent systems and traditional quantitative investment methods\nacross diverse evaluation metrics. ContestTrade is open-sourced on GitHub at\nhttps://github.com/FinStep-AI/ContestTrade."
                },
                "authors": [
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Zuoyou Jiang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Yuxiao Bai"
                    },
                    {
                        "name": "Mengting Chen"
                    },
                    {
                        "name": "Xinyang Wang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zuo Bai"
                    }
                ],
                "author_detail": {
                    "name": "Zuo Bai"
                },
                "author": "Zuo Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09779v1",
                "updated": "2025-08-13T13:00:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:00:05Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision\n  Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE."
                },
                "authors": [
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Zejun Li"
                    },
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09776v1",
                "updated": "2025-08-13T12:59:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    59,
                    8,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:59:08Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    59,
                    8,
                    2,
                    225,
                    0
                ],
                "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study"
                },
                "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance."
                },
                "authors": [
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Ege Erdogan"
                    },
                    {
                        "name": "Zineb Attaoui"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09767v1",
                "updated": "2025-08-13T12:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    52,
                    38,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    52,
                    38,
                    2,
                    225,
                    0
                ],
                "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech"
                },
                "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness."
                },
                "authors": [
                    {
                        "name": "Shuhei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Shuhei Kato"
                },
                "author": "Shuhei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12508v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12508v3",
                "updated": "2025-08-13T12:50:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    50,
                    42,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-14T13:45:37Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    45,
                    37,
                    5,
                    165,
                    0
                ],
                "title": "AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose\n  Task Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose\n  Task Solving"
                },
                "summary": "Recent advances in agent systems have demonstrated remarkable capabilities in\nsolving both general-purpose and highly complex tasks. However, most current\nmodels lack mechanisms for coordinating specialized agents and have limited\nability to generalize to new or diverse domains. To this end, we introduce\nAgentOrchestra, a hierarchical multi-agent framework for general-purpose task\nsolving that integrates high-level planning with modular agent collaboration.\nDrawing inspiration from a conductor orchestrating a symphony, and grounded in\nthe principles of extensibility, multimodality, modularity, and coordination,\nit features a central planning agent that decomposes complex objectives and\ndelegates sub-tasks to a team of specialized agents. Each sub-agent is equipped\nwith general programming tools, as well as abilities to tackle a wide range of\nreal-world specific tasks, including data analysis, file operations, web\nnavigation, and interactive reasoning in dynamic multimodal environments.\nNotably, AgentOrchestra introduces an MCP Manager Agent that enables\nintelligent evolution through dynamic tool creation, retrieval, and reuse\nmechanisms, significantly enhancing the system's adaptability and scalability.\nAgentOrchestra supports flexible orchestration through explicit sub-goal\nformulation, inter-agent communication, and adaptive role allocation. We\nevaluate the framework on three widely used benchmarks for assessing LLM-based\nagent systems. Experimental results show that AgentOrchestra consistently\noutperforms flat-agent and monolithic baselines in terms of task success rate\nand adaptability. On the GAIA benchmark testing dataset, AgentOrchestra\nachieves an average score of 83.39\\%, ranking among the top general-purpose\nagents. These results highlight the effectiveness of hierarchical organization\nand role specialization in building scalable and general-purpose LLM-based\nagent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in agent systems have demonstrated remarkable capabilities in\nsolving both general-purpose and highly complex tasks. However, most current\nmodels lack mechanisms for coordinating specialized agents and have limited\nability to generalize to new or diverse domains. To this end, we introduce\nAgentOrchestra, a hierarchical multi-agent framework for general-purpose task\nsolving that integrates high-level planning with modular agent collaboration.\nDrawing inspiration from a conductor orchestrating a symphony, and grounded in\nthe principles of extensibility, multimodality, modularity, and coordination,\nit features a central planning agent that decomposes complex objectives and\ndelegates sub-tasks to a team of specialized agents. Each sub-agent is equipped\nwith general programming tools, as well as abilities to tackle a wide range of\nreal-world specific tasks, including data analysis, file operations, web\nnavigation, and interactive reasoning in dynamic multimodal environments.\nNotably, AgentOrchestra introduces an MCP Manager Agent that enables\nintelligent evolution through dynamic tool creation, retrieval, and reuse\nmechanisms, significantly enhancing the system's adaptability and scalability.\nAgentOrchestra supports flexible orchestration through explicit sub-goal\nformulation, inter-agent communication, and adaptive role allocation. We\nevaluate the framework on three widely used benchmarks for assessing LLM-based\nagent systems. Experimental results show that AgentOrchestra consistently\noutperforms flat-agent and monolithic baselines in terms of task success rate\nand adaptability. On the GAIA benchmark testing dataset, AgentOrchestra\nachieves an average score of 83.39\\%, ranking among the top general-purpose\nagents. These results highlight the effectiveness of hierarchical organization\nand role specialization in building scalable and general-purpose LLM-based\nagent systems."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Yuzhen Xiao"
                    },
                    {
                        "name": "Yongcong Li"
                    },
                    {
                        "name": "Ce Cui"
                    },
                    {
                        "name": "Yilei Zhao"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yahui Zhou"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12508v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12508v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09762v1",
                "updated": "2025-08-13T12:47:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    47,
                    33,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:47:33Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    47,
                    33,
                    2,
                    225,
                    0
                ],
                "title": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to\n  Sacrifice Itself for Human Safety?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to\n  Sacrifice Itself for Human Safety?"
                },
                "summary": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities."
                },
                "authors": [
                    {
                        "name": "Manuel Herrador"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Herrador"
                },
                "author": "Manuel Herrador",
                "arxiv_comment": "10 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05371v2",
                "updated": "2025-08-13T12:45:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    45,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-03-07T12:25:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    25,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in\n  LLMs"
                },
                "summary": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We compute 8 steering vectors, each corresponding to a different social\nbias axis, such as age, gender, or race, on a training subset of the BBQ\ndataset and compare the effectiveness of these to 3 additional bias mitigation\nmethods across 4 datasets. When optimized on the BBQ dataset, our individually\ntuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on\nCLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and\nSelf-Debias in all cases, and improvements over fine-tuning in 12 out of 17\nevaluations. In addition, steering vectors showed the lowest impact on MMLU\nscores of the four bias mitigation methods tested. The work presents the first\nsystematic investigation of steering vectors for bias mitigation, and we\ndemonstrate that they are a powerful and computationally efficient strategy for\nreducing bias in LLMs, with broader implications for enhancing AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We compute 8 steering vectors, each corresponding to a different social\nbias axis, such as age, gender, or race, on a training subset of the BBQ\ndataset and compare the effectiveness of these to 3 additional bias mitigation\nmethods across 4 datasets. When optimized on the BBQ dataset, our individually\ntuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on\nCLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and\nSelf-Debias in all cases, and improvements over fine-tuning in 12 out of 17\nevaluations. In addition, steering vectors showed the lowest impact on MMLU\nscores of the four bias mitigation methods tested. The work presents the first\nsystematic investigation of steering vectors for bias mitigation, and we\ndemonstrate that they are a powerful and computationally efficient strategy for\nreducing bias in LLMs, with broader implications for enhancing AI safety."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Irtaza Khalid"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Submitted to AACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09758v1",
                "updated": "2025-08-13T12:39:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    39,
                    20,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:39:20Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    39,
                    20,
                    2,
                    225,
                    0
                ],
                "title": "sanba: An R Package for Bayesian Clustering of Distributions via Shared\n  Atoms Nested Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sanba: An R Package for Bayesian Clustering of Distributions via Shared\n  Atoms Nested Models"
                },
                "summary": "Nested data structures arise when observations are grouped into distinct\nunits, such as patients within hospitals or students within schools. Accounting\nfor this hierarchical organization is essential for valid inference, as\nignoring it can lead to biased estimates and poor generalization. This article\naddresses the challenge of clustering both individual observations and their\ncorresponding groups while flexibly estimating group-specific densities.\nBayesian nested mixture models offer a principled and robust framework for this\ntask. However, their practical use has often been limited by computational\ncomplexity. To overcome this barrier, we present sanba, an R package for\nBayesian analysis of grouped data using nested mixture models with a shared set\nof atoms, a structure recently introduced in the statistical literature. The\npackage provides multiple inference strategies, including state-of-the-art\nMarkov Chain Monte Carlo routines and variational inference algorithms tailored\nfor large-scale datasets. All core functions are implemented in C++ and\nseamlessly integrated into R, making sanba a fast and user-friendly tool for\nfitting nested mixture models with modern Bayesian algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested data structures arise when observations are grouped into distinct\nunits, such as patients within hospitals or students within schools. Accounting\nfor this hierarchical organization is essential for valid inference, as\nignoring it can lead to biased estimates and poor generalization. This article\naddresses the challenge of clustering both individual observations and their\ncorresponding groups while flexibly estimating group-specific densities.\nBayesian nested mixture models offer a principled and robust framework for this\ntask. However, their practical use has often been limited by computational\ncomplexity. To overcome this barrier, we present sanba, an R package for\nBayesian analysis of grouped data using nested mixture models with a shared set\nof atoms, a structure recently introduced in the statistical literature. The\npackage provides multiple inference strategies, including state-of-the-art\nMarkov Chain Monte Carlo routines and variational inference algorithms tailored\nfor large-scale datasets. All core functions are implemented in C++ and\nseamlessly integrated into R, making sanba a fast and user-friendly tool for\nfitting nested mixture models with modern Bayesian algorithms."
                },
                "authors": [
                    {
                        "name": "Francesco Denti"
                    },
                    {
                        "name": "Laura D'Angelo"
                    }
                ],
                "author_detail": {
                    "name": "Laura D'Angelo"
                },
                "author": "Laura D'Angelo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09755v1",
                "updated": "2025-08-13T12:35:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    35,
                    4,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:35:04Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    35,
                    4,
                    2,
                    225,
                    0
                ],
                "title": "Transforming Questions and Documents for Semantically Aligned\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Questions and Documents for Semantically Aligned\n  Retrieval-Augmented Generation"
                },
                "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios."
                },
                "authors": [
                    {
                        "name": "Seokgi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seokgi Lee"
                },
                "author": "Seokgi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09752v1",
                "updated": "2025-08-13T12:31:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    31,
                    27,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:31:27Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    31,
                    27,
                    2,
                    225,
                    0
                ],
                "title": "$$-Parametrization for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$$-Parametrization for Mixture of Experts"
                },
                "summary": "Recent years have seen a growing interest and adoption of LLMs, with\n$\\mu$Transfer becoming a key technique for tuning hyperparameters in\nlarge-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a\nleading architecture in extremely large models. However, the intersection of\nthese two advancements has remained unexplored. In this work, we derive a\n$\\mu$-Parameterization ($\\mu$P) for MoE, providing theoretical guarantees for\nfeature learning across model widths in both the router and experts. We\nempirically validate our parameterization and further investigate how scaling\nthe number of experts and granularity affects the optimal learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen a growing interest and adoption of LLMs, with\n$\\mu$Transfer becoming a key technique for tuning hyperparameters in\nlarge-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a\nleading architecture in extremely large models. However, the intersection of\nthese two advancements has remained unexplored. In this work, we derive a\n$\\mu$-Parameterization ($\\mu$P) for MoE, providing theoretical guarantees for\nfeature learning across model widths in both the router and experts. We\nempirically validate our parameterization and further investigate how scaling\nthe number of experts and granularity affects the optimal learning rate."
                },
                "authors": [
                    {
                        "name": "Jan Maanicki"
                    },
                    {
                        "name": "Kamil Ciebiera"
                    },
                    {
                        "name": "Mateusz Boru"
                    },
                    {
                        "name": "Maciej Piro"
                    },
                    {
                        "name": "Jan Ludziejewski"
                    },
                    {
                        "name": "Maciej Stefaniak"
                    },
                    {
                        "name": "Micha Krutul"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Marek Cygan"
                    },
                    {
                        "name": "Kamil Adamczewski"
                    },
                    {
                        "name": "Jakub Krajewski"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Krajewski"
                },
                "author": "Jakub Krajewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10417v2",
                "updated": "2025-08-13T12:11:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    11,
                    26,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-09T20:40:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    20,
                    40,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs\n  Performance"
                },
                "summary": "Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Abdelrahman A. Ali"
                    },
                    {
                        "name": "Aya E. Fouda"
                    },
                    {
                        "name": "Radwa J. Hanafy"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09732v1",
                "updated": "2025-08-13T11:56:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    56,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:56:22Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    56,
                    22,
                    2,
                    225,
                    0
                ],
                "title": "Predictive Uncertainty for Runtime Assurance of a Real-Time Computer\n  Vision-Based Landing System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Uncertainty for Runtime Assurance of a Real-Time Computer\n  Vision-Based Landing System"
                },
                "summary": "Recent advances in data-driven computer vision have enabled robust autonomous\nnavigation capabilities for civil aviation, including automated landing and\nrunway detection. However, ensuring that these systems meet the robustness and\nsafety requirements for aviation applications remains a major challenge. In\nthis work, we present a practical vision-based pipeline for aircraft pose\nestimation from runway images that represents a step toward the ability to\ncertify these systems for use in safety-critical aviation applications. Our\napproach features three key innovations: (i) an efficient, flexible neural\narchitecture based on a spatial Soft Argmax operator for probabilistic keypoint\nregression, supporting diverse vision backbones with real-time inference; (ii)\na principled loss function producing calibrated predictive uncertainties, which\nare evaluated via sharpness and calibration metrics; and (iii) an adaptation of\nResidual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling\nruntime detection and rejection of faulty model outputs. We implement and\nevaluate our pose estimation pipeline on a dataset of runway images. We show\nthat our model outperforms baseline architectures in terms of accuracy while\nalso producing well-calibrated uncertainty estimates with sub-pixel precision\nthat can be used downstream for fault detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data-driven computer vision have enabled robust autonomous\nnavigation capabilities for civil aviation, including automated landing and\nrunway detection. However, ensuring that these systems meet the robustness and\nsafety requirements for aviation applications remains a major challenge. In\nthis work, we present a practical vision-based pipeline for aircraft pose\nestimation from runway images that represents a step toward the ability to\ncertify these systems for use in safety-critical aviation applications. Our\napproach features three key innovations: (i) an efficient, flexible neural\narchitecture based on a spatial Soft Argmax operator for probabilistic keypoint\nregression, supporting diverse vision backbones with real-time inference; (ii)\na principled loss function producing calibrated predictive uncertainties, which\nare evaluated via sharpness and calibration metrics; and (iii) an adaptation of\nResidual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling\nruntime detection and rejection of faulty model outputs. We implement and\nevaluate our pose estimation pipeline on a dataset of runway images. We show\nthat our model outperforms baseline architectures in terms of accuracy while\nalso producing well-calibrated uncertainty estimates with sub-pixel precision\nthat can be used downstream for fault detection."
                },
                "authors": [
                    {
                        "name": "Romeo Valentin"
                    },
                    {
                        "name": "Sydney M. Katz"
                    },
                    {
                        "name": "Artur B. Carneiro"
                    },
                    {
                        "name": "Don Walker"
                    },
                    {
                        "name": "Mykel J. Kochenderfer"
                    }
                ],
                "author_detail": {
                    "name": "Mykel J. Kochenderfer"
                },
                "author": "Mykel J. Kochenderfer",
                "arxiv_comment": "8 pages, 5 figures, accepted at DASC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17659v3",
                "updated": "2025-08-13T11:46:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    46,
                    4,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-23T16:24:57Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    24,
                    57,
                    2,
                    204,
                    0
                ],
                "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale."
                },
                "authors": [
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yunhan Tang"
                    },
                    {
                        "name": "Yijie Wang"
                    },
                    {
                        "name": "Zhihao Yuan"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "We are withdrawing this preprint because it is undergoing a major\n  revision and restructuring. We feel that the current version does not convey\n  our core contributions and methodology with sufficient clarity and accuracy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09726v1",
                "updated": "2025-08-13T11:43:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    43,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:43:49Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    43,
                    49,
                    2,
                    225,
                    0
                ],
                "title": "Sample More to Think Less: Group Filtered Policy Optimization for\n  Concise Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample More to Think Less: Group Filtered Policy Optimization for\n  Concise Reasoning"
                },
                "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning."
                },
                "authors": [
                    {
                        "name": "Vaishnavi Shrivastava"
                    },
                    {
                        "name": "Ahmed Awadallah"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Shivam Garg"
                    },
                    {
                        "name": "Harkirat Behl"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09724v1",
                "updated": "2025-08-13T11:41:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    41,
                    1,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:41:01Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    41,
                    1,
                    2,
                    225,
                    0
                ],
                "title": "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge"
                },
                "summary": "Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but\nit is prone to preference bias, where judges systematically favor certain\noutputs, such as their own. This bias leads to inconsistent and skewed rankings\nacross different judges. To address this, we first empirically demonstrate\nsignificant and heterogeneous biases in cross-model evaluations. We then\npropose UDA (Unsupervised Debiasing Alignment), a framework that reduces\ninter-judge disagreement by dynamically adjusting the Elo rating system. For\neach pairwise comparison, a compact neural network learns to adaptively set the\nK-factor and refine win probabilities. Crucially, UDA operates in a fully\nunsupervised manner, guided solely by the objective of minimizing the\ndispersion among the Elo trajectories of all judges. This forces an alignment\ntowards a collective consensus, which serves as an unsupervised proxy for a\nmore stable and reproducible evaluation. In addition, we provide theoretical\nmotivation demonstrating how alignment towards a consensus can reduce aggregate\nsystem bias. Experiments show that UDA significantly reduces the inter-judge\nrating standard deviation by up to 63.4% and improves the average correlation\nwith human judgments by 24.7%. Notably, UDA elevates the performance of poorly\nperforming judges to achieve parity with high-quality ones, fostering a more\nrobust and reliable evaluation ecosystem. Code and data are available at\nhttps://anonymous.4open.science/r/62AB93CD-23B4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but\nit is prone to preference bias, where judges systematically favor certain\noutputs, such as their own. This bias leads to inconsistent and skewed rankings\nacross different judges. To address this, we first empirically demonstrate\nsignificant and heterogeneous biases in cross-model evaluations. We then\npropose UDA (Unsupervised Debiasing Alignment), a framework that reduces\ninter-judge disagreement by dynamically adjusting the Elo rating system. For\neach pairwise comparison, a compact neural network learns to adaptively set the\nK-factor and refine win probabilities. Crucially, UDA operates in a fully\nunsupervised manner, guided solely by the objective of minimizing the\ndispersion among the Elo trajectories of all judges. This forces an alignment\ntowards a collective consensus, which serves as an unsupervised proxy for a\nmore stable and reproducible evaluation. In addition, we provide theoretical\nmotivation demonstrating how alignment towards a consensus can reduce aggregate\nsystem bias. Experiments show that UDA significantly reduces the inter-judge\nrating standard deviation by up to 63.4% and improves the average correlation\nwith human judgments by 24.7%. Notably, UDA elevates the performance of poorly\nperforming judges to achieve parity with high-quality ones, fostering a more\nrobust and reliable evaluation ecosystem. Code and data are available at\nhttps://anonymous.4open.science/r/62AB93CD-23B4."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Lindong Wu"
                    },
                    {
                        "name": "Wenbo Yu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03743v2",
                "updated": "2025-08-13T11:40:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    40,
                    35,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-04T18:00:00Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    18,
                    0,
                    0,
                    4,
                    185,
                    0
                ],
                "title": "Bimetric gravity improves the fit to DESI BAO and eases the Hubble\n  tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimetric gravity improves the fit to DESI BAO and eases the Hubble\n  tension"
                },
                "summary": "We investigate whether the latest combination of DESI DR2 baryon acoustic\noscillation (BAO) measurements, cosmic microwave background (CMB) data (Planck\n2018 + ACT), and Type Ia supernovae (SNe Ia) compilations (Pantheon+, Union3,\nand DES Y5) favor a dynamical dark energy component, and explore if such a\nscenario can simultaneously help resolve the Hubble tension. We contrast two\nframeworks: the widely used phenomenological $w_0 w_a$CDM model, and bimetric\ngravity, a fundamental modification of general relativity that naturally gives\nrise to phantom dark energy. The $w_0 w_a$CDM model is moderately preferred\nover $\\Lambda$CDM, at the $2$-$4 \\, \\sigma$ level, when fitting DESI DR2 + CMB\n+ SNe Ia, but it exacerbates the Hubble tension. By comparison, bimetric\ngravity provides a modest improvement in fit quality, at the $1 \\, \\sigma$\nlevel, but, by inferring $H_0 = 69.0 \\pm 0.4 \\, \\mathrm{km/s/Mpc}$, it\npartially eases the Hubble tension, from a $5 \\,\\sigma$ discrepancy to a $3.7\n\\, \\sigma$ tension. Including locally calibrated SNe Ia brings the overall\npreference for the bimetric model over $\\Lambda$CDM to the $2 \\, \\sigma$ level,\ncomparable to that of the $w_0 w_a$CDM model when including the local SN Ia\ncalibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the latest combination of DESI DR2 baryon acoustic\noscillation (BAO) measurements, cosmic microwave background (CMB) data (Planck\n2018 + ACT), and Type Ia supernovae (SNe Ia) compilations (Pantheon+, Union3,\nand DES Y5) favor a dynamical dark energy component, and explore if such a\nscenario can simultaneously help resolve the Hubble tension. We contrast two\nframeworks: the widely used phenomenological $w_0 w_a$CDM model, and bimetric\ngravity, a fundamental modification of general relativity that naturally gives\nrise to phantom dark energy. The $w_0 w_a$CDM model is moderately preferred\nover $\\Lambda$CDM, at the $2$-$4 \\, \\sigma$ level, when fitting DESI DR2 + CMB\n+ SNe Ia, but it exacerbates the Hubble tension. By comparison, bimetric\ngravity provides a modest improvement in fit quality, at the $1 \\, \\sigma$\nlevel, but, by inferring $H_0 = 69.0 \\pm 0.4 \\, \\mathrm{km/s/Mpc}$, it\npartially eases the Hubble tension, from a $5 \\,\\sigma$ discrepancy to a $3.7\n\\, \\sigma$ tension. Including locally calibrated SNe Ia brings the overall\npreference for the bimetric model over $\\Lambda$CDM to the $2 \\, \\sigma$ level,\ncomparable to that of the $w_0 w_a$CDM model when including the local SN Ia\ncalibration."
                },
                "authors": [
                    {
                        "name": "Marcus Hgs"
                    },
                    {
                        "name": "Edvard Mrtsell"
                    }
                ],
                "author_detail": {
                    "name": "Edvard Mrtsell"
                },
                "author": "Edvard Mrtsell",
                "arxiv_comment": "v2: added references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09721v1",
                "updated": "2025-08-13T11:24:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    24,
                    24,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:24:24Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    24,
                    24,
                    2,
                    225,
                    0
                ],
                "title": "Structured Kernel Regression VAE: A Computationally Efficient Surrogate\n  for GP-VAEs in ICA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Kernel Regression VAE: A Computationally Efficient Surrogate\n  for GP-VAEs in ICA"
                },
                "summary": "The interpretability of generative models is considered a key factor in\ndemonstrating their effectiveness and controllability. The generated data are\nbelieved to be determined by latent variables that are not directly observable.\nTherefore, disentangling, decoupling, decomposing, causal inference, or\nperforming Independent Component Analysis (ICA) in the latent variable space\nhelps uncover the independent factors that influence the attributes or features\naffecting the generated outputs, thereby enhancing the interpretability of\ngenerative models. As a generative model, Variational Autoencoders (VAEs)\ncombine with variational Bayesian inference algorithms. Using VAEs, the inverse\nprocess of ICA can be equivalently framed as a variational inference process.\nIn some studies, Gaussian processes (GPs) have been introduced as priors for\neach dimension of latent variables in VAEs, structuring and separating each\ndimension from temporal or spatial perspectives, and encouraging different\ndimensions to control various attributes of the generated data. However, GPs\nimpose a significant computational burden, resulting in substantial resource\nconsumption when handling large datasets. Essentially, GPs model different\ntemporal or spatial structures through various kernel functions. Structuring\nthe priors of latent variables via kernel functions-so that different kernel\nfunctions model the correlations among sequence points within different latent\ndimensions-is at the core of achieving disentanglement in VAEs. The proposed\nStructured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more\nefficient way, avoiding the costly kernel matrix inversion required in GPs.\nThis research demonstrates that, while maintaining ICA performance, SKR-VAE\nachieves greater computational efficiency and significantly reduced\ncomputational burden compared to GP-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The interpretability of generative models is considered a key factor in\ndemonstrating their effectiveness and controllability. The generated data are\nbelieved to be determined by latent variables that are not directly observable.\nTherefore, disentangling, decoupling, decomposing, causal inference, or\nperforming Independent Component Analysis (ICA) in the latent variable space\nhelps uncover the independent factors that influence the attributes or features\naffecting the generated outputs, thereby enhancing the interpretability of\ngenerative models. As a generative model, Variational Autoencoders (VAEs)\ncombine with variational Bayesian inference algorithms. Using VAEs, the inverse\nprocess of ICA can be equivalently framed as a variational inference process.\nIn some studies, Gaussian processes (GPs) have been introduced as priors for\neach dimension of latent variables in VAEs, structuring and separating each\ndimension from temporal or spatial perspectives, and encouraging different\ndimensions to control various attributes of the generated data. However, GPs\nimpose a significant computational burden, resulting in substantial resource\nconsumption when handling large datasets. Essentially, GPs model different\ntemporal or spatial structures through various kernel functions. Structuring\nthe priors of latent variables via kernel functions-so that different kernel\nfunctions model the correlations among sequence points within different latent\ndimensions-is at the core of achieving disentanglement in VAEs. The proposed\nStructured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more\nefficient way, avoiding the costly kernel matrix inversion required in GPs.\nThis research demonstrates that, while maintaining ICA performance, SKR-VAE\nachieves greater computational efficiency and significantly reduced\ncomputational burden compared to GP-VAE."
                },
                "authors": [
                    {
                        "name": "Yuan-Hao Wei"
                    },
                    {
                        "name": "Fu-Hao Deng"
                    },
                    {
                        "name": "Lin-Yong Cui"
                    },
                    {
                        "name": "Yan-Jie Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan-Jie Sun"
                },
                "author": "Yan-Jie Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12867v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12867v4",
                "updated": "2025-08-13T11:23:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    23,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-17T11:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting"
                },
                "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Dataset, code, checkpoints, and demo samples\nare available at https://github.com/yanghaha0908/EmoVoice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Dataset, code, checkpoints, and demo samples\nare available at https://github.com/yanghaha0908/EmoVoice."
                },
                "authors": [
                    {
                        "name": "Guanrou Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "ShiLiang Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "Accepted at ACMMM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12867v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12867v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09719v1",
                "updated": "2025-08-13T11:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    19,
                    30,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    19,
                    30,
                    2,
                    225,
                    0
                ],
                "title": "Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models"
                },
                "summary": "Large, publicly available clinical datasets have emerged as a novel resource\nfor understanding disease heterogeneity and to explore personalization of\ntherapy. These datasets are derived from data not originally collected for\nresearch purposes and, as a result, are often incomplete and lack critical\nlabels. Many AI tools have been developed to retrospectively label these\ndatasets, such as by performing disease classification; however, they often\nsuffer from limited interpretability. Previous work has attempted to explain\npredictions using Concept Bottleneck Models (CBMs), which learn interpretable\nconcepts that map to higher-level clinical ideas, facilitating human\nevaluation. However, these models often experience performance limitations when\nthe concepts fail to adequately explain or characterize the task. We use the\nidentification of Acute Respiratory Distress Syndrome (ARDS) as a challenging\ntest case to demonstrate the value of incorporating contextual information from\nclinical notes to improve CBM performance. Our approach leverages a Large\nLanguage Model (LLM) to process clinical notes and generate additional\nconcepts, resulting in a 10% performance gain over existing methods.\nAdditionally, it facilitates the learning of more comprehensive concepts,\nthereby reducing the risk of information leakage and reliance on spurious\nshortcuts, thus improving the characterization of ARDS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large, publicly available clinical datasets have emerged as a novel resource\nfor understanding disease heterogeneity and to explore personalization of\ntherapy. These datasets are derived from data not originally collected for\nresearch purposes and, as a result, are often incomplete and lack critical\nlabels. Many AI tools have been developed to retrospectively label these\ndatasets, such as by performing disease classification; however, they often\nsuffer from limited interpretability. Previous work has attempted to explain\npredictions using Concept Bottleneck Models (CBMs), which learn interpretable\nconcepts that map to higher-level clinical ideas, facilitating human\nevaluation. However, these models often experience performance limitations when\nthe concepts fail to adequately explain or characterize the task. We use the\nidentification of Acute Respiratory Distress Syndrome (ARDS) as a challenging\ntest case to demonstrate the value of incorporating contextual information from\nclinical notes to improve CBM performance. Our approach leverages a Large\nLanguage Model (LLM) to process clinical notes and generate additional\nconcepts, resulting in a 10% performance gain over existing methods.\nAdditionally, it facilitates the learning of more comprehensive concepts,\nthereby reducing the risk of information leakage and reliance on spurious\nshortcuts, thus improving the characterization of ARDS."
                },
                "authors": [
                    {
                        "name": "Anish Narain"
                    },
                    {
                        "name": "Ritam Majumdar"
                    },
                    {
                        "name": "Nikita Narayanan"
                    },
                    {
                        "name": "Dominic Marshall"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "32 pages, 7 figures, accepted at Machine Learning for Healthcare\n  Conference (MLHC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07976v2",
                "updated": "2025-08-13T11:06:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    6,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-11T13:36:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    36,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL"
                },
                "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Minyang Xie"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Banghua Zhu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09105v2",
                "updated": "2025-08-13T11:05:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    5,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T17:32:24Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    32,
                    24,
                    1,
                    224,
                    0
                ],
                "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling"
                },
                "summary": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems."
                },
                "authors": [
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Jianjie Huang"
                    },
                    {
                        "name": "Jingzhi Li"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09713v1",
                "updated": "2025-08-13T11:04:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    4,
                    48,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:04:48Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    4,
                    48,
                    2,
                    225,
                    0
                ],
                "title": "Evaluating the Role of Large Language Models in Legal Practice in India",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Role of Large Language Models in Legal Practice in India"
                },
                "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law."
                },
                "authors": [
                    {
                        "name": "Rahul Hemrajani"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Hemrajani"
                },
                "arxiv_affiliation": "National Law School of India University, Bengaluru",
                "author": "Rahul Hemrajani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09702v1",
                "updated": "2025-08-13T10:56:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    56,
                    24,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T10:56:24Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    56,
                    24,
                    2,
                    225,
                    0
                ],
                "title": "$\\text{M}^3\\text{PDB}$: A Multimodal, Multi-Label, Multilingual Prompt\n  Database for Speech Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\text{M}^3\\text{PDB}$: A Multimodal, Multi-Label, Multilingual Prompt\n  Database for Speech Generation"
                },
                "summary": "Recent advancements in zero-shot speech generation have enabled models to\nsynthesize speech that mimics speaker identity and speaking style from speech\nprompts. However, these models' effectiveness is significantly limited in\nreal-world scenarios where high-quality speech prompts are absent, incomplete,\nor out of domain. This issue arises primarily from a significant quality\nmismatch between the speech data utilized for model training and the input\nprompt speech during inference. To address this, we introduce\n$\\text{M}^3\\text{PDB}$, the first large-scale, multi-modal, multi-label, and\nmultilingual prompt database designed for robust prompt selection in speech\ngeneration. Our dataset construction leverages a novel multi-modal, multi-agent\nannotation framework, enabling precise and hierarchical labeling across diverse\nmodalities. Furthermore, we propose a lightweight yet effective prompt\nselection strategy tailored for real-time, resource-constrained inference\nsettings. Experimental results demonstrate that our proposed database and\nselection strategy effectively support various challenging speech generation\nscenarios. We hope our work can inspire the community to shift focus from\nimproving performance on standard benchmarks to addressing more realistic and\ndiverse application scenarios in speech generation. Code and dataset are\navailable at: https://github.com/hizening/M3PDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in zero-shot speech generation have enabled models to\nsynthesize speech that mimics speaker identity and speaking style from speech\nprompts. However, these models' effectiveness is significantly limited in\nreal-world scenarios where high-quality speech prompts are absent, incomplete,\nor out of domain. This issue arises primarily from a significant quality\nmismatch between the speech data utilized for model training and the input\nprompt speech during inference. To address this, we introduce\n$\\text{M}^3\\text{PDB}$, the first large-scale, multi-modal, multi-label, and\nmultilingual prompt database designed for robust prompt selection in speech\ngeneration. Our dataset construction leverages a novel multi-modal, multi-agent\nannotation framework, enabling precise and hierarchical labeling across diverse\nmodalities. Furthermore, we propose a lightweight yet effective prompt\nselection strategy tailored for real-time, resource-constrained inference\nsettings. Experimental results demonstrate that our proposed database and\nselection strategy effectively support various challenging speech generation\nscenarios. We hope our work can inspire the community to shift focus from\nimproving performance on standard benchmarks to addressing more realistic and\ndiverse application scenarios in speech generation. Code and dataset are\navailable at: https://github.com/hizening/M3PDB."
                },
                "authors": [
                    {
                        "name": "Boyu Zhu"
                    },
                    {
                        "name": "Cheng Gong"
                    },
                    {
                        "name": "Muyang Wu"
                    },
                    {
                        "name": "Ruihao Jing"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Xiaolei Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18785v2",
                "updated": "2025-08-13T10:49:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    49,
                    32,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-23T15:54:28Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    54,
                    28,
                    0,
                    174,
                    0
                ],
                "title": "SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy\n  Prediction in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy\n  Prediction in Autonomous Driving"
                },
                "summary": "Perception systems in autonomous driving rely on sensors such as LiDAR and\ncameras to perceive the 3D environment. However, due to occlusions and data\nsparsity, these sensors often fail to capture complete information. Semantic\nOccupancy Prediction (SOP) addresses this challenge by inferring both occupancy\nand semantics of unobserved regions. Existing transformer-based SOP methods\nlack explicit modeling of spatial structure in attention computation, resulting\nin limited geometric awareness and poor performance in sparse or occluded\nareas. To this end, we propose Spatially-aware Window Attention (SWA), a novel\nmechanism that incorporates local spatial context into attention. SWA\nsignificantly improves scene completion and achieves state-of-the-art results\non LiDAR-based SOP benchmarks. We further validate its generality by\nintegrating SWA into a camera-based SOP pipeline, where it also yields\nconsistent gains across modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception systems in autonomous driving rely on sensors such as LiDAR and\ncameras to perceive the 3D environment. However, due to occlusions and data\nsparsity, these sensors often fail to capture complete information. Semantic\nOccupancy Prediction (SOP) addresses this challenge by inferring both occupancy\nand semantics of unobserved regions. Existing transformer-based SOP methods\nlack explicit modeling of spatial structure in attention computation, resulting\nin limited geometric awareness and poor performance in sparse or occluded\nareas. To this end, we propose Spatially-aware Window Attention (SWA), a novel\nmechanism that incorporates local spatial context into attention. SWA\nsignificantly improves scene completion and achieves state-of-the-art results\non LiDAR-based SOP benchmarks. We further validate its generality by\nintegrating SWA into a camera-based SOP pipeline, where it also yields\nconsistent gains across modalities."
                },
                "authors": [
                    {
                        "name": "Helin Cao"
                    },
                    {
                        "name": "Rafael Materla"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "2025 IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC), Vienna, Austria, Oct 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18798v2",
                "updated": "2025-08-13T10:44:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    44,
                    59,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-23T16:03:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by\n  Object-Centric Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by\n  Object-Centric Awareness"
                },
                "summary": "Autonomous driving perception faces significant challenges due to occlusions\nand incomplete scene data in the environment. To overcome these issues, the\ntask of semantic occupancy prediction (SOP) is proposed, which aims to jointly\ninfer both the geometry and semantic labels of a scene from images. However,\nconventional camera-based methods typically treat all categories equally and\nprimarily rely on local features, leading to suboptimal predictions, especially\nfor dynamic foreground objects. To address this, we propose Object-Centric SOP\n(OC-SOP), a framework that integrates high-level object-centric cues extracted\nvia a detection branch into the semantic occupancy prediction pipeline. This\nobject-centric integration significantly enhances the prediction accuracy for\nforeground objects and achieves state-of-the-art performance among all\ncategories on SemanticKITTI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving perception faces significant challenges due to occlusions\nand incomplete scene data in the environment. To overcome these issues, the\ntask of semantic occupancy prediction (SOP) is proposed, which aims to jointly\ninfer both the geometry and semantic labels of a scene from images. However,\nconventional camera-based methods typically treat all categories equally and\nprimarily rely on local features, leading to suboptimal predictions, especially\nfor dynamic foreground objects. To address this, we propose Object-Centric SOP\n(OC-SOP), a framework that integrates high-level object-centric cues extracted\nvia a detection branch into the semantic occupancy prediction pipeline. This\nobject-centric integration significantly enhances the prediction accuracy for\nforeground objects and achieves state-of-the-art performance among all\ncategories on SemanticKITTI."
                },
                "authors": [
                    {
                        "name": "Helin Cao"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "2025 IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC), Vienna, Austria, Oct 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22557v2",
                "updated": "2025-08-13T10:28:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    28,
                    17,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-27T18:15:56Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    15,
                    56,
                    4,
                    178,
                    0
                ],
                "title": "MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for\n  Cipher-Based Jailbreak Attacks for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for\n  Cipher-Based Jailbreak Attacks for LLMs"
                },
                "summary": "As large language models (LLMs) grow more capable, they face growing\nvulnerability to sophisticated jailbreak attacks. While developers invest\nheavily in alignment finetuning and safety guardrails, researchers continue\npublishing novel attacks, driving progress through adversarial iteration. This\ndynamic mirrors a strategic game of continual evolution. However, two major\nchallenges hinder jailbreak development: the high cost of querying top-tier\nLLMs and the short lifespan of effective attacks due to frequent safety\nupdates. These factors limit cost-efficiency and practical impact of research\nin jailbreak attacks. To address this, we propose MetaCipher, a low-cost,\nmulti-agent jailbreak framework that generalizes across LLMs with varying\nsafety measures. Using reinforcement learning, MetaCipher is modular and\nadaptive, supporting extensibility to future strategies. Within as few as 10\nqueries, MetaCipher achieves state-of-the-art attack success rates on recent\nmalicious prompt benchmarks, outperforming prior jailbreak methods. We conduct\na large-scale empirical evaluation across diverse victim models and benchmarks,\ndemonstrating its robustness and adaptability. Warning: This paper contains\nmodel outputs that may be offensive or harmful, shown solely to demonstrate\njailbreak efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow more capable, they face growing\nvulnerability to sophisticated jailbreak attacks. While developers invest\nheavily in alignment finetuning and safety guardrails, researchers continue\npublishing novel attacks, driving progress through adversarial iteration. This\ndynamic mirrors a strategic game of continual evolution. However, two major\nchallenges hinder jailbreak development: the high cost of querying top-tier\nLLMs and the short lifespan of effective attacks due to frequent safety\nupdates. These factors limit cost-efficiency and practical impact of research\nin jailbreak attacks. To address this, we propose MetaCipher, a low-cost,\nmulti-agent jailbreak framework that generalizes across LLMs with varying\nsafety measures. Using reinforcement learning, MetaCipher is modular and\nadaptive, supporting extensibility to future strategies. Within as few as 10\nqueries, MetaCipher achieves state-of-the-art attack success rates on recent\nmalicious prompt benchmarks, outperforming prior jailbreak methods. We conduct\na large-scale empirical evaluation across diverse victim models and benchmarks,\ndemonstrating its robustness and adaptability. Warning: This paper contains\nmodel outputs that may be offensive or harmful, shown solely to demonstrate\njailbreak efficacy."
                },
                "authors": [
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13744v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13744v6",
                "updated": "2025-08-13T10:26:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    26,
                    26,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty"
                },
                "summary": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximization frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-risks and by emphasizing\n'constraints' over 'preference'. This decomposes viable economic asset-pricing\ninto that of model and non-model risks separately, leading to a unique and\nconvenient model-risk pricing formula. Its parameter, a dynamically conserved\nconstant of model-risk inference, allows an integrated representation of\nex-ante risk-pricing and bias such that their ex-post impacts are disentangled\nvia well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns\nacquire a fresh significance: peak-reward reveals ex-ante risk-premia, and\npeak-location, bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximization frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-risks and by emphasizing\n'constraints' over 'preference'. This decomposes viable economic asset-pricing\ninto that of model and non-model risks separately, leading to a unique and\nconvenient model-risk pricing formula. Its parameter, a dynamically conserved\nconstant of model-risk inference, allows an integrated representation of\nex-ante risk-pricing and bias such that their ex-post impacts are disentangled\nvia well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns\nacquire a fresh significance: peak-reward reveals ex-ante risk-premia, and\npeak-location, bias."
                },
                "authors": [
                    {
                        "name": "Ken Kangda Wren"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kangda Wren"
                },
                "author": "Ken Kangda Wren",
                "arxiv_comment": "20 pages of main text (including title and abstract), 9 pages of\n  Appendix and Bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13744v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13744v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.MF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.MF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09681v1",
                "updated": "2025-08-13T10:20:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    20,
                    24,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T10:20:24Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    20,
                    24,
                    2,
                    225,
                    0
                ],
                "title": "Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in\n  surgical vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in\n  surgical vision"
                },
                "summary": "We proposed a novel test-time optimisation (TTO) approach framed by a\nNeRF-based architecture for long-term 3D point tracking. Most current methods\nin point tracking struggle to obtain consistent motion or are limited to 2D\nmotion. TTO approaches frame the solution for long-term tracking as optimising\na function that aggregates correspondences from other specialised\nstate-of-the-art methods. Unlike the state-of-the-art on TTO, we propose\nparametrising such a function with our new invertible Neural Radiance Field\n(InvNeRF) architecture to perform both 2D and 3D tracking in surgical\nscenarios. Our approach allows us to exploit the advantages of a\nrendering-based approach by supervising the reprojection of pixel\ncorrespondences. It adapts strategies from recent rendering-based methods to\nobtain a bidirectional deformable-canonical mapping, to efficiently handle a\ndefined workspace, and to guide the rays' density. It also presents our\nmulti-scale HexPlanes for fast inference and a new algorithm for efficient\npixel sampling and convergence criteria. We present results in the STIR and\nSCARE datasets, for evaluating point tracking and testing the integration of\nkinematic data in our pipeline, respectively. In 2D point tracking, our\napproach surpasses the precision and accuracy of the TTO state-of-the-art\nmethods by nearly 50% on average precision, while competing with other\napproaches. In 3D point tracking, this is the first TTO approach, surpassing\nfeed-forward methods while incorporating the benefits of a deformable\nNeRF-based reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We proposed a novel test-time optimisation (TTO) approach framed by a\nNeRF-based architecture for long-term 3D point tracking. Most current methods\nin point tracking struggle to obtain consistent motion or are limited to 2D\nmotion. TTO approaches frame the solution for long-term tracking as optimising\na function that aggregates correspondences from other specialised\nstate-of-the-art methods. Unlike the state-of-the-art on TTO, we propose\nparametrising such a function with our new invertible Neural Radiance Field\n(InvNeRF) architecture to perform both 2D and 3D tracking in surgical\nscenarios. Our approach allows us to exploit the advantages of a\nrendering-based approach by supervising the reprojection of pixel\ncorrespondences. It adapts strategies from recent rendering-based methods to\nobtain a bidirectional deformable-canonical mapping, to efficiently handle a\ndefined workspace, and to guide the rays' density. It also presents our\nmulti-scale HexPlanes for fast inference and a new algorithm for efficient\npixel sampling and convergence criteria. We present results in the STIR and\nSCARE datasets, for evaluating point tracking and testing the integration of\nkinematic data in our pipeline, respectively. In 2D point tracking, our\napproach surpasses the precision and accuracy of the TTO state-of-the-art\nmethods by nearly 50% on average precision, while competing with other\napproaches. In 3D point tracking, this is the first TTO approach, surpassing\nfeed-forward methods while incorporating the benefits of a deformable\nNeRF-based reconstruction."
                },
                "authors": [
                    {
                        "name": "Gerardo Loza"
                    },
                    {
                        "name": "Junlei Hu"
                    },
                    {
                        "name": "Dominic Jones"
                    },
                    {
                        "name": "Sharib Ali"
                    },
                    {
                        "name": "Pietro Valdastri"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Valdastri"
                },
                "author": "Pietro Valdastri",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09864v3",
                "updated": "2025-08-13T10:18:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    18,
                    32,
                    2,
                    225,
                    0
                ],
                "published": "2024-06-14T09:22:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    9,
                    22,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data"
                },
                "summary": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique multimodal dataset,\nfeaturing audio, image, and textual data from 50 classes, specifically designed\nfor learning from uncertain data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the research community to design more\ntrustworthy and robust machine learning approaches for safety critical\napplications. The code and instructions for downloading and processing the\ndataset can be found at: https://github.com/bezirganyan/LUMA/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique multimodal dataset,\nfeaturing audio, image, and textual data from 50 classes, specifically designed\nfor learning from uncertain data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the research community to design more\ntrustworthy and robust machine learning approaches for safety critical\napplications. The code and instructions for downloading and processing the\ndataset can be found at: https://github.com/bezirganyan/LUMA/ ."
                },
                "authors": [
                    {
                        "name": "Grigor Bezirganyan"
                    },
                    {
                        "name": "Sana Sellami"
                    },
                    {
                        "name": "Laure Berti-quille"
                    },
                    {
                        "name": "Sbastien Fournier"
                    }
                ],
                "author_detail": {
                    "name": "Sbastien Fournier"
                },
                "author": "Sbastien Fournier",
                "arxiv_doi": "10.1145/3726302.3730302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_journal_ref": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval. 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22748v2",
                "updated": "2025-08-13T10:10:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    10,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-30T15:05:05Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    5,
                    5,
                    2,
                    211,
                    0
                ],
                "title": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index"
                },
                "summary": "We draw on Eloundou et al. (2024) to develop the Generative AI Susceptibility\nIndex (GAISI), a task-based measure of UK job exposure to large language models\n(LLMs), such as ChatGPT. GAISI is derived from probabilistic task ratings by\nLLMs and linked to worker-reported task data from the Skills and Employment\nSurveys. It reflects the share of job activities where an LLM or LLM-powered\nsystem can reduce task completion time by at least 25% beyond existing\nproductivity tools. The index demonstrates high reliability, strong alignment\nwith AI capabilities, and superior predictive power compared to existing\nexposure measures. By 2023-24, nearly all UK jobs exhibited some exposure, yet\nonly a minority were heavily affected. Aggregate exposure has risen since 2017,\nprimarily due to occupational shifts rather than changes in task profiles. The\nprice premium for AI-exposed tasks declined relative to 2017, measuring\napproximately 12% lower in 2023-24. Job postings fell following the release of\nChatGPT, with job postings 5.5% lower in 2025-Q2 than if pre-GPT hiring\npatterns had persisted. GAISI offers a robust framework for assessing AI's\nimpact on work, providing early evidence that displacement effects may already\noutweigh productivity gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We draw on Eloundou et al. (2024) to develop the Generative AI Susceptibility\nIndex (GAISI), a task-based measure of UK job exposure to large language models\n(LLMs), such as ChatGPT. GAISI is derived from probabilistic task ratings by\nLLMs and linked to worker-reported task data from the Skills and Employment\nSurveys. It reflects the share of job activities where an LLM or LLM-powered\nsystem can reduce task completion time by at least 25% beyond existing\nproductivity tools. The index demonstrates high reliability, strong alignment\nwith AI capabilities, and superior predictive power compared to existing\nexposure measures. By 2023-24, nearly all UK jobs exhibited some exposure, yet\nonly a minority were heavily affected. Aggregate exposure has risen since 2017,\nprimarily due to occupational shifts rather than changes in task profiles. The\nprice premium for AI-exposed tasks declined relative to 2017, measuring\napproximately 12% lower in 2023-24. Job postings fell following the release of\nChatGPT, with job postings 5.5% lower in 2025-Q2 than if pre-GPT hiring\npatterns had persisted. GAISI offers a robust framework for assessing AI's\nimpact on work, providing early evidence that displacement effects may already\noutweigh productivity gains."
                },
                "authors": [
                    {
                        "name": "Golo Henseke"
                    },
                    {
                        "name": "Rhys Davies"
                    },
                    {
                        "name": "Alan Felstead"
                    },
                    {
                        "name": "Duncan Gallie"
                    },
                    {
                        "name": "Francis Green"
                    },
                    {
                        "name": "Ying Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zhou"
                },
                "author": "Ying Zhou",
                "arxiv_comment": "51 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02195v2",
                "updated": "2025-08-13T10:03:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    3,
                    30,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-03T00:40:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    0,
                    40,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "SymCERE: Symmetric Contrastive Learning for Robust Review-Enhanced\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymCERE: Symmetric Contrastive Learning for Robust Review-Enhanced\n  Recommendation"
                },
                "summary": "Modern recommendation systems can achieve high performance by fusing user\nbehavior graphs (via GNNs) and review texts (via LLMs). However, this fusion\nfaces three significant issues: (1) False Negatives in contrastive learning can\ndegrade the training signal by penalizing similar items; (2) Popularity Bias,\noften encoded as embedding magnitude, can distort similarity scores; and (3)\nSignal Ambiguity, which arises from the conflation of objective facts with\nsubjective sentiment in reviews. These interconnected issues can prevent models\nfrom learning users' true preferences. In this paper, we propose SymCERE\n(Symmetric SINCERE), a contrastive learning method that addresses these three\nissues simultaneously through its structural design. First, we introduce a\nsymmetric application of the SINCERE loss for cross-modal alignment, which is\ndesigned to eliminate false negatives in recommendation. Second, by integrating\nthis with L2 normalisation under a \"magnitude-as-noise\" hypothesis, we aim to\nmitigate popularity bias by forcing the model to encode preferences primarily\nin the vector's direction. Experiments on 15 datasets from three distinct\nplatforms (e-commerce, local reviews, and travel) demonstrate that SymCERE\noutperforms several strong baselines, achieving a relative improvement of up to\n43.6% on NDCG@10. Furthermore, a detailed LIME analysis shows that the model\nlearns to anchor alignment on objective, informative vocabulary (e.g., \"OEM,\"\n\"compatible,\" \"gasket\"), while placing less emphasis on generic sentiment\n(e.g., \"good,\" \"great\"). This suggests that effective semantic alignment stems\nfrom understanding factual product attributes, offering a path toward more\naccurate recommendation systems. The code is available at:\nhttps://anonymous.4open.science/r/ReviewGNN-2E1E.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommendation systems can achieve high performance by fusing user\nbehavior graphs (via GNNs) and review texts (via LLMs). However, this fusion\nfaces three significant issues: (1) False Negatives in contrastive learning can\ndegrade the training signal by penalizing similar items; (2) Popularity Bias,\noften encoded as embedding magnitude, can distort similarity scores; and (3)\nSignal Ambiguity, which arises from the conflation of objective facts with\nsubjective sentiment in reviews. These interconnected issues can prevent models\nfrom learning users' true preferences. In this paper, we propose SymCERE\n(Symmetric SINCERE), a contrastive learning method that addresses these three\nissues simultaneously through its structural design. First, we introduce a\nsymmetric application of the SINCERE loss for cross-modal alignment, which is\ndesigned to eliminate false negatives in recommendation. Second, by integrating\nthis with L2 normalisation under a \"magnitude-as-noise\" hypothesis, we aim to\nmitigate popularity bias by forcing the model to encode preferences primarily\nin the vector's direction. Experiments on 15 datasets from three distinct\nplatforms (e-commerce, local reviews, and travel) demonstrate that SymCERE\noutperforms several strong baselines, achieving a relative improvement of up to\n43.6% on NDCG@10. Furthermore, a detailed LIME analysis shows that the model\nlearns to anchor alignment on objective, informative vocabulary (e.g., \"OEM,\"\n\"compatible,\" \"gasket\"), while placing less emphasis on generic sentiment\n(e.g., \"good,\" \"great\"). This suggests that effective semantic alignment stems\nfrom understanding factual product attributes, offering a path toward more\naccurate recommendation systems. The code is available at:\nhttps://anonymous.4open.science/r/ReviewGNN-2E1E."
                },
                "authors": [
                    {
                        "name": "Toyotaro Suzumura"
                    },
                    {
                        "name": "Hisashi Ikari"
                    },
                    {
                        "name": "Hiroki Kanezashi"
                    },
                    {
                        "name": "Md Mostafizur Rahman"
                    },
                    {
                        "name": "Yu Hirate"
                    }
                ],
                "author_detail": {
                    "name": "Yu Hirate"
                },
                "author": "Yu Hirate",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10024v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10024v4",
                "updated": "2025-08-13T10:02:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    2,
                    30,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-14T08:06:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    6,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles"
                },
                "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."
                },
                "authors": [
                    {
                        "name": "Shaolun Ruan"
                    },
                    {
                        "name": "Rui Sheng"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Tim Dwyer"
                    },
                    {
                        "name": "Jiannan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiannan Li"
                },
                "author": "Jiannan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10024v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10024v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09670v1",
                "updated": "2025-08-13T09:58:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    58,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    58,
                    10,
                    2,
                    225,
                    0
                ],
                "title": "MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR\n  Advancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR\n  Advancement"
                },
                "summary": "Recent advances demonstrate that reinforcement learning with verifiable\nrewards (RLVR) significantly enhances the reasoning capabilities of large\nlanguage models (LLMs). However, standard RLVR faces challenges with reward\nsparsity, where zero rewards from consistently incorrect candidate answers\nprovide no learning signal, particularly in challenging tasks. To address this,\nwe propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative\nframework that utilizes diverse expert prompts as system prompts to generate a\nbroader range of responses, substantially increasing the likelihood of\nidentifying correct solutions. Additionally, we introduce an inter-expert\nmutual learning mechanism that facilitates knowledge sharing and transfer among\nexperts, further boosting the model's performance through RLVR. Extensive\nexperiments across multiple reasoning benchmarks show that MEML-GRPO delivers\nsignificant improvements, achieving an average performance gain of 4.89% with\nQwen and 11.33% with Llama, effectively overcoming the core limitations of\ntraditional RLVR methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that reinforcement learning with verifiable\nrewards (RLVR) significantly enhances the reasoning capabilities of large\nlanguage models (LLMs). However, standard RLVR faces challenges with reward\nsparsity, where zero rewards from consistently incorrect candidate answers\nprovide no learning signal, particularly in challenging tasks. To address this,\nwe propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative\nframework that utilizes diverse expert prompts as system prompts to generate a\nbroader range of responses, substantially increasing the likelihood of\nidentifying correct solutions. Additionally, we introduce an inter-expert\nmutual learning mechanism that facilitates knowledge sharing and transfer among\nexperts, further boosting the model's performance through RLVR. Extensive\nexperiments across multiple reasoning benchmarks show that MEML-GRPO delivers\nsignificant improvements, achieving an average performance gain of 4.89% with\nQwen and 11.33% with Llama, effectively overcoming the core limitations of\ntraditional RLVR methods."
                },
                "authors": [
                    {
                        "name": "Weitao Jia"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Weijie Yin"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Teng Fu"
                    },
                    {
                        "name": "Changhong Jin"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Xiaohui Lv"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09669v1",
                "updated": "2025-08-13T09:57:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    57,
                    24,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:57:24Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    57,
                    24,
                    2,
                    225,
                    0
                ],
                "title": "Trapping, chaos and averaging in bubbling AdS spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trapping, chaos and averaging in bubbling AdS spaces"
                },
                "summary": "We discuss chaos and ensemble averaging in 1/2 BPS bubbling $AdS$ spaces of\nLin, Lunin and Maldacena (LLM) by studying trapped and escaping null geodesics\nand estimating their decay rates. We find typical chaotic scattering behavior\nand confirm the Pesin relation between escape rates, Lyapunov exponents and\nKolmogorov-Sinai entropy. On the other hand, for geodesics in coarse-grained\n(grayscale) LLM geometries (which exhibit a naked singularity) chaos is\nstrongly suppressed, which is consistent with orbits and escape rates averaged\nover microscopic backgrounds. Also the singularities in these grayscale\ngeometries produce an attractive potential and have some similarities to black\nhole throats trapping geodesics for a long time. Overall, averaging over the\nensembles of LLM geometries brings us closer toward the typical behavior of\ngeodesics in black hole backgrounds, but some important differences remain, in\nparticular the existence of a threshold timescale when the averaging fails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss chaos and ensemble averaging in 1/2 BPS bubbling $AdS$ spaces of\nLin, Lunin and Maldacena (LLM) by studying trapped and escaping null geodesics\nand estimating their decay rates. We find typical chaotic scattering behavior\nand confirm the Pesin relation between escape rates, Lyapunov exponents and\nKolmogorov-Sinai entropy. On the other hand, for geodesics in coarse-grained\n(grayscale) LLM geometries (which exhibit a naked singularity) chaos is\nstrongly suppressed, which is consistent with orbits and escape rates averaged\nover microscopic backgrounds. Also the singularities in these grayscale\ngeometries produce an attractive potential and have some similarities to black\nhole throats trapping geodesics for a long time. Overall, averaging over the\nensembles of LLM geometries brings us closer toward the typical behavior of\ngeodesics in black hole backgrounds, but some important differences remain, in\nparticular the existence of a threshold timescale when the averaging fails."
                },
                "authors": [
                    {
                        "name": "David Berenstein"
                    },
                    {
                        "name": "Mihailo ubrovi"
                    },
                    {
                        "name": "Vladan Djuki"
                    }
                ],
                "author_detail": {
                    "name": "Vladan Djuki"
                },
                "author": "Vladan Djuki",
                "arxiv_comment": "33 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09666v1",
                "updated": "2025-08-13T09:56:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    56,
                    8,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:56:08Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    56,
                    8,
                    2,
                    225,
                    0
                ],
                "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought\n  Distillation"
                },
                "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs."
                },
                "authors": [
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Qingyue Yuan"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01330v2",
                "updated": "2025-08-13T09:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    51,
                    20,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-03T13:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    9,
                    21,
                    0,
                    34,
                    0
                ],
                "title": "Accelerating Linear Recurrent Neural Networks for the Edge with\n  Unstructured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Linear Recurrent Neural Networks for the Edge with\n  Unstructured Sparsity"
                },
                "summary": "Linear recurrent neural networks enable powerful long-range sequence modeling\nwith constant memory usage and time-per-token during inference. These\narchitectures hold promise for streaming applications at the edge, but\ndeployment in resource-constrained environments requires hardware-aware\noptimizations to minimize latency and energy consumption. Unstructured sparsity\noffers a compelling solution, enabling substantial reductions in compute and\nmemory requirements--when accelerated by compatible hardware platforms. In this\npaper, we conduct a scaling study to investigate the Pareto front of\nperformance and efficiency across inference compute budgets. We find that\nhighly sparse linear RNNs consistently achieve better efficiency-performance\ntrade-offs than dense baselines, with 2x less compute and 36% less memory at\niso-accuracy. Our models achieve state-of-the-art results on a real-time\nstreaming task for audio denoising. By quantizing our sparse models to\nfixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic\nchip for real-time processing, we translate model compression into tangible\ngains of 42x lower latency and 149x lower energy consumption compared to a\ndense model on an edge GPU. Our findings showcase the transformative potential\nof unstructured sparsity, paving the way for highly efficient recurrent neural\nnetworks in real-world, resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear recurrent neural networks enable powerful long-range sequence modeling\nwith constant memory usage and time-per-token during inference. These\narchitectures hold promise for streaming applications at the edge, but\ndeployment in resource-constrained environments requires hardware-aware\noptimizations to minimize latency and energy consumption. Unstructured sparsity\noffers a compelling solution, enabling substantial reductions in compute and\nmemory requirements--when accelerated by compatible hardware platforms. In this\npaper, we conduct a scaling study to investigate the Pareto front of\nperformance and efficiency across inference compute budgets. We find that\nhighly sparse linear RNNs consistently achieve better efficiency-performance\ntrade-offs than dense baselines, with 2x less compute and 36% less memory at\niso-accuracy. Our models achieve state-of-the-art results on a real-time\nstreaming task for audio denoising. By quantizing our sparse models to\nfixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic\nchip for real-time processing, we translate model compression into tangible\ngains of 42x lower latency and 149x lower energy consumption compared to a\ndense model on an edge GPU. Our findings showcase the transformative potential\nof unstructured sparsity, paving the way for highly efficient recurrent neural\nnetworks in real-world, resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Alessandro Pierro"
                    },
                    {
                        "name": "Steven Abreu"
                    },
                    {
                        "name": "Jonathan Timcheck"
                    },
                    {
                        "name": "Philipp Stratmann"
                    },
                    {
                        "name": "Andreas Wild"
                    },
                    {
                        "name": "Sumit Bam Shrestha"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Bam Shrestha"
                },
                "author": "Sumit Bam Shrestha",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12843v3",
                "updated": "2025-08-13T09:50:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    50,
                    32,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-17T12:11:04Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    11,
                    4,
                    1,
                    352,
                    0
                ],
                "title": "SLTNet: Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLTNet: Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks"
                },
                "summary": "Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at most 9.06% and 9.39% mIoU,\nrespectively, with extremely 4.58x lower energy consumption and 114 FPS\ninference speed. Our code is open-sourced and available at\nhttps://github.com/longxianlei/SLTNet-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at most 9.06% and 9.39% mIoU,\nrespectively, with extremely 4.58x lower energy consumption and 114 FPS\ninference speed. Our code is open-sourced and available at\nhttps://github.com/longxianlei/SLTNet-v1.0."
                },
                "authors": [
                    {
                        "name": "Xianlei Long"
                    },
                    {
                        "name": "Xiaxin Zhu"
                    },
                    {
                        "name": "Fangming Guo"
                    },
                    {
                        "name": "Wanyi Zhang"
                    },
                    {
                        "name": "Qingyi Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Fuqiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Fuqiang Gu"
                },
                "author": "Fuqiang Gu",
                "arxiv_comment": "Accepted by IROS 2025 (2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11511v2",
                "updated": "2025-08-13T17:53:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    53,
                    18,
                    2,
                    225,
                    0
                ],
                "published": "2024-07-16T08:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    49,
                    35,
                    1,
                    198,
                    0
                ],
                "title": "Multi-Step Reasoning with Large Language Models, a Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning with Large Language Models, a Survey"
                },
                "summary": "Language models with billions of parameters exhibit in-context learning\nabilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks.\n  The research on LLM reasoning abilities started with the question whether\nLLMs can solve grade school math word problems, and has expanded to other tasks\nin the past few years. This paper reviews the field of multi-step reasoning\nwith LLMs. We propose a taxonomy that identifies different ways to generate,\nevaluate, and control multi-step reasoning. We provide an in-depth coverage of\ncore approaches and open problems, and we propose a research agenda for the\nnear future.\n  We find that multi-step reasoning approaches have progressed beyond math word\nproblems, and can now successfully solve challenges in logic, combinatorial\ngames, and robotics, sometimes by first generating code that is then executed\nby external tools. Many studies in multi-step methods are using reinforcement\nlearning for finetuning, external optimization loops, in context reinforcement\nlearning, and self-reflection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models with billions of parameters exhibit in-context learning\nabilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks.\n  The research on LLM reasoning abilities started with the question whether\nLLMs can solve grade school math word problems, and has expanded to other tasks\nin the past few years. This paper reviews the field of multi-step reasoning\nwith LLMs. We propose a taxonomy that identifies different ways to generate,\nevaluate, and control multi-step reasoning. We provide an in-depth coverage of\ncore approaches and open problems, and we propose a research agenda for the\nnear future.\n  We find that multi-step reasoning approaches have progressed beyond math word\nproblems, and can now successfully solve challenges in logic, combinatorial\ngames, and robotics, sometimes by first generating code that is then executed\nby external tools. Many studies in multi-step methods are using reinforcement\nlearning for finetuning, external optimization loops, in context reinforcement\nlearning, and self-reflection."
                },
                "authors": [
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Annie Wong"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Joost Broekens"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Back"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Back"
                },
                "author": "Thomas Back",
                "arxiv_comment": "revised version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09968v1",
                "updated": "2025-08-13T17:33:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    33,
                    37,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:33:37Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    33,
                    37,
                    2,
                    225,
                    0
                ],
                "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"
                },
                "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise"
                },
                "authors": [
                    {
                        "name": "Luca Eyring"
                    },
                    {
                        "name": "Shyamgopal Karthik"
                    },
                    {
                        "name": "Alexey Dosovitskiy"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Project page: https://noisehypernetworks.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09967v1",
                "updated": "2025-08-13T17:32:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    32,
                    42,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:32:42Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    32,
                    42,
                    2,
                    225,
                    0
                ],
                "title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image\n  Classification"
                },
                "summary": "Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC."
                },
                "authors": [
                    {
                        "name": "Tianqi Xiang"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Qixiang Zhang"
                    },
                    {
                        "name": "Xiaomeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomeng Li"
                },
                "author": "Xiaomeng Li",
                "arxiv_comment": "Accepted in MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06399v2",
                "updated": "2025-08-13T17:20:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    20,
                    50,
                    2,
                    225,
                    0
                ],
                "published": "2025-01-11T01:12:23Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    1,
                    12,
                    23,
                    5,
                    11,
                    0
                ],
                "title": "GenAI Confessions: Black-box Membership Inference for Generative Image\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI Confessions: Black-box Membership Inference for Generative Image\n  Models"
                },
                "summary": "From a simple text prompt, generative-AI image models can create stunningly\nrealistic and creative images bounded, it seems, by only our imagination. These\nmodels have achieved this remarkable feat thanks, in part, to the ingestion of\nbillions of images collected from nearly every corner of the internet. Many\ncreators have understandably expressed concern over how their intellectual\nproperty has been ingested without their permission or a mechanism to opt out\nof training. As a result, questions of fair use and copyright infringement have\nquickly emerged. We describe a method that allows us to determine if a model\nwas trained on a specific image or set of images. This method is\ncomputationally efficient and assumes no explicit knowledge of the model\narchitecture or weights (so-called black-box membership inference). We\nanticipate that this method will be crucial for auditing existing models and,\nlooking ahead, ensuring the fairer development and deployment of generative AI\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From a simple text prompt, generative-AI image models can create stunningly\nrealistic and creative images bounded, it seems, by only our imagination. These\nmodels have achieved this remarkable feat thanks, in part, to the ingestion of\nbillions of images collected from nearly every corner of the internet. Many\ncreators have understandably expressed concern over how their intellectual\nproperty has been ingested without their permission or a mechanism to opt out\nof training. As a result, questions of fair use and copyright infringement have\nquickly emerged. We describe a method that allows us to determine if a model\nwas trained on a specific image or set of images. This method is\ncomputationally efficient and assumes no explicit knowledge of the model\narchitecture or weights (so-called black-box membership inference). We\nanticipate that this method will be crucial for auditing existing models and,\nlooking ahead, ensuring the fairer development and deployment of generative AI\nmodels."
                },
                "authors": [
                    {
                        "name": "Matyas Bohacek"
                    },
                    {
                        "name": "Hany Farid"
                    }
                ],
                "author_detail": {
                    "name": "Hany Farid"
                },
                "author": "Hany Farid",
                "arxiv_comment": "https://genai-confessions.github.io",
                "arxiv_journal_ref": "ICCV-W 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09958v1",
                "updated": "2025-08-13T17:19:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    19,
                    41,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:19:41Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    19,
                    41,
                    2,
                    225,
                    0
                ],
                "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks"
                },
                "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Eddie Zhang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "arxiv_comment": "Submitted to AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03611v2",
                "updated": "2025-08-13T17:17:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    17,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-05T16:27:10Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    27,
                    10,
                    1,
                    217,
                    0
                ],
                "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and\n  Predictive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block: Balancing Load in LLM Serving with Context, Knowledge and\n  Predictive Scheduling"
                },
                "summary": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "12 pages, 8 figures excluding appendix. V1: Fix some typos and\n  grammar issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09956v2",
                "updated": "2025-08-14T01:29:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    1,
                    29,
                    55,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T17:17:17Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    17,
                    17,
                    2,
                    225,
                    0
                ],
                "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering"
                },
                "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology."
                },
                "authors": [
                    {
                        "name": "Fares Antaki"
                    },
                    {
                        "name": "David Mikhail"
                    },
                    {
                        "name": "Daniel Milad"
                    },
                    {
                        "name": "Danny A Mammo"
                    },
                    {
                        "name": "Sumit Sharma"
                    },
                    {
                        "name": "Sunil K Srivastava"
                    },
                    {
                        "name": "Bing Yu Chen"
                    },
                    {
                        "name": "Samir Touma"
                    },
                    {
                        "name": "Mertcan Sevgi"
                    },
                    {
                        "name": "Jonathan El-Khoury"
                    },
                    {
                        "name": "Pearse A Keane"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yih Chung Tham"
                    },
                    {
                        "name": "Renaud Duval"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Duval"
                },
                "author": "Renaud Duval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09949v1",
                "updated": "2025-08-13T17:08:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    8,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:08:22Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    8,
                    22,
                    2,
                    225,
                    0
                ],
                "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning"
                },
                "summary": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance."
                },
                "authors": [
                    {
                        "name": "Trevine Oorloff"
                    },
                    {
                        "name": "Vishwanath Sindagi"
                    },
                    {
                        "name": "Wele Gedara Chaminda Bandara"
                    },
                    {
                        "name": "Ali Shafahi"
                    },
                    {
                        "name": "Amin Ghiasi"
                    },
                    {
                        "name": "Charan Prakash"
                    },
                    {
                        "name": "Reza Ardekani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Ardekani"
                },
                "author": "Reza Ardekani",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09945v1",
                "updated": "2025-08-13T17:00:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    0,
                    44,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T17:00:44Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    0,
                    44,
                    2,
                    225,
                    0
                ],
                "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models"
                },
                "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets."
                },
                "authors": [
                    {
                        "name": "Lingjie Jiang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09937v1",
                "updated": "2025-08-13T16:42:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    42,
                    1,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:42:01Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    42,
                    1,
                    2,
                    225,
                    0
                ],
                "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions."
                },
                "authors": [
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Momin Abbas"
                    },
                    {
                        "name": "Maysa Malfiza Garcia de Macedo"
                    },
                    {
                        "name": "Marcelo Carpinette Grave"
                    },
                    {
                        "name": "Luan Soares de Souza"
                    },
                    {
                        "name": "Tiago Machado"
                    },
                    {
                        "name": "Rogerio A de Paula"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Heloisa Caroline de Souza Pereira Candello"
                    },
                    {
                        "name": "Rebecka Nordenlow"
                    },
                    {
                        "name": "Aminat Adebiyi"
                    }
                ],
                "author_detail": {
                    "name": "Aminat Adebiyi"
                },
                "author": "Aminat Adebiyi",
                "arxiv_doi": "10.48550/arXiv.2508.09937",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2508.09937",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In submission",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21817v3",
                "updated": "2025-08-14T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    16,
                    52,
                    3,
                    226,
                    0
                ],
                "published": "2025-07-29T13:51:46Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    13,
                    51,
                    46,
                    1,
                    210,
                    0
                ],
                "title": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on\n  Vulnerability Datasets Detect Top 25 CWE Weaknesses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of Distribution, Out of Luck: How Well Can LLMs Trained on\n  Vulnerability Datasets Detect Top 25 CWE Weaknesses?"
                },
                "summary": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from the same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 33% when evaluated on independent data, with some performing close to\nrandom guessing. To address these limitations, we present a three-part\nsolution. First, we introduce a manually curated test dataset, BenchVul,\ncovering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a\nhigh-quality training dataset, TitanVul, comprising 38,863 functions by\naggregating seven public sources and applying deduplication and validation\nusing a novel multi-agent LLM framework. Third, we propose a Realistic\nVulnerability Generation (RVG) framework, which synthesizes context-aware\nvulnerability examples for underrepresented but critical CWE types through\nsimulated development workflows. Our evaluation shows the strengths of each\ncomponent in closing the generalization gap. First, BenchVul shows the\nlimitations of self-testing: models trained on existing datasets, such as\nBigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to\n0.519 and from 0.713 to 0.607). Second, training models on TitanVul\ndemonstrates improved generalization, with model performance increasing from\n0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul.\nThird, supplementing TitanVul with RVG-generated data yields further gains,\nincreasing model performance by 14.0% to 0.874.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated vulnerability detection research has made substantial progress, yet\nits real-world impact remains limited. Current vulnerability datasets suffer\nfrom issues including label inaccuracy rates of 20-71%, extensive duplication,\nand poor coverage of critical CWE types. These issues create a significant\n\"generalization gap\" where models achieve misleading self-testing performance\n(measured on held-out data from the same dataset for training) by exploiting\nspurious correlations rather than learning true vulnerability patterns. Our\nanalysis reveals that many models experience substantial performance drops of\nup to 33% when evaluated on independent data, with some performing close to\nrandom guessing. To address these limitations, we present a three-part\nsolution. First, we introduce a manually curated test dataset, BenchVul,\ncovering the MITRE Top 25 Most Dangerous CWEs. Second, we construct a\nhigh-quality training dataset, TitanVul, comprising 38,863 functions by\naggregating seven public sources and applying deduplication and validation\nusing a novel multi-agent LLM framework. Third, we propose a Realistic\nVulnerability Generation (RVG) framework, which synthesizes context-aware\nvulnerability examples for underrepresented but critical CWE types through\nsimulated development workflows. Our evaluation shows the strengths of each\ncomponent in closing the generalization gap. First, BenchVul shows the\nlimitations of self-testing: models trained on existing datasets, such as\nBigVul and CVEfixes, experience performance drops on BenchVul (from 0.776 to\n0.519 and from 0.713 to 0.607). Second, training models on TitanVul\ndemonstrates improved generalization, with model performance increasing from\n0.584 when evaluated on the same dataset to 0.767 when tested on BenchVul.\nThird, supplementing TitanVul with RVG-generated data yields further gains,\nincreasing model performance by 14.0% to 0.874."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ngoc Tan Bui"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Chengran Yang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Jinfeng Jiang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Chiok Yew Ho"
                    },
                    {
                        "name": "Jie Tan"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Yide Yin"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09932v2",
                "updated": "2025-08-14T13:25:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    25,
                    18,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T16:33:02Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    33,
                    2,
                    2,
                    225,
                    0
                ],
                "title": "Mathematical Computation and Reasoning Errors by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Computation and Reasoning Errors by Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Edith Aurora Graf"
                    }
                ],
                "author_detail": {
                    "name": "Edith Aurora Graf"
                },
                "author": "Edith Aurora Graf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09911v1",
                "updated": "2025-08-13T16:07:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    7,
                    45,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:07:45Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    7,
                    45,
                    2,
                    225,
                    0
                ],
                "title": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous\n  Deliberation on Perspectivist Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous\n  Deliberation on Perspectivist Data"
                },
                "summary": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets."
                },
                "authors": [
                    {
                        "name": "Malik Khadar"
                    },
                    {
                        "name": "Daniel Runningen"
                    },
                    {
                        "name": "Julia Tang"
                    },
                    {
                        "name": "Stevie Chancellor"
                    },
                    {
                        "name": "Harmanpreet Kaur"
                    }
                ],
                "author_detail": {
                    "name": "Harmanpreet Kaur"
                },
                "author": "Harmanpreet Kaur",
                "arxiv_doi": "10.1145/3757707",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757707",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear at CSCW 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09904v1",
                "updated": "2025-08-13T16:02:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    2,
                    55,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T16:02:55Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    16,
                    2,
                    55,
                    2,
                    225,
                    0
                ],
                "title": "Beyond Nave Prompting: Strategies for Improved Zero-shot\n  Context-aided Forecasting with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Nave Prompting: Strategies for Improved Zero-shot\n  Context-aided Forecasting with LLMs"
                },
                "summary": "Forecasting in real-world settings requires models to integrate not only\nhistorical data but also relevant contextual information, often available in\ntextual form. While recent work has shown that large language models (LLMs) can\nbe effective context-aided forecasters via na\\\"ive direct prompting, their full\npotential remains underexplored. We address this gap with 4 strategies,\nproviding new insights into the zero-shot capabilities of LLMs in this setting.\nReDP improves interpretability by eliciting explicit reasoning traces, allowing\nus to assess the model's reasoning over the context independently from its\nforecast accuracy. CorDP leverages LLMs solely to refine existing forecasts\nwith context, enhancing their applicability in real-world forecasting\npipelines. IC-DP proposes embedding historical examples of context-aided\nforecasting tasks in the prompt, substantially improving accuracy even for the\nlargest models. Finally, RouteDP optimizes resource efficiency by using LLMs to\nestimate task difficulty, and routing the most challenging tasks to larger\nmodels. Evaluated on different kinds of context-aided forecasting tasks from\nthe CiK benchmark, our strategies demonstrate distinct benefits over na\\\"ive\nprompting across LLMs of different sizes and families. These results open the\ndoor to further simple yet effective improvements in LLM-based context-aided\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting in real-world settings requires models to integrate not only\nhistorical data but also relevant contextual information, often available in\ntextual form. While recent work has shown that large language models (LLMs) can\nbe effective context-aided forecasters via na\\\"ive direct prompting, their full\npotential remains underexplored. We address this gap with 4 strategies,\nproviding new insights into the zero-shot capabilities of LLMs in this setting.\nReDP improves interpretability by eliciting explicit reasoning traces, allowing\nus to assess the model's reasoning over the context independently from its\nforecast accuracy. CorDP leverages LLMs solely to refine existing forecasts\nwith context, enhancing their applicability in real-world forecasting\npipelines. IC-DP proposes embedding historical examples of context-aided\nforecasting tasks in the prompt, substantially improving accuracy even for the\nlargest models. Finally, RouteDP optimizes resource efficiency by using LLMs to\nestimate task difficulty, and routing the most challenging tasks to larger\nmodels. Evaluated on different kinds of context-aided forecasting tasks from\nthe CiK benchmark, our strategies demonstrate distinct benefits over na\\\"ive\nprompting across LLMs of different sizes and families. These results open the\ndoor to further simple yet effective improvements in LLM-based context-aided\nforecasting."
                },
                "authors": [
                    {
                        "name": "Arjun Ashok"
                    },
                    {
                        "name": "Andrew Robert Williams"
                    },
                    {
                        "name": "Vincent Zhihao Zheng"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Alexandre Drouin"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Drouin"
                },
                "author": "Alexandre Drouin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09897v1",
                "updated": "2025-08-13T15:56:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    56,
                    16,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:56:16Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    56,
                    16,
                    2,
                    225,
                    0
                ],
                "title": "Finetuning Large Language Model as an Effective Symbolic Regressor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning Large Language Model as an Effective Symbolic Regressor"
                },
                "summary": "Deriving governing equations from observational data, known as Symbolic\nRegression (SR), is a cornerstone of scientific discovery. Large Language\nModels (LLMs) have shown promise in this task by leveraging their vast\ncross-disciplinary scientific knowledge. However, existing LLM-based methods\nprimarily rely on direct inference or prompt engineering, often requiring\nexcessive inference iterations to converge on correct formulas or failing to\ntreating complex equation targets. These limitations in effectiveness and\ngeneralization stem from an inherent tension between pre-trained LLMs'\nproficiency in approximate reasoning and the high-precision demands of SR\ntasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR\ncapability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning\nremains a critical barrier. We thus introduce SymbArena, specifically\nengineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse\nequations formulated as corpora of 1.83 billion tokens for LLM utilization,\nenabling effective training and inference. Further, SymbArena proposes a\nheuristics metric to precisely quantify form-level consistency, going beyond\nexisting SR numerical-oriented evaluation strategies. With this benchmark, we\nexplore mainstream LLM fine-tuning techniques for SR tasks and establish\nSymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental\nresults validate SymbolicChat as the first LLM to exceed traditional numerical\nmethods in both numerical precision and symbolic form accuracy, outperforming\nthe second-best LLM baseline with improvements of 2-fold gains in R2 score and\n8.37% in form-level consistency score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving governing equations from observational data, known as Symbolic\nRegression (SR), is a cornerstone of scientific discovery. Large Language\nModels (LLMs) have shown promise in this task by leveraging their vast\ncross-disciplinary scientific knowledge. However, existing LLM-based methods\nprimarily rely on direct inference or prompt engineering, often requiring\nexcessive inference iterations to converge on correct formulas or failing to\ntreating complex equation targets. These limitations in effectiveness and\ngeneralization stem from an inherent tension between pre-trained LLMs'\nproficiency in approximate reasoning and the high-precision demands of SR\ntasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR\ncapability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning\nremains a critical barrier. We thus introduce SymbArena, specifically\nengineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse\nequations formulated as corpora of 1.83 billion tokens for LLM utilization,\nenabling effective training and inference. Further, SymbArena proposes a\nheuristics metric to precisely quantify form-level consistency, going beyond\nexisting SR numerical-oriented evaluation strategies. With this benchmark, we\nexplore mainstream LLM fine-tuning techniques for SR tasks and establish\nSymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental\nresults validate SymbolicChat as the first LLM to exceed traditional numerical\nmethods in both numerical precision and symbolic form accuracy, outperforming\nthe second-best LLM baseline with improvements of 2-fold gains in R2 score and\n8.37% in form-level consistency score."
                },
                "authors": [
                    {
                        "name": "Yingfan Hua"
                    },
                    {
                        "name": "Ruikun Li"
                    },
                    {
                        "name": "Jun Yao"
                    },
                    {
                        "name": "Guohang Zhuang"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09893v1",
                "updated": "2025-08-13T15:51:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    51,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:51:05Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    51,
                    5,
                    2,
                    225,
                    0
                ],
                "title": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"
                },
                "summary": "Regulatory compliance question answering (QA) requires precise, verifiable\ninformation, and domain-specific expertise, posing challenges for Large\nLanguage Models (LLMs). In this work, we present a novel multi-agent framework\nthat integrates a Knowledge Graph (KG) of Regulatory triplets with\nRetrieval-Augmented Generation (RAG) to address these demands. First, agents\nbuild and maintain an ontology-free KG by extracting subject--predicate--object\n(SPO) triplets from regulatory documents and systematically cleaning,\nnormalizing, deduplicating, and updating them. Second, these triplets are\nembedded and stored along with their corresponding textual sections and\nmetadata in a single enriched vector database, allowing for both graph-based\nreasoning and efficient information retrieval. Third, an orchestrated agent\npipeline leverages triplet-level retrieval for question answering, ensuring\nhigh semantic alignment between user queries and the factual\n\"who-did-what-to-whom\" core captured by the graph. Our hybrid system\noutperforms conventional methods in complex regulatory queries, ensuring\nfactual correctness with embedded triplets, enabling traceability through a\nunified vector database, and enhancing understanding through subgraph\nvisualization, providing a robust foundation for compliance-driven and broader\naudit-focused applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regulatory compliance question answering (QA) requires precise, verifiable\ninformation, and domain-specific expertise, posing challenges for Large\nLanguage Models (LLMs). In this work, we present a novel multi-agent framework\nthat integrates a Knowledge Graph (KG) of Regulatory triplets with\nRetrieval-Augmented Generation (RAG) to address these demands. First, agents\nbuild and maintain an ontology-free KG by extracting subject--predicate--object\n(SPO) triplets from regulatory documents and systematically cleaning,\nnormalizing, deduplicating, and updating them. Second, these triplets are\nembedded and stored along with their corresponding textual sections and\nmetadata in a single enriched vector database, allowing for both graph-based\nreasoning and efficient information retrieval. Third, an orchestrated agent\npipeline leverages triplet-level retrieval for question answering, ensuring\nhigh semantic alignment between user queries and the factual\n\"who-did-what-to-whom\" core captured by the graph. Our hybrid system\noutperforms conventional methods in complex regulatory queries, ensuring\nfactual correctness with embedded triplets, enabling traceability through a\nunified vector database, and enhancing understanding through subgraph\nvisualization, providing a robust foundation for compliance-driven and broader\naudit-focused applications."
                },
                "authors": [
                    {
                        "name": "Bhavik Agarwal"
                    },
                    {
                        "name": "Hemant Sunil Jomraj"
                    },
                    {
                        "name": "Simone Kaplunov"
                    },
                    {
                        "name": "Jack Krolick"
                    },
                    {
                        "name": "Viktoria Rojkova"
                    }
                ],
                "author_detail": {
                    "name": "Viktoria Rojkova"
                },
                "author": "Viktoria Rojkova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23077v3",
                "updated": "2025-08-13T15:48:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    48,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2025-03-29T13:27:46Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    13,
                    27,
                    46,
                    5,
                    88,
                    0
                ],
                "title": "Efficient Inference for Large Reasoning Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Large Reasoning Models: A Survey"
                },
                "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in solving complex tasks. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. The overview structure of\nthis paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from reasoning scenarios, object functions, and performance \\&\nefficiency aspects. Besides, we present open challenges in this field,\nincluding human-centric controllable reasoning, trade-off between\ninterpretability and efficiency of reasoning, ensuring the safety of efficient\nreasoning, and broader applications of efficient reasoning. In addition, we\nhighlight key insights for enhancing LRMs' inference efficiency via techniques\nsuch as model merging, new architectures, and agent routers. We hope this work\nserves as a valuable guide, helping researchers overcome challenges in this\nvibrant field. A collection of efficient reasoning methods for LRMs (papers and\ncodes) is provided at this link:\nhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in solving complex tasks. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. The overview structure of\nthis paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from reasoning scenarios, object functions, and performance \\&\nefficiency aspects. Besides, we present open challenges in this field,\nincluding human-centric controllable reasoning, trade-off between\ninterpretability and efficiency of reasoning, ensuring the safety of efficient\nreasoning, and broader applications of efficient reasoning. In addition, we\nhighlight key insights for enhancing LRMs' inference efficiency via techniques\nsuch as model merging, new architectures, and agent routers. We hope this work\nserves as a valuable guide, helping researchers overcome challenges in this\nvibrant field. A collection of efficient reasoning methods for LRMs (papers and\ncodes) is provided at this link:\nhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs."
                },
                "authors": [
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Ruihan Gong"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Stan Z. Li"
                    },
                    {
                        "name": "Keqin Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqin Li"
                },
                "author": "Keqin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09889v1",
                "updated": "2025-08-13T15:46:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    46,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:46:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    46,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving"
                },
                "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems."
                },
                "authors": [
                    {
                        "name": "Zhitian Xie"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Chengyue Yu"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09883v1",
                "updated": "2025-08-13T15:32:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    32,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:32:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    32,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Beyond Scaling Law: A Data-Efficient Distillation Framework for\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scaling Law: A Data-Efficient Distillation Framework for\n  Reasoning"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable reasoning capabilities in\ntasks such as algorithmic coding and mathematical problem-solving. Recent\nmethods have improved reasoning through expanded corpus and multistage training\ncombining reinforcement learning and supervised fine-tuning. Although some\nmethods suggest that small but targeted dataset can incentivize reasoning via\nonly distillation, a reasoning scaling laws is still taking shape, increasing\ncomputational costs. To address this, we propose a data-efficient distillation\nframework (DED) that optimizes the Pareto frontier of reasoning distillation.\nInspired by the on-policy learning and diverse roll-out strategies of\nreinforcement learning, the key idea of our approach is threefold: (1) We\nidentify that benchmark scores alone do not determine an effective teacher\nmodel. Through comprehensive comparisons of leading reasoning LLMs, we develop\na method to select an optimal teacher model. (2) While scaling distillation can\nenhance reasoning, it often degrades out-of-domain performance. A carefully\ncurated, smaller corpus achieves a balanced trade-off between in-domain and\nout-of-domain capabilities. (3) Diverse reasoning trajectories encourage the\nstudent model to develop robust reasoning skills. We validate our method\nthrough evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and\ncode generation (LiveCodeBench), achieving state-of-the-art results with only\n0.8k carefully curated examples, bypassing the need for extensive scaling. Our\nsystematic analysis demonstrates that DED outperforms existing methods by\nconsidering factors beyond superficial hardness, token length, or teacher model\ncapability. This work offers a practical and efficient pathway to advanced\nreasoning while preserving general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable reasoning capabilities in\ntasks such as algorithmic coding and mathematical problem-solving. Recent\nmethods have improved reasoning through expanded corpus and multistage training\ncombining reinforcement learning and supervised fine-tuning. Although some\nmethods suggest that small but targeted dataset can incentivize reasoning via\nonly distillation, a reasoning scaling laws is still taking shape, increasing\ncomputational costs. To address this, we propose a data-efficient distillation\nframework (DED) that optimizes the Pareto frontier of reasoning distillation.\nInspired by the on-policy learning and diverse roll-out strategies of\nreinforcement learning, the key idea of our approach is threefold: (1) We\nidentify that benchmark scores alone do not determine an effective teacher\nmodel. Through comprehensive comparisons of leading reasoning LLMs, we develop\na method to select an optimal teacher model. (2) While scaling distillation can\nenhance reasoning, it often degrades out-of-domain performance. A carefully\ncurated, smaller corpus achieves a balanced trade-off between in-domain and\nout-of-domain capabilities. (3) Diverse reasoning trajectories encourage the\nstudent model to develop robust reasoning skills. We validate our method\nthrough evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and\ncode generation (LiveCodeBench), achieving state-of-the-art results with only\n0.8k carefully curated examples, bypassing the need for extensive scaling. Our\nsystematic analysis demonstrates that DED outperforms existing methods by\nconsidering factors beyond superficial hardness, token length, or teacher model\ncapability. This work offers a practical and efficient pathway to advanced\nreasoning while preserving general capabilities."
                },
                "authors": [
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Xiaoguang Jiang"
                    },
                    {
                        "name": "Huiyang Li"
                    },
                    {
                        "name": "Jucai Zhai"
                    },
                    {
                        "name": "Dengfeng Liu"
                    },
                    {
                        "name": "Qiaobo Hao"
                    },
                    {
                        "name": "Huang Liu"
                    },
                    {
                        "name": "Zhiguo Yang"
                    },
                    {
                        "name": "Ji Xie"
                    },
                    {
                        "name": "Ninglun Gu"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Kailai Zhang"
                    },
                    {
                        "name": "Yelun Bao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09874v1",
                "updated": "2025-08-13T15:16:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    16,
                    29,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:16:29Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    16,
                    29,
                    2,
                    225,
                    0
                ],
                "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain."
                },
                "authors": [
                    {
                        "name": "Jiaqi Cao"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Rubin Wei"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09865v1",
                "updated": "2025-08-13T15:01:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    1,
                    59,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T15:01:59Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    1,
                    59,
                    2,
                    225,
                    0
                ],
                "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource\n  Urdu Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource\n  Urdu Transcription"
                },
                "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems."
                },
                "authors": [
                    {
                        "name": "Abdul Rehman Antall"
                    },
                    {
                        "name": "Naveed Akhtar"
                    }
                ],
                "author_detail": {
                    "name": "Naveed Akhtar"
                },
                "author": "Naveed Akhtar",
                "arxiv_comment": "8 pages, 3 figures, 1 table, including references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06950v3",
                "updated": "2025-08-13T14:59:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    59,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-09T11:56:59Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    56,
                    59,
                    5,
                    221,
                    0
                ],
                "title": "Large Language Models Do Not Simulate Human Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Do Not Simulate Human Psychology"
                },
                "summary": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application."
                },
                "authors": [
                    {
                        "name": "Sarah Schrder"
                    },
                    {
                        "name": "Thekla Morgenroth"
                    },
                    {
                        "name": "Ulrike Kuhl"
                    },
                    {
                        "name": "Valerie Vaquet"
                    },
                    {
                        "name": "Benjamin Paaen"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Paaen"
                },
                "author": "Benjamin Paaen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19147v2",
                "updated": "2025-08-13T14:59:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    59,
                    1,
                    2,
                    225,
                    0
                ],
                "published": "2024-11-28T13:46:49Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    46,
                    49,
                    3,
                    333,
                    0
                ],
                "title": "Spectrum Efficiency and Processing Latency Trade-offs in Panel-Based LIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectrum Efficiency and Processing Latency Trade-offs in Panel-Based LIS"
                },
                "summary": "The next generation wireless systems will face stringent new requirements,\nincluding ultra-low latency, high data rates and enhanced reliability. Large\nIntelligent Surfaces, is one proposed solution that has the potential to solve\nthese high demands. The real-life deployment of such systems involves different\ndesign considerations with non-trivial trade-offs. This paper investigates the\ntrade-off between spectral efficiency and processing latency, considering\ndifferent antenna distribution schemes and detection algorithms. A latency\nmodel for the physical layer processing has been developed, using real FPGA and\napplication-specific instruction processor (ASIP) hardware implementation\nresults. Simulation results using an indoor environment show that distributing\nantennas throughout the scenario improves overall reliability, while the impact\nfrom this on latency is limited both when using zero-forcing (ZF) and Minimum\nMean Square Error (MMSE) detection. Changing the detection algorithm to\nmaximum-ratio combining (MRC) from ZF or MMSE, however, reduces the latency\nsignificantly, even if a larger number of antennas are needed to achieve a\nsimilar spectrum efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next generation wireless systems will face stringent new requirements,\nincluding ultra-low latency, high data rates and enhanced reliability. Large\nIntelligent Surfaces, is one proposed solution that has the potential to solve\nthese high demands. The real-life deployment of such systems involves different\ndesign considerations with non-trivial trade-offs. This paper investigates the\ntrade-off between spectral efficiency and processing latency, considering\ndifferent antenna distribution schemes and detection algorithms. A latency\nmodel for the physical layer processing has been developed, using real FPGA and\napplication-specific instruction processor (ASIP) hardware implementation\nresults. Simulation results using an indoor environment show that distributing\nantennas throughout the scenario improves overall reliability, while the impact\nfrom this on latency is limited both when using zero-forcing (ZF) and Minimum\nMean Square Error (MMSE) detection. Changing the detection algorithm to\nmaximum-ratio combining (MRC) from ZF or MMSE, however, reduces the latency\nsignificantly, even if a larger number of antennas are needed to achieve a\nsimilar spectrum efficiency."
                },
                "authors": [
                    {
                        "name": "Lina Tinnerberg"
                    },
                    {
                        "name": "Dumitra Iancu"
                    },
                    {
                        "name": "Ove Edfors"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Juan Vidal Alegra"
                    }
                ],
                "author_detail": {
                    "name": "Juan Vidal Alegra"
                },
                "author": "Juan Vidal Alegra",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication,\n  copyright information may be affected upon publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09857v1",
                "updated": "2025-08-13T14:49:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    49,
                    54,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:49:54Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    49,
                    54,
                    2,
                    225,
                    0
                ],
                "title": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video\n  VAE Train Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video\n  VAE Train Better"
                },
                "summary": "Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection."
                },
                "authors": [
                    {
                        "name": "Yupeng Zhou"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Ziheng Ouyang"
                    },
                    {
                        "name": "Yuming Chen"
                    },
                    {
                        "name": "Ruoyi Du"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Qibin Hou"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Hou"
                },
                "author": "Qibin Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09848v2",
                "updated": "2025-08-14T02:08:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    2,
                    8,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-13T14:28:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    28,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts"
                },
                "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Tsz Ting Chung"
                    },
                    {
                        "name": "Chulun Zhou"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Haoshu Lu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09834v1",
                "updated": "2025-08-13T14:13:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    13,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:13:46Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    13,
                    46,
                    2,
                    225,
                    0
                ],
                "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiaoyu Mo"
                    },
                    {
                        "name": "Daizong Liu"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Wenliang Chen"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09832v1",
                "updated": "2025-08-13T14:07:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    7,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T14:07:05Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    7,
                    5,
                    2,
                    225,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models in Fine-Grained Review\n  Comment Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models in Fine-Grained Review\n  Comment Classification"
                },
                "summary": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process."
                },
                "authors": [
                    {
                        "name": "Linh Nguyen"
                    },
                    {
                        "name": "Chunhua Liu"
                    },
                    {
                        "name": "Hong Yi Lin"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    }
                ],
                "author_detail": {
                    "name": "Patanamon Thongtanunam"
                },
                "author": "Patanamon Thongtanunam",
                "arxiv_comment": "Accepted at 2025 IEEE International Conference on Source Code\n  Analysis & Manipulation (SCAM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09820v1",
                "updated": "2025-08-13T13:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    44,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    44,
                    2,
                    225,
                    0
                ],
                "title": "Provable In-Context Vector Arithmetic via Retrieving Task Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable In-Context Vector Arithmetic via Retrieving Task Concepts"
                },
                "summary": "In-context learning (ICL) has garnered significant attention for its ability\nto grasp functions/tasks from demonstrations. Recent studies suggest the\npresence of a latent task/function vector in LLMs during ICL. Merullo et al.\n(2024) showed that LLMs leverage this vector alongside the residual stream for\nWord2Vec-like vector arithmetic, solving factual-recall ICL tasks.\nAdditionally, recent work empirically highlighted the key role of\nQuestion-Answer data in enhancing factual-recall capabilities. Despite these\ninsights, a theoretical explanation remains elusive. To move one step forward,\nwe propose a theoretical framework building on empirically grounded\nhierarchical concept modeling. We develop an optimization theory, showing how\nnonlinear residual transformers trained via gradient descent on cross-entropy\nloss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss\nconvergence and show the strong generalization, including robustness to concept\nrecombination and distribution shifts. These results elucidate the advantages\nof transformers over static embedding predecessors. Empirical simulations\ncorroborate our theoretical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has garnered significant attention for its ability\nto grasp functions/tasks from demonstrations. Recent studies suggest the\npresence of a latent task/function vector in LLMs during ICL. Merullo et al.\n(2024) showed that LLMs leverage this vector alongside the residual stream for\nWord2Vec-like vector arithmetic, solving factual-recall ICL tasks.\nAdditionally, recent work empirically highlighted the key role of\nQuestion-Answer data in enhancing factual-recall capabilities. Despite these\ninsights, a theoretical explanation remains elusive. To move one step forward,\nwe propose a theoretical framework building on empirically grounded\nhierarchical concept modeling. We develop an optimization theory, showing how\nnonlinear residual transformers trained via gradient descent on cross-entropy\nloss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss\nconvergence and show the strong generalization, including robustness to concept\nrecombination and distribution shifts. These results elucidate the advantages\nof transformers over static embedding predecessors. Empirical simulations\ncorroborate our theoretical insights."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    },
                    {
                        "name": "Taiji Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Taiji Suzuki"
                },
                "author": "Taiji Suzuki",
                "arxiv_comment": "Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09818v1",
                "updated": "2025-08-13T13:54:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    16,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:16Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    16,
                    2,
                    225,
                    0
                ],
                "title": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior\n  Understanding from Motion and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior\n  Understanding from Motion and Video"
                },
                "summary": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation."
                },
                "authors": [
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md Yeasin Rahat"
                    },
                    {
                        "name": "Nafiz Fahad"
                    },
                    {
                        "name": "Abir Ahmed"
                    },
                    {
                        "name": "Liew Tze Hui"
                    }
                ],
                "author_detail": {
                    "name": "Liew Tze Hui"
                },
                "author": "Liew Tze Hui",
                "arxiv_comment": "Accepted in ICCVDM '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09815v1",
                "updated": "2025-08-13T13:47:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    47,
                    55,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:47:55Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    47,
                    55,
                    2,
                    225,
                    0
                ],
                "title": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights\n  from Multi-Agent Security Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights\n  from Multi-Agent Security Research"
                },
                "summary": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments."
                },
                "authors": [
                    {
                        "name": "Klaudia Krawiecka"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14368v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14368v5",
                "updated": "2025-08-13T13:44:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    44,
                    46,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-18T22:04:56Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    4,
                    56,
                    2,
                    353,
                    0
                ],
                "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation"
                },
                "summary": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Francis Ferraro"
                    }
                ],
                "author_detail": {
                    "name": "Francis Ferraro"
                },
                "author": "Francis Ferraro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14368v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14368v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03012v2",
                "updated": "2025-08-13T13:42:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    42,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-01-06T13:37:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    37,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Analyzing Finetuning Representation Shift for Multimodal LLMs Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Finetuning Representation Shift for Multimodal LLMs Steering"
                },
                "summary": "Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in\nunderstanding multimodal inputs. However, understanding and interpreting the\nbehavior of such complex models is a challenging task, not to mention the\ndynamic shifts that may occur during fine-tuning, or due to covariate shift\nbetween datasets. In this work, we apply concept-level analysis towards MLLM\nunderstanding. More specifically, we propose to map hidden states to\ninterpretable visual and textual concepts. This enables us to more efficiently\ncompare certain semantic dynamics, such as the shift from an original and\nfine-tuned model, revealing concept alteration and potential biases that may\noccur during fine-tuning. We also demonstrate the use of shift vectors to\ncapture these concepts changes. These shift vectors allow us to recover\nfine-tuned concepts by applying simple, computationally inexpensive additive\nconcept shifts in the original model. Finally, our findings also have direct\napplications for MLLM steering, which can be used for model debiasing as well\nas enforcing safety in MLLM output. All in all, we propose a novel,\ntraining-free, ready-to-use framework for MLLM behavior interpretability and\ncontrol. Our implementation is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in\nunderstanding multimodal inputs. However, understanding and interpreting the\nbehavior of such complex models is a challenging task, not to mention the\ndynamic shifts that may occur during fine-tuning, or due to covariate shift\nbetween datasets. In this work, we apply concept-level analysis towards MLLM\nunderstanding. More specifically, we propose to map hidden states to\ninterpretable visual and textual concepts. This enables us to more efficiently\ncompare certain semantic dynamics, such as the shift from an original and\nfine-tuned model, revealing concept alteration and potential biases that may\noccur during fine-tuning. We also demonstrate the use of shift vectors to\ncapture these concepts changes. These shift vectors allow us to recover\nfine-tuned concepts by applying simple, computationally inexpensive additive\nconcept shifts in the original model. Finally, our findings also have direct\napplications for MLLM steering, which can be used for model debiasing as well\nas enforcing safety in MLLM output. All in all, we propose a novel,\ntraining-free, ready-to-use framework for MLLM behavior interpretability and\ncontrol. Our implementation is publicly available."
                },
                "authors": [
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "ICCV 2025. The first three authors contributed equally. Project page\n  and code: https://pegah-\n  kh.github.io/projects/lmm-finetuning-analysis-and-steering/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09959v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09959v5",
                "updated": "2025-08-14T09:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    16,
                    33,
                    3,
                    226,
                    0
                ],
                "published": "2023-08-19T09:27:46Z",
                "published_parsed": [
                    2023,
                    8,
                    19,
                    9,
                    27,
                    46,
                    5,
                    231,
                    0
                ],
                "title": "Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth\n  Reservations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth\n  Reservations"
                },
                "summary": "To realize the long-standing vision of providing quality-of-service (QoS)\nguarantees on a public Internet, this paper introduces Hummingbird: a\nlightweight QoS-system that provides fine-grained inter-domain reservations for\nend hosts.\n  Hummingbird enables flexible and composable reservations with end-to-end\nguarantees, and addresses an often overlooked, but crucial, aspect of\nbandwidth-reservation systems: incentivization of network providers.\nHummingbird represents bandwidth reservations as tradable assets, allowing\nmarkets to emerge. These markets then ensure fair and efficient resource\nallocation and encourage deployment by remunerating providers. This\nincentivization is facilitated by decoupling reservations from network\nidentities, which enables novel control-plane mechanisms and allows the design\nof a control plane based on smart contracts.\n  Hummingbird also provides an efficient reservation data plane, which\nstreamlines the processing on routers and thus simplifies the implementation,\ndeployment, and traffic policing, while maintaining robust security properties.\nOur prototype implementation demonstrates the efficiency and scalability of\nHummingbird's asset-based control plane, and our high-speed software\nimplementation can fill a 160 Gbps link with Hummingbird packets on commodity\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To realize the long-standing vision of providing quality-of-service (QoS)\nguarantees on a public Internet, this paper introduces Hummingbird: a\nlightweight QoS-system that provides fine-grained inter-domain reservations for\nend hosts.\n  Hummingbird enables flexible and composable reservations with end-to-end\nguarantees, and addresses an often overlooked, but crucial, aspect of\nbandwidth-reservation systems: incentivization of network providers.\nHummingbird represents bandwidth reservations as tradable assets, allowing\nmarkets to emerge. These markets then ensure fair and efficient resource\nallocation and encourage deployment by remunerating providers. This\nincentivization is facilitated by decoupling reservations from network\nidentities, which enables novel control-plane mechanisms and allows the design\nof a control plane based on smart contracts.\n  Hummingbird also provides an efficient reservation data plane, which\nstreamlines the processing on routers and thus simplifies the implementation,\ndeployment, and traffic policing, while maintaining robust security properties.\nOur prototype implementation demonstrates the efficiency and scalability of\nHummingbird's asset-based control plane, and our high-speed software\nimplementation can fill a 160 Gbps link with Hummingbird packets on commodity\nhardware."
                },
                "authors": [
                    {
                        "name": "Karl Wst"
                    },
                    {
                        "name": "Giacomo Giuliari"
                    },
                    {
                        "name": "Markus Legner"
                    },
                    {
                        "name": "Jean-Pierre Smith"
                    },
                    {
                        "name": "Marc Wyss"
                    },
                    {
                        "name": "Jules Bachmann"
                    },
                    {
                        "name": "Juan A. Garcia-Pardo"
                    },
                    {
                        "name": "Adrian Perrig"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Perrig"
                },
                "author": "Adrian Perrig",
                "arxiv_doi": "10.1145/3718958.3750495",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3718958.3750495",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09959v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09959v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 15 figures",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.2; C.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11790v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11790v4",
                "updated": "2025-08-13T13:29:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    29,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2025-01-20T23:41:22Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    41,
                    22,
                    0,
                    20,
                    0
                ],
                "title": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables\n  Questions"
                },
                "summary": "Recent studies have raised significant concerns regarding the reliability of\ncurrent mathematics benchmarks, highlighting issues such as simplistic design\nand potential data contamination. Consequently, developing a reliable benchmark\nthat effectively evaluates large language models' (LLMs) genuine capabilities\nin mathematical reasoning remains a critical challenge. To address these\nconcerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking\nLLMs with Random Variables in mathematical reasoning. Specifically, we build\nquestion-generating functions to produce random variable questions (RVQs),\nwhose background content mirrors original benchmark problems, but with\nrandomized variable combinations, rendering them \"unseen\" to LLMs. Models must\ncompletely understand the inherent question pattern to correctly answer RVQs\nwith diverse variable combinations. Thus, an LLM's genuine reasoning capability\nis reflected through its accuracy and robustness on RV-Bench. We conducted\nextensive experiments on over 30 representative LLMs across more than 1,000\nRVQs. Our findings propose that LLMs exhibit a proficiency imbalance between\nencountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals\nthat proficiency generalization across similar mathematical reasoning tasks is\nlimited, but we verified it can still be effectively elicited through test-time\nscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have raised significant concerns regarding the reliability of\ncurrent mathematics benchmarks, highlighting issues such as simplistic design\nand potential data contamination. Consequently, developing a reliable benchmark\nthat effectively evaluates large language models' (LLMs) genuine capabilities\nin mathematical reasoning remains a critical challenge. To address these\nconcerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking\nLLMs with Random Variables in mathematical reasoning. Specifically, we build\nquestion-generating functions to produce random variable questions (RVQs),\nwhose background content mirrors original benchmark problems, but with\nrandomized variable combinations, rendering them \"unseen\" to LLMs. Models must\ncompletely understand the inherent question pattern to correctly answer RVQs\nwith diverse variable combinations. Thus, an LLM's genuine reasoning capability\nis reflected through its accuracy and robustness on RV-Bench. We conducted\nextensive experiments on over 30 representative LLMs across more than 1,000\nRVQs. Our findings propose that LLMs exhibit a proficiency imbalance between\nencountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals\nthat proficiency generalization across similar mathematical reasoning tasks is\nlimited, but we verified it can still be effectively elicited through test-time\nscaling."
                },
                "authors": [
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Su Dong"
                    },
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Yujing Zhang"
                    },
                    {
                        "name": "Zhu Wang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11790v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11790v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v5",
                "updated": "2025-08-13T13:27:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    27,
                    26,
                    2,
                    225,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08712v2",
                "updated": "2025-08-13T13:24:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    24,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T07:56:04Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    7,
                    56,
                    4,
                    1,
                    224,
                    0
                ],
                "title": "A Survey on Parallel Text Generation: From Parallel Decoding to\n  Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Parallel Text Generation: From Parallel Decoding to\n  Diffusion Language Models"
                },
                "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation."
                },
                "authors": [
                    {
                        "name": "Lingzhe Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Chiming Duan"
                    },
                    {
                        "name": "Minghua He"
                    },
                    {
                        "name": "Leyi Pan"
                    },
                    {
                        "name": "Pei Xiao"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Aiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Aiwei Liu"
                },
                "author": "Aiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09791v1",
                "updated": "2025-08-13T13:22:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    22,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:22:49Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    22,
                    49,
                    2,
                    225,
                    0
                ],
                "title": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration\n  Recommendations"
                },
                "summary": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses."
                },
                "authors": [
                    {
                        "name": "Junxiao Han"
                    },
                    {
                        "name": "Yarong Wang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00554v2",
                "updated": "2025-08-13T13:17:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    17,
                    6,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-01T11:48:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism"
                },
                "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multi-agent systems and traditional quantitative investment methods\nacross diverse evaluation metrics. ContestTrade is open-sourced on GitHub at\nhttps://github.com/FinStep-AI/ContestTrade.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multi-agent systems and traditional quantitative investment methods\nacross diverse evaluation metrics. ContestTrade is open-sourced on GitHub at\nhttps://github.com/FinStep-AI/ContestTrade."
                },
                "authors": [
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Zuoyou Jiang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Yuxiao Bai"
                    },
                    {
                        "name": "Mengting Chen"
                    },
                    {
                        "name": "Xinyang Wang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zuo Bai"
                    }
                ],
                "author_detail": {
                    "name": "Zuo Bai"
                },
                "author": "Zuo Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09786v1",
                "updated": "2025-08-13T13:12:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    12,
                    18,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:12:18Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    12,
                    18,
                    2,
                    225,
                    0
                ],
                "title": "Adoption of Explainable Natural Language Processing: Perspectives from\n  Industry and Academia on Practices and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adoption of Explainable Natural Language Processing: Perspectives from\n  Industry and Academia on Practices and Challenges"
                },
                "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice."
                },
                "authors": [
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Tobias Mller"
                    },
                    {
                        "name": "Roksoliana Rabets"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09779v1",
                "updated": "2025-08-13T13:00:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:00:05Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision\n  Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE."
                },
                "authors": [
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Zejun Li"
                    },
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09776v1",
                "updated": "2025-08-13T12:59:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    59,
                    8,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:59:08Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    59,
                    8,
                    2,
                    225,
                    0
                ],
                "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study"
                },
                "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance."
                },
                "authors": [
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Ege Erdogan"
                    },
                    {
                        "name": "Zineb Attaoui"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09769v1",
                "updated": "2025-08-13T12:53:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    53,
                    27,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    53,
                    27,
                    2,
                    225,
                    0
                ],
                "title": "An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven\n  Schedules in 5G Time-Sensitive Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven\n  Schedules in 5G Time-Sensitive Networks"
                },
                "summary": "Current standardization efforts are advancing the integration of 5G and\nTime-Sensitive Networking (TSN) to facilitate the deployment of safety-critical\nindustrial applications that require real-time communication. However, there\nremains a fundamental disconnect between the probabilistic 5G delay\ncharacteristics and the often idealistic delay models used to synthesize 5G-TSN\nnetwork configurations. For time-driven schedules in particular, any delay\noutlier unforeseen during schedule synthesis can jeopardize the robustness of\ntheir real-time guarantees. To address this challenge, we present the\n(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time\nguarantees during unstable network conditions that do not match the expected\ndelay characteristics. It augments the primary time-driven schedule with a\ndynamic priority-driven scheme to elevate the priority of m out of k\nconsecutive frames if they are delayed. Our evaluations demonstrate that weakly\nhard real-time guarantees are essential to uphold the quality of control within\na networked control system. At the same time, only a small overhead is imposed\nwhen the primary schedule can provide stronger quality of service guarantees.\nOur (m,k)-firm Elevation Policy thereby yields a robust but light-weight\nfallback mechanism to serve applications with meaningful guarantees during\nunstable network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current standardization efforts are advancing the integration of 5G and\nTime-Sensitive Networking (TSN) to facilitate the deployment of safety-critical\nindustrial applications that require real-time communication. However, there\nremains a fundamental disconnect between the probabilistic 5G delay\ncharacteristics and the often idealistic delay models used to synthesize 5G-TSN\nnetwork configurations. For time-driven schedules in particular, any delay\noutlier unforeseen during schedule synthesis can jeopardize the robustness of\ntheir real-time guarantees. To address this challenge, we present the\n(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time\nguarantees during unstable network conditions that do not match the expected\ndelay characteristics. It augments the primary time-driven schedule with a\ndynamic priority-driven scheme to elevate the priority of m out of k\nconsecutive frames if they are delayed. Our evaluations demonstrate that weakly\nhard real-time guarantees are essential to uphold the quality of control within\na networked control system. At the same time, only a small overhead is imposed\nwhen the primary schedule can provide stronger quality of service guarantees.\nOur (m,k)-firm Elevation Policy thereby yields a robust but light-weight\nfallback mechanism to serve applications with meaningful guarantees during\nunstable network conditions."
                },
                "authors": [
                    {
                        "name": "Simon Egger"
                    },
                    {
                        "name": "Robin Laidig"
                    },
                    {
                        "name": "Heiko Geppert"
                    },
                    {
                        "name": "Lucas Haug"
                    },
                    {
                        "name": "Jona Herrmann"
                    },
                    {
                        "name": "Frank Drr"
                    },
                    {
                        "name": "Christian Becker"
                    }
                ],
                "author_detail": {
                    "name": "Christian Becker"
                },
                "author": "Christian Becker",
                "arxiv_comment": "23 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09767v1",
                "updated": "2025-08-13T12:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    52,
                    38,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    52,
                    38,
                    2,
                    225,
                    0
                ],
                "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in\n  Multilingual Text-to-Speech"
                },
                "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness."
                },
                "authors": [
                    {
                        "name": "Shuhei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Shuhei Kato"
                },
                "author": "Shuhei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12508v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12508v3",
                "updated": "2025-08-13T12:50:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    50,
                    42,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-14T13:45:37Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    45,
                    37,
                    5,
                    165,
                    0
                ],
                "title": "AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose\n  Task Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose\n  Task Solving"
                },
                "summary": "Recent advances in agent systems have demonstrated remarkable capabilities in\nsolving both general-purpose and highly complex tasks. However, most current\nmodels lack mechanisms for coordinating specialized agents and have limited\nability to generalize to new or diverse domains. To this end, we introduce\nAgentOrchestra, a hierarchical multi-agent framework for general-purpose task\nsolving that integrates high-level planning with modular agent collaboration.\nDrawing inspiration from a conductor orchestrating a symphony, and grounded in\nthe principles of extensibility, multimodality, modularity, and coordination,\nit features a central planning agent that decomposes complex objectives and\ndelegates sub-tasks to a team of specialized agents. Each sub-agent is equipped\nwith general programming tools, as well as abilities to tackle a wide range of\nreal-world specific tasks, including data analysis, file operations, web\nnavigation, and interactive reasoning in dynamic multimodal environments.\nNotably, AgentOrchestra introduces an MCP Manager Agent that enables\nintelligent evolution through dynamic tool creation, retrieval, and reuse\nmechanisms, significantly enhancing the system's adaptability and scalability.\nAgentOrchestra supports flexible orchestration through explicit sub-goal\nformulation, inter-agent communication, and adaptive role allocation. We\nevaluate the framework on three widely used benchmarks for assessing LLM-based\nagent systems. Experimental results show that AgentOrchestra consistently\noutperforms flat-agent and monolithic baselines in terms of task success rate\nand adaptability. On the GAIA benchmark testing dataset, AgentOrchestra\nachieves an average score of 83.39\\%, ranking among the top general-purpose\nagents. These results highlight the effectiveness of hierarchical organization\nand role specialization in building scalable and general-purpose LLM-based\nagent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in agent systems have demonstrated remarkable capabilities in\nsolving both general-purpose and highly complex tasks. However, most current\nmodels lack mechanisms for coordinating specialized agents and have limited\nability to generalize to new or diverse domains. To this end, we introduce\nAgentOrchestra, a hierarchical multi-agent framework for general-purpose task\nsolving that integrates high-level planning with modular agent collaboration.\nDrawing inspiration from a conductor orchestrating a symphony, and grounded in\nthe principles of extensibility, multimodality, modularity, and coordination,\nit features a central planning agent that decomposes complex objectives and\ndelegates sub-tasks to a team of specialized agents. Each sub-agent is equipped\nwith general programming tools, as well as abilities to tackle a wide range of\nreal-world specific tasks, including data analysis, file operations, web\nnavigation, and interactive reasoning in dynamic multimodal environments.\nNotably, AgentOrchestra introduces an MCP Manager Agent that enables\nintelligent evolution through dynamic tool creation, retrieval, and reuse\nmechanisms, significantly enhancing the system's adaptability and scalability.\nAgentOrchestra supports flexible orchestration through explicit sub-goal\nformulation, inter-agent communication, and adaptive role allocation. We\nevaluate the framework on three widely used benchmarks for assessing LLM-based\nagent systems. Experimental results show that AgentOrchestra consistently\noutperforms flat-agent and monolithic baselines in terms of task success rate\nand adaptability. On the GAIA benchmark testing dataset, AgentOrchestra\nachieves an average score of 83.39\\%, ranking among the top general-purpose\nagents. These results highlight the effectiveness of hierarchical organization\nand role specialization in building scalable and general-purpose LLM-based\nagent systems."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Yuzhen Xiao"
                    },
                    {
                        "name": "Yongcong Li"
                    },
                    {
                        "name": "Ce Cui"
                    },
                    {
                        "name": "Yilei Zhao"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yahui Zhou"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12508v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12508v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09762v1",
                "updated": "2025-08-13T12:47:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    47,
                    33,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:47:33Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    47,
                    33,
                    2,
                    225,
                    0
                ],
                "title": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to\n  Sacrifice Itself for Human Safety?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to\n  Sacrifice Itself for Human Safety?"
                },
                "summary": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities."
                },
                "authors": [
                    {
                        "name": "Manuel Herrador"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Herrador"
                },
                "author": "Manuel Herrador",
                "arxiv_comment": "10 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05371v2",
                "updated": "2025-08-13T12:45:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    45,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-03-07T12:25:29Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    25,
                    29,
                    4,
                    66,
                    0
                ],
                "title": "Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in\n  LLMs"
                },
                "summary": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We compute 8 steering vectors, each corresponding to a different social\nbias axis, such as age, gender, or race, on a training subset of the BBQ\ndataset and compare the effectiveness of these to 3 additional bias mitigation\nmethods across 4 datasets. When optimized on the BBQ dataset, our individually\ntuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on\nCLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and\nSelf-Debias in all cases, and improvements over fine-tuning in 12 out of 17\nevaluations. In addition, steering vectors showed the lowest impact on MMLU\nscores of the four bias mitigation methods tested. The work presents the first\nsystematic investigation of steering vectors for bias mitigation, and we\ndemonstrate that they are a powerful and computationally efficient strategy for\nreducing bias in LLMs, with broader implications for enhancing AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We compute 8 steering vectors, each corresponding to a different social\nbias axis, such as age, gender, or race, on a training subset of the BBQ\ndataset and compare the effectiveness of these to 3 additional bias mitigation\nmethods across 4 datasets. When optimized on the BBQ dataset, our individually\ntuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on\nCLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and\nSelf-Debias in all cases, and improvements over fine-tuning in 12 out of 17\nevaluations. In addition, steering vectors showed the lowest impact on MMLU\nscores of the four bias mitigation methods tested. The work presents the first\nsystematic investigation of steering vectors for bias mitigation, and we\ndemonstrate that they are a powerful and computationally efficient strategy for\nreducing bias in LLMs, with broader implications for enhancing AI safety."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Irtaza Khalid"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Submitted to AACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09755v1",
                "updated": "2025-08-13T12:35:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    35,
                    4,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:35:04Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    35,
                    4,
                    2,
                    225,
                    0
                ],
                "title": "Transforming Questions and Documents for Semantically Aligned\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Questions and Documents for Semantically Aligned\n  Retrieval-Augmented Generation"
                },
                "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios."
                },
                "authors": [
                    {
                        "name": "Seokgi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seokgi Lee"
                },
                "author": "Seokgi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09753v1",
                "updated": "2025-08-13T12:34:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    34,
                    15,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:34:15Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    34,
                    15,
                    2,
                    225,
                    0
                ],
                "title": "TriForecaster: A Mixture of Experts Framework for Multi-Region Electric\n  Load Forecasting with Tri-dimensional Specialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForecaster: A Mixture of Experts Framework for Multi-Region Electric\n  Load Forecasting with Tri-dimensional Specialization"
                },
                "summary": "Electric load forecasting is pivotal for power system operation, planning and\ndecision-making. The rise of smart grids and meters has provided more detailed\nand high-quality load data at multiple levels of granularity, from home to bus\nand cities. Motivated by similar patterns of loads across different cities in a\nprovince in eastern China, in this paper we focus on the Multi-Region Electric\nLoad Forecasting (MRELF) problem, targeting accurate short-term load\nforecasting for multiple sub-regions within a large region. We identify three\nchallenges for MRELF, including regional variation, contextual variation, and\ntemporal variation. To address them, we propose TriForecaster, a new framework\nleveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning\n(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer\nand Context-Time Specializer (CTSpecializer) layers, enabling dynamic\ncooperation and specialization of expert models across regional, contextual,\nand temporal dimensions. Based on evaluation on four real-world MRELF datasets\nwith varied granularity, TriForecaster outperforms state-of-the-art models by\nachieving an average forecast error reduction of 22.4\\%, thereby demonstrating\nits flexibility and broad applicability. In particular, the deployment of\nTriForecaster on the eForecaster platform in eastern China exemplifies its\npractical utility, effectively providing city-level, short-term load forecasts\nfor 17 cities, supporting a population exceeding 110 million and daily\nelectricity usage over 100 gigawatt-hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric load forecasting is pivotal for power system operation, planning and\ndecision-making. The rise of smart grids and meters has provided more detailed\nand high-quality load data at multiple levels of granularity, from home to bus\nand cities. Motivated by similar patterns of loads across different cities in a\nprovince in eastern China, in this paper we focus on the Multi-Region Electric\nLoad Forecasting (MRELF) problem, targeting accurate short-term load\nforecasting for multiple sub-regions within a large region. We identify three\nchallenges for MRELF, including regional variation, contextual variation, and\ntemporal variation. To address them, we propose TriForecaster, a new framework\nleveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning\n(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer\nand Context-Time Specializer (CTSpecializer) layers, enabling dynamic\ncooperation and specialization of expert models across regional, contextual,\nand temporal dimensions. Based on evaluation on four real-world MRELF datasets\nwith varied granularity, TriForecaster outperforms state-of-the-art models by\nachieving an average forecast error reduction of 22.4\\%, thereby demonstrating\nits flexibility and broad applicability. In particular, the deployment of\nTriForecaster on the eForecaster platform in eastern China exemplifies its\npractical utility, effectively providing city-level, short-term load forecasts\nfor 17 cities, supporting a population exceeding 110 million and daily\nelectricity usage over 100 gigawatt-hours."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Zhu"
                    },
                    {
                        "name": "Zhipeng Zeng"
                    },
                    {
                        "name": "Qiming Chen"
                    },
                    {
                        "name": "Linxiao Yang"
                    },
                    {
                        "name": "Peiyuan Liu"
                    },
                    {
                        "name": "Weiqi Chen"
                    },
                    {
                        "name": "Liang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Liang Sun"
                },
                "author": "Liang Sun",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09752v1",
                "updated": "2025-08-13T12:31:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    31,
                    27,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:31:27Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    31,
                    27,
                    2,
                    225,
                    0
                ],
                "title": "$$-Parametrization for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$$-Parametrization for Mixture of Experts"
                },
                "summary": "Recent years have seen a growing interest and adoption of LLMs, with\n$\\mu$Transfer becoming a key technique for tuning hyperparameters in\nlarge-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a\nleading architecture in extremely large models. However, the intersection of\nthese two advancements has remained unexplored. In this work, we derive a\n$\\mu$-Parameterization ($\\mu$P) for MoE, providing theoretical guarantees for\nfeature learning across model widths in both the router and experts. We\nempirically validate our parameterization and further investigate how scaling\nthe number of experts and granularity affects the optimal learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen a growing interest and adoption of LLMs, with\n$\\mu$Transfer becoming a key technique for tuning hyperparameters in\nlarge-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a\nleading architecture in extremely large models. However, the intersection of\nthese two advancements has remained unexplored. In this work, we derive a\n$\\mu$-Parameterization ($\\mu$P) for MoE, providing theoretical guarantees for\nfeature learning across model widths in both the router and experts. We\nempirically validate our parameterization and further investigate how scaling\nthe number of experts and granularity affects the optimal learning rate."
                },
                "authors": [
                    {
                        "name": "Jan Maanicki"
                    },
                    {
                        "name": "Kamil Ciebiera"
                    },
                    {
                        "name": "Mateusz Boru"
                    },
                    {
                        "name": "Maciej Piro"
                    },
                    {
                        "name": "Jan Ludziejewski"
                    },
                    {
                        "name": "Maciej Stefaniak"
                    },
                    {
                        "name": "Micha Krutul"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Marek Cygan"
                    },
                    {
                        "name": "Kamil Adamczewski"
                    },
                    {
                        "name": "Jakub Krajewski"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Krajewski"
                },
                "author": "Jakub Krajewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09743v1",
                "updated": "2025-08-13T12:13:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    13,
                    28,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T12:13:28Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    13,
                    28,
                    2,
                    225,
                    0
                ],
                "title": "HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge\n  Transfer in Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge\n  Transfer in Neural Networks"
                },
                "summary": "A prevailing trend in neural network research suggests that model performance\nimproves with increasing depth and capacity - often at the cost of\nintegrability and efficiency. In this paper, we propose a strategy to optimize\nsmall, deployable models by enhancing their capabilities through structured\nknowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a\nbiologically inspired framework for modular and selective transfer of\ntask-relevant features from a larger, pretrained parent network to a smaller\nchild model. Unlike standard knowledge distillation, which enforces uniform\nimitation of teacher outputs, HKT draws inspiration from biological inheritance\nmechanisms - such as memory RNA transfer in planarians - to guide a multi-stage\nprocess of feature transfer. Neural network blocks are treated as functional\ncarriers, and knowledge is transmitted through three biologically motivated\ncomponents: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention\n(GA) mechanism governs the integration of inherited and native representations,\nensuring both alignment and selectivity. We evaluate HKT across diverse vision\ntasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),\nand semantic segmentation (LiTS), demonstrating that it significantly improves\nchild model performance while preserving its compactness. The results show that\nHKT consistently outperforms conventional distillation approaches, offering a\ngeneral-purpose, interpretable, and scalable solution for deploying\nhigh-performance neural networks in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prevailing trend in neural network research suggests that model performance\nimproves with increasing depth and capacity - often at the cost of\nintegrability and efficiency. In this paper, we propose a strategy to optimize\nsmall, deployable models by enhancing their capabilities through structured\nknowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a\nbiologically inspired framework for modular and selective transfer of\ntask-relevant features from a larger, pretrained parent network to a smaller\nchild model. Unlike standard knowledge distillation, which enforces uniform\nimitation of teacher outputs, HKT draws inspiration from biological inheritance\nmechanisms - such as memory RNA transfer in planarians - to guide a multi-stage\nprocess of feature transfer. Neural network blocks are treated as functional\ncarriers, and knowledge is transmitted through three biologically motivated\ncomponents: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention\n(GA) mechanism governs the integration of inherited and native representations,\nensuring both alignment and selectivity. We evaluate HKT across diverse vision\ntasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),\nand semantic segmentation (LiTS), demonstrating that it significantly improves\nchild model performance while preserving its compactness. The results show that\nHKT consistently outperforms conventional distillation approaches, offering a\ngeneral-purpose, interpretable, and scalable solution for deploying\nhigh-performance neural networks in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yanick Chistian Tchenko"
                    },
                    {
                        "name": "Felix Mohr"
                    },
                    {
                        "name": "Hicham Hadj Abdelkader"
                    },
                    {
                        "name": "Hedi Tabia"
                    }
                ],
                "author_detail": {
                    "name": "Hedi Tabia"
                },
                "author": "Hedi Tabia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10417v2",
                "updated": "2025-08-13T12:11:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    11,
                    26,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-09T20:40:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    20,
                    40,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs\n  Performance"
                },
                "summary": "Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Abdelrahman A. Ali"
                    },
                    {
                        "name": "Aya E. Fouda"
                    },
                    {
                        "name": "Radwa J. Hanafy"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09724v1",
                "updated": "2025-08-13T11:41:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    41,
                    1,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:41:01Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    41,
                    1,
                    2,
                    225,
                    0
                ],
                "title": "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge"
                },
                "summary": "Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but\nit is prone to preference bias, where judges systematically favor certain\noutputs, such as their own. This bias leads to inconsistent and skewed rankings\nacross different judges. To address this, we first empirically demonstrate\nsignificant and heterogeneous biases in cross-model evaluations. We then\npropose UDA (Unsupervised Debiasing Alignment), a framework that reduces\ninter-judge disagreement by dynamically adjusting the Elo rating system. For\neach pairwise comparison, a compact neural network learns to adaptively set the\nK-factor and refine win probabilities. Crucially, UDA operates in a fully\nunsupervised manner, guided solely by the objective of minimizing the\ndispersion among the Elo trajectories of all judges. This forces an alignment\ntowards a collective consensus, which serves as an unsupervised proxy for a\nmore stable and reproducible evaluation. In addition, we provide theoretical\nmotivation demonstrating how alignment towards a consensus can reduce aggregate\nsystem bias. Experiments show that UDA significantly reduces the inter-judge\nrating standard deviation by up to 63.4% and improves the average correlation\nwith human judgments by 24.7%. Notably, UDA elevates the performance of poorly\nperforming judges to achieve parity with high-quality ones, fostering a more\nrobust and reliable evaluation ecosystem. Code and data are available at\nhttps://anonymous.4open.science/r/62AB93CD-23B4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but\nit is prone to preference bias, where judges systematically favor certain\noutputs, such as their own. This bias leads to inconsistent and skewed rankings\nacross different judges. To address this, we first empirically demonstrate\nsignificant and heterogeneous biases in cross-model evaluations. We then\npropose UDA (Unsupervised Debiasing Alignment), a framework that reduces\ninter-judge disagreement by dynamically adjusting the Elo rating system. For\neach pairwise comparison, a compact neural network learns to adaptively set the\nK-factor and refine win probabilities. Crucially, UDA operates in a fully\nunsupervised manner, guided solely by the objective of minimizing the\ndispersion among the Elo trajectories of all judges. This forces an alignment\ntowards a collective consensus, which serves as an unsupervised proxy for a\nmore stable and reproducible evaluation. In addition, we provide theoretical\nmotivation demonstrating how alignment towards a consensus can reduce aggregate\nsystem bias. Experiments show that UDA significantly reduces the inter-judge\nrating standard deviation by up to 63.4% and improves the average correlation\nwith human judgments by 24.7%. Notably, UDA elevates the performance of poorly\nperforming judges to achieve parity with high-quality ones, fostering a more\nrobust and reliable evaluation ecosystem. Code and data are available at\nhttps://anonymous.4open.science/r/62AB93CD-23B4."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Lindong Wu"
                    },
                    {
                        "name": "Wenbo Yu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12867v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12867v4",
                "updated": "2025-08-13T11:23:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    23,
                    57,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-17T11:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting"
                },
                "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Dataset, code, checkpoints, and demo samples\nare available at https://github.com/yanghaha0908/EmoVoice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Dataset, code, checkpoints, and demo samples\nare available at https://github.com/yanghaha0908/EmoVoice."
                },
                "authors": [
                    {
                        "name": "Guanrou Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "ShiLiang Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "Accepted at ACMMM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12867v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12867v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09719v1",
                "updated": "2025-08-13T11:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    19,
                    30,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    19,
                    30,
                    2,
                    225,
                    0
                ],
                "title": "Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models"
                },
                "summary": "Large, publicly available clinical datasets have emerged as a novel resource\nfor understanding disease heterogeneity and to explore personalization of\ntherapy. These datasets are derived from data not originally collected for\nresearch purposes and, as a result, are often incomplete and lack critical\nlabels. Many AI tools have been developed to retrospectively label these\ndatasets, such as by performing disease classification; however, they often\nsuffer from limited interpretability. Previous work has attempted to explain\npredictions using Concept Bottleneck Models (CBMs), which learn interpretable\nconcepts that map to higher-level clinical ideas, facilitating human\nevaluation. However, these models often experience performance limitations when\nthe concepts fail to adequately explain or characterize the task. We use the\nidentification of Acute Respiratory Distress Syndrome (ARDS) as a challenging\ntest case to demonstrate the value of incorporating contextual information from\nclinical notes to improve CBM performance. Our approach leverages a Large\nLanguage Model (LLM) to process clinical notes and generate additional\nconcepts, resulting in a 10% performance gain over existing methods.\nAdditionally, it facilitates the learning of more comprehensive concepts,\nthereby reducing the risk of information leakage and reliance on spurious\nshortcuts, thus improving the characterization of ARDS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large, publicly available clinical datasets have emerged as a novel resource\nfor understanding disease heterogeneity and to explore personalization of\ntherapy. These datasets are derived from data not originally collected for\nresearch purposes and, as a result, are often incomplete and lack critical\nlabels. Many AI tools have been developed to retrospectively label these\ndatasets, such as by performing disease classification; however, they often\nsuffer from limited interpretability. Previous work has attempted to explain\npredictions using Concept Bottleneck Models (CBMs), which learn interpretable\nconcepts that map to higher-level clinical ideas, facilitating human\nevaluation. However, these models often experience performance limitations when\nthe concepts fail to adequately explain or characterize the task. We use the\nidentification of Acute Respiratory Distress Syndrome (ARDS) as a challenging\ntest case to demonstrate the value of incorporating contextual information from\nclinical notes to improve CBM performance. Our approach leverages a Large\nLanguage Model (LLM) to process clinical notes and generate additional\nconcepts, resulting in a 10% performance gain over existing methods.\nAdditionally, it facilitates the learning of more comprehensive concepts,\nthereby reducing the risk of information leakage and reliance on spurious\nshortcuts, thus improving the characterization of ARDS."
                },
                "authors": [
                    {
                        "name": "Anish Narain"
                    },
                    {
                        "name": "Ritam Majumdar"
                    },
                    {
                        "name": "Nikita Narayanan"
                    },
                    {
                        "name": "Dominic Marshall"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "32 pages, 7 figures, accepted at Machine Learning for Healthcare\n  Conference (MLHC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02420v2",
                "updated": "2025-08-13T11:10:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    10,
                    11,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-03T09:21:48Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    21,
                    48,
                    3,
                    93,
                    0
                ],
                "title": "On learning racing policies with reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On learning racing policies with reinforcement learning"
                },
                "summary": "Fully autonomous vehicles promise enhanced safety and efficiency. However,\nensuring reliable operation in challenging corner cases requires control\nalgorithms capable of performing at the vehicle limits. We address this\nrequirement by considering the task of autonomous racing and propose solving it\nby learning a racing policy using Reinforcement Learning (RL). Our approach\nleverages domain randomization, actuator dynamics modeling, and policy\narchitecture design to enable reliable and safe zero-shot deployment on a real\nplatform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a\nstate-of-the-art Model Predictive Control (MPC), but, to the best of our\nknowledge, also represents the first instance of an RL policy outperforming\nexpert human drivers in RC racing. This work identifies the key factors driving\nthis performance improvement, providing critical insights for the design of\nrobust RL-based control strategies for autonomous vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully autonomous vehicles promise enhanced safety and efficiency. However,\nensuring reliable operation in challenging corner cases requires control\nalgorithms capable of performing at the vehicle limits. We address this\nrequirement by considering the task of autonomous racing and propose solving it\nby learning a racing policy using Reinforcement Learning (RL). Our approach\nleverages domain randomization, actuator dynamics modeling, and policy\narchitecture design to enable reliable and safe zero-shot deployment on a real\nplatform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a\nstate-of-the-art Model Predictive Control (MPC), but, to the best of our\nknowledge, also represents the first instance of an RL policy outperforming\nexpert human drivers in RC racing. This work identifies the key factors driving\nthis performance improvement, providing critical insights for the design of\nrobust RL-based control strategies for autonomous vehicles."
                },
                "authors": [
                    {
                        "name": "Grzegorz Czechmanowski"
                    },
                    {
                        "name": "Jan Wgrzynowski"
                    },
                    {
                        "name": "Piotr Kicki"
                    },
                    {
                        "name": "Krzysztof Walas"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Walas"
                },
                "author": "Krzysztof Walas",
                "arxiv_comment": "This paper has been accepted for publication in the Proceedings of\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07976v2",
                "updated": "2025-08-13T11:06:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    6,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-11T13:36:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    36,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL"
                },
                "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Minyang Xie"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Banghua Zhu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09105v2",
                "updated": "2025-08-13T11:05:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    5,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T17:32:24Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    32,
                    24,
                    1,
                    224,
                    0
                ],
                "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling"
                },
                "summary": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems."
                },
                "authors": [
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Jianjie Huang"
                    },
                    {
                        "name": "Jingzhi Li"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09713v1",
                "updated": "2025-08-13T11:04:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    4,
                    48,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T11:04:48Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    4,
                    48,
                    2,
                    225,
                    0
                ],
                "title": "Evaluating the Role of Large Language Models in Legal Practice in India",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Role of Large Language Models in Legal Practice in India"
                },
                "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law."
                },
                "authors": [
                    {
                        "name": "Rahul Hemrajani"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Hemrajani"
                },
                "arxiv_affiliation": "National Law School of India University, Bengaluru",
                "author": "Rahul Hemrajani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09694v1",
                "updated": "2025-08-13T10:47:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    47,
                    16,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T10:47:16Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    47,
                    16,
                    2,
                    225,
                    0
                ],
                "title": "A Long-Baseline Atom Interferometer at CERN LHC Point 4: Implementation\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Long-Baseline Atom Interferometer at CERN LHC Point 4: Implementation\n  Study"
                },
                "summary": "Building on the feasibility study in CERN-PBC Report-2018-002 (Arduini et al.\n2018), this report supported by the Physics Beyond Colliders (PBC) Study Group\ndescribes the technical implementation of modifications to the PX46 shaft at\nLHC Point 4 during LS3 (June 2026 - June 2030) that would enable it to\naccommodate the installation and operation of a vertical long-baseline Atom\nInterferometer during Run 4 without affecting LHC operations. We specify in\ndetail the necessary civil-engineering work, installation of bespoke radiation\nshielding, deployment of access-control systems and safety alarms, and design\nof a mobile elevator platform. Our comprehensive technical assessment\nidentifies no fundamental obstacles or showstoppers to implementation. Refined\ncost estimates and a critical-path schedule confirm that, from formal approval,\nall interventions can be completed within a 1.5-year window. These preparations\nwould ensure seamless, concurrent operation of the Atom Interferometer\nexperiment and the HL-LHC, with all technical challenges successfully addressed\nthrough established engineering solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the feasibility study in CERN-PBC Report-2018-002 (Arduini et al.\n2018), this report supported by the Physics Beyond Colliders (PBC) Study Group\ndescribes the technical implementation of modifications to the PX46 shaft at\nLHC Point 4 during LS3 (June 2026 - June 2030) that would enable it to\naccommodate the installation and operation of a vertical long-baseline Atom\nInterferometer during Run 4 without affecting LHC operations. We specify in\ndetail the necessary civil-engineering work, installation of bespoke radiation\nshielding, deployment of access-control systems and safety alarms, and design\nof a mobile elevator platform. Our comprehensive technical assessment\nidentifies no fundamental obstacles or showstoppers to implementation. Refined\ncost estimates and a critical-path schedule confirm that, from formal approval,\nall interventions can be completed within a 1.5-year window. These preparations\nwould ensure seamless, concurrent operation of the Atom Interferometer\nexperiment and the HL-LHC, with all technical challenges successfully addressed\nthrough established engineering solutions."
                },
                "authors": [
                    {
                        "name": "G. Arduini"
                    },
                    {
                        "name": "O. Buchmller"
                    },
                    {
                        "name": "T. A. Bud"
                    },
                    {
                        "name": "S. Calatroni"
                    },
                    {
                        "name": "O. Crespo-Lopez"
                    },
                    {
                        "name": "A. Devienne"
                    },
                    {
                        "name": "J. Ellis"
                    },
                    {
                        "name": "T. Hakulinen"
                    },
                    {
                        "name": "A. Infantino"
                    },
                    {
                        "name": "D. Lafarge"
                    },
                    {
                        "name": "A. P. Marion"
                    }
                ],
                "author_detail": {
                    "name": "A. P. Marion"
                },
                "author": "A. P. Marion",
                "arxiv_comment": "CERN-PBC Report-2025-004, 10 pages, multiple figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07152v2",
                "updated": "2025-08-13T10:33:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    33,
                    43,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-10T03:01:10Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    3,
                    1,
                    10,
                    6,
                    222,
                    0
                ],
                "title": "Inversion of Arctic dual-channel sound speed profile based on random\n  airgun signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion of Arctic dual-channel sound speed profile based on random\n  airgun signal"
                },
                "summary": "For the unique dual-channel sound speed profiles of the Canadian Basin and\nthe Chukchi Plateau in the Arctic, based on the propagation characteristics of\nrefracted normal modes under dual-channel sound speed profiles, an inversion\nmethod using refracted normal modes for dual-channel sound speed profiles is\nproposed. This method proposes a dual-parameter representation method for\ndual-channel sound speed profiles, tailored to the characteristics of\ndual-channel sound speed profiles. A dispersion structure extraction method is\nproposed for the dispersion structure characteristics of refracted normal modes\nunder dual-channel sound speed profiles. Combining the parameter representation\nmethod of sound speed profiles and the dispersion structure extraction method,\nan inversion method for dual-channel sound speed profiles is proposed. For the\ncommon horizontal variation of sound speed profiles in long-distance acoustic\npropagation, a method for inverting horizontally varying dual-channel sound\nspeed profiles is proposed. Finally, this article verifies the effectiveness of\nthe dual-channel sound speed profile inversion method using the Arctic\nlow-frequency long-range acoustic propagation experiment. Compared with\nprevious sound speed profile inversion methods, the method proposed in this\narticle has the advantages of fewer inversion parameters and faster inversion\nspeed. It can be implemented using only a single hydrophone passively receiving\nrandom air gun signals, and it also solves the inversion problem of horizontal\nvariation of sound speed profiles. It has significant advantages such as low\ncost, easy deployment, and fast computation speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the unique dual-channel sound speed profiles of the Canadian Basin and\nthe Chukchi Plateau in the Arctic, based on the propagation characteristics of\nrefracted normal modes under dual-channel sound speed profiles, an inversion\nmethod using refracted normal modes for dual-channel sound speed profiles is\nproposed. This method proposes a dual-parameter representation method for\ndual-channel sound speed profiles, tailored to the characteristics of\ndual-channel sound speed profiles. A dispersion structure extraction method is\nproposed for the dispersion structure characteristics of refracted normal modes\nunder dual-channel sound speed profiles. Combining the parameter representation\nmethod of sound speed profiles and the dispersion structure extraction method,\nan inversion method for dual-channel sound speed profiles is proposed. For the\ncommon horizontal variation of sound speed profiles in long-distance acoustic\npropagation, a method for inverting horizontally varying dual-channel sound\nspeed profiles is proposed. Finally, this article verifies the effectiveness of\nthe dual-channel sound speed profile inversion method using the Arctic\nlow-frequency long-range acoustic propagation experiment. Compared with\nprevious sound speed profile inversion methods, the method proposed in this\narticle has the advantages of fewer inversion parameters and faster inversion\nspeed. It can be implemented using only a single hydrophone passively receiving\nrandom air gun signals, and it also solves the inversion problem of horizontal\nvariation of sound speed profiles. It has significant advantages such as low\ncost, easy deployment, and fast computation speed."
                },
                "authors": [
                    {
                        "name": "Jinbao Weng"
                    },
                    {
                        "name": "Yubo Qi"
                    },
                    {
                        "name": "Yanming Yang"
                    },
                    {
                        "name": "Hongtao Wen"
                    },
                    {
                        "name": "Hongtao Zhou"
                    },
                    {
                        "name": "Benqing Chen"
                    },
                    {
                        "name": "Dewei Xu"
                    },
                    {
                        "name": "Ruichao Xue"
                    },
                    {
                        "name": "Caigao Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Caigao Zeng"
                },
                "arxiv_affiliation": "Fujian Provincial Key Laboratory of Marine Physical and Geological Processes, Xiamen, Fujian, China",
                "author": "Caigao Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22557v2",
                "updated": "2025-08-13T10:28:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    28,
                    17,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-27T18:15:56Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    15,
                    56,
                    4,
                    178,
                    0
                ],
                "title": "MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for\n  Cipher-Based Jailbreak Attacks for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for\n  Cipher-Based Jailbreak Attacks for LLMs"
                },
                "summary": "As large language models (LLMs) grow more capable, they face growing\nvulnerability to sophisticated jailbreak attacks. While developers invest\nheavily in alignment finetuning and safety guardrails, researchers continue\npublishing novel attacks, driving progress through adversarial iteration. This\ndynamic mirrors a strategic game of continual evolution. However, two major\nchallenges hinder jailbreak development: the high cost of querying top-tier\nLLMs and the short lifespan of effective attacks due to frequent safety\nupdates. These factors limit cost-efficiency and practical impact of research\nin jailbreak attacks. To address this, we propose MetaCipher, a low-cost,\nmulti-agent jailbreak framework that generalizes across LLMs with varying\nsafety measures. Using reinforcement learning, MetaCipher is modular and\nadaptive, supporting extensibility to future strategies. Within as few as 10\nqueries, MetaCipher achieves state-of-the-art attack success rates on recent\nmalicious prompt benchmarks, outperforming prior jailbreak methods. We conduct\na large-scale empirical evaluation across diverse victim models and benchmarks,\ndemonstrating its robustness and adaptability. Warning: This paper contains\nmodel outputs that may be offensive or harmful, shown solely to demonstrate\njailbreak efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow more capable, they face growing\nvulnerability to sophisticated jailbreak attacks. While developers invest\nheavily in alignment finetuning and safety guardrails, researchers continue\npublishing novel attacks, driving progress through adversarial iteration. This\ndynamic mirrors a strategic game of continual evolution. However, two major\nchallenges hinder jailbreak development: the high cost of querying top-tier\nLLMs and the short lifespan of effective attacks due to frequent safety\nupdates. These factors limit cost-efficiency and practical impact of research\nin jailbreak attacks. To address this, we propose MetaCipher, a low-cost,\nmulti-agent jailbreak framework that generalizes across LLMs with varying\nsafety measures. Using reinforcement learning, MetaCipher is modular and\nadaptive, supporting extensibility to future strategies. Within as few as 10\nqueries, MetaCipher achieves state-of-the-art attack success rates on recent\nmalicious prompt benchmarks, outperforming prior jailbreak methods. We conduct\na large-scale empirical evaluation across diverse victim models and benchmarks,\ndemonstrating its robustness and adaptability. Warning: This paper contains\nmodel outputs that may be offensive or harmful, shown solely to demonstrate\njailbreak efficacy."
                },
                "authors": [
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09683v1",
                "updated": "2025-08-13T10:22:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    22,
                    20,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T10:22:20Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    22,
                    20,
                    2,
                    225,
                    0
                ],
                "title": "Procedural Generation and Games at the Dawn of Fault Tolerant Quantum\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Generation and Games at the Dawn of Fault Tolerant Quantum\n  Computing"
                },
                "summary": "Quantum computers have long been more of a toy for researchers than a tool\nfor solving complex problems. However, recent advances in the field make\nexploiting the advantages of fault-tolerant quantum computers feasible in the\nnext 5 to 10 years. It is now time to begin imagining how such devices could be\nused in practice for game development and deployment. In this work we identify\nprocedural content generation as a very promising area of application and\nexploration. We examine a selection of algorithmic approaches used in classical\nprocedural content generation and propose promising quantum algorithms that\ncould provide an alternative approach or a computational advantage. We then end\nwith a hypothetical game that exploits a recent quantum algorithm for computing\nthe Jones polynomial exponentially faster than classical computers could.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computers have long been more of a toy for researchers than a tool\nfor solving complex problems. However, recent advances in the field make\nexploiting the advantages of fault-tolerant quantum computers feasible in the\nnext 5 to 10 years. It is now time to begin imagining how such devices could be\nused in practice for game development and deployment. In this work we identify\nprocedural content generation as a very promising area of application and\nexploration. We examine a selection of algorithmic approaches used in classical\nprocedural content generation and propose promising quantum algorithms that\ncould provide an alternative approach or a computational advantage. We then end\nwith a hypothetical game that exploits a recent quantum algorithm for computing\nthe Jones polynomial exponentially faster than classical computers could."
                },
                "authors": [
                    {
                        "name": "Daniel Bultrini"
                    },
                    {
                        "name": "James Wootton"
                    }
                ],
                "author_detail": {
                    "name": "James Wootton"
                },
                "author": "James Wootton",
                "arxiv_comment": "IEEE CoG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09864v3",
                "updated": "2025-08-13T10:18:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    18,
                    32,
                    2,
                    225,
                    0
                ],
                "published": "2024-06-14T09:22:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    9,
                    22,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal\n  Data"
                },
                "summary": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique multimodal dataset,\nfeaturing audio, image, and textual data from 50 classes, specifically designed\nfor learning from uncertain data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the research community to design more\ntrustworthy and robust machine learning approaches for safety critical\napplications. The code and instructions for downloading and processing the\ndataset can be found at: https://github.com/bezirganyan/LUMA/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique multimodal dataset,\nfeaturing audio, image, and textual data from 50 classes, specifically designed\nfor learning from uncertain data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the research community to design more\ntrustworthy and robust machine learning approaches for safety critical\napplications. The code and instructions for downloading and processing the\ndataset can be found at: https://github.com/bezirganyan/LUMA/ ."
                },
                "authors": [
                    {
                        "name": "Grigor Bezirganyan"
                    },
                    {
                        "name": "Sana Sellami"
                    },
                    {
                        "name": "Laure Berti-quille"
                    },
                    {
                        "name": "Sbastien Fournier"
                    }
                ],
                "author_detail": {
                    "name": "Sbastien Fournier"
                },
                "author": "Sbastien Fournier",
                "arxiv_doi": "10.1145/3726302.3730302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_journal_ref": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval. 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22748v2",
                "updated": "2025-08-13T10:10:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    10,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-30T15:05:05Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    5,
                    5,
                    2,
                    211,
                    0
                ],
                "title": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index"
                },
                "summary": "We draw on Eloundou et al. (2024) to develop the Generative AI Susceptibility\nIndex (GAISI), a task-based measure of UK job exposure to large language models\n(LLMs), such as ChatGPT. GAISI is derived from probabilistic task ratings by\nLLMs and linked to worker-reported task data from the Skills and Employment\nSurveys. It reflects the share of job activities where an LLM or LLM-powered\nsystem can reduce task completion time by at least 25% beyond existing\nproductivity tools. The index demonstrates high reliability, strong alignment\nwith AI capabilities, and superior predictive power compared to existing\nexposure measures. By 2023-24, nearly all UK jobs exhibited some exposure, yet\nonly a minority were heavily affected. Aggregate exposure has risen since 2017,\nprimarily due to occupational shifts rather than changes in task profiles. The\nprice premium for AI-exposed tasks declined relative to 2017, measuring\napproximately 12% lower in 2023-24. Job postings fell following the release of\nChatGPT, with job postings 5.5% lower in 2025-Q2 than if pre-GPT hiring\npatterns had persisted. GAISI offers a robust framework for assessing AI's\nimpact on work, providing early evidence that displacement effects may already\noutweigh productivity gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We draw on Eloundou et al. (2024) to develop the Generative AI Susceptibility\nIndex (GAISI), a task-based measure of UK job exposure to large language models\n(LLMs), such as ChatGPT. GAISI is derived from probabilistic task ratings by\nLLMs and linked to worker-reported task data from the Skills and Employment\nSurveys. It reflects the share of job activities where an LLM or LLM-powered\nsystem can reduce task completion time by at least 25% beyond existing\nproductivity tools. The index demonstrates high reliability, strong alignment\nwith AI capabilities, and superior predictive power compared to existing\nexposure measures. By 2023-24, nearly all UK jobs exhibited some exposure, yet\nonly a minority were heavily affected. Aggregate exposure has risen since 2017,\nprimarily due to occupational shifts rather than changes in task profiles. The\nprice premium for AI-exposed tasks declined relative to 2017, measuring\napproximately 12% lower in 2023-24. Job postings fell following the release of\nChatGPT, with job postings 5.5% lower in 2025-Q2 than if pre-GPT hiring\npatterns had persisted. GAISI offers a robust framework for assessing AI's\nimpact on work, providing early evidence that displacement effects may already\noutweigh productivity gains."
                },
                "authors": [
                    {
                        "name": "Golo Henseke"
                    },
                    {
                        "name": "Rhys Davies"
                    },
                    {
                        "name": "Alan Felstead"
                    },
                    {
                        "name": "Duncan Gallie"
                    },
                    {
                        "name": "Francis Green"
                    },
                    {
                        "name": "Ying Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zhou"
                },
                "author": "Ying Zhou",
                "arxiv_comment": "51 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02195v2",
                "updated": "2025-08-13T10:03:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    3,
                    30,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-03T00:40:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    0,
                    40,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "SymCERE: Symmetric Contrastive Learning for Robust Review-Enhanced\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymCERE: Symmetric Contrastive Learning for Robust Review-Enhanced\n  Recommendation"
                },
                "summary": "Modern recommendation systems can achieve high performance by fusing user\nbehavior graphs (via GNNs) and review texts (via LLMs). However, this fusion\nfaces three significant issues: (1) False Negatives in contrastive learning can\ndegrade the training signal by penalizing similar items; (2) Popularity Bias,\noften encoded as embedding magnitude, can distort similarity scores; and (3)\nSignal Ambiguity, which arises from the conflation of objective facts with\nsubjective sentiment in reviews. These interconnected issues can prevent models\nfrom learning users' true preferences. In this paper, we propose SymCERE\n(Symmetric SINCERE), a contrastive learning method that addresses these three\nissues simultaneously through its structural design. First, we introduce a\nsymmetric application of the SINCERE loss for cross-modal alignment, which is\ndesigned to eliminate false negatives in recommendation. Second, by integrating\nthis with L2 normalisation under a \"magnitude-as-noise\" hypothesis, we aim to\nmitigate popularity bias by forcing the model to encode preferences primarily\nin the vector's direction. Experiments on 15 datasets from three distinct\nplatforms (e-commerce, local reviews, and travel) demonstrate that SymCERE\noutperforms several strong baselines, achieving a relative improvement of up to\n43.6% on NDCG@10. Furthermore, a detailed LIME analysis shows that the model\nlearns to anchor alignment on objective, informative vocabulary (e.g., \"OEM,\"\n\"compatible,\" \"gasket\"), while placing less emphasis on generic sentiment\n(e.g., \"good,\" \"great\"). This suggests that effective semantic alignment stems\nfrom understanding factual product attributes, offering a path toward more\naccurate recommendation systems. The code is available at:\nhttps://anonymous.4open.science/r/ReviewGNN-2E1E.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommendation systems can achieve high performance by fusing user\nbehavior graphs (via GNNs) and review texts (via LLMs). However, this fusion\nfaces three significant issues: (1) False Negatives in contrastive learning can\ndegrade the training signal by penalizing similar items; (2) Popularity Bias,\noften encoded as embedding magnitude, can distort similarity scores; and (3)\nSignal Ambiguity, which arises from the conflation of objective facts with\nsubjective sentiment in reviews. These interconnected issues can prevent models\nfrom learning users' true preferences. In this paper, we propose SymCERE\n(Symmetric SINCERE), a contrastive learning method that addresses these three\nissues simultaneously through its structural design. First, we introduce a\nsymmetric application of the SINCERE loss for cross-modal alignment, which is\ndesigned to eliminate false negatives in recommendation. Second, by integrating\nthis with L2 normalisation under a \"magnitude-as-noise\" hypothesis, we aim to\nmitigate popularity bias by forcing the model to encode preferences primarily\nin the vector's direction. Experiments on 15 datasets from three distinct\nplatforms (e-commerce, local reviews, and travel) demonstrate that SymCERE\noutperforms several strong baselines, achieving a relative improvement of up to\n43.6% on NDCG@10. Furthermore, a detailed LIME analysis shows that the model\nlearns to anchor alignment on objective, informative vocabulary (e.g., \"OEM,\"\n\"compatible,\" \"gasket\"), while placing less emphasis on generic sentiment\n(e.g., \"good,\" \"great\"). This suggests that effective semantic alignment stems\nfrom understanding factual product attributes, offering a path toward more\naccurate recommendation systems. The code is available at:\nhttps://anonymous.4open.science/r/ReviewGNN-2E1E."
                },
                "authors": [
                    {
                        "name": "Toyotaro Suzumura"
                    },
                    {
                        "name": "Hisashi Ikari"
                    },
                    {
                        "name": "Hiroki Kanezashi"
                    },
                    {
                        "name": "Md Mostafizur Rahman"
                    },
                    {
                        "name": "Yu Hirate"
                    }
                ],
                "author_detail": {
                    "name": "Yu Hirate"
                },
                "author": "Yu Hirate",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10024v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10024v4",
                "updated": "2025-08-13T10:02:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    10,
                    2,
                    30,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-14T08:06:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    6,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles"
                },
                "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."
                },
                "authors": [
                    {
                        "name": "Shaolun Ruan"
                    },
                    {
                        "name": "Rui Sheng"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Tim Dwyer"
                    },
                    {
                        "name": "Jiannan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiannan Li"
                },
                "author": "Jiannan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10024v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10024v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09670v1",
                "updated": "2025-08-13T09:58:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    58,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    58,
                    10,
                    2,
                    225,
                    0
                ],
                "title": "MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR\n  Advancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR\n  Advancement"
                },
                "summary": "Recent advances demonstrate that reinforcement learning with verifiable\nrewards (RLVR) significantly enhances the reasoning capabilities of large\nlanguage models (LLMs). However, standard RLVR faces challenges with reward\nsparsity, where zero rewards from consistently incorrect candidate answers\nprovide no learning signal, particularly in challenging tasks. To address this,\nwe propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative\nframework that utilizes diverse expert prompts as system prompts to generate a\nbroader range of responses, substantially increasing the likelihood of\nidentifying correct solutions. Additionally, we introduce an inter-expert\nmutual learning mechanism that facilitates knowledge sharing and transfer among\nexperts, further boosting the model's performance through RLVR. Extensive\nexperiments across multiple reasoning benchmarks show that MEML-GRPO delivers\nsignificant improvements, achieving an average performance gain of 4.89% with\nQwen and 11.33% with Llama, effectively overcoming the core limitations of\ntraditional RLVR methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that reinforcement learning with verifiable\nrewards (RLVR) significantly enhances the reasoning capabilities of large\nlanguage models (LLMs). However, standard RLVR faces challenges with reward\nsparsity, where zero rewards from consistently incorrect candidate answers\nprovide no learning signal, particularly in challenging tasks. To address this,\nwe propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative\nframework that utilizes diverse expert prompts as system prompts to generate a\nbroader range of responses, substantially increasing the likelihood of\nidentifying correct solutions. Additionally, we introduce an inter-expert\nmutual learning mechanism that facilitates knowledge sharing and transfer among\nexperts, further boosting the model's performance through RLVR. Extensive\nexperiments across multiple reasoning benchmarks show that MEML-GRPO delivers\nsignificant improvements, achieving an average performance gain of 4.89% with\nQwen and 11.33% with Llama, effectively overcoming the core limitations of\ntraditional RLVR methods."
                },
                "authors": [
                    {
                        "name": "Weitao Jia"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Weijie Yin"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Teng Fu"
                    },
                    {
                        "name": "Changhong Jin"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Xiaohui Lv"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09669v1",
                "updated": "2025-08-13T09:57:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    57,
                    24,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:57:24Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    57,
                    24,
                    2,
                    225,
                    0
                ],
                "title": "Trapping, chaos and averaging in bubbling AdS spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trapping, chaos and averaging in bubbling AdS spaces"
                },
                "summary": "We discuss chaos and ensemble averaging in 1/2 BPS bubbling $AdS$ spaces of\nLin, Lunin and Maldacena (LLM) by studying trapped and escaping null geodesics\nand estimating their decay rates. We find typical chaotic scattering behavior\nand confirm the Pesin relation between escape rates, Lyapunov exponents and\nKolmogorov-Sinai entropy. On the other hand, for geodesics in coarse-grained\n(grayscale) LLM geometries (which exhibit a naked singularity) chaos is\nstrongly suppressed, which is consistent with orbits and escape rates averaged\nover microscopic backgrounds. Also the singularities in these grayscale\ngeometries produce an attractive potential and have some similarities to black\nhole throats trapping geodesics for a long time. Overall, averaging over the\nensembles of LLM geometries brings us closer toward the typical behavior of\ngeodesics in black hole backgrounds, but some important differences remain, in\nparticular the existence of a threshold timescale when the averaging fails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss chaos and ensemble averaging in 1/2 BPS bubbling $AdS$ spaces of\nLin, Lunin and Maldacena (LLM) by studying trapped and escaping null geodesics\nand estimating their decay rates. We find typical chaotic scattering behavior\nand confirm the Pesin relation between escape rates, Lyapunov exponents and\nKolmogorov-Sinai entropy. On the other hand, for geodesics in coarse-grained\n(grayscale) LLM geometries (which exhibit a naked singularity) chaos is\nstrongly suppressed, which is consistent with orbits and escape rates averaged\nover microscopic backgrounds. Also the singularities in these grayscale\ngeometries produce an attractive potential and have some similarities to black\nhole throats trapping geodesics for a long time. Overall, averaging over the\nensembles of LLM geometries brings us closer toward the typical behavior of\ngeodesics in black hole backgrounds, but some important differences remain, in\nparticular the existence of a threshold timescale when the averaging fails."
                },
                "authors": [
                    {
                        "name": "David Berenstein"
                    },
                    {
                        "name": "Mihailo ubrovi"
                    },
                    {
                        "name": "Vladan Djuki"
                    }
                ],
                "author_detail": {
                    "name": "Vladan Djuki"
                },
                "author": "Vladan Djuki",
                "arxiv_comment": "33 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09666v1",
                "updated": "2025-08-13T09:56:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    56,
                    8,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:56:08Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    56,
                    8,
                    2,
                    225,
                    0
                ],
                "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought\n  Distillation"
                },
                "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs."
                },
                "authors": [
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Qingyue Yuan"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01330v2",
                "updated": "2025-08-13T09:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    51,
                    20,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-03T13:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    9,
                    21,
                    0,
                    34,
                    0
                ],
                "title": "Accelerating Linear Recurrent Neural Networks for the Edge with\n  Unstructured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Linear Recurrent Neural Networks for the Edge with\n  Unstructured Sparsity"
                },
                "summary": "Linear recurrent neural networks enable powerful long-range sequence modeling\nwith constant memory usage and time-per-token during inference. These\narchitectures hold promise for streaming applications at the edge, but\ndeployment in resource-constrained environments requires hardware-aware\noptimizations to minimize latency and energy consumption. Unstructured sparsity\noffers a compelling solution, enabling substantial reductions in compute and\nmemory requirements--when accelerated by compatible hardware platforms. In this\npaper, we conduct a scaling study to investigate the Pareto front of\nperformance and efficiency across inference compute budgets. We find that\nhighly sparse linear RNNs consistently achieve better efficiency-performance\ntrade-offs than dense baselines, with 2x less compute and 36% less memory at\niso-accuracy. Our models achieve state-of-the-art results on a real-time\nstreaming task for audio denoising. By quantizing our sparse models to\nfixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic\nchip for real-time processing, we translate model compression into tangible\ngains of 42x lower latency and 149x lower energy consumption compared to a\ndense model on an edge GPU. Our findings showcase the transformative potential\nof unstructured sparsity, paving the way for highly efficient recurrent neural\nnetworks in real-world, resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear recurrent neural networks enable powerful long-range sequence modeling\nwith constant memory usage and time-per-token during inference. These\narchitectures hold promise for streaming applications at the edge, but\ndeployment in resource-constrained environments requires hardware-aware\noptimizations to minimize latency and energy consumption. Unstructured sparsity\noffers a compelling solution, enabling substantial reductions in compute and\nmemory requirements--when accelerated by compatible hardware platforms. In this\npaper, we conduct a scaling study to investigate the Pareto front of\nperformance and efficiency across inference compute budgets. We find that\nhighly sparse linear RNNs consistently achieve better efficiency-performance\ntrade-offs than dense baselines, with 2x less compute and 36% less memory at\niso-accuracy. Our models achieve state-of-the-art results on a real-time\nstreaming task for audio denoising. By quantizing our sparse models to\nfixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic\nchip for real-time processing, we translate model compression into tangible\ngains of 42x lower latency and 149x lower energy consumption compared to a\ndense model on an edge GPU. Our findings showcase the transformative potential\nof unstructured sparsity, paving the way for highly efficient recurrent neural\nnetworks in real-world, resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Alessandro Pierro"
                    },
                    {
                        "name": "Steven Abreu"
                    },
                    {
                        "name": "Jonathan Timcheck"
                    },
                    {
                        "name": "Philipp Stratmann"
                    },
                    {
                        "name": "Andreas Wild"
                    },
                    {
                        "name": "Sumit Bam Shrestha"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Bam Shrestha"
                },
                "author": "Sumit Bam Shrestha",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09663v1",
                "updated": "2025-08-13T09:50:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    50,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:50:10Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    50,
                    10,
                    2,
                    225,
                    0
                ],
                "title": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for\n  Kubernetes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for\n  Kubernetes"
                },
                "summary": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to\nsupport increasingly complex and multi-tenant scientific workflows. These\nsystems require reconciliation of the isolation requirements of native cloud\nworkloads and the performance demands of HPC applications. In this context,\nnetworking hardware is a critical boundary component: it is the conduit for\nhigh-throughput, low-latency communication and enables isolation across\ntenants. HPE Slingshot is a high-speed network interconnect that provides up to\n200 Gbps of throughput per port and targets high-performance computing (HPC)\nsystems. The Slingshot host software, including hardware drivers and network\nmiddleware libraries, is designed to meet HPC deployments, which predominantly\nuse single-tenant access modes. Hence, the Slingshot stack is not suited for\nsecure use in multi-tenant deployments, such as converged HPC-Cloud\ndeployments. In this paper, we design and implement an extension to the\nSlingshot stack targeting converged deployments on the basis of Kubernetes. Our\nintegration provides secure, container-granular, and multi-tenant access to\nSlingshot RDMA networking capabilities at minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to\nsupport increasingly complex and multi-tenant scientific workflows. These\nsystems require reconciliation of the isolation requirements of native cloud\nworkloads and the performance demands of HPC applications. In this context,\nnetworking hardware is a critical boundary component: it is the conduit for\nhigh-throughput, low-latency communication and enables isolation across\ntenants. HPE Slingshot is a high-speed network interconnect that provides up to\n200 Gbps of throughput per port and targets high-performance computing (HPC)\nsystems. The Slingshot host software, including hardware drivers and network\nmiddleware libraries, is designed to meet HPC deployments, which predominantly\nuse single-tenant access modes. Hence, the Slingshot stack is not suited for\nsecure use in multi-tenant deployments, such as converged HPC-Cloud\ndeployments. In this paper, we design and implement an extension to the\nSlingshot stack targeting converged deployments on the basis of Kubernetes. Our\nintegration provides secure, container-granular, and multi-tenant access to\nSlingshot RDMA networking capabilities at minimal overhead."
                },
                "authors": [
                    {
                        "name": "Philipp A. Friese"
                    },
                    {
                        "name": "Ahmed Eleliemy"
                    },
                    {
                        "name": "Utz-Uwe Haus"
                    },
                    {
                        "name": "Martin Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schulz"
                },
                "author": "Martin Schulz",
                "arxiv_comment": "10 pages, 12 figures, 1 table, 3 listings, to be published in IEEE\n  Cluster 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09662v1",
                "updated": "2025-08-13T09:48:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    48,
                    23,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:48:23Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    48,
                    23,
                    2,
                    225,
                    0
                ],
                "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability\n  Coverage Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiEval: Efficient and Generalizable Model Evaluation via Capability\n  Coverage Maximization"
                },
                "summary": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Yaoning Wang"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Yugang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yugang Jiang"
                },
                "author": "Yugang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09660v1",
                "updated": "2025-08-13T09:44:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    44,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:44:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    44,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Anomaly Detection for IoT Global Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly Detection for IoT Global Connectivity"
                },
                "summary": "Internet of Things (IoT) application providers rely on Mobile Network\nOperators (MNOs) and roaming infrastructures to deliver their services\nglobally. In this complex ecosystem, where the end-to-end communication path\ntraverses multiple entities, it has become increasingly challenging to\nguarantee communication availability and reliability. Further, most platform\noperators use a reactive approach to communication issues, responding to user\ncomplaints only after incidents have become severe, compromising service\nquality. This paper presents our experience in the design and deployment of\nANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity\nservice of a large global roaming platform. ANCHOR assists engineers by\nfiltering vast amounts of data to identify potential problematic clients (i.e.,\nthose with connectivity issues affecting several of their IoT devices),\nenabling proactive issue resolution before the service is critically impacted.\nWe first describe the IoT service, infrastructure, and network visibility of\nthe IoT connectivity provider we operate. Second, we describe the main\nchallenges and operational requirements for designing an unsupervised anomaly\ndetection solution on this platform. Following these guidelines, we propose\ndifferent statistical rules, and machine- and deep-learning models for IoT\nverticals anomaly detection based on passive signaling traffic. We describe the\nsteps we followed working with the operational teams on the design and\nevaluation of our solution on the operational platform, and report an\nevaluation on operational IoT customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet of Things (IoT) application providers rely on Mobile Network\nOperators (MNOs) and roaming infrastructures to deliver their services\nglobally. In this complex ecosystem, where the end-to-end communication path\ntraverses multiple entities, it has become increasingly challenging to\nguarantee communication availability and reliability. Further, most platform\noperators use a reactive approach to communication issues, responding to user\ncomplaints only after incidents have become severe, compromising service\nquality. This paper presents our experience in the design and deployment of\nANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity\nservice of a large global roaming platform. ANCHOR assists engineers by\nfiltering vast amounts of data to identify potential problematic clients (i.e.,\nthose with connectivity issues affecting several of their IoT devices),\nenabling proactive issue resolution before the service is critically impacted.\nWe first describe the IoT service, infrastructure, and network visibility of\nthe IoT connectivity provider we operate. Second, we describe the main\nchallenges and operational requirements for designing an unsupervised anomaly\ndetection solution on this platform. Following these guidelines, we propose\ndifferent statistical rules, and machine- and deep-learning models for IoT\nverticals anomaly detection based on passive signaling traffic. We describe the\nsteps we followed working with the operational teams on the design and\nevaluation of our solution on the operational platform, and report an\nevaluation on operational IoT customers."
                },
                "authors": [
                    {
                        "name": "Jesus Omaa Iglesias"
                    },
                    {
                        "name": "Carlos Segura Perales"
                    },
                    {
                        "name": "Stefan Geiler"
                    },
                    {
                        "name": "Diego Perino"
                    },
                    {
                        "name": "Andra Lutu"
                    }
                ],
                "author_detail": {
                    "name": "Andra Lutu"
                },
                "author": "Andra Lutu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14332v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14332v4",
                "updated": "2025-08-13T09:39:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    39,
                    24,
                    2,
                    225,
                    0
                ],
                "published": "2024-10-18T09:44:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "ViCToR: Improving Visual Comprehension via Token Reconstruction for\n  Pretraining LMMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViCToR: Improving Visual Comprehension via Token Reconstruction for\n  Pretraining LMMs"
                },
                "summary": "Large Multimodal Models (LMMs) often face a modality representation gap\nduring pretraining: while language embeddings remain stable, visual\nrepresentations are highly sensitive to contextual noise (e.g., background\nclutter). To address this issue, we introduce a visual comprehension stage,\nwhich we call ViCToR (Visual Comprehension via Token Reconstruction), a novel\npretraining framework for LMMs. ViCToR employs a learnable visual token pool\nand utilizes the Hungarian matching algorithm to select semantically relevant\ntokens from this pool for visual token replacement. Furthermore, by integrating\na visual token reconstruction loss with dense semantic supervision, ViCToR can\nlearn tokens which retain high visual detail, thereby enhancing the large\nlanguage model's (LLM's) understanding of visual information. After pretraining\non 3 million publicly accessible images and captions, ViCToR achieves\nstate-of-the-art results, improving over LLaVA-NeXT-8B by 10.4%, 3.2%, and 7.2%\non the MMStar, SEED$^I$, and RealWorldQA benchmarks, respectively. Code is\navailable at https://github.com/deepglint/Victor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) often face a modality representation gap\nduring pretraining: while language embeddings remain stable, visual\nrepresentations are highly sensitive to contextual noise (e.g., background\nclutter). To address this issue, we introduce a visual comprehension stage,\nwhich we call ViCToR (Visual Comprehension via Token Reconstruction), a novel\npretraining framework for LMMs. ViCToR employs a learnable visual token pool\nand utilizes the Hungarian matching algorithm to select semantically relevant\ntokens from this pool for visual token replacement. Furthermore, by integrating\na visual token reconstruction loss with dense semantic supervision, ViCToR can\nlearn tokens which retain high visual detail, thereby enhancing the large\nlanguage model's (LLM's) understanding of visual information. After pretraining\non 3 million publicly accessible images and captions, ViCToR achieves\nstate-of-the-art results, improving over LLaVA-NeXT-8B by 10.4%, 3.2%, and 7.2%\non the MMStar, SEED$^I$, and RealWorldQA benchmarks, respectively. Code is\navailable at https://github.com/deepglint/Victor."
                },
                "authors": [
                    {
                        "name": "Yin Xie"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Peirou Liang"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Yongle Zhao"
                    },
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Roy Miles"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "10 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14332v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14332v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09653v1",
                "updated": "2025-08-13T09:37:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    37,
                    7,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:37:07Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    37,
                    7,
                    2,
                    225,
                    0
                ],
                "title": "On Negative-aware Preference Optimization for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Negative-aware Preference Optimization for Recommendation"
                },
                "summary": "Recommendation systems leverage user interaction data to suggest relevant\nitems while filtering out irrelevant (negative) ones. The rise of large\nlanguage models (LLMs) has garnered increasing attention for their potential in\nrecommendation tasks. However, existing methods for optimizing LLM-based\nrecommenders face challenges in effectively utilizing negative samples. Simply\nintegrating large numbers of negative samples can improve ranking accuracy and\nmitigate popularity bias but often leads to increased computational overhead\nand memory costs. Additionally, current approaches fail to account for the\nvarying informativeness of negative samples, leading to suboptimal optimization\nperformance. To address these issues, we propose NAPO\n(\\textbf{N}egative-\\textbf{A}ware \\textbf{P}reference \\textbf{O}ptimization),\nan enhanced framework for preference optimization in LLM-based recommendation.\nNAPO introduces two key innovations: (1) in-batch negative sharing, which\nexpands the pool of negative samples without additional memory overhead, and\n(2) dynamic reward margin adjustment, which adapts model updates based on the\nconfidence of negative samples. Extensive experiments on three public datasets\ndemonstrate that NAPO outperforms existing methods in both recommendation\naccuracy and popularity bias reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation systems leverage user interaction data to suggest relevant\nitems while filtering out irrelevant (negative) ones. The rise of large\nlanguage models (LLMs) has garnered increasing attention for their potential in\nrecommendation tasks. However, existing methods for optimizing LLM-based\nrecommenders face challenges in effectively utilizing negative samples. Simply\nintegrating large numbers of negative samples can improve ranking accuracy and\nmitigate popularity bias but often leads to increased computational overhead\nand memory costs. Additionally, current approaches fail to account for the\nvarying informativeness of negative samples, leading to suboptimal optimization\nperformance. To address these issues, we propose NAPO\n(\\textbf{N}egative-\\textbf{A}ware \\textbf{P}reference \\textbf{O}ptimization),\nan enhanced framework for preference optimization in LLM-based recommendation.\nNAPO introduces two key innovations: (1) in-batch negative sharing, which\nexpands the pool of negative samples without additional memory overhead, and\n(2) dynamic reward margin adjustment, which adapts model updates based on the\nconfidence of negative samples. Extensive experiments on three public datasets\ndemonstrate that NAPO outperforms existing methods in both recommendation\naccuracy and popularity bias reduction."
                },
                "authors": [
                    {
                        "name": "Chenlu Ding"
                    },
                    {
                        "name": "Daoxuan Liu"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xingyu Hu"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Yongkang Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09649v1",
                "updated": "2025-08-13T09:31:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    31,
                    6,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:31:06Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    31,
                    6,
                    2,
                    225,
                    0
                ],
                "title": "The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025\n  Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025\n  Challenge"
                },
                "summary": "Accurate intraoperative image guidance is critical for achieving maximal safe\nresection in brain tumor surgery, yet neuronavigation systems based on\npreoperative MRI lose accuracy during the procedure due to brain shift.\nAligning post-resection intraoperative ultrasound (iUS) with preoperative MRI\ncan restore spatial accuracy by estimating brain shift deformations, but it\nremains a challenging problem given the large anatomical and topological\nchanges and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge\nprovides the largest public benchmark for this task, built upon the ReMIND\ndataset. It offers 99 training cases, 5 validation cases, and 10 private test\ncases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.\nData are provided without annotations for training, while validation and test\nperformance are evaluated on manually annotated anatomical landmarks. Metrics\ninclude target registration error (TRE), robustness to worst-case landmark\nmisalignment (TRE30), and runtime. By establishing a standardized evaluation\nframework for this clinically critical and technically complex problem,\nReMIND2Reg aims to accelerate the development of robust, generalizable, and\nclinically deployable multimodal registration algorithms for image-guided\nneurosurgery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate intraoperative image guidance is critical for achieving maximal safe\nresection in brain tumor surgery, yet neuronavigation systems based on\npreoperative MRI lose accuracy during the procedure due to brain shift.\nAligning post-resection intraoperative ultrasound (iUS) with preoperative MRI\ncan restore spatial accuracy by estimating brain shift deformations, but it\nremains a challenging problem given the large anatomical and topological\nchanges and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge\nprovides the largest public benchmark for this task, built upon the ReMIND\ndataset. It offers 99 training cases, 5 validation cases, and 10 private test\ncases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.\nData are provided without annotations for training, while validation and test\nperformance are evaluated on manually annotated anatomical landmarks. Metrics\ninclude target registration error (TRE), robustness to worst-case landmark\nmisalignment (TRE30), and runtime. By establishing a standardized evaluation\nframework for this clinically critical and technically complex problem,\nReMIND2Reg aims to accelerate the development of robust, generalizable, and\nclinically deployable multimodal registration algorithms for image-guided\nneurosurgery."
                },
                "authors": [
                    {
                        "name": "Reuben Dorent"
                    },
                    {
                        "name": "Laura Rigolo"
                    },
                    {
                        "name": "Colin P. Galvin"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Mattias P. Heinrich"
                    },
                    {
                        "name": "Aaron Carass"
                    },
                    {
                        "name": "Olivier Colliot"
                    },
                    {
                        "name": "Demian Wassermann"
                    },
                    {
                        "name": "Alexandra Golby"
                    },
                    {
                        "name": "Tina Kapur"
                    },
                    {
                        "name": "William Wells"
                    }
                ],
                "author_detail": {
                    "name": "William Wells"
                },
                "author": "William Wells",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09648v1",
                "updated": "2025-08-13T09:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    30,
                    41,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    30,
                    41,
                    2,
                    225,
                    0
                ],
                "title": "ReqInOne: A Large Language Model-Based Agent for Software Requirements\n  Specification Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReqInOne: A Large Language Model-Based Agent for Software Requirements\n  Specification Generation"
                },
                "summary": "Software Requirements Specification (SRS) is one of the most important\ndocuments in software projects, but writing it manually is time-consuming and\noften leads to ambiguity. Existing automated methods rely heavily on manual\nanalysis, while recent Large Language Model (LLM)-based approaches suffer from\nhallucinations and limited controllability. In this paper, we propose ReqInOne,\nan LLM-based agent that follows the common steps taken by human requirements\nengineers when writing an SRS to convert natural language into a structured\nSRS. ReqInOne adopts a modular architecture by decomposing SRS generation into\nthree tasks: summary, requirement extraction, and requirement classification,\neach supported by tailored prompt templates to improve the quality and\nconsistency of LLM outputs.\n  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the\ngenerated SRSs against those produced by the holistic GPT-4-based method from\nprior work as well as by entry-level requirements engineers. Expert evaluations\nshow that ReqInOne produces more accurate and well-structured SRS documents.\nThe performance advantage of ReqInOne benefits from its modular design, and\nexperimental results further demonstrate that its requirement classification\ncomponent achieves comparable or even better results than the state-of-the-art\nrequirement classification model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Requirements Specification (SRS) is one of the most important\ndocuments in software projects, but writing it manually is time-consuming and\noften leads to ambiguity. Existing automated methods rely heavily on manual\nanalysis, while recent Large Language Model (LLM)-based approaches suffer from\nhallucinations and limited controllability. In this paper, we propose ReqInOne,\nan LLM-based agent that follows the common steps taken by human requirements\nengineers when writing an SRS to convert natural language into a structured\nSRS. ReqInOne adopts a modular architecture by decomposing SRS generation into\nthree tasks: summary, requirement extraction, and requirement classification,\neach supported by tailored prompt templates to improve the quality and\nconsistency of LLM outputs.\n  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the\ngenerated SRSs against those produced by the holistic GPT-4-based method from\nprior work as well as by entry-level requirements engineers. Expert evaluations\nshow that ReqInOne produces more accurate and well-structured SRS documents.\nThe performance advantage of ReqInOne benefits from its modular design, and\nexperimental results further demonstrate that its requirement classification\ncomponent achieves comparable or even better results than the state-of-the-art\nrequirement classification model."
                },
                "authors": [
                    {
                        "name": "Taohong Zhu"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Youcheng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Youcheng Sun"
                },
                "author": "Youcheng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19860v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19860v3",
                "updated": "2025-08-13T09:30:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    30,
                    16,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-28T14:50:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback"
                },
                "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks."
                },
                "authors": [
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19860v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19860v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21518v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21518v3",
                "updated": "2025-08-13T09:26:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    26,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-04-30T11:13:52Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    11,
                    13,
                    52,
                    2,
                    120,
                    0
                ],
                "title": "Confidential Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Serverless Computing"
                },
                "summary": "Although serverless computing offers compelling cost and deployment\nsimplicity advantages, a significant challenge remains in securely managing\nsensitive data as it flows through the network of ephemeral function executions\nin serverless computing environments within untrusted clouds. While\nConfidential Virtual Machines (CVMs) offer a promising secure execution\nenvironment, their integration with serverless architectures currently faces\nfundamental limitations in key areas: security, performance, and resource\nefficiency. We present WALLET, a confidential computing system for secure\nserverless deployments to overcome these limitations. By employing nested\nconfidential execution and a decoupled guest OS within CVMs, WALLET runs each\nfunction in a minimal \"trustlet\", significantly improving security through a\nreduced Trusted Computing Base (TCB). Furthermore, by leveraging a data-centric\nI/O architecture built upon a lightweight LibOS, WALLET optimizes network\ncommunication to address performance and resource efficiency challenges. Our\nevaluation shows that compared to CVM-based deployments, WALLET has 4.3x\nsmaller TCB, improves end-to-end latency (15-93%), achieves higher function\ndensity (up to 907x), and reduces inter-function communication (up to 27x) and\nfunction chaining latency (16.7-30.2x); thus, WALLET offers a practical system\nfor confidential serverless computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although serverless computing offers compelling cost and deployment\nsimplicity advantages, a significant challenge remains in securely managing\nsensitive data as it flows through the network of ephemeral function executions\nin serverless computing environments within untrusted clouds. While\nConfidential Virtual Machines (CVMs) offer a promising secure execution\nenvironment, their integration with serverless architectures currently faces\nfundamental limitations in key areas: security, performance, and resource\nefficiency. We present WALLET, a confidential computing system for secure\nserverless deployments to overcome these limitations. By employing nested\nconfidential execution and a decoupled guest OS within CVMs, WALLET runs each\nfunction in a minimal \"trustlet\", significantly improving security through a\nreduced Trusted Computing Base (TCB). Furthermore, by leveraging a data-centric\nI/O architecture built upon a lightweight LibOS, WALLET optimizes network\ncommunication to address performance and resource efficiency challenges. Our\nevaluation shows that compared to CVM-based deployments, WALLET has 4.3x\nsmaller TCB, improves end-to-end latency (15-93%), achieves higher function\ndensity (up to 907x), and reduces inter-function communication (up to 27x) and\nfunction chaining latency (16.7-30.2x); thus, WALLET offers a practical system\nfor confidential serverless computing."
                },
                "authors": [
                    {
                        "name": "Patrick Sabanic"
                    },
                    {
                        "name": "Masanori Misono"
                    },
                    {
                        "name": "Teofil Bodea"
                    },
                    {
                        "name": "Julian Pritzi"
                    },
                    {
                        "name": "Michael Hackl"
                    },
                    {
                        "name": "Dimitrios Stavrakakis"
                    },
                    {
                        "name": "Pramod Bhatotia"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Bhatotia"
                },
                "author": "Pramod Bhatotia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21518v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21518v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08331v2",
                "updated": "2025-08-13T09:11:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    11,
                    34,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-10T11:24:40Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    11,
                    24,
                    40,
                    6,
                    222,
                    0
                ],
                "title": "miRKatAI: An Integrated Database and Multi-agent AI system for microRNA\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "miRKatAI: An Integrated Database and Multi-agent AI system for microRNA\n  Research"
                },
                "summary": "MicroRNAs (miRs) are robust regulators of gene expression, implicated in most\nbiological processes. microRNAs predominantly downregulate the expression of\ngenes post-transcriptionally and each miR is predicted to target several\nhundred genes. The accurate identification and annotation of miR-mRNA target\ninteractions is central to understanding miRs function and their therapeutic\npotential. However, computational target prediction is challenging due to\nimperfect complementarity of miRs with their targets and the growing volume and\nheterogeneity of experimental data present challenges in accessing,\nintegrating, and analysing miR-target interaction information across biological\ncontexts. This creates a need for integrated resources and intelligent query\ntools.\n  We present the miRKat Suite, comprising miRKatDB, a comprehensive, curated\ndatabase of predicted and validated miR-target interactions and associated\nannotations, and miRKatAI, a multi-agent system powered by large language\nmodels (LLMs) and LangGraph. miRKatDB integrates data from multiple publicly\navailable sources, providing a comprehensive foundation for miR studies,\nincluding miR target genes and changes in levels of tissue expression\npreviously reported. miRKatAI offers a natural language interface for complex\nquerying of miRKatDB, facilitates grounded information retrieval from\nestablished sources in the field, and supports basic data visualisation. The\nmiRKat Suite aims to accelerate miR research by streamlining data access,\nenhancing exploratory analysis, and supporting hypothesis generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MicroRNAs (miRs) are robust regulators of gene expression, implicated in most\nbiological processes. microRNAs predominantly downregulate the expression of\ngenes post-transcriptionally and each miR is predicted to target several\nhundred genes. The accurate identification and annotation of miR-mRNA target\ninteractions is central to understanding miRs function and their therapeutic\npotential. However, computational target prediction is challenging due to\nimperfect complementarity of miRs with their targets and the growing volume and\nheterogeneity of experimental data present challenges in accessing,\nintegrating, and analysing miR-target interaction information across biological\ncontexts. This creates a need for integrated resources and intelligent query\ntools.\n  We present the miRKat Suite, comprising miRKatDB, a comprehensive, curated\ndatabase of predicted and validated miR-target interactions and associated\nannotations, and miRKatAI, a multi-agent system powered by large language\nmodels (LLMs) and LangGraph. miRKatDB integrates data from multiple publicly\navailable sources, providing a comprehensive foundation for miR studies,\nincluding miR target genes and changes in levels of tissue expression\npreviously reported. miRKatAI offers a natural language interface for complex\nquerying of miRKatDB, facilitates grounded information retrieval from\nestablished sources in the field, and supports basic data visualisation. The\nmiRKat Suite aims to accelerate miR research by streamlining data access,\nenhancing exploratory analysis, and supporting hypothesis generation."
                },
                "authors": [
                    {
                        "name": "Karen Guerrero-Vazquez"
                    },
                    {
                        "name": "Jacopo Umberto Verga"
                    },
                    {
                        "name": "Pilib O Broin"
                    },
                    {
                        "name": "Eva Szegezdi"
                    },
                    {
                        "name": "Katarzyna Goljanek-Whysall"
                    }
                ],
                "author_detail": {
                    "name": "Katarzyna Goljanek-Whysall"
                },
                "author": "Katarzyna Goljanek-Whysall",
                "arxiv_comment": "10 pages, 1 figure, app note",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07770v2",
                "updated": "2025-08-13T09:09:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    9,
                    33,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-11T08:56:19Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    56,
                    19,
                    0,
                    223,
                    0
                ],
                "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction\n  and Mobile Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentWorld: An Interactive Simulation Platform for Scene Construction\n  and Mobile Robotic Manipulation"
                },
                "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/"
                },
                "authors": [
                    {
                        "name": "Yizheng Zhang"
                    },
                    {
                        "name": "Zhenjun Yu"
                    },
                    {
                        "name": "Jiaxin Lai"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Lei Han"
                    }
                ],
                "author_detail": {
                    "name": "Lei Han"
                },
                "author": "Lei Han",
                "arxiv_comment": "Accepted by Conference on Robot Learning 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09631v1",
                "updated": "2025-08-13T09:06:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    6,
                    59,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T09:06:59Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    6,
                    59,
                    2,
                    225,
                    0
                ],
                "title": "AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated strong capabilities\nin translating natural language into database queries, especially when dealing\nwith complex graph-structured data. However, real-world queries often contain\ninherent ambiguities, and the interconnected nature of graph structures can\namplify these challenges, leading to unintended or incorrect query results. To\nsystematically evaluate LLMs on this front, we propose a taxonomy of\ngraph-query ambiguities, comprising three primary types: Attribute Ambiguity,\nRelationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided\ninto Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a\nnovel benchmark of real-world ambiguous queries paired with expert-verified\ngraph query answers. Evaluating 9 representative LLMs shows that even top\nmodels struggle with ambiguous graph queries. Our findings reveal a critical\ngap in ambiguity handling and motivate future work on specialized resolution\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated strong capabilities\nin translating natural language into database queries, especially when dealing\nwith complex graph-structured data. However, real-world queries often contain\ninherent ambiguities, and the interconnected nature of graph structures can\namplify these challenges, leading to unintended or incorrect query results. To\nsystematically evaluate LLMs on this front, we propose a taxonomy of\ngraph-query ambiguities, comprising three primary types: Attribute Ambiguity,\nRelationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided\ninto Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a\nnovel benchmark of real-world ambiguous queries paired with expert-verified\ngraph query answers. Evaluating 9 representative LLMs shows that even top\nmodels struggle with ambiguous graph queries. Our findings reveal a critical\ngap in ambiguity handling and motivate future work on specialized resolution\ntechniques."
                },
                "authors": [
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Sebastian Schelter"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00938v2",
                "updated": "2025-08-13T09:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    5,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-01T16:43:57Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    43,
                    57,
                    1,
                    182,
                    0
                ],
                "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy."
                },
                "authors": [
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "10 pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v3",
                "updated": "2025-08-13T09:00:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Pan"
                },
                "author": "Jianfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08329v3",
                "updated": "2025-08-14T01:34:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    1,
                    34,
                    8,
                    3,
                    226,
                    0
                ],
                "published": "2025-04-11T07:51:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    51,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models"
                },
                "summary": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of vocabulary. This problem limits the generalizability of\nEHR foundation models and the integration of models trained with different\nvocabularies. To alleviate this problem, we propose a set of novel medical\nconcept representations (MedRep) for EHR foundation models based on the\nobservational medical outcome partnership (OMOP) common data model (CDM). For\nconcept representation learning, we enrich the information of each concept with\na minimal definition through large language model (LLM) prompts and complement\nthe text-based representations through the graph ontology of OMOP vocabulary.\nOur approach outperforms the vanilla EHR foundation model and the model with a\npreviously introduced medical code tokenizer in diverse prediction tasks. We\nalso demonstrate the generalizability of MedRep through external validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of vocabulary. This problem limits the generalizability of\nEHR foundation models and the integration of models trained with different\nvocabularies. To alleviate this problem, we propose a set of novel medical\nconcept representations (MedRep) for EHR foundation models based on the\nobservational medical outcome partnership (OMOP) common data model (CDM). For\nconcept representation learning, we enrich the information of each concept with\na minimal definition through large language model (LLM) prompts and complement\nthe text-based representations through the graph ontology of OMOP vocabulary.\nOur approach outperforms the vanilla EHR foundation model and the model with a\npreviously introduced medical code tokenizer in diverse prediction tasks. We\nalso demonstrate the generalizability of MedRep through external validation."
                },
                "authors": [
                    {
                        "name": "Junmo Kim"
                    },
                    {
                        "name": "Namkyeong Lee"
                    },
                    {
                        "name": "Jiwon Kim"
                    },
                    {
                        "name": "Kwangsoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kwangsoo Kim"
                },
                "author": "Kwangsoo Kim",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09622v1",
                "updated": "2025-08-13T08:53:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    53,
                    17,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T08:53:17Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    53,
                    17,
                    2,
                    225,
                    0
                ],
                "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific\n  Abstracts in Russian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific\n  Abstracts in Russian"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025."
                },
                "authors": [
                    {
                        "name": "Tatiana Batura"
                    },
                    {
                        "name": "Elena Bruches"
                    },
                    {
                        "name": "Milana Shvenk"
                    },
                    {
                        "name": "Valentin Malykh"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Malykh"
                },
                "author": "Valentin Malykh",
                "arxiv_comment": "AINL 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09621v1",
                "updated": "2025-08-13T08:53:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    53,
                    13,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T08:53:13Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    53,
                    13,
                    2,
                    225,
                    0
                ],
                "title": "Interpretable Robot Control via Structured Behavior Trees and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Robot Control via Structured Behavior Trees and Large\n  Language Models"
                },
                "summary": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite."
                },
                "authors": [
                    {
                        "name": "Ingrid Mava Chekam"
                    },
                    {
                        "name": "Ines Pastor-Martinez"
                    },
                    {
                        "name": "Ali Tourani"
                    },
                    {
                        "name": "Jose Andres Millan-Romera"
                    },
                    {
                        "name": "Laura Ribeiro"
                    },
                    {
                        "name": "Pedro Miguel Bastos Soares"
                    },
                    {
                        "name": "Holger Voos"
                    },
                    {
                        "name": "Jose Luis Sanchez-Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Jose Luis Sanchez-Lopez"
                },
                "author": "Jose Luis Sanchez-Lopez",
                "arxiv_comment": "15 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23486v3",
                "updated": "2025-08-13T08:51:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    51,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-31T12:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains"
                },
                "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Shirui Wang"
                    },
                    {
                        "name": "Zhihui Tang"
                    },
                    {
                        "name": "Huaxia Yang"
                    },
                    {
                        "name": "Qiuhong Gong"
                    },
                    {
                        "name": "Tiantian Gu"
                    },
                    {
                        "name": "Hongyang Ma"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Wubin Sun"
                    },
                    {
                        "name": "Zeliang Lian"
                    },
                    {
                        "name": "Kehang Mao"
                    },
                    {
                        "name": "Yinan Jiang"
                    },
                    {
                        "name": "Zhicheng Huang"
                    },
                    {
                        "name": "Lingyun Ma"
                    },
                    {
                        "name": "Wenjie Shen"
                    },
                    {
                        "name": "Yajie Ji"
                    },
                    {
                        "name": "Yunhui Tan"
                    },
                    {
                        "name": "Chunbo Wang"
                    },
                    {
                        "name": "Yunlu Gao"
                    },
                    {
                        "name": "Qianling Ye"
                    },
                    {
                        "name": "Rui Lin"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Lijuan Niu"
                    },
                    {
                        "name": "Zhihao Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Mengran Lang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Huimin Zhang"
                    },
                    {
                        "name": "Haitao Shen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Qiguang Zhao"
                    },
                    {
                        "name": "Si-Xuan Liu"
                    },
                    {
                        "name": "Lina Zhou"
                    },
                    {
                        "name": "Hua Gao"
                    },
                    {
                        "name": "Dongqiang Ye"
                    },
                    {
                        "name": "Lingmin Meng"
                    },
                    {
                        "name": "Youtao Yu"
                    },
                    {
                        "name": "Naixin Liang"
                    },
                    {
                        "name": "Jianxiong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxiong Wu"
                },
                "author": "Jianxiong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09614v1",
                "updated": "2025-08-13T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    45,
                    4,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T08:45:04Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    45,
                    4,
                    2,
                    225,
                    0
                ],
                "title": "How Persuasive Could LLMs Be? A First Study Combining\n  Linguistic-Rhetorical Analysis and User Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Persuasive Could LLMs Be? A First Study Combining\n  Linguistic-Rhetorical Analysis and User Experiments"
                },
                "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research."
                },
                "authors": [
                    {
                        "name": "Daniel Raffini"
                    },
                    {
                        "name": "Agnese Macori"
                    },
                    {
                        "name": "Lorenzo Porcaro"
                    },
                    {
                        "name": "Tiziana Catarci"
                    },
                    {
                        "name": "Marco Angelini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Angelini"
                },
                "author": "Marco Angelini",
                "arxiv_comment": "9-pages",
                "arxiv_journal_ref": "20th International Conference on Artificial Intelligence and Law\n  (ICAIL)LCIC-CLAIRvoyantS Workshop, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10960v3",
                "updated": "2025-08-13T08:43:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    43,
                    45,
                    2,
                    225,
                    0
                ],
                "published": "2025-06-12T17:57:05Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    57,
                    5,
                    3,
                    163,
                    0
                ],
                "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark"
                },
                "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench."
                },
                "authors": [
                    {
                        "name": "Kangwei Liu"
                    },
                    {
                        "name": "Siyuan Cheng"
                    },
                    {
                        "name": "Bozhong Tian"
                    },
                    {
                        "name": "Xiaozhuan Liang"
                    },
                    {
                        "name": "Yuyang Yin"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04005v2",
                "updated": "2025-08-13T08:42:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    42,
                    40,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-05T11:17:20Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    11,
                    17,
                    20,
                    5,
                    186,
                    0
                ],
                "title": "Exploring a Gamified Personality Assessment Method through Interaction\n  with Multi-Personality LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring a Gamified Personality Assessment Method through Interaction\n  with Multi-Personality LLM Agents"
                },
                "summary": "The execution of effective and imperceptible personality assessments is\nreceiving increasing attention in psychology and human-computer interaction\nfields. This study explores an interactive approach for personality assessment,\nfocusing on the multiplicity of personality representation. We propose a\nframework of gamified personality assessment through multi-personality\nrepresentations (Multi-PR GPA). The framework leverages Large Language Models\nto empower virtual agents with diverse personalities. These agents elicit\nmultifaceted human personality representations through engaging in interactive\ngames. Drawing upon the multi-type textual data generated throughout the\ninteraction, it achieves two ways of personality assessments (i.e., Direct\nAssessment and Que-based Assessment) and provides interpretable insights.\nGrounded in the classic Big Five theory, we implemented a prototype system and\nconducted a user study to assess the efficacy of Multi-PR GPA. The results\nunderscore the effectiveness of our approach in personality assessment and\ndemonstrate that it achieves superior performance when considering the\nmultiplicity of personality representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The execution of effective and imperceptible personality assessments is\nreceiving increasing attention in psychology and human-computer interaction\nfields. This study explores an interactive approach for personality assessment,\nfocusing on the multiplicity of personality representation. We propose a\nframework of gamified personality assessment through multi-personality\nrepresentations (Multi-PR GPA). The framework leverages Large Language Models\nto empower virtual agents with diverse personalities. These agents elicit\nmultifaceted human personality representations through engaging in interactive\ngames. Drawing upon the multi-type textual data generated throughout the\ninteraction, it achieves two ways of personality assessments (i.e., Direct\nAssessment and Que-based Assessment) and provides interpretable insights.\nGrounded in the classic Big Five theory, we implemented a prototype system and\nconducted a user study to assess the efficacy of Multi-PR GPA. The results\nunderscore the effectiveness of our approach in personality assessment and\ndemonstrate that it achieves superior performance when considering the\nmultiplicity of personality representation."
                },
                "authors": [
                    {
                        "name": "Baiqiao Zhang"
                    },
                    {
                        "name": "Xiangxian Li"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Xinyu Gai"
                    },
                    {
                        "name": "Juan Liu"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "Yong-jin Liu"
                    },
                    {
                        "name": "Yulong Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Bian"
                },
                "author": "Yulong Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01191v3",
                "updated": "2025-08-13T08:41:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    41,
                    33,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-02T04:37:28Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    4,
                    37,
                    28,
                    5,
                    214,
                    0
                ],
                "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens"
                },
                "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning."
                },
                "authors": [
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Yancheng Wang"
                    },
                    {
                        "name": "Yingzhen Yang"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06433v2",
                "updated": "2025-08-13T08:33:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    33,
                    50,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-08T16:20:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    20,
                    56,
                    4,
                    220,
                    0
                ],
                "title": "Memp: Exploring Agent Procedural Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memp: Exploring Agent Procedural Memory"
                },
                "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06296v2",
                "updated": "2025-08-13T08:27:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    27,
                    12,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-08T13:15:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    15,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "LLM Robustness Leaderboard v1 --Technical report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Robustness Leaderboard v1 --Technical report"
                },
                "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community."
                },
                "authors": [
                    {
                        "name": "Pierre Peign - Lefebvre"
                    },
                    {
                        "name": "Quentin Feuillade-Montixi"
                    },
                    {
                        "name": "Tom David"
                    },
                    {
                        "name": "Nicolas Miailhe"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Miailhe"
                },
                "author": "Nicolas Miailhe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09594v1",
                "updated": "2025-08-13T08:18:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    18,
                    13,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T08:18:13Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    18,
                    13,
                    2,
                    225,
                    0
                ],
                "title": "LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round\n  Annotation"
                },
                "summary": "Modern computing systems, such as HDFS and Spark, produce vast quantities of\nlogs that developers use for tasks like anomaly detection and error analysis.\nTo simplify log analysis, template generation methods have been proposed to\nstandardize log formats, transforming unstructured data into structured\ntemplates. Existing heuristic-based methods and neural network-based methods\nsuffer from low accuracy problems due to the reliance on handcrafted heuristics\nor specific log patterns in training sets. Recently, large language models\n(LLMs) have shown great potential in log template generation. However, they\noften struggle with ambiguous, complex, or highly specific log content, which\ncan lead to errors in generating accurate templates. To address these\nchallenges, we propose LLMLog, a multi-round annotation framework with adaptive\nin-context learning. We first propose an edit-distance-based similarity metric\nto evaluate log similarity. Then, we introduce a method to select the most\ninformative $k$ unlabeled logs for annotation by considering both the\nrepresentativeness of the logs and the confidence of LLM predictions.\nAdditionally, we design an adaptive context selection strategy that adaptively\nselects labeled logs to ensure comprehensive keyword coverage for unlabeled\nlogs. These labeled logs serve as the context for LLMs to better understand the\nunlabeled logs, thereby enhancing the accuracy of template generation.\nExtensive experiments on sixteen datasets demonstrate that LLMLog outperforms\nthe state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computing systems, such as HDFS and Spark, produce vast quantities of\nlogs that developers use for tasks like anomaly detection and error analysis.\nTo simplify log analysis, template generation methods have been proposed to\nstandardize log formats, transforming unstructured data into structured\ntemplates. Existing heuristic-based methods and neural network-based methods\nsuffer from low accuracy problems due to the reliance on handcrafted heuristics\nor specific log patterns in training sets. Recently, large language models\n(LLMs) have shown great potential in log template generation. However, they\noften struggle with ambiguous, complex, or highly specific log content, which\ncan lead to errors in generating accurate templates. To address these\nchallenges, we propose LLMLog, a multi-round annotation framework with adaptive\nin-context learning. We first propose an edit-distance-based similarity metric\nto evaluate log similarity. Then, we introduce a method to select the most\ninformative $k$ unlabeled logs for annotation by considering both the\nrepresentativeness of the logs and the confidence of LLM predictions.\nAdditionally, we design an adaptive context selection strategy that adaptively\nselects labeled logs to ensure comprehensive keyword coverage for unlabeled\nlogs. These labeled logs serve as the context for LLMs to better understand the\nunlabeled logs, thereby enhancing the accuracy of template generation.\nExtensive experiments on sixteen datasets demonstrate that LLMLog outperforms\nthe state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Fei Teng"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09591v1",
                "updated": "2025-08-13T08:16:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    16,
                    31,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T08:16:31Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    16,
                    31,
                    2,
                    225,
                    0
                ],
                "title": "HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication\n  and Expert Swap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication\n  and Expert Swap"
                },
                "summary": "The sparsely activated mixture-of-experts (MoE) transformer has become a\ncommon architecture for large language models (LLMs) due to its sparsity, which\nrequires fewer computational demands while easily scaling the model size. In\nMoE models, each MoE layer requires to dynamically choose tokens to activate\nparticular experts for computation while the activated experts may not be\nlocated in the same device or GPU as the token. However, this leads to\nsubstantial communication and load imbalances across all GPUs, which obstructs\nthe scalability of distributed systems within a GPU cluster. To this end, we\nintroduce HierMoE to accelerate the training of MoE models by two\ntopology-aware techniques: 1) token deduplication to reduce the communication\ntraffic, and 2) expert swap to balance the workloads among all GPUs. To enable\nthe above two proposed approaches to be more general, we build theoretical\nmodels aimed at achieving the best token duplication and expert swap strategy\nunder different model configurations and hardware environments. We implement\nour prototype HierMoE system atop Megatron-LM and conduct experiments on a\n32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results\nshow that our HierMoE achieves $1.55\\times$ to $3.32\\times$ faster\ncommunication and delivers $1.18\\times$ to $1.27\\times$ faster end-to-end\ntraining compared to state-of-the-art MoE training systems, Tutel-2DH,\nSmartMoE, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sparsely activated mixture-of-experts (MoE) transformer has become a\ncommon architecture for large language models (LLMs) due to its sparsity, which\nrequires fewer computational demands while easily scaling the model size. In\nMoE models, each MoE layer requires to dynamically choose tokens to activate\nparticular experts for computation while the activated experts may not be\nlocated in the same device or GPU as the token. However, this leads to\nsubstantial communication and load imbalances across all GPUs, which obstructs\nthe scalability of distributed systems within a GPU cluster. To this end, we\nintroduce HierMoE to accelerate the training of MoE models by two\ntopology-aware techniques: 1) token deduplication to reduce the communication\ntraffic, and 2) expert swap to balance the workloads among all GPUs. To enable\nthe above two proposed approaches to be more general, we build theoretical\nmodels aimed at achieving the best token duplication and expert swap strategy\nunder different model configurations and hardware environments. We implement\nour prototype HierMoE system atop Megatron-LM and conduct experiments on a\n32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results\nshow that our HierMoE achieves $1.55\\times$ to $3.32\\times$ faster\ncommunication and delivers $1.18\\times$ to $1.27\\times$ faster end-to-end\ntraining compared to state-of-the-art MoE training systems, Tutel-2DH,\nSmartMoE, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "Wenxiang Lin"
                    },
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08961v2",
                "updated": "2025-08-13T08:08:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    8,
                    40,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T14:23:26Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    23,
                    26,
                    1,
                    224,
                    0
                ],
                "title": "DualSpeechLM: Towards Unified Speech Understanding and Generation via\n  Dual Speech Token Modeling with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualSpeechLM: Towards Unified Speech Understanding and Generation via\n  Dual Speech Token Modeling with Large Language Models"
                },
                "summary": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Yiwen Shao"
                    },
                    {
                        "name": "Hangting Chen"
                    },
                    {
                        "name": "Jiankun Zhao"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Helen Meng"
                    },
                    {
                        "name": "Xixin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xixin Wu"
                },
                "author": "Xixin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18835v2",
                "updated": "2025-08-13T08:07:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    7,
                    49,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-25T08:43:00Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    8,
                    43,
                    0,
                    2,
                    360,
                    0
                ],
                "title": "AUCAD: Automated Construction of Alignment Dataset from Log-Related\n  Issues for Enhancing LLM-based Log Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUCAD: Automated Construction of Alignment Dataset from Log-Related\n  Issues for Enhancing LLM-based Log Generation"
                },
                "summary": "Log statements have become an integral part of modern software systems. Prior\nresearch efforts have focused on supporting the decisions of placing log\nstatements, such as where/what to log. With the increasing adoption of Large\nLanguage Models (LLMs) for code-related tasks such as code completion or\ngeneration, automated approaches for generating log statements have gained much\nmomentum. However, the performance of these approaches still has a long way to\ngo. This paper explores enhancing the performance of LLM-based solutions for\nautomated log statement generation by post-training LLMs with a purpose-built\ndataset. Thus the primary contribution is a novel approach called AUCAD, which\nautomatically constructs such a dataset with information extracting from\nlog-related issues. Researchers have long noticed that a significant portion of\nthe issues in the open-source community are related to log statements. However,\ndistilling this portion of data requires manual efforts, which is\nlabor-intensive and costly, rendering it impractical. Utilizing our approach,\nwe automatically extract log-related issues from 1,537 entries of log data\nacross 88 projects and identify 808 code snippets (i.e., methods) with\nretrievable source code both before and after modification of each issue\n(including log statements) to construct a dataset. Each entry in the dataset\nconsists of a data pair representing high-quality and problematic log\nstatements, respectively. With this dataset, we proceed to post-train multiple\nLLMs (primarily from the Llama series) for automated log statement generation.\nBoth human and experimental evaluations indicate that these models\nsignificantly outperform existing LLM-based solutions, thereby validating the\nefficacy of our method for constructing a post-training dataset to enhance\nLLM-based log statement generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log statements have become an integral part of modern software systems. Prior\nresearch efforts have focused on supporting the decisions of placing log\nstatements, such as where/what to log. With the increasing adoption of Large\nLanguage Models (LLMs) for code-related tasks such as code completion or\ngeneration, automated approaches for generating log statements have gained much\nmomentum. However, the performance of these approaches still has a long way to\ngo. This paper explores enhancing the performance of LLM-based solutions for\nautomated log statement generation by post-training LLMs with a purpose-built\ndataset. Thus the primary contribution is a novel approach called AUCAD, which\nautomatically constructs such a dataset with information extracting from\nlog-related issues. Researchers have long noticed that a significant portion of\nthe issues in the open-source community are related to log statements. However,\ndistilling this portion of data requires manual efforts, which is\nlabor-intensive and costly, rendering it impractical. Utilizing our approach,\nwe automatically extract log-related issues from 1,537 entries of log data\nacross 88 projects and identify 808 code snippets (i.e., methods) with\nretrievable source code both before and after modification of each issue\n(including log statements) to construct a dataset. Each entry in the dataset\nconsists of a data pair representing high-quality and problematic log\nstatements, respectively. With this dataset, we proceed to post-train multiple\nLLMs (primarily from the Llama series) for automated log statement generation.\nBoth human and experimental evaluations indicate that these models\nsignificantly outperform existing LLM-based solutions, thereby validating the\nefficacy of our method for constructing a post-training dataset to enhance\nLLM-based log statement generation."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Dongjun Yu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Guoping Rong"
                    },
                    {
                        "name": "Yongda Yu"
                    },
                    {
                        "name": "Haifeng Shen"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Dong Shao"
                    },
                    {
                        "name": "Hongyu Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Kuang"
                },
                "author": "Hongyu Kuang",
                "arxiv_doi": "10.1145/3755881.3755889",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3755881.3755889",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.18835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In the 16th International Conference on Internetware 2025. 13 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21966v2",
                "updated": "2025-08-13T08:00:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    0,
                    47,
                    2,
                    225,
                    0
                ],
                "published": "2025-05-28T04:36:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    4,
                    36,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "MapStory: Prototyping Editable Map Animations with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapStory: Prototyping Editable Map Animations with LLM Agents"
                },
                "summary": "We introduce MapStory, an LLM-powered animation prototyping tool that\ngenerates editable map animation sequences directly from natural language text\nby leveraging a dual-agent LLM architecture. Given a user written script,\nMapStory automatically produces a scene breakdown, which decomposes the text\ninto key map animation primitives such as camera movements, visual highlights,\nand animated elements. Our system includes a researcher agent that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nautomatic extraction of relevant regions, paths, and coordinates while allowing\nusers to edit and query for changes or additional information to refine the\nresults. Additionally, users can fine-tune parameters of these primitive blocks\nthrough an interactive timeline editor. We detail the system's design and\narchitecture, informed by formative interviews with professional animators and\nby an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MapStory, an LLM-powered animation prototyping tool that\ngenerates editable map animation sequences directly from natural language text\nby leveraging a dual-agent LLM architecture. Given a user written script,\nMapStory automatically produces a scene breakdown, which decomposes the text\ninto key map animation primitives such as camera movements, visual highlights,\nand animated elements. Our system includes a researcher agent that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nautomatic extraction of relevant regions, paths, and coordinates while allowing\nusers to edit and query for changes or additional information to refine the\nresults. Additionally, users can fine-tune parameters of these primitive blocks\nthrough an interactive timeline editor. We detail the system's design and\narchitecture, informed by formative interviews with professional animators and\nby an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."
                },
                "authors": [
                    {
                        "name": "Aditya Gunturu"
                    },
                    {
                        "name": "Ben Pearman"
                    },
                    {
                        "name": "Keiichi Ihara"
                    },
                    {
                        "name": "Morteza Faraji"
                    },
                    {
                        "name": "Bryan Wang"
                    },
                    {
                        "name": "Rubaiat Habib Kazi"
                    },
                    {
                        "name": "Ryo Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Suzuki"
                },
                "author": "Ryo Suzuki",
                "arxiv_comment": "UIST 2025. Project page:\n  https://adigunturu.github.io/MapStory-UIST25/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2, H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]