[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v2",
                "updated": "2025-05-07T13:07:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    7,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v6",
                "updated": "2025-05-07T01:29:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    1,
                    29,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v6",
                "updated": "2025-05-06T19:28:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    19,
                    28,
                    56,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03763v1",
                "updated": "2025-04-21T00:21:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    21,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:21:08Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    21,
                    8,
                    0,
                    111,
                    0
                ],
                "title": "Splitwiser: Efficient LM inference with constrained resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Splitwiser: Efficient LM inference with constrained resources"
                },
                "summary": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml)."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Adney Cardoza"
                    },
                    {
                        "name": "Melissa Capo"
                    }
                ],
                "author_detail": {
                    "name": "Melissa Capo"
                },
                "author": "Melissa Capo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03756v1",
                "updated": "2025-04-19T13:17:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    13,
                    17,
                    34,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T13:17:34Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    13,
                    17,
                    34,
                    5,
                    109,
                    0
                ],
                "title": "Improving the Serving Performance of Multi-LoRA Large Language Models\n  via Efficient LoRA and KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Serving Performance of Multi-LoRA Large Language Models\n  via Efficient LoRA and KV Cache Management"
                },
                "summary": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works."
                },
                "authors": [
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Jiuchen Shi"
                    },
                    {
                        "name": "Yixiao Wang"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.05473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05473v1",
                "updated": "2025-05-08T17:59:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    59,
                    47,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:59:47Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    59,
                    47,
                    3,
                    128,
                    0
                ],
                "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and\n  Endpoint Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and\n  Endpoint Diffusion"
                },
                "summary": "Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty."
                },
                "authors": [
                    {
                        "name": "Qitao Zhao"
                    },
                    {
                        "name": "Amy Lin"
                    },
                    {
                        "name": "Jeff Tan"
                    },
                    {
                        "name": "Jason Y. Zhang"
                    },
                    {
                        "name": "Deva Ramanan"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Tulsiani"
                },
                "author": "Shubham Tulsiani",
                "arxiv_comment": "CVPR 2025. Project website: https://qitaozhao.github.io/DiffusionSfM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05470v1",
                "updated": "2025-05-08T17:58:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    58,
                    45,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:58:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    58,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-GRPO: Training Flow Matching Models via Online RL"
                },
                "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Gongye Liu"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "arxiv_comment": "Code: https://github.com/yifan123/flow_grpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05469v1",
                "updated": "2025-05-08T17:58:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    58,
                    18,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:58:18Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    58,
                    18,
                    3,
                    128,
                    0
                ],
                "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Physically Stable and Buildable LEGO Designs from Text"
                },
                "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/."
                },
                "authors": [
                    {
                        "name": "Ava Pun"
                    },
                    {
                        "name": "Kangle Deng"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Deva Ramanan"
                    },
                    {
                        "name": "Changliu Liu"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Yan Zhu"
                },
                "author": "Jun-Yan Zhu",
                "arxiv_comment": "Project page: https://avalovelace1.github.io/LegoGPT/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05467v1",
                "updated": "2025-05-08T17:57:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    57,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:57:40Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    57,
                    40,
                    3,
                    128,
                    0
                ],
                "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant"
                },
                "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Bo Feng"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Ping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Huang"
                },
                "author": "Ping Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05465v1",
                "updated": "2025-05-08T17:56:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:56:57Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    57,
                    3,
                    128,
                    0
                ],
                "title": "ComPO: Preference Alignment via Comparison Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComPO: Preference Alignment via Comparison Oracles"
                },
                "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}."
                },
                "authors": [
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wotao Yin"
                    },
                    {
                        "name": "Tianyi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Lin"
                },
                "author": "Tianyi Lin",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05464v1",
                "updated": "2025-05-08T17:56:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:56:23Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    23,
                    3,
                    128,
                    0
                ],
                "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging"
                },
                "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."
                },
                "authors": [
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "arxiv_comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14073v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14073v4",
                "updated": "2025-05-08T17:55:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    55,
                    15,
                    3,
                    128,
                    0
                ],
                "published": "2023-12-21T17:50:10Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    17,
                    50,
                    10,
                    3,
                    355,
                    0
                ],
                "title": "Nonparametric Bayesian intensity estimation for covariate-driven\n  inhomogeneous point processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Bayesian intensity estimation for covariate-driven\n  inhomogeneous point processes"
                },
                "summary": "This work studies nonparametric Bayesian estimation of the intensity function\nof an inhomogeneous Poisson point process in the important case where the\nintensity depends on covariates, based on the observation of a single\nrealisation of the point pattern over a large area. It is shown how the\npresence of covariates allows to borrow information from far away locations in\nthe observation window, enabling consistent inference in the growing domain\nasymptotics. In particular, optimal posterior contraction rates under both\nglobal and point-wise loss functions are derived. The rates in global loss are\nobtained under conditions on the prior distribution resembling those in the\nwell established theory of Bayesian nonparametrics, combined with concentration\ninequalities for functionals of stationary processes to control certain random\ncovariate-dependent loss functions appearing in the analysis. The local rates\nare derived with an ad-hoc study that builds on recent advances in the theory\nof P\\'olya tree priors, extended to the present multivariate setting with a\nnovel construction that makes use of the random geometry induced by the\ncovariates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies nonparametric Bayesian estimation of the intensity function\nof an inhomogeneous Poisson point process in the important case where the\nintensity depends on covariates, based on the observation of a single\nrealisation of the point pattern over a large area. It is shown how the\npresence of covariates allows to borrow information from far away locations in\nthe observation window, enabling consistent inference in the growing domain\nasymptotics. In particular, optimal posterior contraction rates under both\nglobal and point-wise loss functions are derived. The rates in global loss are\nobtained under conditions on the prior distribution resembling those in the\nwell established theory of Bayesian nonparametrics, combined with concentration\ninequalities for functionals of stationary processes to control certain random\ncovariate-dependent loss functions appearing in the analysis. The local rates\nare derived with an ad-hoc study that builds on recent advances in the theory\nof P\\'olya tree priors, extended to the present multivariate setting with a\nnovel construction that makes use of the random geometry induced by the\ncovariates."
                },
                "authors": [
                    {
                        "name": "Matteo Giordano"
                    },
                    {
                        "name": "Alisa Kirichenko"
                    },
                    {
                        "name": "Judith Rousseau"
                    }
                ],
                "author_detail": {
                    "name": "Judith Rousseau"
                },
                "author": "Judith Rousseau",
                "arxiv_comment": "62 pages, to appear in Bernoulli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14073v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14073v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05454v1",
                "updated": "2025-05-08T17:45:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    45,
                    17,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:45:17Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    45,
                    17,
                    3,
                    128,
                    0
                ],
                "title": "Resolution of the Solar Convective Conundrum? New Results Using the\n  Time-Distance Deep-Focus Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolution of the Solar Convective Conundrum? New Results Using the\n  Time-Distance Deep-Focus Method"
                },
                "summary": "We re-examine the deep-focus methodology of time-distance helioseismology\npreviously used to estimate the power spectrum of the solar convection at a\ndepth of about 30 Mm, which was found to be significantly weaker than predicted\nby theory and simulations. The Global Acoustic, Linearized Euler (GALE) and\nEulerian Lagrangian (EULAG) codes are used to generate ground-truth simulations\nthrough which the accuracy of the convective power spectrum can be evaluated.\nThis validation process shows that the power spectrum diverges significantly\nfrom ground truth beyond spatial scales corresponding to the spherical harmonic\ndegree $\\ell=15$ - $30$ because of the limited resolution of helioseismic\nmeasurements. However, the power estimated at larger spatial scales ($\\ell<15$)\nis sufficiently accurate. We then apply the methodology to solar data and find\na spectrum that is substantially stronger than previously reported. We discuss\nsome possible differences in methodology that might have led to the initial\nunder-estimation of solar convective power. The new spectra are in line with\nrecent hydrodynamic and magnetohydrodynamic simulations of solar convection and\nalso consistent with the previous inferences obtained by the ring-diagram\nlocal-helioseismology method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We re-examine the deep-focus methodology of time-distance helioseismology\npreviously used to estimate the power spectrum of the solar convection at a\ndepth of about 30 Mm, which was found to be significantly weaker than predicted\nby theory and simulations. The Global Acoustic, Linearized Euler (GALE) and\nEulerian Lagrangian (EULAG) codes are used to generate ground-truth simulations\nthrough which the accuracy of the convective power spectrum can be evaluated.\nThis validation process shows that the power spectrum diverges significantly\nfrom ground truth beyond spatial scales corresponding to the spherical harmonic\ndegree $\\ell=15$ - $30$ because of the limited resolution of helioseismic\nmeasurements. However, the power estimated at larger spatial scales ($\\ell<15$)\nis sufficiently accurate. We then apply the methodology to solar data and find\na spectrum that is substantially stronger than previously reported. We discuss\nsome possible differences in methodology that might have led to the initial\nunder-estimation of solar convective power. The new spectra are in line with\nrecent hydrodynamic and magnetohydrodynamic simulations of solar convection and\nalso consistent with the previous inferences obtained by the ring-diagram\nlocal-helioseismology method."
                },
                "authors": [
                    {
                        "name": "John T. Stefan"
                    },
                    {
                        "name": "Alexander G. Kosovichev"
                    },
                    {
                        "name": "Gustavo Guerrero"
                    },
                    {
                        "name": "Andrey M. Stejko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey M. Stejko"
                },
                "author": "Andrey M. Stejko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05453v1",
                "updated": "2025-05-08T17:44:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    44,
                    45,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:44:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    44,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Conversational Process Model Redesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Process Model Redesign"
                },
                "summary": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria."
                },
                "authors": [
                    {
                        "name": "Nataliia Klievtsova"
                    },
                    {
                        "name": "Timotheus Kampik"
                    },
                    {
                        "name": "Juergen Mangler"
                    },
                    {
                        "name": "Stefanie Rinderle-Ma"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Rinderle-Ma"
                },
                "author": "Stefanie Rinderle-Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05445v1",
                "updated": "2025-05-08T17:36:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    36,
                    36,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:36:36Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    36,
                    36,
                    3,
                    128,
                    0
                ],
                "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations"
                },
                "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18039v2",
                "updated": "2025-05-08T17:34:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    34,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-25T03:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    12,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Nuoqian Xiao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05441v1",
                "updated": "2025-05-08T17:31:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:31:28Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    28,
                    3,
                    128,
                    0
                ],
                "title": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based\n  Interaction in Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based\n  Interaction in Virtual Reality"
                },
                "summary": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments."
                },
                "authors": [
                    {
                        "name": "Xiyun Hu"
                    },
                    {
                        "name": "Dizhi Ma"
                    },
                    {
                        "name": "Fengming He"
                    },
                    {
                        "name": "Zhengzhe Zhu"
                    },
                    {
                        "name": "Shao-Kang Hsia"
                    },
                    {
                        "name": "Chenfei Zhu"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Karthik Ramani"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Ramani"
                },
                "author": "Karthik Ramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05440v1",
                "updated": "2025-05-08T17:31:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:31:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation"
                },
                "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation."
                },
                "authors": [
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Xavier Hu"
                    },
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05438v1",
                "updated": "2025-05-08T17:27:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    27,
                    44,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:27:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    27,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Scalable Bernoulli factories for Bayesian inference with intractable\n  likelihoods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bernoulli factories for Bayesian inference with intractable\n  likelihoods"
                },
                "summary": "Bernoulli factory MCMC algorithms implement accept-reject Markov chains\nwithout explicit computation of acceptance probabilities, and are used to\ntarget posterior distributions associated with intractable likelihood models.\nThese algorithms often mix better than alternatives based on data augmentation\nor acceptance probability estimation. However, we show that their computational\nperformance typically deteriorates exponentially with data size. To address\nthis, we propose a simple divide-and-conquer Bernoulli factory MCMC algorithm\nand prove that it has polynomial complexity of degree between 1 and 2, with the\nexact degree depending on the existence of efficient unbiased estimators of the\nintractable likelihood ratio. We demonstrate the effectiveness of our approach\nwith applications to Bayesian inference in two intractable likelihood models,\nand observe respective polynomial cost of degree 1.2 and 1 in the data size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bernoulli factory MCMC algorithms implement accept-reject Markov chains\nwithout explicit computation of acceptance probabilities, and are used to\ntarget posterior distributions associated with intractable likelihood models.\nThese algorithms often mix better than alternatives based on data augmentation\nor acceptance probability estimation. However, we show that their computational\nperformance typically deteriorates exponentially with data size. To address\nthis, we propose a simple divide-and-conquer Bernoulli factory MCMC algorithm\nand prove that it has polynomial complexity of degree between 1 and 2, with the\nexact degree depending on the existence of efficient unbiased estimators of the\nintractable likelihood ratio. We demonstrate the effectiveness of our approach\nwith applications to Bayesian inference in two intractable likelihood models,\nand observe respective polynomial cost of degree 1.2 and 1 in the data size."
                },
                "authors": [
                    {
                        "name": "Timothée Stumpf-Fétizon"
                    },
                    {
                        "name": "Flávio B. Gonçalves"
                    }
                ],
                "author_detail": {
                    "name": "Flávio B. Gonçalves"
                },
                "author": "Flávio B. Gonçalves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05429v1",
                "updated": "2025-05-08T17:19:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    19,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:19:11Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    19,
                    11,
                    3,
                    128,
                    0
                ],
                "title": "Theoretical modeling of approximate universality of tidally deformed\n  neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical modeling of approximate universality of tidally deformed\n  neutron stars"
                },
                "summary": "Quasi-universal relations are known to exist among various neutron star\nobservables that do not depend sensitively on the underlying nuclear matter\nequations of state. For example, some of these relations imply that the tidally\ninduced multipole moments are approximately characterized by the electric-type\nquadrupolar tidal deformability. Such relations can be used to reduce the\nnumber of independent tidal parameters in gravitational-waveform modeling,\nthereby allowing us to infer extreme nuclear matter properties more accurately\nand test General Relativity in an insensitive manner to uncertainties in\nnuclear physics. We present a comprehensive theoretical investigation into\napproximate universality of neutron stars. Our approach employs a semi-analytic\nrelativistic stellar interior model, which extends the Tolman VII solution,\nthereby enabling a refined exploration of the tidal properties of nonrotating\nstars within a semi-analytic framework. The derived power-law relations among\nvarious tidal deformabilities -- referred to as the universal Love relations --\nagree well with expressions in previous work found empirically. We elucidate\nhow the equation-of-state dependence is suppressed in a particular combination\nof macroscopic physical parameters induced by perturbations and demonstrate\nthat the relation between the moment of inertia and electric-type quadrupolar\ntidal deformability (I-Love relation) rests on the same underlying mechanism.\nOur findings indicate that the approximate universality of neutron stars can be\nattributed to low compressibility, consistent with some of the previous studies\non the possible origin of the universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-universal relations are known to exist among various neutron star\nobservables that do not depend sensitively on the underlying nuclear matter\nequations of state. For example, some of these relations imply that the tidally\ninduced multipole moments are approximately characterized by the electric-type\nquadrupolar tidal deformability. Such relations can be used to reduce the\nnumber of independent tidal parameters in gravitational-waveform modeling,\nthereby allowing us to infer extreme nuclear matter properties more accurately\nand test General Relativity in an insensitive manner to uncertainties in\nnuclear physics. We present a comprehensive theoretical investigation into\napproximate universality of neutron stars. Our approach employs a semi-analytic\nrelativistic stellar interior model, which extends the Tolman VII solution,\nthereby enabling a refined exploration of the tidal properties of nonrotating\nstars within a semi-analytic framework. The derived power-law relations among\nvarious tidal deformabilities -- referred to as the universal Love relations --\nagree well with expressions in previous work found empirically. We elucidate\nhow the equation-of-state dependence is suppressed in a particular combination\nof macroscopic physical parameters induced by perturbations and demonstrate\nthat the relation between the moment of inertia and electric-type quadrupolar\ntidal deformability (I-Love relation) rests on the same underlying mechanism.\nOur findings indicate that the approximate universality of neutron stars can be\nattributed to low compressibility, consistent with some of the previous studies\non the possible origin of the universality."
                },
                "authors": [
                    {
                        "name": "Takuya Katagiri"
                    },
                    {
                        "name": "Gowtham Rishi Mukkamala"
                    },
                    {
                        "name": "Kent Yagi"
                    }
                ],
                "author_detail": {
                    "name": "Kent Yagi"
                },
                "author": "Kent Yagi",
                "arxiv_comment": "22pages, 16figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05427v1",
                "updated": "2025-05-08T17:15:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    15,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:15:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    15,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for\n  High-Quality LLM Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-FineWeb: Efficient Data Filtering and Verification for\n  High-Quality LLM Training Data"
                },
                "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency."
                },
                "authors": [
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Peijun Tang"
                    },
                    {
                        "name": "Hongya Lyu"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "The datasets are available on\n  https://huggingface.co/datasets/openbmb/UltraFineWeb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05423v1",
                "updated": "2025-05-08T17:12:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    12,
                    56,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:12:56Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    12,
                    56,
                    3,
                    128,
                    0
                ],
                "title": "TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering"
                },
                "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Lieve Macken"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08684v2",
                "updated": "2025-05-08T17:11:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    11,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2024-03-13T16:41:02Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    16,
                    41,
                    2,
                    2,
                    73,
                    0
                ],
                "title": "Bubble breakup probability in turbulent flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bubble breakup probability in turbulent flows"
                },
                "summary": "Bubbles drive gas and chemical transfers in various industrial and\ngeophysical contexts, in which flows are typically turbulent. As gas and\nchemical transfers are bubble size dependent, their quantification requires a\nprediction of bubble breakup. The most common idea, introduced by Kolmogorov\nand Hinze, is to consider a sharp limit between breaking and non breaking\nbubbles, given by $\\mathrm{We}_c\\approx 1$, where the Weber number\n$\\mathrm{We}$ is the ratio between inertial and capillary forces at the bubble\nscale. Yet, due to the inherent stochasticity of the flow every bubble might in\nreality break. In this work, we use a stochastic linear model previously\ndeveloped to infer the breakup probability of bubbles in turbulence as function\nof both We and the residence time. This allows us to introduce a definition of\nthe critical Weber number accounting for the time spent by bubbles within a\nturbulent region. We show that bubble breakup is a memoryless process, whose\nbreakup rate varies exponentially with $\\mathrm{We}^{-1}$. The linear model\nsuccessfully reproduces experimental breakup rates from the literature. We show\nthat the stochastic nature of bubble breakup is central when the residence time\nof bubbles is smaller than ten correlation times of turbulence at the bubble\nscale: the transition between breaking and non breaking bubbles is smooth and\nmost bubbles can break. For large residence times, the original vision of\nKolmogorov and Hinze is recovered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bubbles drive gas and chemical transfers in various industrial and\ngeophysical contexts, in which flows are typically turbulent. As gas and\nchemical transfers are bubble size dependent, their quantification requires a\nprediction of bubble breakup. The most common idea, introduced by Kolmogorov\nand Hinze, is to consider a sharp limit between breaking and non breaking\nbubbles, given by $\\mathrm{We}_c\\approx 1$, where the Weber number\n$\\mathrm{We}$ is the ratio between inertial and capillary forces at the bubble\nscale. Yet, due to the inherent stochasticity of the flow every bubble might in\nreality break. In this work, we use a stochastic linear model previously\ndeveloped to infer the breakup probability of bubbles in turbulence as function\nof both We and the residence time. This allows us to introduce a definition of\nthe critical Weber number accounting for the time spent by bubbles within a\nturbulent region. We show that bubble breakup is a memoryless process, whose\nbreakup rate varies exponentially with $\\mathrm{We}^{-1}$. The linear model\nsuccessfully reproduces experimental breakup rates from the literature. We show\nthat the stochastic nature of bubble breakup is central when the residence time\nof bubbles is smaller than ten correlation times of turbulence at the bubble\nscale: the transition between breaking and non breaking bubbles is smooth and\nmost bubbles can break. For large residence times, the original vision of\nKolmogorov and Hinze is recovered."
                },
                "authors": [
                    {
                        "name": "Aliénor Rivière"
                    },
                    {
                        "name": "Stéphane Perrard"
                    }
                ],
                "author_detail": {
                    "name": "Stéphane Perrard"
                },
                "author": "Stéphane Perrard",
                "arxiv_comment": "4 figures + A supplementary Material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05420v1",
                "updated": "2025-05-08T17:09:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    9,
                    14,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:09:14Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    9,
                    14,
                    3,
                    128,
                    0
                ],
                "title": "Robustly optimal dynamics for active matter reservoir computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustly optimal dynamics for active matter reservoir computing"
                },
                "summary": "We study the information processing abilities of active matter in the\nreservoir computing (RC) paradigm, using a model that is externally driven to\ninfer the future state of a chaotic signal. The simulated system closely\nfollows a previously reported model. We uncover an exceptional dynamical regime\nof agent dynamics that has been overlooked heretofore. It appears robustly\noptimal across varying physical parameters and inference tasks, thus providing\nvaluable insights into computation and inference with physical systems more\ngenerally. The ability to form effective mechanisms for information processing\nare primarily determined by the system's own intrinsic relaxation abilities.\nThese are identifiable when probing the system without a specific inference\ngoal and manifest when testing minimalistic single-particle reservoirs. The\nregime that achieves optimal computation is situated just below the critical\ndamping threshold, involving a microscopic dynamical relaxation with multiple\nstages. The optimal system is adaptable under chaotic external driving, due to\na diversity in response mechanisms that emerge like rapid alternations between\nquasi-stationary and highly nonlinear dynamical states. Both coherent and\nincoherent dynamics contribute to their operation, partly at dissimilar scales\nof space and delay time. Correlations on agent dynamics can indicate the\nbest-performing regimes and onsets of tight relationships between the\nresponding system and the fluctuating driver. As this model of computation is\ninterpretable in physical terms, it facilitates re-framing inquiries regarding\nlearning and unconventional computing with a fresh rationale for many-body\nphysics out of equilibrium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the information processing abilities of active matter in the\nreservoir computing (RC) paradigm, using a model that is externally driven to\ninfer the future state of a chaotic signal. The simulated system closely\nfollows a previously reported model. We uncover an exceptional dynamical regime\nof agent dynamics that has been overlooked heretofore. It appears robustly\noptimal across varying physical parameters and inference tasks, thus providing\nvaluable insights into computation and inference with physical systems more\ngenerally. The ability to form effective mechanisms for information processing\nare primarily determined by the system's own intrinsic relaxation abilities.\nThese are identifiable when probing the system without a specific inference\ngoal and manifest when testing minimalistic single-particle reservoirs. The\nregime that achieves optimal computation is situated just below the critical\ndamping threshold, involving a microscopic dynamical relaxation with multiple\nstages. The optimal system is adaptable under chaotic external driving, due to\na diversity in response mechanisms that emerge like rapid alternations between\nquasi-stationary and highly nonlinear dynamical states. Both coherent and\nincoherent dynamics contribute to their operation, partly at dissimilar scales\nof space and delay time. Correlations on agent dynamics can indicate the\nbest-performing regimes and onsets of tight relationships between the\nresponding system and the fluctuating driver. As this model of computation is\ninterpretable in physical terms, it facilitates re-framing inquiries regarding\nlearning and unconventional computing with a fresh rationale for many-body\nphysics out of equilibrium."
                },
                "authors": [
                    {
                        "name": "Mario U. Gaimann"
                    },
                    {
                        "name": "Miriam Klopotek"
                    }
                ],
                "author_detail": {
                    "name": "Miriam Klopotek"
                },
                "author": "Miriam Klopotek",
                "arxiv_comment": "55 pages, 30 figures. Supplementary Videos:\n  https://doi.org/10.18419/DARUS-4619. Replication Data:\n  https://doi.org/10.18419/DARUS-4620",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.AO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05413v1",
                "updated": "2025-05-08T16:54:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    54,
                    48,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:54:48Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    54,
                    48,
                    3,
                    128,
                    0
                ],
                "title": "DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional\n  Computing"
                },
                "summary": "Hyperdimensional Computing (HDC) is emerging as a promising approach for edge\nAI, offering a balance between accuracy and efficiency. However, current\nHDC-based applications often rely on high-precision models and/or encoding\nmatrices to achieve competitive performance, which imposes significant\ncomputational and memory demands, especially for ultra-low power devices. While\nrecent efforts use techniques like precision reduction and pruning to increase\nthe efficiency, most require retraining to maintain performance, making them\nexpensive and impractical. To address this issue, we propose a novel Post\nTraining Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),\nwhich aims at compressing the end-to-end HDC system, achieving near floating\npoint performance without the need of retraining. DPQ-HD reduces computational\nand memory overhead by uniquely combining the above three compression\ntechniques and efficiently adapts to hardware constraints. Additionally, we\nintroduce an energy-efficient inference approach that progressively evaluates\nsimilarity scores such as cosine similarity and performs early exit to reduce\nthe computation, accelerating prediction inference while maintaining accuracy.\nWe demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image\nand graph classification tasks with only a 1-2% drop in accuracy compared to\nuncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing\npost-training compression methods and performs better or at par with\nretraining-based state-of-the-art techniques, requiring significantly less\noverall optimization time (up to 100x) and faster inference (up to 56x) on a\nmicrocontroller",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is emerging as a promising approach for edge\nAI, offering a balance between accuracy and efficiency. However, current\nHDC-based applications often rely on high-precision models and/or encoding\nmatrices to achieve competitive performance, which imposes significant\ncomputational and memory demands, especially for ultra-low power devices. While\nrecent efforts use techniques like precision reduction and pruning to increase\nthe efficiency, most require retraining to maintain performance, making them\nexpensive and impractical. To address this issue, we propose a novel Post\nTraining Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),\nwhich aims at compressing the end-to-end HDC system, achieving near floating\npoint performance without the need of retraining. DPQ-HD reduces computational\nand memory overhead by uniquely combining the above three compression\ntechniques and efficiently adapts to hardware constraints. Additionally, we\nintroduce an energy-efficient inference approach that progressively evaluates\nsimilarity scores such as cosine similarity and performs early exit to reduce\nthe computation, accelerating prediction inference while maintaining accuracy.\nWe demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image\nand graph classification tasks with only a 1-2% drop in accuracy compared to\nuncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing\npost-training compression methods and performs better or at par with\nretraining-based state-of-the-art techniques, requiring significantly less\noverall optimization time (up to 100x) and faster inference (up to 56x) on a\nmicrocontroller"
                },
                "authors": [
                    {
                        "name": "Nilesh Prasad Pandey"
                    },
                    {
                        "name": "Shriniwas Kulkarni"
                    },
                    {
                        "name": "David Wang"
                    },
                    {
                        "name": "Onat Gungor"
                    },
                    {
                        "name": "Flavio Ponzina"
                    },
                    {
                        "name": "Tajana Rosing"
                    }
                ],
                "author_detail": {
                    "name": "Tajana Rosing"
                },
                "author": "Tajana Rosing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05505v2",
                "updated": "2025-05-08T16:52:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    52,
                    55,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-07T15:22:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    22,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "Correctness Coverage Evaluation for Medical Multiple-Choice Question\n  Answering Based on the Enhanced Conformal Prediction Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Coverage Evaluation for Medical Multiple-Choice Question\n  Answering Based on the Enhanced Conformal Prediction Framework"
                },
                "summary": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs."
                },
                "authors": [
                    {
                        "name": "Yusong Ke"
                    },
                    {
                        "name": "Hongru Lin"
                    },
                    {
                        "name": "Yuting Ruan"
                    },
                    {
                        "name": "Junya Tang"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "arxiv_comment": "Published by Mathematics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05408v1",
                "updated": "2025-05-08T16:50:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    50,
                    6,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:50:06Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    50,
                    6,
                    3,
                    128,
                    0
                ],
                "title": "Crosslingual Reasoning through Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crosslingual Reasoning through Test-Time Scaling"
                },
                "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts."
                },
                "authors": [
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "M. Farid Adilazuarda"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Ruochen Zhang"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Carsten Eickhoff"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Stephen H. Bach"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05406v1",
                "updated": "2025-05-08T16:46:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    46,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:46:24Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    46,
                    24,
                    3,
                    128,
                    0
                ],
                "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than\n  Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than\n  Humans?"
                },
                "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting."
                },
                "authors": [
                    {
                        "name": "Valeria Pastorino"
                    },
                    {
                        "name": "Nafise Sadat Moosavi"
                    }
                ],
                "author_detail": {
                    "name": "Nafise Sadat Moosavi"
                },
                "author": "Nafise Sadat Moosavi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07074v3",
                "updated": "2025-05-08T16:40:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    40,
                    59,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-09T17:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning"
                },
                "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Yichuan Li"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Kyumin Lee"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13325v3",
                "updated": "2025-05-08T16:33:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    33,
                    22,
                    3,
                    128,
                    0
                ],
                "published": "2024-05-22T03:56:55Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    3,
                    56,
                    55,
                    2,
                    143,
                    0
                ],
                "title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event\n  Argument Extraction with Slot Querying",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event\n  Argument Extraction with Slot Querying"
                },
                "summary": "Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components."
                },
                "authors": [
                    {
                        "name": "Guanghui Wang"
                    },
                    {
                        "name": "Dexi Liu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Qizhi Wan"
                    },
                    {
                        "name": "Rong Hu"
                    },
                    {
                        "name": "Xiping Liu"
                    },
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "arxiv_comment": "Published as a conference paper in COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10729v2",
                "updated": "2025-05-08T16:29:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    29,
                    53,
                    3,
                    128,
                    0
                ],
                "published": "2024-11-16T07:46:28Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    46,
                    28,
                    5,
                    321,
                    0
                ],
                "title": "On-device Anomaly Detection in Conveyor Belt Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device Anomaly Detection in Conveyor Belt Operations"
                },
                "summary": "Conveyor belts are crucial in mining operations by enabling the continuous\nand efficient movement of bulk materials over long distances, which directly\nimpacts productivity. While detecting anomalies in specific conveyor belt\ncomponents has been widely studied, identifying the root causes of these\nfailures, such as changing production conditions and operator errors, remains\ncritical. Continuous monitoring of mining conveyor belt work cycles is still at\nan early stage and requires robust solutions. Recently, an anomaly detection\nmethod for duty cycle operations of a mining conveyor belt has been proposed.\nBased on its limited performance and unevaluated long-term proper operation,\nthis study proposes two novel methods for classifying normal and abnormal duty\ncycles. The proposed approaches are pattern recognition systems that make use\nof threshold-based duty-cycle detection mechanisms, manually extracted\nfeatures, pattern-matching, and supervised tiny machine learning models. The\nexplored low-computational models include decision tree, random forest, extra\ntrees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer\nperceptron. A comprehensive evaluation of the former and proposed approaches is\ncarried out on two datasets. Both proposed methods outperform the former\nmethod, with the best-performing approach being dataset-dependent. The\nheuristic rule-based approach achieves the highest performance in the same\ndataset used for algorithm training, with 97.3% for normal cycles and 80.2% for\nabnormal cycles. The ML-based approach performs better on a dataset including\nthe effects of machine aging, scoring 91.3% for normal cycles and 67.9% for\nabnormal cycles. Implemented on two low-power microcontrollers, the methods\ndemonstrate efficient, real-time operation with energy consumption of 13.3 and\n20.6 ${\\mu}$J during inference. These results offer valuable insights for\ndetecting ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conveyor belts are crucial in mining operations by enabling the continuous\nand efficient movement of bulk materials over long distances, which directly\nimpacts productivity. While detecting anomalies in specific conveyor belt\ncomponents has been widely studied, identifying the root causes of these\nfailures, such as changing production conditions and operator errors, remains\ncritical. Continuous monitoring of mining conveyor belt work cycles is still at\nan early stage and requires robust solutions. Recently, an anomaly detection\nmethod for duty cycle operations of a mining conveyor belt has been proposed.\nBased on its limited performance and unevaluated long-term proper operation,\nthis study proposes two novel methods for classifying normal and abnormal duty\ncycles. The proposed approaches are pattern recognition systems that make use\nof threshold-based duty-cycle detection mechanisms, manually extracted\nfeatures, pattern-matching, and supervised tiny machine learning models. The\nexplored low-computational models include decision tree, random forest, extra\ntrees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer\nperceptron. A comprehensive evaluation of the former and proposed approaches is\ncarried out on two datasets. Both proposed methods outperform the former\nmethod, with the best-performing approach being dataset-dependent. The\nheuristic rule-based approach achieves the highest performance in the same\ndataset used for algorithm training, with 97.3% for normal cycles and 80.2% for\nabnormal cycles. The ML-based approach performs better on a dataset including\nthe effects of machine aging, scoring 91.3% for normal cycles and 67.9% for\nabnormal cycles. Implemented on two low-power microcontrollers, the methods\ndemonstrate efficient, real-time operation with energy consumption of 13.3 and\n20.6 ${\\mu}$J during inference. These results offer valuable insights for\ndetecting ..."
                },
                "authors": [
                    {
                        "name": "Luciano S. Martinez-Rau"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bengt Oelmann"
                    },
                    {
                        "name": "Sebastian Bader"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Bader"
                },
                "author": "Sebastian Bader",
                "arxiv_comment": "Preprint submitted to IEEE Sensors Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05391v1",
                "updated": "2025-05-08T16:27:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    27,
                    27,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:27:27Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    27,
                    27,
                    3,
                    128,
                    0
                ],
                "title": "EDmamba: A Simple yet Effective Event Denoising Method with State Space\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDmamba: A Simple yet Effective Event Denoising Method with State Space\n  Model"
                },
                "summary": "Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster."
                },
                "authors": [
                    {
                        "name": "Ciyu Ruan"
                    },
                    {
                        "name": "Zihang Gong"
                    },
                    {
                        "name": "Ruishan Guo"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02107v2",
                "updated": "2025-05-08T16:22:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    22,
                    26,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-02T20:11:54Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    20,
                    11,
                    54,
                    2,
                    92,
                    0
                ],
                "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining"
                },
                "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains."
                },
                "authors": [
                    {
                        "name": "Jeffrey Li"
                    },
                    {
                        "name": "Mohammadreza Armandpour"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Vaishaal Shankar"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Fartash Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Fartash Faghri"
                },
                "author": "Fartash Faghri",
                "arxiv_comment": "Code available at: https://github.com/apple/ml-tic-lm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05374v1",
                "updated": "2025-05-08T16:09:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:09:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "OcularAge: A Comparative Study of Iris and Periocular Images for\n  Pediatric Age Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OcularAge: A Comparative Study of Iris and Periocular Images for\n  Pediatric Age Estimation"
                },
                "summary": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications."
                },
                "authors": [
                    {
                        "name": "Naveenkumar G Venkataswamy"
                    },
                    {
                        "name": "Poorna Ravi"
                    },
                    {
                        "name": "Stephanie Schuckers"
                    },
                    {
                        "name": "Masudul H. Imtiaz"
                    }
                ],
                "author_detail": {
                    "name": "Masudul H. Imtiaz"
                },
                "author": "Masudul H. Imtiaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05360v1",
                "updated": "2025-05-08T15:53:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    53,
                    34,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:53:34Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    53,
                    34,
                    3,
                    128,
                    0
                ],
                "title": "DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning"
                },
                "summary": "We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD."
                },
                "authors": [
                    {
                        "name": "Wenru Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07944v2",
                "updated": "2025-05-08T15:48:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    48,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-12T07:06:38Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    6,
                    38,
                    2,
                    164,
                    0
                ],
                "title": "Enhancing Differential Testing With LLMs For Testing Deep Learning\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Differential Testing With LLMs For Testing Deep Learning\n  Libraries"
                },
                "summary": "Differential testing offers a promising strategy to alleviate the test oracle\nproblem by comparing the test results between alternative implementations.\nHowever, existing differential testing techniques for deep learning (DL)\nlibraries are limited by the key challenges of finding alternative\nimplementations (called counterparts) for a given API and subsequently\ngenerating diverse test inputs. To address the two challenges, this paper\nintroduces DLLens, an LLM-enhanced differential testing technique for DL\nlibraries. To address the first challenge, DLLens incorporates an LLM-based\ncounterpart synthesis workflow, with the insight that the counterpart of a\ngiven DL library API's computation could be successfully synthesized through\ncertain composition and adaptation of the APIs from another DL library. To\naddress the second challenge, DLLens incorporates a static analysis technique\nthat extracts the path constraints from the implementations of a given API and\nits counterpart to guide diverse test input generation. The extraction is\nfacilitated by LLM's knowledge of the concerned DL library and its upstream\nlibraries.\n  We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our\nevaluation shows that DLLens synthesizes counterparts for 1.84 times as many\nAPIs as those found by state-of-the-art techniques on these libraries.\nMoreover, under the same time budget, DLLens covers 7.23% more branches and\ndetects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly\nsampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 59 are confirmed by developers, including 46\nconfirmed as previously unknown bugs, and 10 of these previously unknown bugs\nhave been fixed in the latest version of TensorFlow and PyTorch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential testing offers a promising strategy to alleviate the test oracle\nproblem by comparing the test results between alternative implementations.\nHowever, existing differential testing techniques for deep learning (DL)\nlibraries are limited by the key challenges of finding alternative\nimplementations (called counterparts) for a given API and subsequently\ngenerating diverse test inputs. To address the two challenges, this paper\nintroduces DLLens, an LLM-enhanced differential testing technique for DL\nlibraries. To address the first challenge, DLLens incorporates an LLM-based\ncounterpart synthesis workflow, with the insight that the counterpart of a\ngiven DL library API's computation could be successfully synthesized through\ncertain composition and adaptation of the APIs from another DL library. To\naddress the second challenge, DLLens incorporates a static analysis technique\nthat extracts the path constraints from the implementations of a given API and\nits counterpart to guide diverse test input generation. The extraction is\nfacilitated by LLM's knowledge of the concerned DL library and its upstream\nlibraries.\n  We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our\nevaluation shows that DLLens synthesizes counterparts for 1.84 times as many\nAPIs as those found by state-of-the-art techniques on these libraries.\nMoreover, under the same time budget, DLLens covers 7.23% more branches and\ndetects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly\nsampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 59 are confirmed by developers, including 46\nconfirmed as previously unknown bugs, and 10 of these previously unknown bugs\nhave been fixed in the latest version of TensorFlow and PyTorch."
                },
                "authors": [
                    {
                        "name": "Meiziniu Li"
                    },
                    {
                        "name": "Dongze Li"
                    },
                    {
                        "name": "Jianmeng Liu"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yongqiang Tian"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "arxiv_comment": "This work has been accepted by ACM TOSEM. Manuscript under final\n  preparation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05346v1",
                "updated": "2025-05-08T15:33:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    33,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:33:24Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    33,
                    24,
                    3,
                    128,
                    0
                ],
                "title": "Analysis of the accuracy of GNSS inferred precipitable water vapour\n  against that from a 210 GHz WVR at the H.E.S.S. site",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of the accuracy of GNSS inferred precipitable water vapour\n  against that from a 210 GHz WVR at the H.E.S.S. site"
                },
                "summary": "The High Energy Stereoscopic System (H.E.S.S.) site and the Gamsberg Mountain\nhave been identified as potential sites for the Africa Millimetre Telescope\n(AMT). The AMT is poised to observe at millimetre and possibly at submillimetre\nwavelengths. At these wavelengths, precipitable water vapour (PWV) in the\natmosphere is the main source of opacity during observations and therefore\nneeds to be accurately assessed at the potential sites for the AMT. In order to\ninvestigate the PWV conditions for the AMT, identical Global Navigation\nSatellite System (GNSS) stations were installed and used to assess the PWV at\nthe two potential sites. In this study, the accuracy of those PWV measurements\nby the GNSS stations was assessed by comparing the H.E.S.S. installed GNSS\nstation PWV measurements to that from a 210 GHz Water Vapour Radiometer (WVR)\nalso installed at the H.E.S.S. site. A correlation of 98% and an offset of 0.34\nmm was found between the GNSS station and the 210 GHz WVR PWV data when on-site\npressure and the Nevada Geodetic Laboratory (NGL) weighted mean temperature\n($\\mathrm{T_m}$) were used calculate the GNSS station PWV data. In comparison,\nthe offset reduces to 0.15 mm when on-site derived $\\mathrm{T_m}$ and pressure\nwere used to calculate the GNSS station PWV. The results show that the GNSS\nstation with on-site meteorological data can be used with high accuracy to\nreliably determine the PWV conditions at the H.E.S.S. site.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The High Energy Stereoscopic System (H.E.S.S.) site and the Gamsberg Mountain\nhave been identified as potential sites for the Africa Millimetre Telescope\n(AMT). The AMT is poised to observe at millimetre and possibly at submillimetre\nwavelengths. At these wavelengths, precipitable water vapour (PWV) in the\natmosphere is the main source of opacity during observations and therefore\nneeds to be accurately assessed at the potential sites for the AMT. In order to\ninvestigate the PWV conditions for the AMT, identical Global Navigation\nSatellite System (GNSS) stations were installed and used to assess the PWV at\nthe two potential sites. In this study, the accuracy of those PWV measurements\nby the GNSS stations was assessed by comparing the H.E.S.S. installed GNSS\nstation PWV measurements to that from a 210 GHz Water Vapour Radiometer (WVR)\nalso installed at the H.E.S.S. site. A correlation of 98% and an offset of 0.34\nmm was found between the GNSS station and the 210 GHz WVR PWV data when on-site\npressure and the Nevada Geodetic Laboratory (NGL) weighted mean temperature\n($\\mathrm{T_m}$) were used calculate the GNSS station PWV data. In comparison,\nthe offset reduces to 0.15 mm when on-site derived $\\mathrm{T_m}$ and pressure\nwere used to calculate the GNSS station PWV. The results show that the GNSS\nstation with on-site meteorological data can be used with high accuracy to\nreliably determine the PWV conditions at the H.E.S.S. site."
                },
                "authors": [
                    {
                        "name": "Lott Frans"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Heino Falcke"
                    },
                    {
                        "name": "Tiziana Venturi"
                    }
                ],
                "author_detail": {
                    "name": "Tiziana Venturi"
                },
                "author": "Tiziana Venturi",
                "arxiv_doi": "10.1093/rasti/rzaf012",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/rasti/rzaf012",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 12 figures, published by RAS Techniques and Instruments",
                "arxiv_journal_ref": "RAS Techniques and Instruments, Volume 4, 2025, rzaf012",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05343v1",
                "updated": "2025-05-08T15:32:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    32,
                    4,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:32:04Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    32,
                    4,
                    3,
                    128,
                    0
                ],
                "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization"
                },
                "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Sooyoung Park"
                    },
                    {
                        "name": "Arda Senocak"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "arxiv_comment": "Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is\n  available at https://github.com/swimmiing/ACL-SSL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05335v1",
                "updated": "2025-05-08T15:27:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    27,
                    43,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:27:43Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    27,
                    43,
                    3,
                    128,
                    0
                ],
                "title": "FLAM: Frame-Wise Language-Audio Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAM: Frame-Wise Language-Audio Modeling"
                },
                "summary": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yusong Wu"
                    },
                    {
                        "name": "Christos Tsirigotis"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Cheng-Zhi Anna Huang"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Oriol Nieto"
                    },
                    {
                        "name": "Prem Seetharaman"
                    },
                    {
                        "name": "Justin Salamon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Salamon"
                },
                "author": "Justin Salamon",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05327v1",
                "updated": "2025-05-08T15:17:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    17,
                    37,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:17:37Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    17,
                    37,
                    3,
                    128,
                    0
                ],
                "title": "ICon: In-Context Contribution for Automatic Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICon: In-Context Contribution for Automatic Data Selection"
                },
                "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones."
                },
                "authors": [
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Linli Yao"
                    },
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05315v1",
                "updated": "2025-05-08T15:01:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    1,
                    6,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    1,
                    6,
                    3,
                    128,
                    0
                ],
                "title": "Scalable Chain of Thoughts via Elastic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Chain of Thoughts via Elastic Reasoning"
                },
                "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05310v1",
                "updated": "2025-05-08T14:59:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    59,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:59:40Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    59,
                    40,
                    3,
                    128,
                    0
                ],
                "title": "A comparative analysis of GNSS-inferred precipitable water vapour at the\n  potential sites for the Africa Millimetre Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparative analysis of GNSS-inferred precipitable water vapour at the\n  potential sites for the Africa Millimetre Telescope"
                },
                "summary": "The Event Horizon Telescope (EHT) is a network of antennas across the globe\ncurrently used to image super-massive black holes (SMBHs) at a frequency of 230\nGHz. Since the release of the image of M87$^\\ast$ in 2019 and, subsequently,\nthat of Sgr A$^\\ast$ in 2022 by the EHT collaboration, the focus has shifted to\ndynamically imaging SMBHs. This has led to a search for potential sites to\nextend and fill in the gaps within the EHT network. The Gamsberg Mountain and\nthe H.E.S.S. site are both located within the Khomas highlands and have been\nidentified as potential sites for the Africa Millimetre Telescope (AMT).\nPrecipitable water vapour (PWV) in the atmosphere is the main source of opacity\nand noise from atmospheric emissions when observing at millimetre to\nsub-millimetre wavelengths. This study aims to establish the PWV content and\nthe atmospheric transmission at 86, 230, and 345 GHz at the AMT potential sites\nusing Global Navigation Satellite System (GNSS) derived PWV data. Results show\nboth sites have potential for observations at 86 and 230 GHz, with 345 GHz\npossible at the Gamsberg Mountain during winter. The overall median PWV of\n14.27 mm and 9.25 mm was calculated at the H.E.S.S. site and the Gamsberg\nMountain, respectively. The EHT window had PWV medians of 16.62 mm and 11.20 mm\nat the H.E.S.S. site and Gamsberg Mountain, respectively. Among the two sites,\nthe Gamsberg Mountain had the lowest PWV conditions, therefore making it the\nmost suitable site for the AMT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Event Horizon Telescope (EHT) is a network of antennas across the globe\ncurrently used to image super-massive black holes (SMBHs) at a frequency of 230\nGHz. Since the release of the image of M87$^\\ast$ in 2019 and, subsequently,\nthat of Sgr A$^\\ast$ in 2022 by the EHT collaboration, the focus has shifted to\ndynamically imaging SMBHs. This has led to a search for potential sites to\nextend and fill in the gaps within the EHT network. The Gamsberg Mountain and\nthe H.E.S.S. site are both located within the Khomas highlands and have been\nidentified as potential sites for the Africa Millimetre Telescope (AMT).\nPrecipitable water vapour (PWV) in the atmosphere is the main source of opacity\nand noise from atmospheric emissions when observing at millimetre to\nsub-millimetre wavelengths. This study aims to establish the PWV content and\nthe atmospheric transmission at 86, 230, and 345 GHz at the AMT potential sites\nusing Global Navigation Satellite System (GNSS) derived PWV data. Results show\nboth sites have potential for observations at 86 and 230 GHz, with 345 GHz\npossible at the Gamsberg Mountain during winter. The overall median PWV of\n14.27 mm and 9.25 mm was calculated at the H.E.S.S. site and the Gamsberg\nMountain, respectively. The EHT window had PWV medians of 16.62 mm and 11.20 mm\nat the H.E.S.S. site and Gamsberg Mountain, respectively. Among the two sites,\nthe Gamsberg Mountain had the lowest PWV conditions, therefore making it the\nmost suitable site for the AMT."
                },
                "authors": [
                    {
                        "name": "Lott Frans"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Heino Falcke"
                    },
                    {
                        "name": "Tiziana Venturi"
                    }
                ],
                "author_detail": {
                    "name": "Tiziana Venturi"
                },
                "author": "Tiziana Venturi",
                "arxiv_doi": "10.1093/mnras/staf103",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf103",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 12 figures, article published in MNRAS",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 537,\n  Issue 2, February 2025, Pages 1357-1368",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05298v1",
                "updated": "2025-05-08T14:41:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    41,
                    7,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:41:07Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    41,
                    7,
                    3,
                    128,
                    0
                ],
                "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with\n  Us by Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Reasonable Parrots: Why Large Language Models Should Argue with\n  Us by Design"
                },
                "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation."
                },
                "authors": [
                    {
                        "name": "Elena Musi"
                    },
                    {
                        "name": "Nadin Kokciyan"
                    },
                    {
                        "name": "Khalid Al-Khatib"
                    },
                    {
                        "name": "Davide Ceolin"
                    },
                    {
                        "name": "Emmanuelle Dietz"
                    },
                    {
                        "name": "Klara Gutekunst"
                    },
                    {
                        "name": "Annette Hautli-Janisz"
                    },
                    {
                        "name": "Cristian Manuel Santibañez Yañez"
                    },
                    {
                        "name": "Jodi Schneider"
                    },
                    {
                        "name": "Jonas Scholz"
                    },
                    {
                        "name": "Cor Steging"
                    },
                    {
                        "name": "Jacky Visser"
                    },
                    {
                        "name": "Henning Wachsmuth"
                    }
                ],
                "author_detail": {
                    "name": "Henning Wachsmuth"
                },
                "author": "Henning Wachsmuth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08478v2",
                "updated": "2025-05-08T14:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    34,
                    54,
                    3,
                    128,
                    0
                ],
                "published": "2024-12-11T15:45:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning"
                },
                "summary": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area."
                },
                "authors": [
                    {
                        "name": "Patrick Sattler"
                    },
                    {
                        "name": "Johannes Zirngibl"
                    },
                    {
                        "name": "Fahad Hilal"
                    },
                    {
                        "name": "Oliver Gasser"
                    },
                    {
                        "name": "Kevin Vermeulen"
                    },
                    {
                        "name": "Georg Carle"
                    },
                    {
                        "name": "Mattijs Jonker"
                    }
                ],
                "author_detail": {
                    "name": "Mattijs Jonker"
                },
                "author": "Mattijs Jonker",
                "arxiv_doi": "10.1145/3730977",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730977",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05288v1",
                "updated": "2025-05-08T14:29:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:29:11Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    11,
                    3,
                    128,
                    0
                ],
                "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes"
                },
                "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdelreheem"
                    },
                    {
                        "name": "Filippo Aleotti"
                    },
                    {
                        "name": "Jamie Watson"
                    },
                    {
                        "name": "Zawar Qureshi"
                    },
                    {
                        "name": "Abdelrahman Eldesokey"
                    },
                    {
                        "name": "Peter Wonka"
                    },
                    {
                        "name": "Gabriel Brostow"
                    },
                    {
                        "name": "Sara Vicente"
                    },
                    {
                        "name": "Guillermo Garcia-Hernando"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Garcia-Hernando"
                },
                "author": "Guillermo Garcia-Hernando",
                "arxiv_comment": "Tech report. Project page: https://nianticlabs.github.io/placeit3d/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05286v1",
                "updated": "2025-05-08T14:28:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    28,
                    47,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:28:47Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    28,
                    47,
                    3,
                    128,
                    0
                ],
                "title": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow"
                },
                "summary": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow."
                },
                "authors": [
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11174v2",
                "updated": "2025-05-08T14:23:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    23,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-15T13:27:57Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    27,
                    57,
                    1,
                    105,
                    0
                ],
                "title": "Algorithmic thresholds in combinatorial optimization depend on the time\n  scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic thresholds in combinatorial optimization depend on the time\n  scaling"
                },
                "summary": "In the last decades, many efforts have focused on analyzing typical-case\nhardness in optimization and inference problems. Some recent work has pointed\nout that polynomial algorithms exist, running with a time that grows more than\nlinearly with the system size, which can do better than linear algorithms,\nfinding solutions to random problems in a wider range of parameters. However, a\ntheory for polynomial and superlinear algorithms is in general lacking. In this\npaper, we examine the performance of the Simulated Annealing algorithm, a\nstandard, versatile, and robust choice for solving optimization and inference\nproblems, in the prototypical random $K$-Sat problem. For the first time, we\nshow that the algorithmic thresholds depend on the time scaling of the\nalgorithm with the size of the system. Indeed, one can identify not just one,\nbut different thresholds for linear, quadratic, cubic regimes (and so on). This\nobservation opens new directions in studying the typical case hardness in\noptimization problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last decades, many efforts have focused on analyzing typical-case\nhardness in optimization and inference problems. Some recent work has pointed\nout that polynomial algorithms exist, running with a time that grows more than\nlinearly with the system size, which can do better than linear algorithms,\nfinding solutions to random problems in a wider range of parameters. However, a\ntheory for polynomial and superlinear algorithms is in general lacking. In this\npaper, we examine the performance of the Simulated Annealing algorithm, a\nstandard, versatile, and robust choice for solving optimization and inference\nproblems, in the prototypical random $K$-Sat problem. For the first time, we\nshow that the algorithmic thresholds depend on the time scaling of the\nalgorithm with the size of the system. Indeed, one can identify not just one,\nbut different thresholds for linear, quadratic, cubic regimes (and so on). This\nobservation opens new directions in studying the typical case hardness in\noptimization problems."
                },
                "authors": [
                    {
                        "name": "M. C. Angelini"
                    },
                    {
                        "name": "M. Avila-González"
                    },
                    {
                        "name": "F. D'Amico"
                    },
                    {
                        "name": "D. Machado"
                    },
                    {
                        "name": "R. Mulet"
                    },
                    {
                        "name": "F. Ricci-Tersenghi"
                    }
                ],
                "author_detail": {
                    "name": "F. Ricci-Tersenghi"
                },
                "author": "F. Ricci-Tersenghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12838v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12838v3",
                "updated": "2025-05-08T14:21:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    21,
                    55,
                    3,
                    128,
                    0
                ],
                "published": "2024-02-20T09:05:43Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    9,
                    5,
                    43,
                    1,
                    51,
                    0
                ],
                "title": "Extending the Scope of Inference About Predictive Ability to Machine\n  Learning Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the Scope of Inference About Predictive Ability to Machine\n  Learning Methods"
                },
                "summary": "The use of machine learning methods for predictive purposes has increased\ndramatically over the past two decades, but uncertainty quantification for\npredictive comparisons remains elusive. This paper addresses this gap by\nextending the classic inference theory for predictive ability in time series to\nmodern machine learners, such as the Lasso or Deep Learning. We investigate\nunder which conditions such extensions are possible. For standard out-of-sample\nasymptotic inference to be valid with machine learning, two key properties must\nhold: (I) a zero-mean condition for the score of the prediction loss function\nand (ii) a \"fast rate\" of convergence for the machine learner. Absent any of\nthese conditions, the estimation risk may be unbounded, and inferences invalid\nand very sensitive to sample splitting. For accurate inferences, we recommend\nan 80%-20% training-test splitting rule. We illustrate the wide applicability\nof our results with three applications: high-dimensional time series\nregressions with the Lasso, Deep learning for binary outcomes, and a new\nout-of-sample test for the Martingale Difference Hypothesis (MDH). The\ntheoretical results are supported by extensive Monte Carlo simulations and an\nempirical application evaluating the MDH of some major exchange rates at daily\nand higher frequencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of machine learning methods for predictive purposes has increased\ndramatically over the past two decades, but uncertainty quantification for\npredictive comparisons remains elusive. This paper addresses this gap by\nextending the classic inference theory for predictive ability in time series to\nmodern machine learners, such as the Lasso or Deep Learning. We investigate\nunder which conditions such extensions are possible. For standard out-of-sample\nasymptotic inference to be valid with machine learning, two key properties must\nhold: (I) a zero-mean condition for the score of the prediction loss function\nand (ii) a \"fast rate\" of convergence for the machine learner. Absent any of\nthese conditions, the estimation risk may be unbounded, and inferences invalid\nand very sensitive to sample splitting. For accurate inferences, we recommend\nan 80%-20% training-test splitting rule. We illustrate the wide applicability\nof our results with three applications: high-dimensional time series\nregressions with the Lasso, Deep learning for binary outcomes, and a new\nout-of-sample test for the Martingale Difference Hypothesis (MDH). The\ntheoretical results are supported by extensive Monte Carlo simulations and an\nempirical application evaluating the MDH of some major exchange rates at daily\nand higher frequencies."
                },
                "authors": [
                    {
                        "name": "Juan Carlos Escanciano"
                    },
                    {
                        "name": "Ricardo Parra"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Parra"
                },
                "author": "Ricardo Parra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12838v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16354v2",
                "updated": "2025-05-08T14:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    16,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-20T17:13:51Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    13,
                    51,
                    3,
                    79,
                    0
                ],
                "title": "Hypercyclicity of Weighted shifts on weighted Bergman and Dirichlet\n  spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypercyclicity of Weighted shifts on weighted Bergman and Dirichlet\n  spaces"
                },
                "summary": "Let $B_w$ and $F_w$ denote, respectively, the weighted backward and forward\nshift operators defined on the weighted Bergman space $A^p_{\\phi}$, or the\nweighted Dirichlet space ${D}^p_{\\phi}$ of the unit disc, where the weight\nfunction $\\phi(z)$ is mostly radial. We first obtain sufficient conditions for\n$B_w$ and $F_w$ to be continuous on these spaces. For radial weights, we derive\nnorm estimates for coefficient functionals on $A^p_{\\phi}$ and $D^p_{\\phi}$,\nand using those estimates we infer when the weighted shifts or their adjoints\nare hypercyclic and chaotic. We also deal with a non-radial case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $B_w$ and $F_w$ denote, respectively, the weighted backward and forward\nshift operators defined on the weighted Bergman space $A^p_{\\phi}$, or the\nweighted Dirichlet space ${D}^p_{\\phi}$ of the unit disc, where the weight\nfunction $\\phi(z)$ is mostly radial. We first obtain sufficient conditions for\n$B_w$ and $F_w$ to be continuous on these spaces. For radial weights, we derive\nnorm estimates for coefficient functionals on $A^p_{\\phi}$ and $D^p_{\\phi}$,\nand using those estimates we infer when the weighted shifts or their adjoints\nare hypercyclic and chaotic. We also deal with a non-radial case."
                },
                "authors": [
                    {
                        "name": "Bibhash Kumar Das"
                    },
                    {
                        "name": "Aneesh Mundayadan"
                    }
                ],
                "author_detail": {
                    "name": "Aneesh Mundayadan"
                },
                "author": "Aneesh Mundayadan",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.FA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "47A16, 46E22, 32K05, 47B32, 47B37, 37A99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05269v1",
                "updated": "2025-05-08T14:15:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    15,
                    53,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:15:53Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    15,
                    53,
                    3,
                    128,
                    0
                ],
                "title": "A Two-Sample Test of Text Generation Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-Sample Test of Text Generation Similarity"
                },
                "summary": "The surge in digitized text data requires reliable inferential methods on\nobserved textual patterns. This article proposes a novel two-sample text test\nfor comparing similarity between two groups of documents. The hypothesis is\nwhether the probabilistic mapping generating the textual data is identical\nacross two groups of documents. The proposed test aims to assess text\nsimilarity by comparing the entropy of the documents. Entropy is estimated\nusing neural network-based language models. The test statistic is derived from\nan estimation-and-inference framework, where the entropy is first approximated\nusing an estimation set, followed by inference on the remaining data set. We\nshowed theoretically that under mild conditions, the test statistic\nasymptotically follows a normal distribution. A multiple data-splitting\nstrategy is proposed to enhance test power, which combines p-values into a\nunified decision. Various simulation studies and a real data example\ndemonstrated that the proposed two-sample text test maintains the nominal Type\none error rate while offering greater power compared to existing methods. The\nproposed method provides a novel solution to assert differences in document\nclasses, particularly in fields where large-scale textual information is\ncrucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in digitized text data requires reliable inferential methods on\nobserved textual patterns. This article proposes a novel two-sample text test\nfor comparing similarity between two groups of documents. The hypothesis is\nwhether the probabilistic mapping generating the textual data is identical\nacross two groups of documents. The proposed test aims to assess text\nsimilarity by comparing the entropy of the documents. Entropy is estimated\nusing neural network-based language models. The test statistic is derived from\nan estimation-and-inference framework, where the entropy is first approximated\nusing an estimation set, followed by inference on the remaining data set. We\nshowed theoretically that under mild conditions, the test statistic\nasymptotically follows a normal distribution. A multiple data-splitting\nstrategy is proposed to enhance test power, which combines p-values into a\nunified decision. Various simulation studies and a real data example\ndemonstrated that the proposed two-sample text test maintains the nominal Type\none error rate while offering greater power compared to existing methods. The\nproposed method provides a novel solution to assert differences in document\nclasses, particularly in fields where large-scale textual information is\ncrucial."
                },
                "authors": [
                    {
                        "name": "Jingbin Xu"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Meimei Liu"
                    },
                    {
                        "name": "Feng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Feng Guo"
                },
                "author": "Feng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05262v1",
                "updated": "2025-05-08T14:07:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    7,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:07:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    7,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State\n  Modelling and Adversarial Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State\n  Modelling and Adversarial Exploration"
                },
                "summary": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks."
                },
                "authors": [
                    {
                        "name": "Andreas Kontogiannis"
                    },
                    {
                        "name": "Konstantinos Papathanasiou"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michael M. Zavlanos"
                    },
                    {
                        "name": "George Vouros"
                    }
                ],
                "author_detail": {
                    "name": "George Vouros"
                },
                "author": "George Vouros",
                "arxiv_comment": "Accepted (Poster) at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05261v1",
                "updated": "2025-05-08T14:06:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    6,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:06:38Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    6,
                    38,
                    3,
                    128,
                    0
                ],
                "title": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving\n  two-stage stochastic programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving\n  two-stage stochastic programming"
                },
                "summary": "Two-stage stochastic programming (2SP) offers a basic framework for modelling\ndecision-making under uncertainty, yet scalability remains a challenge due to\nthe computational complexity of recourse function evaluation. Existing\nlearning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP)\nemploy neural networks (NNs) as recourse function surrogates but rely on\ncomputationally intensive mixed-integer programming (MIP) formulations. We\npropose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks\n(ICNNs) to exploit linear programming (LP) representability in convex 2SP\nproblems. By architecturally enforcing convexity and enabling exact inference\nthrough LP, our approach eliminates the need for integer variables inherent to\nthe conventional MIP-based formulation while retaining an exact embedding of\nthe ICNN surrogate within the 2SP framework. This results in a more\ncomputationally efficient alternative that maintains solution quality.\nComprehensive experiments reveal that ICNNs incur only marginally longer\ntraining times while achieving validation accuracy on par with their MIP-based\ncounterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits\nconsiderably faster solution times than the MIP-based formulations while\npreserving solution quality, with these advantages becoming significantly more\npronounced as problem scale increases. For the most challenging instances, the\nmethod achieves speedups of up to 100$\\times$ and solution quality superior to\nMIP-based formulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage stochastic programming (2SP) offers a basic framework for modelling\ndecision-making under uncertainty, yet scalability remains a challenge due to\nthe computational complexity of recourse function evaluation. Existing\nlearning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP)\nemploy neural networks (NNs) as recourse function surrogates but rely on\ncomputationally intensive mixed-integer programming (MIP) formulations. We\npropose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks\n(ICNNs) to exploit linear programming (LP) representability in convex 2SP\nproblems. By architecturally enforcing convexity and enabling exact inference\nthrough LP, our approach eliminates the need for integer variables inherent to\nthe conventional MIP-based formulation while retaining an exact embedding of\nthe ICNN surrogate within the 2SP framework. This results in a more\ncomputationally efficient alternative that maintains solution quality.\nComprehensive experiments reveal that ICNNs incur only marginally longer\ntraining times while achieving validation accuracy on par with their MIP-based\ncounterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits\nconsiderably faster solution times than the MIP-based formulations while\npreserving solution quality, with these advantages becoming significantly more\npronounced as problem scale increases. For the most challenging instances, the\nmethod achieves speedups of up to 100$\\times$ and solution quality superior to\nMIP-based formulations."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Fabricio Oliveira"
                    }
                ],
                "author_detail": {
                    "name": "Fabricio Oliveira"
                },
                "author": "Fabricio Oliveira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05257v1",
                "updated": "2025-05-08T14:04:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    4,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:04:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    4,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "First Light and Reionization Epoch Simulations (FLARES) -- XVIII: the\n  ionising emissivities and hydrogen recombination line properties of early AGN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Light and Reionization Epoch Simulations (FLARES) -- XVIII: the\n  ionising emissivities and hydrogen recombination line properties of early AGN"
                },
                "summary": "One of the most remarkable results from the \\emph{James Webb Space Telescope}\nhas been the discovery of a large population of compact sources exhibiting\nstrong broad H$\\alpha$ emission, typically interpreted to be low-luminosity\nbroad-line (Type 1) active galactic nuclei (BLAGN). An important question is\nwhether these observations are in tension with galaxy formation models, and if\nso how? While comparisons have been made using physical properties (i.e.~black\nhole mass and accretion rate) inferred from observations, these require the use\nof SED modelling assumptions, or locally inferred scaling relations, which may\nbe unjustified, at least in the distant high-redshift Universe. In this work we\ntake an alternative approach and forward model predictions from the First Light\nAnd Reionisation Epoch Simulations (FLARES) suite of cosmological\nhydrodynamical zoom simulations to predict the observable properties of BLAGN.\nWe achieve this by first coupling \\flares\\ with the \\qsosed\\ model to predict\nthe ionising photon luminosities of high-redshift ($z>5$) AGN. To model the\nobserved broad H$\\alpha$ emission we then assume a constant conversion factor\nand covering fraction, and the fraction of AGN that have observable\nbroad-lines. With a reasonable choice of these parameters, \\flares\\ is able to\nreproduce observational constraints on the H$\\alpha$ luminosity function and\nequivalent width distribution at $z=5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most remarkable results from the \\emph{James Webb Space Telescope}\nhas been the discovery of a large population of compact sources exhibiting\nstrong broad H$\\alpha$ emission, typically interpreted to be low-luminosity\nbroad-line (Type 1) active galactic nuclei (BLAGN). An important question is\nwhether these observations are in tension with galaxy formation models, and if\nso how? While comparisons have been made using physical properties (i.e.~black\nhole mass and accretion rate) inferred from observations, these require the use\nof SED modelling assumptions, or locally inferred scaling relations, which may\nbe unjustified, at least in the distant high-redshift Universe. In this work we\ntake an alternative approach and forward model predictions from the First Light\nAnd Reionisation Epoch Simulations (FLARES) suite of cosmological\nhydrodynamical zoom simulations to predict the observable properties of BLAGN.\nWe achieve this by first coupling \\flares\\ with the \\qsosed\\ model to predict\nthe ionising photon luminosities of high-redshift ($z>5$) AGN. To model the\nobserved broad H$\\alpha$ emission we then assume a constant conversion factor\nand covering fraction, and the fraction of AGN that have observable\nbroad-lines. With a reasonable choice of these parameters, \\flares\\ is able to\nreproduce observational constraints on the H$\\alpha$ luminosity function and\nequivalent width distribution at $z=5$."
                },
                "authors": [
                    {
                        "name": "Stephen M. Wilkins"
                    },
                    {
                        "name": "Aswin P. Vijayan"
                    },
                    {
                        "name": "Scott Hagen"
                    },
                    {
                        "name": "Joseph Caruana"
                    },
                    {
                        "name": "Christopher J. Conselice"
                    },
                    {
                        "name": "Chris Done"
                    },
                    {
                        "name": "Michaela Hirschmann"
                    },
                    {
                        "name": "Dimitrios Irodotou"
                    },
                    {
                        "name": "Christopher C. Lovell"
                    },
                    {
                        "name": "Jorryt Matthee"
                    },
                    {
                        "name": "Adèle Plat"
                    },
                    {
                        "name": "William J. Roper"
                    },
                    {
                        "name": "Anthony J. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Anthony J. Taylor"
                },
                "author": "Anthony J. Taylor",
                "arxiv_comment": "15 pages, to be submitted to the Open Journal of Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00717v2",
                "updated": "2025-05-08T13:46:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    46,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-17T15:01:01Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    1,
                    1,
                    3,
                    107,
                    0
                ],
                "title": "Thinning-Stable Point Processes as a Model for Spatial Burstiness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinning-Stable Point Processes as a Model for Spatial Burstiness"
                },
                "summary": "In modern telecommunications, spatial burstiness of data traffic\n  poses challenges to traditional Poisson-based models. This paper\n  describes application of thinning-stable point processes,\n  which provide a more appropriate framework for modeling bursty\n  spatial data. We discuss their properties, representation, inference\n  methods, and applications, demonstrating the advantages over\n  classical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern telecommunications, spatial burstiness of data traffic\n  poses challenges to traditional Poisson-based models. This paper\n  describes application of thinning-stable point processes,\n  which provide a more appropriate framework for modeling bursty\n  spatial data. We discuss their properties, representation, inference\n  methods, and applications, demonstrating the advantages over\n  classical approaches."
                },
                "authors": [
                    {
                        "name": "Sergei Zuyev"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Zuyev"
                },
                "author": "Sergei Zuyev",
                "arxiv_comment": "7 pages, 2 figures. Accepted for WiOpt+25 Conference",
                "arxiv_journal_ref": "WiOpt-2025 Conference paper, 26-29th of May, 2025, Link\\\"oping,\n  Sweden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04728v2",
                "updated": "2025-05-08T13:42:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    42,
                    18,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-07T07:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "title": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models"
                },
                "summary": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks."
                },
                "authors": [
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Yuhuan Yuan"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Fuxiang Frank Xia"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ge Lin"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Accepted by TMLR2025 (32 pages, 6 figures)",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12421v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12421v3",
                "updated": "2025-05-08T13:39:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    39,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2023-11-21T08:21:55Z",
                "published_parsed": [
                    2023,
                    11,
                    21,
                    8,
                    21,
                    55,
                    1,
                    325,
                    0
                ],
                "title": "Two Views Are Better than One: Monocular 3D Pose Estimation with\n  Multiview Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Views Are Better than One: Monocular 3D Pose Estimation with\n  Multiview Consistency"
                },
                "summary": "Deducing a 3D human pose from a single 2D image is inherently challenging\nbecause multiple 3D poses can correspond to the same 2D representation. 3D data\ncan resolve this pose ambiguity, but it is expensive to record and requires an\nintricate setup that is often restricted to controlled lab environments. We\npropose a method that improves the performance of deep learning-based monocular\n3D human pose estimation models by using multiview data only during training,\nbut not during inference. We introduce a novel loss function, consistency loss,\nwhich operates on two synchronized views. This approach is simpler than\nprevious models that require 3D ground truth or intrinsic and extrinsic camera\nparameters. Our consistency loss penalizes differences in two pose sequences\nafter rigid alignment. We also demonstrate that our consistency loss\nsubstantially improves performance for fine-tuning without requiring 3D data.\nFurthermore, we show that using our consistency loss can yield state-of-the-art\nperformance when training models from scratch in a semi-supervised manner. Our\nfindings provide a simple way to capture new data, e.g in a new domain. This\ndata can be added using off-the-shelf cameras with no calibration requirements.\nWe make all our code and data publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deducing a 3D human pose from a single 2D image is inherently challenging\nbecause multiple 3D poses can correspond to the same 2D representation. 3D data\ncan resolve this pose ambiguity, but it is expensive to record and requires an\nintricate setup that is often restricted to controlled lab environments. We\npropose a method that improves the performance of deep learning-based monocular\n3D human pose estimation models by using multiview data only during training,\nbut not during inference. We introduce a novel loss function, consistency loss,\nwhich operates on two synchronized views. This approach is simpler than\nprevious models that require 3D ground truth or intrinsic and extrinsic camera\nparameters. Our consistency loss penalizes differences in two pose sequences\nafter rigid alignment. We also demonstrate that our consistency loss\nsubstantially improves performance for fine-tuning without requiring 3D data.\nFurthermore, we show that using our consistency loss can yield state-of-the-art\nperformance when training models from scratch in a semi-supervised manner. Our\nfindings provide a simple way to capture new data, e.g in a new domain. This\ndata can be added using off-the-shelf cameras with no calibration requirements.\nWe make all our code and data publicly available."
                },
                "authors": [
                    {
                        "name": "Christian Keilstrup Ingwersen"
                    },
                    {
                        "name": "Rasmus Tirsgaard"
                    },
                    {
                        "name": "Rasmus Nylander"
                    },
                    {
                        "name": "Janus Nørtoft Jensen"
                    },
                    {
                        "name": "Anders Bjorholm Dahl"
                    },
                    {
                        "name": "Morten Rieger Hannemose"
                    }
                ],
                "author_detail": {
                    "name": "Morten Rieger Hannemose"
                },
                "author": "Morten Rieger Hannemose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12421v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12421v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15236v2",
                "updated": "2025-05-08T13:35:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    35,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-20T00:00:56Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    0,
                    0,
                    56,
                    6,
                    294,
                    0
                ],
                "title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nadvancing natural language understanding and generation, enabling applications\nacross fields beyond healthcare, software engineering, and conversational\nsystems. Despite these advancements in the past few years, LLMs have shown\nconsiderable vulnerabilities, particularly to prompt injection and jailbreaking\nattacks. This review analyzes the state of research on these vulnerabilities\nand presents available defense strategies. We roughly categorize attack\napproaches into prompt-based, model-based, multimodal, and multilingual,\ncovering techniques such as adversarial prompting, backdoor injections, and\ncross-modality exploits. We also review various defense mechanisms, including\nprompt filtering, transformation, alignment techniques, multi-agent defenses,\nand self-regulation, evaluating their strengths and shortcomings. We also\ndiscuss key metrics and benchmarks used to assess LLM safety and robustness,\nnoting challenges like the quantification of attack success in interactive\ncontexts and biases in existing datasets. Identifying current research gaps, we\nsuggest future directions for resilient alignment strategies, advanced defenses\nagainst evolving attacks, automation of jailbreak detection, and consideration\nof ethical and societal impacts. This review emphasizes the need for continued\nresearch and cooperation within the AI community to enhance LLM security and\nensure their safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed artificial intelligence by\nadvancing natural language understanding and generation, enabling applications\nacross fields beyond healthcare, software engineering, and conversational\nsystems. Despite these advancements in the past few years, LLMs have shown\nconsiderable vulnerabilities, particularly to prompt injection and jailbreaking\nattacks. This review analyzes the state of research on these vulnerabilities\nand presents available defense strategies. We roughly categorize attack\napproaches into prompt-based, model-based, multimodal, and multilingual,\ncovering techniques such as adversarial prompting, backdoor injections, and\ncross-modality exploits. We also review various defense mechanisms, including\nprompt filtering, transformation, alignment techniques, multi-agent defenses,\nand self-regulation, evaluating their strengths and shortcomings. We also\ndiscuss key metrics and benchmarks used to assess LLM safety and robustness,\nnoting challenges like the quantification of attack success in interactive\ncontexts and biases in existing datasets. Identifying current research gaps, we\nsuggest future directions for resilient alignment strategies, advanced defenses\nagainst evolving attacks, automation of jailbreak detection, and consideration\nof ethical and societal impacts. This review emphasizes the need for continued\nresearch and cooperation within the AI community to enhance LLM security and\nensure their safe deployment."
                },
                "authors": [
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Lawrence K. Q. Yan"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Caitlyn Heqi Yin"
                },
                "author": "Caitlyn Heqi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05237v1",
                "updated": "2025-05-08T13:32:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    32,
                    9,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:32:09Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    32,
                    9,
                    3,
                    128,
                    0
                ],
                "title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\n  Learning"
                },
                "summary": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain"
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Hengrui Gu"
                    },
                    {
                        "name": "Hangting Ye"
                    },
                    {
                        "name": "Yiwei Dai"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.03571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.03571v3",
                "updated": "2025-05-08T13:30:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    30,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2022-08-06T19:47:32Z",
                "published_parsed": [
                    2022,
                    8,
                    6,
                    19,
                    47,
                    32,
                    5,
                    218,
                    0
                ],
                "title": "Transformer-based assignment decision network for multiple object\n  tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based assignment decision network for multiple object\n  tracking"
                },
                "summary": "Data association is a crucial component for any multiple object tracking\n(MOT) method that follows the tracking-by-detection paradigm. To generate\ncomplete trajectories such methods employ a data association process to\nestablish assignments between detections and existing targets during each\ntimestep. Recent data association approaches try to solve either a\nmulti-dimensional linear assignment task or a network flow minimization problem\nor tackle it via multiple hypotheses tracking. However, during inference an\noptimization step that computes optimal assignments is required for every\nsequence frame inducing additional complexity to any given solution. To this\nend, in the context of this work we introduce Transformer-based Assignment\nDecision Network (TADN) that tackles data association without the need of any\nexplicit optimization during inference. In particular, TADN can directly infer\nassignment pairs between detections and active targets in a single forward pass\nof the network. We have integrated TADN in a rather simple MOT framework,\ndesigned a novel training strategy for efficient end-to-end training and\ndemonstrated the high potential of our approach for online visual\ntracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and\nUA-DETRAC. Our proposed approach demonstrates strong performance in most\nevaluation metrics despite its simple nature as a tracker lacking significant\nauxiliary components such as occlusion handling or re-identification. The\nimplementation of our method is publicly available at\nhttps://github.com/psaltaath/tadn-mot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data association is a crucial component for any multiple object tracking\n(MOT) method that follows the tracking-by-detection paradigm. To generate\ncomplete trajectories such methods employ a data association process to\nestablish assignments between detections and existing targets during each\ntimestep. Recent data association approaches try to solve either a\nmulti-dimensional linear assignment task or a network flow minimization problem\nor tackle it via multiple hypotheses tracking. However, during inference an\noptimization step that computes optimal assignments is required for every\nsequence frame inducing additional complexity to any given solution. To this\nend, in the context of this work we introduce Transformer-based Assignment\nDecision Network (TADN) that tackles data association without the need of any\nexplicit optimization during inference. In particular, TADN can directly infer\nassignment pairs between detections and active targets in a single forward pass\nof the network. We have integrated TADN in a rather simple MOT framework,\ndesigned a novel training strategy for efficient end-to-end training and\ndemonstrated the high potential of our approach for online visual\ntracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and\nUA-DETRAC. Our proposed approach demonstrates strong performance in most\nevaluation metrics despite its simple nature as a tracker lacking significant\nauxiliary components such as occlusion handling or re-identification. The\nimplementation of our method is publicly available at\nhttps://github.com/psaltaath/tadn-mot."
                },
                "authors": [
                    {
                        "name": "Athena Psalta"
                    },
                    {
                        "name": "Vasileios Tsironis"
                    },
                    {
                        "name": "Konstantinos Karantzalos"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Karantzalos"
                },
                "author": "Konstantinos Karantzalos",
                "arxiv_comment": "Preprint version. Under consideration at Computer Vision and Image\n  Understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.03571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.03571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16698v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16698v3",
                "updated": "2025-05-08T13:28:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    28,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2024-12-21T16:54:28Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    16,
                    54,
                    28,
                    5,
                    356,
                    0
                ],
                "title": "Interact with me: Joint Egocentric Forecasting of Intent to Interact,\n  Attitude and Social Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interact with me: Joint Egocentric Forecasting of Intent to Interact,\n  Attitude and Social Actions"
                },
                "summary": "For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Tongfei Bian"
                    },
                    {
                        "name": "Yiming Ma"
                    },
                    {
                        "name": "Mathieu Chollet"
                    },
                    {
                        "name": "Victor Sanchez"
                    },
                    {
                        "name": "Tanaya Guha"
                    }
                ],
                "author_detail": {
                    "name": "Tanaya Guha"
                },
                "author": "Tanaya Guha",
                "arxiv_comment": "Accepted to ICME, 2025. Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16698v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16698v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02542v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02542v4",
                "updated": "2025-05-08T13:24:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    24,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-04T12:12:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    12,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "LREA: Low-Rank Efficient Attention on Modeling Long-Term User Behaviors\n  for CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LREA: Low-Rank Efficient Attention on Modeling Long-Term User Behaviors\n  for CTR Prediction"
                },
                "summary": "With the rapid growth of user historical behavior data, user interest\nmodeling has become a prominent aspect in Click-Through Rate (CTR) prediction,\nfocusing on learning user intent representations. However, this complexity\nposes computational challenges, requiring a balance between model performance\nand acceptable response times for online services. Traditional methods often\nutilize filtering techniques. These techniques can lead to the loss of\nsignificant information by prioritizing top K items based on item attributes or\nemploying low-precision attention mechanisms. In this study, we introduce LREA,\na novel attention mechanism that overcomes the limitations of existing\napproaches while ensuring computational efficiency. LREA leverages low-rank\nmatrix decomposition to optimize runtime performance and incorporates a\nspecially designed loss function to maintain attention capabilities while\npreserving information integrity. During the inference phase, matrix absorption\nand pre-storage strategies are employed to effectively meet runtime\nconstraints. The results of extensive offline and online experiments\ndemonstrate that our method outperforms state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of user historical behavior data, user interest\nmodeling has become a prominent aspect in Click-Through Rate (CTR) prediction,\nfocusing on learning user intent representations. However, this complexity\nposes computational challenges, requiring a balance between model performance\nand acceptable response times for online services. Traditional methods often\nutilize filtering techniques. These techniques can lead to the loss of\nsignificant information by prioritizing top K items based on item attributes or\nemploying low-precision attention mechanisms. In this study, we introduce LREA,\na novel attention mechanism that overcomes the limitations of existing\napproaches while ensuring computational efficiency. LREA leverages low-rank\nmatrix decomposition to optimize runtime performance and incorporates a\nspecially designed loss function to maintain attention capabilities while\npreserving information integrity. During the inference phase, matrix absorption\nand pre-storage strategies are employed to effectively meet runtime\nconstraints. The results of extensive offline and online experiments\ndemonstrate that our method outperforms state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Xiaochen Li"
                    },
                    {
                        "name": "Jinxin Hu"
                    },
                    {
                        "name": "Hong Wen"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiaoyi Zeng"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_doi": "10.1145/3726302.3730228",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730228",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02542v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02542v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by SIGIR 2025 Short Paper Track",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13701v2",
                "updated": "2025-05-08T13:22:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    22,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-18T14:02:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    14,
                    2,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Inverse Inference on Cooperative Control of Networked Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Inference on Cooperative Control of Networked Dynamical Systems"
                },
                "summary": "Recent years have witnessed the rapid advancement of understanding the\ncontrol mechanism of networked dynamical systems (NDSs), which are governed by\ncomponents such as nodal dynamics and topology. This paper reveals that the\ncritical components in continuous-time state feedback cooperative control of\nNDSs can be inferred merely from discrete observations. In particular, we\nadvocate a bi-level inference framework to estimate the global closed-loop\nsystem and extract the components, respectively. The novelty lies in bridging\nthe gap from discrete observations to the continuous-time model and effectively\ndecoupling the concerned components. Specifically, in the first level, we\ndesign a causality-based estimator for the discrete-time closed-loop system\nmatrix, which can achieve asymptotically unbiased performance when the NDS is\nstable. In the second level, we introduce a matrix logarithm based method to\nrecover the continuous-time counterpart matrix, providing new sampling period\nguarantees and establishing the recovery error bound. By utilizing graph\nproperties of the NDS, we develop least square based procedures to decouple the\nconcerned components with up to a scalar ambiguity. Furthermore, we employ\ninverse optimal control techniques to reconstruct the objective function\ndriving the control process, deriving necessary conditions for the solutions.\nNumerical simulations demonstrate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid advancement of understanding the\ncontrol mechanism of networked dynamical systems (NDSs), which are governed by\ncomponents such as nodal dynamics and topology. This paper reveals that the\ncritical components in continuous-time state feedback cooperative control of\nNDSs can be inferred merely from discrete observations. In particular, we\nadvocate a bi-level inference framework to estimate the global closed-loop\nsystem and extract the components, respectively. The novelty lies in bridging\nthe gap from discrete observations to the continuous-time model and effectively\ndecoupling the concerned components. Specifically, in the first level, we\ndesign a causality-based estimator for the discrete-time closed-loop system\nmatrix, which can achieve asymptotically unbiased performance when the NDS is\nstable. In the second level, we introduce a matrix logarithm based method to\nrecover the continuous-time counterpart matrix, providing new sampling period\nguarantees and establishing the recovery error bound. By utilizing graph\nproperties of the NDS, we develop least square based procedures to decouple the\nconcerned components with up to a scalar ambiguity. Furthermore, we employ\ninverse optimal control techniques to reconstruct the objective function\ndriving the control process, deriving necessary conditions for the solutions.\nNumerical simulations demonstrate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Yushan Li"
                    },
                    {
                        "name": "Jianping He"
                    },
                    {
                        "name": "Dimos V. Dimarogonas"
                    }
                ],
                "author_detail": {
                    "name": "Dimos V. Dimarogonas"
                },
                "author": "Dimos V. Dimarogonas",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20784v2",
                "updated": "2025-05-08T13:18:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    18,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-29T14:01:10Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    1,
                    10,
                    1,
                    119,
                    0
                ],
                "title": "Approximate Lifted Model Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Lifted Model Construction"
                },
                "summary": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice."
                },
                "authors": [
                    {
                        "name": "Malte Luttermann"
                    },
                    {
                        "name": "Jan Speller"
                    },
                    {
                        "name": "Marcel Gehrke"
                    },
                    {
                        "name": "Tanya Braun"
                    },
                    {
                        "name": "Ralf Möller"
                    },
                    {
                        "name": "Mattis Hartwig"
                    }
                ],
                "author_detail": {
                    "name": "Mattis Hartwig"
                },
                "author": "Mattis Hartwig",
                "arxiv_comment": "Extended version of paper accepted to the Proceedings of the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05225v1",
                "updated": "2025-05-08T13:16:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    16,
                    49,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    16,
                    49,
                    3,
                    128,
                    0
                ],
                "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation"
                },
                "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jason Zhang"
                },
                "author": "Chen Jason Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00831v3",
                "updated": "2025-05-08T13:12:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    12,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-01T19:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    44,
                    36,
                    3,
                    121,
                    0
                ],
                "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation"
                },
                "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."
                },
                "authors": [
                    {
                        "name": "Quang P. M. Pham"
                    },
                    {
                        "name": "Khoi T. N. Nguyen"
                    },
                    {
                        "name": "Nhi H. Doan"
                    },
                    {
                        "name": "Cuong A. Pham"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Dezhen Song"
                    }
                ],
                "author_detail": {
                    "name": "Dezhen Song"
                },
                "author": "Dezhen Song",
                "arxiv_comment": "Paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05207v1",
                "updated": "2025-05-08T13:02:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    2,
                    39,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:02:39Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    2,
                    39,
                    3,
                    128,
                    0
                ],
                "title": "A Fourier-based inference method for learning interaction kernels in\n  particle systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fourier-based inference method for learning interaction kernels in\n  particle systems"
                },
                "summary": "We consider the problem of inferring the interaction kernel of stochastic\ninteracting particle systems from observations of a single particle. We adopt a\nsemi-parametric approach and represent the interaction kernel in terms of a\ngeneralized Fourier series. The basis functions in this expansion are tailored\nto the problem at hand and are chosen to be orthogonal polynomials with respect\nto the invariant measure of the mean-field dynamics. The generalized Fourier\ncoefficients are obtained as the solution of an appropriate linear system whose\ncoefficients depend on the moments of the invariant measure, and which are\napproximated from the particle trajectory that we observe. We quantify the\napproximation error in the Lebesgue space weighted by the invariant measure and\nstudy the asymptotic properties of the estimator in the joint limit as the\nobservation interval and the number of particles tend to infinity, i.e. the\njoint large time-mean field limit. We also explore the regime where an\nincreasing number of generalized Fourier coefficients is needed to represent\nthe interaction kernel. Our theoretical results are supported by extensive\nnumerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of inferring the interaction kernel of stochastic\ninteracting particle systems from observations of a single particle. We adopt a\nsemi-parametric approach and represent the interaction kernel in terms of a\ngeneralized Fourier series. The basis functions in this expansion are tailored\nto the problem at hand and are chosen to be orthogonal polynomials with respect\nto the invariant measure of the mean-field dynamics. The generalized Fourier\ncoefficients are obtained as the solution of an appropriate linear system whose\ncoefficients depend on the moments of the invariant measure, and which are\napproximated from the particle trajectory that we observe. We quantify the\napproximation error in the Lebesgue space weighted by the invariant measure and\nstudy the asymptotic properties of the estimator in the joint limit as the\nobservation interval and the number of particles tend to infinity, i.e. the\njoint large time-mean field limit. We also explore the regime where an\nincreasing number of generalized Fourier coefficients is needed to represent\nthe interaction kernel. Our theoretical results are supported by extensive\nnumerical simulations."
                },
                "authors": [
                    {
                        "name": "Grigorios A. Pavliotis"
                    },
                    {
                        "name": "Andrea Zanoni"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanoni"
                },
                "author": "Andrea Zanoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05203v1",
                "updated": "2025-05-08T13:00:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    0,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:00:24Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    0,
                    24,
                    3,
                    128,
                    0
                ],
                "title": "LAPSO: A Unified Optimization View for Learning-Augmented Power System\n  Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPSO: A Unified Optimization View for Learning-Augmented Power System\n  Operations"
                },
                "summary": "With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp."
                },
                "authors": [
                    {
                        "name": "Wangkun Xu"
                    },
                    {
                        "name": "Zhongda Chu"
                    },
                    {
                        "name": "Fei Teng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Teng"
                },
                "author": "Fei Teng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05196v1",
                "updated": "2025-05-08T12:53:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    53,
                    42,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:53:42Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    53,
                    42,
                    3,
                    128,
                    0
                ],
                "title": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based\n  Retrieval-Augmented Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based\n  Retrieval-Augmented Recommender Systems"
                },
                "summary": "We present a systematic study of provider-side data poisoning in\nretrieval-augmented recommender systems (RAG-based). By modifying only a small\nfraction of tokens within item descriptions -- for instance, adding emotional\nkeywords or borrowing phrases from semantically related items -- an attacker\ncan significantly promote or demote targeted items. We formalize these attacks\nunder token-edit and semantic-similarity constraints, and we examine their\neffectiveness in both promotion (long-tail items) and demotion (short-head\nitems) scenarios. Our experiments on MovieLens, using two large language model\n(LLM) retrieval modules, show that even subtle attacks shift final rankings and\nitem exposures while eluding naive detection. The results underscore the\nvulnerability of RAG-based pipelines to small-scale metadata rewrites and\nemphasize the need for robust textual consistency checks and provenance\ntracking to thwart stealthy provider-side poisoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of provider-side data poisoning in\nretrieval-augmented recommender systems (RAG-based). By modifying only a small\nfraction of tokens within item descriptions -- for instance, adding emotional\nkeywords or borrowing phrases from semantically related items -- an attacker\ncan significantly promote or demote targeted items. We formalize these attacks\nunder token-edit and semantic-similarity constraints, and we examine their\neffectiveness in both promotion (long-tail items) and demotion (short-head\nitems) scenarios. Our experiments on MovieLens, using two large language model\n(LLM) retrieval modules, show that even subtle attacks shift final rankings and\nitem exposures while eluding naive detection. The results underscore the\nvulnerability of RAG-based pipelines to small-scale metadata rewrites and\nemphasize the need for robust textual consistency checks and provenance\ntracking to thwart stealthy provider-side poisoning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Nazary"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    },
                    {
                        "name": "Eugenio Di Sciascio"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Di Sciascio"
                },
                "author": "Eugenio Di Sciascio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05195v1",
                "updated": "2025-05-08T12:52:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    52,
                    2,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:52:02Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    52,
                    2,
                    3,
                    128,
                    0
                ],
                "title": "Concept-Based Unsupervised Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Based Unsupervised Domain Adaptation"
                },
                "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets."
                },
                "authors": [
                    {
                        "name": "Xinyue Xu"
                    },
                    {
                        "name": "Yueying Hu"
                    },
                    {
                        "name": "Hui Tang"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Lu Mi"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaomeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomeng Li"
                },
                "author": "Xiaomeng Li",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05190v1",
                "updated": "2025-05-08T12:39:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    39,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:39:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    39,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "Revealing Weaknesses in Text Watermarking Through Self-Information\n  Rewrite Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Weaknesses in Text Watermarking Through Self-Information\n  Rewrite Attacks"
                },
                "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."
                },
                "authors": [
                    {
                        "name": "Yixin Cheng"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Yangming Li"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "arxiv_comment": "ICML 2025 Accpeted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05189v1",
                "updated": "2025-05-08T12:37:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    37,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:37:51Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    37,
                    51,
                    3,
                    128,
                    0
                ],
                "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models"
                },
                "summary": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}."
                },
                "authors": [
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jianchen Hu"
                    },
                    {
                        "name": "Meng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Zhang"
                },
                "author": "Meng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05181v1",
                "updated": "2025-05-08T12:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    32,
                    29,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    32,
                    29,
                    3,
                    128,
                    0
                ],
                "title": "Stochastic Variational Propagation: Local, Scalable and Efficient\n  Alternative to Backpropagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Variational Propagation: Local, Scalable and Efficient\n  Alternative to Backpropagation"
                },
                "summary": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design."
                },
                "authors": [
                    {
                        "name": "Bojian Yin"
                    },
                    {
                        "name": "Federico Corradi"
                    }
                ],
                "author_detail": {
                    "name": "Federico Corradi"
                },
                "author": "Federico Corradi",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05177v1",
                "updated": "2025-05-08T12:28:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    28,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:28:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    28,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "MARK: Memory Augmented Refinement of Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARK: Memory Augmented Refinement of Knowledge"
                },
                "summary": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time."
                },
                "authors": [
                    {
                        "name": "Anish Ganguli"
                    },
                    {
                        "name": "Prabal Deb"
                    },
                    {
                        "name": "Debleena Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Debleena Banerjee"
                },
                "author": "Debleena Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20170v2",
                "updated": "2025-05-08T12:17:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    17,
                    22,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-27T15:07:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    7,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Re-evaluating Open-ended Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Open-ended Evaluation of Large Language Models"
                },
                "summary": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development."
                },
                "authors": [
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Ian Gemp"
                    },
                    {
                        "name": "Luke Marris"
                    },
                    {
                        "name": "Georgios Piliouras"
                    },
                    {
                        "name": "Nicolas Heess"
                    },
                    {
                        "name": "Marc Lanctot"
                    }
                ],
                "author_detail": {
                    "name": "Marc Lanctot"
                },
                "author": "Marc Lanctot",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15169v2",
                "updated": "2025-05-08T11:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    58,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-19T12:51:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    51,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Open-Source Large Language Models on Healthcare Text\n  Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Open-Source Large Language Models on Healthcare Text\n  Classification Tasks"
                },
                "summary": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts."
                },
                "authors": [
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Abeed Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Abeed Sarker"
                },
                "author": "Abeed Sarker",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04611v2",
                "updated": "2025-05-08T11:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    56,
                    7,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T17:55:32Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    55,
                    32,
                    2,
                    127,
                    0
                ],
                "title": "Particle Gibbs without the Gibbs bit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle Gibbs without the Gibbs bit"
                },
                "summary": "Exact parameter and trajectory inference in state-space models is typically\nachieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or\nparticle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly\nproposes a new trajectory and parameter, and accepts or rejects both at once.\nPGibbs instead alternates between sampling from the trajectory, using an\nalgorithm known as conditional sequential Monte Carlo (CSMC) and the parameter\nin a Hastings-within-Gibbs fashion. While particle independent Metropolis\nHastings (PIMH), the parameter-free version of PMMH, is known to be\nstatistically worse than CSMC, PGibbs can induce a slow mixing if the parameter\nand the state trajectory are very correlated. This has made PMMH the method of\nchoice for many practitioners, despite theory and experiments favouring CSMC\nover PIMH for the parameter-free problem. In this article, we describe a\nformulation of PGibbs which bypasses the Gibbs step, essentially marginalizing\nover the trajectory distribution in a fashion similar to PMMH. This is achieved\nby considering the implementation of a CSMC algortihm for the state-space model\nintegrated over the joint distribution of the current parameter and the\nparameter proposal. We illustrate the benefits of method on a simple example\nknown to be challenging for PMMH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact parameter and trajectory inference in state-space models is typically\nachieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or\nparticle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly\nproposes a new trajectory and parameter, and accepts or rejects both at once.\nPGibbs instead alternates between sampling from the trajectory, using an\nalgorithm known as conditional sequential Monte Carlo (CSMC) and the parameter\nin a Hastings-within-Gibbs fashion. While particle independent Metropolis\nHastings (PIMH), the parameter-free version of PMMH, is known to be\nstatistically worse than CSMC, PGibbs can induce a slow mixing if the parameter\nand the state trajectory are very correlated. This has made PMMH the method of\nchoice for many practitioners, despite theory and experiments favouring CSMC\nover PIMH for the parameter-free problem. In this article, we describe a\nformulation of PGibbs which bypasses the Gibbs step, essentially marginalizing\nover the trajectory distribution in a fashion similar to PMMH. This is achieved\nby considering the implementation of a CSMC algortihm for the state-space model\nintegrated over the joint distribution of the current parameter and the\nparameter proposal. We illustrate the benefits of method on a simple example\nknown to be challenging for PMMH."
                },
                "authors": [
                    {
                        "name": "Adrien Corenflos"
                    }
                ],
                "author_detail": {
                    "name": "Adrien Corenflos"
                },
                "author": "Adrien Corenflos",
                "arxiv_comment": "Feedback most welcome. 12 pages, 1 figure. Difference with previous\n  version: fixed a couple of typos + longer simulations to remove noise in the\n  figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05155v1",
                "updated": "2025-05-08T11:51:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    51,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:51:23Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    51,
                    23,
                    3,
                    128,
                    0
                ],
                "title": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning"
                },
                "summary": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Zhihao Zeng"
                    },
                    {
                        "name": "Ziquan Fang"
                    },
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v4",
                "updated": "2025-05-08T11:40:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    40,
                    1,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems"
                },
                "summary": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!"
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21936v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21936v4",
                "updated": "2025-05-08T11:33:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    33,
                    43,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-27T19:26:33Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    19,
                    26,
                    33,
                    3,
                    86,
                    0
                ],
                "title": "Binarity at LOw Metallicity (BLOeM): Enhanced multiplicity of early\n  B-type dwarfs and giants at $Z=0.2\\,{\\rm Z}_\\odot$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarity at LOw Metallicity (BLOeM): Enhanced multiplicity of early\n  B-type dwarfs and giants at $Z=0.2\\,{\\rm Z}_\\odot$"
                },
                "summary": "Early B-type stars ($M_i=8-15$ M$_\\odot$) are frequently in multiple systems,\nas evidenced by spectroscopic campaigns in the Milky Way (MW) and the Large\nMagellanic Cloud (LMC). Previous studies have shown no strong metallicity\ndependence in the close-binary (a>10 au) fraction or orbital-period\ndistributions between the MW's solar metallicity (Z$_\\odot$) and that of the\nLMC (Z=0.5 Z$_\\odot$). However, similar analyses in more metal-poor\nenvironments are still scarce. We focus on 309 early B-type stars (luminosity\nclasses III-V) from the Binarity at LOw Metallicity campaign in the Small\nMagellanic Cloud (SMC, Z=0.2 Z$_\\odot$) using VLT/FLAMES multi-epoch\nspectroscopy. By applying binary detection criteria consistent with previous\nworks, we identify 153 stars (91 SB1, 59 SB2, 3 SB3) exhibiting significant\nradial-velocity (RV) variations, resulting in an observed multiplicity fraction\nof $f^{obs}_{mult}=50\\pm3\\%$. Using Monte Carlo simulations to account for\nobservational biases, we infer an intrinsic close-binary fraction of\n$f_{mult}=80\\pm8\\%$. A Markov chain Monte Carlo analysis of the peak-to-peak RV\ndistribution ($\\Delta{\\rm RV}_{\\rm max}$) confirms a high multiplicity fraction\nof $f_{mult}=78\\pm5\\%$. These findings suggest a possible anti-correlation\nbetween metallicity and the fraction of close B-type binaries, with the SMC\nmultiplicity fraction significantly exceeding previous measurements in the LMC\nand MW. The enhanced fraction of close binaries at SMC's low metallicity may\nhave broad implications for massive-star evolution in the early Universe. More\nfrequent mass transfer and envelope stripping could boost the production of\nexotic transients, stripped supernovae, gravitational-wave progenitors, and\nsustained UV ionising flux, potentially affecting cosmic reionisation.\nTheoretical predictions of binary evolution under metal-poor conditions will\nprovide a key test of our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early B-type stars ($M_i=8-15$ M$_\\odot$) are frequently in multiple systems,\nas evidenced by spectroscopic campaigns in the Milky Way (MW) and the Large\nMagellanic Cloud (LMC). Previous studies have shown no strong metallicity\ndependence in the close-binary (a>10 au) fraction or orbital-period\ndistributions between the MW's solar metallicity (Z$_\\odot$) and that of the\nLMC (Z=0.5 Z$_\\odot$). However, similar analyses in more metal-poor\nenvironments are still scarce. We focus on 309 early B-type stars (luminosity\nclasses III-V) from the Binarity at LOw Metallicity campaign in the Small\nMagellanic Cloud (SMC, Z=0.2 Z$_\\odot$) using VLT/FLAMES multi-epoch\nspectroscopy. By applying binary detection criteria consistent with previous\nworks, we identify 153 stars (91 SB1, 59 SB2, 3 SB3) exhibiting significant\nradial-velocity (RV) variations, resulting in an observed multiplicity fraction\nof $f^{obs}_{mult}=50\\pm3\\%$. Using Monte Carlo simulations to account for\nobservational biases, we infer an intrinsic close-binary fraction of\n$f_{mult}=80\\pm8\\%$. A Markov chain Monte Carlo analysis of the peak-to-peak RV\ndistribution ($\\Delta{\\rm RV}_{\\rm max}$) confirms a high multiplicity fraction\nof $f_{mult}=78\\pm5\\%$. These findings suggest a possible anti-correlation\nbetween metallicity and the fraction of close B-type binaries, with the SMC\nmultiplicity fraction significantly exceeding previous measurements in the LMC\nand MW. The enhanced fraction of close binaries at SMC's low metallicity may\nhave broad implications for massive-star evolution in the early Universe. More\nfrequent mass transfer and envelope stripping could boost the production of\nexotic transients, stripped supernovae, gravitational-wave progenitors, and\nsustained UV ionising flux, potentially affecting cosmic reionisation.\nTheoretical predictions of binary evolution under metal-poor conditions will\nprovide a key test of our results."
                },
                "authors": [
                    {
                        "name": "J. I. Villaseñor"
                    },
                    {
                        "name": "H. Sana"
                    },
                    {
                        "name": "L. Mahy"
                    },
                    {
                        "name": "T. Shenar"
                    },
                    {
                        "name": "J. Bodensteiner"
                    },
                    {
                        "name": "N. Britavskiy"
                    },
                    {
                        "name": "D. J. Lennon"
                    },
                    {
                        "name": "M. Moe"
                    },
                    {
                        "name": "L. R. Patrick"
                    },
                    {
                        "name": "M. Pawlak"
                    },
                    {
                        "name": "D. M. Bowman"
                    },
                    {
                        "name": "P. A. Crowther"
                    },
                    {
                        "name": "S. E. de Mink"
                    },
                    {
                        "name": "K. Deshmukh"
                    },
                    {
                        "name": "C. J. Evans"
                    },
                    {
                        "name": "M. Fabry"
                    },
                    {
                        "name": "M. Fouesneau"
                    },
                    {
                        "name": "A. Herrero"
                    },
                    {
                        "name": "G. Holgado"
                    },
                    {
                        "name": "N. Langer"
                    },
                    {
                        "name": "J. Maíz Apellániz"
                    },
                    {
                        "name": "I. Mandel"
                    },
                    {
                        "name": "L. M. Oskinova"
                    },
                    {
                        "name": "D. Pauli"
                    },
                    {
                        "name": "V. Ramachandran"
                    },
                    {
                        "name": "M. Renzo"
                    },
                    {
                        "name": "H. -W. Rix"
                    },
                    {
                        "name": "D. F. Rocha"
                    },
                    {
                        "name": "A. A. C. S. Sander"
                    },
                    {
                        "name": "F. R. N. Schneider"
                    },
                    {
                        "name": "K. Sen"
                    },
                    {
                        "name": "S. Simón-Díaz"
                    },
                    {
                        "name": "J. Th. van Loon"
                    },
                    {
                        "name": "S. Toonen"
                    },
                    {
                        "name": "J. S. Vink"
                    }
                ],
                "author_detail": {
                    "name": "J. S. Vink"
                },
                "author": "J. S. Vink",
                "arxiv_comment": "16 pages, 18 figures (additional 10 pages and 7 figures in\n  appendices). Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21936v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21936v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19251v3",
                "updated": "2025-05-08T11:09:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    9,
                    33,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-27T14:21:48Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    21,
                    48,
                    6,
                    117,
                    0
                ],
                "title": "Probing the Bounce Energy Scale in Bouncing Cosmologies with Pulsar\n  Timing Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Bounce Energy Scale in Bouncing Cosmologies with Pulsar\n  Timing Arrays"
                },
                "summary": "In this work we constrain the bounce energy scale $\\rho_{s\\downarrow}^{1/4}$\nin a generic framework of bouncing cosmologies using the nanohertz stochastic\ngravitational-wave background recently detected by pulsar timing arrays\n(NANOGrav 15-yr, EPTA DR2, PPTA DR3, IPTA DR2). A full Bayesian fit of the\nanalytic SGWB spectrum for this bounce scenario reveals, for the first time,\ntwo distinct posterior branches in $(\\rho_{s\\downarrow}^{1/4},w_1)$: one near\n$w_1\\approx0.3$ and one at $w_1\\gg1$, where $w_1$ is the contraction phase\nequation of state. We find that the bouncing model attains larger Bayes factors\nagainst each of six conventional SGWB sources (SMBHBs, inflationary GWs, cosmic\nstrings, domain walls, first order phase transitions, scalar induced GWs),\ndemonstrating strong preference of current PTA data for the bounce hypothesis.\nCompared to the more generic dual inflation bounce scenario, the concrete\nbounce realization yields smaller Bayes factors, indicating that PTA\nmeasurements impose tighter constraints when the bounce scale is explicit.\nMoreover, the two posterior branches illuminate distinct theoretical frontiers.\nThe right branch ($w_1\\gg1$) violates the dominant energy condition (DEC),\nthereby providing direct empirical impetus for models with novel early Universe\nphysics, e.g. ghost condensates, higher-derivative or modified gravity\noperators, and extra dimensional effects. Independently, both branches infer\n$\\rho_{s\\downarrow}^{1/4}$ above the Planck scale $M_\\mathrm{pl}$,\ndemonstrating that current PTAs already probe trans-Planckian regimes.\nTogether, these findings offer a rare observational window into UV completions\nof cosmology. We further describe how normalizing flow based machine learning\ncan accelerate such Bayesian analyses as PTA data volumes increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we constrain the bounce energy scale $\\rho_{s\\downarrow}^{1/4}$\nin a generic framework of bouncing cosmologies using the nanohertz stochastic\ngravitational-wave background recently detected by pulsar timing arrays\n(NANOGrav 15-yr, EPTA DR2, PPTA DR3, IPTA DR2). A full Bayesian fit of the\nanalytic SGWB spectrum for this bounce scenario reveals, for the first time,\ntwo distinct posterior branches in $(\\rho_{s\\downarrow}^{1/4},w_1)$: one near\n$w_1\\approx0.3$ and one at $w_1\\gg1$, where $w_1$ is the contraction phase\nequation of state. We find that the bouncing model attains larger Bayes factors\nagainst each of six conventional SGWB sources (SMBHBs, inflationary GWs, cosmic\nstrings, domain walls, first order phase transitions, scalar induced GWs),\ndemonstrating strong preference of current PTA data for the bounce hypothesis.\nCompared to the more generic dual inflation bounce scenario, the concrete\nbounce realization yields smaller Bayes factors, indicating that PTA\nmeasurements impose tighter constraints when the bounce scale is explicit.\nMoreover, the two posterior branches illuminate distinct theoretical frontiers.\nThe right branch ($w_1\\gg1$) violates the dominant energy condition (DEC),\nthereby providing direct empirical impetus for models with novel early Universe\nphysics, e.g. ghost condensates, higher-derivative or modified gravity\noperators, and extra dimensional effects. Independently, both branches infer\n$\\rho_{s\\downarrow}^{1/4}$ above the Planck scale $M_\\mathrm{pl}$,\ndemonstrating that current PTAs already probe trans-Planckian regimes.\nTogether, these findings offer a rare observational window into UV completions\nof cosmology. We further describe how normalizing flow based machine learning\ncan accelerate such Bayesian analyses as PTA data volumes increase."
                },
                "authors": [
                    {
                        "name": "Junrong Lai"
                    },
                    {
                        "name": "Changhong Li"
                    }
                ],
                "author_detail": {
                    "name": "Changhong Li"
                },
                "author": "Changhong Li",
                "arxiv_comment": "30 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18635v2",
                "updated": "2025-05-08T10:58:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    58,
                    9,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-25T20:52:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    20,
                    52,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for\n  LLM and RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for\n  LLM and RAG Systems"
                },
                "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives."
                },
                "authors": [
                    {
                        "name": "Matthew Barker"
                    },
                    {
                        "name": "Andrew Bell"
                    },
                    {
                        "name": "Evan Thomas"
                    },
                    {
                        "name": "James Carr"
                    },
                    {
                        "name": "Thomas Andrews"
                    },
                    {
                        "name": "Umang Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Umang Bhatt"
                },
                "author": "Umang Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20, 68Q32, 90C29, 62P30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; G.1.6; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00654v3",
                "updated": "2025-05-08T10:52:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    52,
                    25,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-01T16:55:44Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    55,
                    44,
                    3,
                    121,
                    0
                ],
                "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Understanding: an Inherent Ambiguity Barrier"
                },
                "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."
                },
                "authors": [
                    {
                        "name": "Daniel N. Nissani"
                    }
                ],
                "author_detail": {
                    "name": "Daniel N. Nissani"
                },
                "arxiv_affiliation": "Nissensohn",
                "author": "Daniel N. Nissani",
                "arxiv_comment": "submitted to NEURAL COMPUTATION",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05122v1",
                "updated": "2025-05-08T10:51:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    51,
                    13,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:51:13Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    51,
                    13,
                    3,
                    128,
                    0
                ],
                "title": "Text2Cypher: Data Pruning using Hard Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher: Data Pruning using Hard Example Selection"
                },
                "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    }
                ],
                "author_detail": {
                    "name": "Makbule Gulcin Ozsoy"
                },
                "author": "Makbule Gulcin Ozsoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02075v3",
                "updated": "2025-05-08T10:46:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    46,
                    52,
                    3,
                    128,
                    0
                ],
                "published": "2023-07-05T07:32:34Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    32,
                    34,
                    2,
                    186,
                    0
                ],
                "title": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment"
                },
                "summary": "Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\ncircumvent the shortage of seed alignments provided for training, recent EA\nmodels utilize pseudo-labeling strategies to iteratively add unaligned entity\npairs predicted with high confidence to the seed alignments for model training.\nHowever, the adverse impact of confirmation bias during pseudo-labeling has\nbeen largely overlooked, thus hindering entity alignment performance. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as\nan effective means to determine entity correspondences and reduce erroneous\nmatches across two KGs. An effective criterion is derived to infer\npseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel\npseudo-label ensembling refines pseudo-labeled alignments by combining\npredictions over multiple models independently trained in parallel. The\nensembled pseudo-labeled alignments are thereafter used to augment seed\nalignments to reinforce subsequent model training for alignment inference. The\neffectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. Our extensive results and\nin-depth analyses demonstrate the superiority of UPL-EA over 15 competitive\nbaselines and its utility as a general pseudo-labeling framework for entity\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\ncircumvent the shortage of seed alignments provided for training, recent EA\nmodels utilize pseudo-labeling strategies to iteratively add unaligned entity\npairs predicted with high confidence to the seed alignments for model training.\nHowever, the adverse impact of confirmation bias during pseudo-labeling has\nbeen largely overlooked, thus hindering entity alignment performance. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as\nan effective means to determine entity correspondences and reduce erroneous\nmatches across two KGs. An effective criterion is derived to infer\npseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel\npseudo-label ensembling refines pseudo-labeled alignments by combining\npredictions over multiple models independently trained in parallel. The\nensembled pseudo-labeled alignments are thereafter used to augment seed\nalignments to reinforce subsequent model training for alignment inference. The\neffectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. Our extensive results and\nin-depth analyses demonstrate the superiority of UPL-EA over 15 competitive\nbaselines and its utility as a general pseudo-labeling framework for entity\nalignment."
                },
                "authors": [
                    {
                        "name": "Qijie Ding"
                    },
                    {
                        "name": "Jie Yin"
                    },
                    {
                        "name": "Daokun Zhang"
                    },
                    {
                        "name": "Junbin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Junbin Gao"
                },
                "author": "Junbin Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.02075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05120v1",
                "updated": "2025-05-08T10:44:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    44,
                    29,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:44:29Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    44,
                    29,
                    3,
                    128,
                    0
                ],
                "title": "Simulating MLB Seasons using Bayesian Inference and Random Walks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating MLB Seasons using Bayesian Inference and Random Walks"
                },
                "summary": "As a dedicated follower of sports statistics and with the MLB season\nbeginning in late March, I set out to predict how many wins each team would\naccumulate by the end of the 162 game season. The goal was to build a\nsimulation framework capable of forecasting the remainder of the season,\nstarting from a 20 game burn-in period to establish initial estimates of team\nstrength. My approach used a Bayesian inference model incorporating team win\npercentage, batting average, and pitching ERA to construct a posterior\ndistribution of win probability for each matchup. For each game, I sampled from\nthe posterior and simulated the outcome using a Bernoulli trial. Because future\nmatchup inputs were unobserved, I forecasted batting averages using random\nwalks and modeled pitching ERA with Kalman filters. After simulating many\nseasons, the model produced a distribution of win totals for all 30 teams and\ncan also be used to estimate each team's probability of making the postseason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a dedicated follower of sports statistics and with the MLB season\nbeginning in late March, I set out to predict how many wins each team would\naccumulate by the end of the 162 game season. The goal was to build a\nsimulation framework capable of forecasting the remainder of the season,\nstarting from a 20 game burn-in period to establish initial estimates of team\nstrength. My approach used a Bayesian inference model incorporating team win\npercentage, batting average, and pitching ERA to construct a posterior\ndistribution of win probability for each matchup. For each game, I sampled from\nthe posterior and simulated the outcome using a Bernoulli trial. Because future\nmatchup inputs were unobserved, I forecasted batting averages using random\nwalks and modeled pitching ERA with Kalman filters. After simulating many\nseasons, the model produced a distribution of win totals for all 30 teams and\ncan also be used to estimate each team's probability of making the postseason."
                },
                "authors": [
                    {
                        "name": "Simon Cha"
                    }
                ],
                "author_detail": {
                    "name": "Simon Cha"
                },
                "author": "Simon Cha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05111v1",
                "updated": "2025-05-08T10:24:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    24,
                    44,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:24:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    24,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Unveiling Language-Specific Features in Large Language Models via Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Language-Specific Features in Large Language Models via Sparse\n  Autoencoders"
                },
                "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs."
                },
                "authors": [
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yidan Zhang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14464v2",
                "updated": "2025-05-08T10:20:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    20,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-18T13:48:01Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    48,
                    1,
                    4,
                    292,
                    0
                ],
                "title": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning"
                },
                "summary": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Cecilia Mascolo"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "arxiv_comment": "Accepted at AHLI CHIL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05108v1",
                "updated": "2025-05-08T10:13:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    13,
                    53,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:13:53Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    13,
                    53,
                    3,
                    128,
                    0
                ],
                "title": "Multi-agent Embodied AI: Advances and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent Embodied AI: Advances and Future Directions"
                },
                "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field."
                },
                "authors": [
                    {
                        "name": "Zhaohan Feng"
                    },
                    {
                        "name": "Ruiqi Xue"
                    },
                    {
                        "name": "Lei Yuan"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Meiqin Liu"
                    },
                    {
                        "name": "Bingzhao Gao"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05103v1",
                "updated": "2025-05-08T10:04:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    4,
                    41,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:04:41Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    4,
                    41,
                    3,
                    128,
                    0
                ],
                "title": "A Weighted Byzantine Fault Tolerance Consensus Driven Trusted Multiple\n  Large Language Models Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Weighted Byzantine Fault Tolerance Consensus Driven Trusted Multiple\n  Large Language Models Network"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of applications. However, individual LLMs often produce inconsistent,\nbiased, or hallucinated outputs due to limitations in their training corpora\nand model architectures. Recently, collaborative frameworks such as the\nMulti-LLM Network (MultiLLMN) have been introduced, enabling multiple LLMs to\ninteract and jointly respond to user queries. Nevertheless, MultiLLMN\narchitectures raise critical concerns regarding the reliability and security of\nthe generated content, particularly in open environments where malicious or\ncompromised LLMs may be present. Moreover, reliance on centralized coordination\nundermines system efficiency and introduces single points of failure. In this\npaper, we propose a novel Trusted MultiLLMN framework, driven by a Weighted\nByzantine Fault Tolerance (WBFT) blockchain consensus mechanism, to ensure the\nreliability, security, and efficiency of multi-LLM collaboration. In WBFT,\nvoting weights are adaptively assigned to each LLM based on its response\nquality and trustworthiness, incentivizing reliable behavior, and reducing the\nimpact of malicious nodes. Extensive simulations demonstrate that WBFT\nsignificantly improves both consensus security and efficiency compared to\nclassical and modern consensus mechanisms, particularly under wireless network\nconditions. Furthermore, our evaluations reveal that Trusted MultiLLMN\nsupported by WBFT can deliver higher-quality and more credible responses than\nboth single LLMs and conventional MultiLLMNs, thereby providing a promising\npath toward building robust, decentralized AI collaboration networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of applications. However, individual LLMs often produce inconsistent,\nbiased, or hallucinated outputs due to limitations in their training corpora\nand model architectures. Recently, collaborative frameworks such as the\nMulti-LLM Network (MultiLLMN) have been introduced, enabling multiple LLMs to\ninteract and jointly respond to user queries. Nevertheless, MultiLLMN\narchitectures raise critical concerns regarding the reliability and security of\nthe generated content, particularly in open environments where malicious or\ncompromised LLMs may be present. Moreover, reliance on centralized coordination\nundermines system efficiency and introduces single points of failure. In this\npaper, we propose a novel Trusted MultiLLMN framework, driven by a Weighted\nByzantine Fault Tolerance (WBFT) blockchain consensus mechanism, to ensure the\nreliability, security, and efficiency of multi-LLM collaboration. In WBFT,\nvoting weights are adaptively assigned to each LLM based on its response\nquality and trustworthiness, incentivizing reliable behavior, and reducing the\nimpact of malicious nodes. Extensive simulations demonstrate that WBFT\nsignificantly improves both consensus security and efficiency compared to\nclassical and modern consensus mechanisms, particularly under wireless network\nconditions. Furthermore, our evaluations reveal that Trusted MultiLLMN\nsupported by WBFT can deliver higher-quality and more credible responses than\nboth single LLMs and conventional MultiLLMNs, thereby providing a promising\npath toward building robust, decentralized AI collaboration networks."
                },
                "authors": [
                    {
                        "name": "Haoxiang Luo"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Schahram Dustdar"
                    }
                ],
                "author_detail": {
                    "name": "Schahram Dustdar"
                },
                "author": "Schahram Dustdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05101v1",
                "updated": "2025-05-08T10:01:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    1,
                    14,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:01:14Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    1,
                    14,
                    3,
                    128,
                    0
                ],
                "title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via\n  Diffusion Models"
                },
                "summary": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Hongyang Zhu"
                    },
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Bo Fu"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08292v2",
                "updated": "2025-05-08T09:33:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    33,
                    59,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-11T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    5,
                    42,
                    1,
                    70,
                    0
                ],
                "title": "Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges"
                },
                "summary": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Qingying Xiao"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Xiangyi Feng"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Bairui Zhang"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Jian Chang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09246v2",
                "updated": "2025-05-08T09:33:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    33,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-12T15:03:00Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    3,
                    0,
                    5,
                    102,
                    0
                ],
                "title": "Type-Constrained Code Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type-Constrained Code Generation with Language Models"
                },
                "summary": "Large language models (LLMs) have achieved notable success in code\ngeneration. However, they still frequently produce uncompilable output because\ntheir next-token inference procedure does not model formal aspects of code.\nAlthough constrained decoding is a promising approach to alleviate this issue,\nit has only been applied to handle either domain-specific languages or\nsyntactic features of general-purpose programming languages. However, LLMs\nfrequently generate code with typing errors, which are beyond the domain of\nsyntax and generally hard to adequately constrain. To address this challenge,\nwe introduce a type-constrained decoding approach that leverages type systems\nto guide code generation. For this purpose, we develop novel prefix automata\nand a search over inhabitable types, forming a sound approach to enforce\nwell-typedness on LLM-generated code. We formalize our approach on a\nfoundational simply-typed language and extend it to TypeScript to demonstrate\npracticality. Our evaluation on the HumanEval and MBPP datasets shows that our\napproach reduces compilation errors by more than half and significantly\nincreases functional correctness in code synthesis, translation, and repair\ntasks across LLMs of various sizes and model families, including\nstate-of-the-art open-weight models with more than 30B parameters. The results\ndemonstrate the generality and effectiveness of our approach in constraining\nLLM code generation with formal rules of type systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved notable success in code\ngeneration. However, they still frequently produce uncompilable output because\ntheir next-token inference procedure does not model formal aspects of code.\nAlthough constrained decoding is a promising approach to alleviate this issue,\nit has only been applied to handle either domain-specific languages or\nsyntactic features of general-purpose programming languages. However, LLMs\nfrequently generate code with typing errors, which are beyond the domain of\nsyntax and generally hard to adequately constrain. To address this challenge,\nwe introduce a type-constrained decoding approach that leverages type systems\nto guide code generation. For this purpose, we develop novel prefix automata\nand a search over inhabitable types, forming a sound approach to enforce\nwell-typedness on LLM-generated code. We formalize our approach on a\nfoundational simply-typed language and extend it to TypeScript to demonstrate\npracticality. Our evaluation on the HumanEval and MBPP datasets shows that our\napproach reduces compilation errors by more than half and significantly\nincreases functional correctness in code synthesis, translation, and repair\ntasks across LLMs of various sizes and model families, including\nstate-of-the-art open-weight models with more than 30B parameters. The results\ndemonstrate the generality and effectiveness of our approach in constraining\nLLM code generation with formal rules of type systems."
                },
                "authors": [
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_doi": "10.1145/3729274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.09246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05081v1",
                "updated": "2025-05-08T09:26:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    26,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T09:26:28Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    26,
                    28,
                    3,
                    128,
                    0
                ],
                "title": "PIDiff: Image Customization for Personalized Identities with Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIDiff: Image Customization for Personalized Identities with Diffusion\n  Models"
                },
                "summary": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task."
                },
                "authors": [
                    {
                        "name": "Jinyu Gu"
                    },
                    {
                        "name": "Haipeng Liu"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16149v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16149v5",
                "updated": "2025-05-08T09:23:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    23,
                    42,
                    3,
                    128,
                    0
                ],
                "published": "2024-03-24T13:43:43Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    13,
                    43,
                    43,
                    6,
                    84,
                    0
                ],
                "title": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey"
                },
                "summary": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs."
                },
                "authors": [
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Yuxin Song"
                    },
                    {
                        "name": "Zihou Liu"
                    },
                    {
                        "name": "Qingyin Tan"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16149v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16149v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12258v3",
                "updated": "2025-05-08T09:15:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    15,
                    15,
                    3,
                    128,
                    0
                ],
                "published": "2024-05-20T11:40:23Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    11,
                    40,
                    23,
                    0,
                    141,
                    0
                ],
                "title": "Scientific Hypothesis Generation by a Large Language Model: Laboratory\n  Validation in Breast Cancer Treatment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Hypothesis Generation by a Large Language Model: Laboratory\n  Validation in Breast Cancer Treatment"
                },
                "summary": "Large language models LLMs have transformed AI and achieved breakthrough\nperformance on a wide range of tasks In science the most interesting\napplication of LLMs is for hypothesis formation A feature of LLMs which results\nfrom their probabilistic structure is that the output text is not necessarily a\nvalid inference from the training text These are termed hallucinations and are\nharmful in many applications In science some hallucinations may be useful novel\nhypotheses whose validity may be tested by laboratory experiments Here we\nexperimentally test the application of LLMs as a source of scientific\nhypotheses using the domain of breast cancer treatment We applied the LLM GPT4\nto hypothesize novel synergistic pairs of FDA-approved noncancer drugs that\ntarget the MCF7 breast cancer cell line relative to the nontumorigenic breast\ncell line MCF10A In the first round of laboratory experiments GPT4 succeeded in\ndiscovering three drug combinations out of twelve tested with synergy scores\nabove the positive controls GPT4 then generated new combinations based on its\ninitial results this generated three more combinations with positive synergy\nscores out of four tested We conclude that LLMs are a valuable source of\nscientific hypotheses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models LLMs have transformed AI and achieved breakthrough\nperformance on a wide range of tasks In science the most interesting\napplication of LLMs is for hypothesis formation A feature of LLMs which results\nfrom their probabilistic structure is that the output text is not necessarily a\nvalid inference from the training text These are termed hallucinations and are\nharmful in many applications In science some hallucinations may be useful novel\nhypotheses whose validity may be tested by laboratory experiments Here we\nexperimentally test the application of LLMs as a source of scientific\nhypotheses using the domain of breast cancer treatment We applied the LLM GPT4\nto hypothesize novel synergistic pairs of FDA-approved noncancer drugs that\ntarget the MCF7 breast cancer cell line relative to the nontumorigenic breast\ncell line MCF10A In the first round of laboratory experiments GPT4 succeeded in\ndiscovering three drug combinations out of twelve tested with synergy scores\nabove the positive controls GPT4 then generated new combinations based on its\ninitial results this generated three more combinations with positive synergy\nscores out of four tested We conclude that LLMs are a valuable source of\nscientific hypotheses."
                },
                "authors": [
                    {
                        "name": "Abbi Abdel-Rehim"
                    },
                    {
                        "name": "Hector Zenil"
                    },
                    {
                        "name": "Oghenejokpeme Orhobor"
                    },
                    {
                        "name": "Marie Fisher"
                    },
                    {
                        "name": "Ross J. Collins"
                    },
                    {
                        "name": "Elizabeth Bourne"
                    },
                    {
                        "name": "Gareth W. Fearnley"
                    },
                    {
                        "name": "Emma Tate"
                    },
                    {
                        "name": "Holly X. Smith"
                    },
                    {
                        "name": "Larisa N. Soldatova"
                    },
                    {
                        "name": "Ross D. King"
                    }
                ],
                "author_detail": {
                    "name": "Ross D. King"
                },
                "author": "Ross D. King",
                "arxiv_comment": "12 pages, 6 tables, 1 figure. Supplementary information available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01324v3",
                "updated": "2025-05-08T09:14:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    14,
                    55,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-02T14:55:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    55,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation"
                },
                "summary": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments."
                },
                "authors": [
                    {
                        "name": "Yukai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yukai Yang"
                },
                "author": "Yukai Yang",
                "arxiv_comment": "37 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for\n  journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G20, 62K99, 62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05073v1",
                "updated": "2025-05-08T09:08:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    8,
                    58,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T09:08:58Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    8,
                    58,
                    3,
                    128,
                    0
                ],
                "title": "RepSNet: A Nucleus Instance Segmentation model based on Boundary\n  Regression and Structural Re-parameterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepSNet: A Nucleus Instance Segmentation model based on Boundary\n  Regression and Structural Re-parameterization"
                },
                "summary": "Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus\ninstance segmentation is a key step in digital pathology analysis and\npathological diagnosis. However, the computational efficiency of the model and\nthe treatment of overlapping targets are the major challenges in the studies of\nthis problem. To this end, a neural network model RepSNet was designed based on\na nucleus boundary regression and a structural re-parameterization scheme for\nsegmenting and classifying the nuclei in H\\&E-stained histopathological images.\nFirst, RepSNet estimates the boundary position information (BPI) of the parent\nnucleus for each pixel. The BPI estimation incorporates the local information\nof the pixel and the contextual information of the parent nucleus. Then, the\nnucleus boundary is estimated by aggregating the BPIs from a series of pixels\nusing a proposed boundary voting mechanism (BVM), and the instance segmentation\nresults are computed from the estimated nucleus boundary using a connected\ncomponent analysis procedure. The BVM intrinsically achieves a kind of\nsynergistic belief enhancement among the BPIs from various pixels. Therefore,\ndifferent from the methods available in literature that obtain nucleus\nboundaries based on a direct pixel recognition scheme, RepSNet computes its\nboundary decisions based on some guidances from macroscopic information using\nan integration mechanism. In addition, RepSNet employs a re-parametrizable\nencoder-decoder structure. This model can not only aggregate features from some\nreceptive fields with various scales which helps segmentation accuracy\nimprovement, but also reduce the parameter amount and computational burdens in\nthe model inference phase through the structural re-parameterization technique.\nExtensive experiments demonstrated the superiorities of RepSNet compared to\nseveral typical benchmark models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus\ninstance segmentation is a key step in digital pathology analysis and\npathological diagnosis. However, the computational efficiency of the model and\nthe treatment of overlapping targets are the major challenges in the studies of\nthis problem. To this end, a neural network model RepSNet was designed based on\na nucleus boundary regression and a structural re-parameterization scheme for\nsegmenting and classifying the nuclei in H\\&E-stained histopathological images.\nFirst, RepSNet estimates the boundary position information (BPI) of the parent\nnucleus for each pixel. The BPI estimation incorporates the local information\nof the pixel and the contextual information of the parent nucleus. Then, the\nnucleus boundary is estimated by aggregating the BPIs from a series of pixels\nusing a proposed boundary voting mechanism (BVM), and the instance segmentation\nresults are computed from the estimated nucleus boundary using a connected\ncomponent analysis procedure. The BVM intrinsically achieves a kind of\nsynergistic belief enhancement among the BPIs from various pixels. Therefore,\ndifferent from the methods available in literature that obtain nucleus\nboundaries based on a direct pixel recognition scheme, RepSNet computes its\nboundary decisions based on some guidances from macroscopic information using\nan integration mechanism. In addition, RepSNet employs a re-parametrizable\nencoder-decoder structure. This model can not only aggregate features from some\nreceptive fields with various scales which helps segmentation accuracy\nimprovement, but also reduce the parameter amount and computational burdens in\nthe model inference phase through the structural re-parameterization technique.\nExtensive experiments demonstrated the superiorities of RepSNet compared to\nseveral typical benchmark models."
                },
                "authors": [
                    {
                        "name": "Shengchun Xiong"
                    },
                    {
                        "name": "Xiangru Li"
                    },
                    {
                        "name": "Yunpeng Zhong"
                    },
                    {
                        "name": "Wanfen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wanfen Peng"
                },
                "author": "Wanfen Peng",
                "arxiv_doi": "10.1007/s11263-024-02332-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11263-024-02332-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 7 figures, 5 tables",
                "arxiv_journal_ref": "Int J Comput Vis (2025)",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05070v1",
                "updated": "2025-05-08T09:06:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    6,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T09:06:28Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    6,
                    28,
                    3,
                    128,
                    0
                ],
                "title": "Performance Evaluation of Large Language Models in Bangla Consumer\n  Health Query Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of Large Language Models in Bangla Consumer\n  Health Query Summarization"
                },
                "summary": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization."
                },
                "authors": [
                    {
                        "name": "Ajwad Abrar"
                    },
                    {
                        "name": "Farzana Tabassum"
                    },
                    {
                        "name": "Sabbir Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Sabbir Ahmed"
                },
                "author": "Sabbir Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05094v2",
                "updated": "2025-05-08T08:59:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    59,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-12-06T14:50:28Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    14,
                    50,
                    28,
                    4,
                    341,
                    0
                ],
                "title": "Asteroseismic Structure Inversions of Main-Sequence Solar-like\n  Oscillators with Convective Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteroseismic Structure Inversions of Main-Sequence Solar-like\n  Oscillators with Convective Cores"
                },
                "summary": "Asteroseismic inferences of main-sequence solar-like oscillators often rely\non best-fit models. However, these models cannot fully reproduce the observed\nmode frequencies, suggesting that the internal structure of the model does not\nfully match that of the star. Asteroseismic structure inversions provide a way\nto test the interior of our stellar models. Recently, structure inversion\ntechniques were used to study 12 stars with radiative cores. In this work, we\nextend that analysis to 43 main-sequence stars with convective cores observed\nby Kepler to look for differences in the sound speed profiles in the inner 30%\nof the star by radius. For around half of our stars, the structure inversions\nshow that our models reproduce the internal structure of the star, where the\ninversions are sensitive, within the observational uncertainties. For the stars\nwhere our inversions reveal significant differences, we find cases where our\nmodel sound speed is too high and cases where our model sound speed is too low.\nWe use the star with the most significant differences to explore several\nchanges to the physics of our model in an attempt to resolve the inferred\ndifferences. These changes include using a different overshoot prescription and\nincluding the effects of diffusion, gravitational settling, and radiative\nlevitation. We find that the resulting changes to the model structure are too\nsmall to resolve the differences shown in our inversions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteroseismic inferences of main-sequence solar-like oscillators often rely\non best-fit models. However, these models cannot fully reproduce the observed\nmode frequencies, suggesting that the internal structure of the model does not\nfully match that of the star. Asteroseismic structure inversions provide a way\nto test the interior of our stellar models. Recently, structure inversion\ntechniques were used to study 12 stars with radiative cores. In this work, we\nextend that analysis to 43 main-sequence stars with convective cores observed\nby Kepler to look for differences in the sound speed profiles in the inner 30%\nof the star by radius. For around half of our stars, the structure inversions\nshow that our models reproduce the internal structure of the star, where the\ninversions are sensitive, within the observational uncertainties. For the stars\nwhere our inversions reveal significant differences, we find cases where our\nmodel sound speed is too high and cases where our model sound speed is too low.\nWe use the star with the most significant differences to explore several\nchanges to the physics of our model in an attempt to resolve the inferred\ndifferences. These changes include using a different overshoot prescription and\nincluding the effects of diffusion, gravitational settling, and radiative\nlevitation. We find that the resulting changes to the model structure are too\nsmall to resolve the differences shown in our inversions."
                },
                "authors": [
                    {
                        "name": "Lynn Buchele"
                    },
                    {
                        "name": "Earl P. Bellinger"
                    },
                    {
                        "name": "Saskia Hekker"
                    },
                    {
                        "name": "Sarbani Basu"
                    }
                ],
                "author_detail": {
                    "name": "Sarbani Basu"
                },
                "arxiv_affiliation": "Yale University",
                "author": "Sarbani Basu",
                "arxiv_comment": "21 pages, 11 figures, updated to the accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21356v2",
                "updated": "2025-05-08T08:58:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    58,
                    12,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-30T06:30:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    30,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing"
                },
                "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."
                },
                "authors": [
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Zhongjie Duan"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05064v1",
                "updated": "2025-05-08T08:56:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    56,
                    46,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:56:46Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    56,
                    46,
                    3,
                    128,
                    0
                ],
                "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterDrum: Watermarking for Data-centric Unlearning Metric"
                },
                "summary": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax."
                },
                "authors": [
                    {
                        "name": "Xinyang Lu"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "Bui Thi Cam Nhung"
                    },
                    {
                        "name": "Rachael Hwee Ling Sim"
                    },
                    {
                        "name": "Fanyu Wen"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05063v1",
                "updated": "2025-05-08T08:55:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    55,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:55:32Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    55,
                    32,
                    3,
                    128,
                    0
                ],
                "title": "CodeMixBench: Evaluating Large Language Models on Code Generation with\n  Code-Mixed Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMixBench: Evaluating Large Language Models on Code Generation with\n  Code-Mixed Prompts"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings."
                },
                "authors": [
                    {
                        "name": "Manik Sheokand"
                    },
                    {
                        "name": "Parth Sawant"
                    }
                ],
                "author_detail": {
                    "name": "Parth Sawant"
                },
                "author": "Parth Sawant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11055v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11055v3",
                "updated": "2025-05-08T08:51:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    51,
                    19,
                    3,
                    128,
                    0
                ],
                "published": "2024-09-17T10:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    31,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant"
                },
                "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove."
                },
                "authors": [
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Jinse Kwon"
                    },
                    {
                        "name": "Jihun Oh"
                    },
                    {
                        "name": "Yongin Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Yongin Kwon"
                },
                "author": "Yongin Kwon",
                "arxiv_comment": "Accepted in IJCAI 2025, 21 pages, 2 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11055v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11055v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05059v1",
                "updated": "2025-05-08T08:50:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    50,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:50:32Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    50,
                    32,
                    3,
                    128,
                    0
                ],
                "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs\n  with Beam Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs\n  with Beam Search"
                },
                "summary": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques."
                },
                "authors": [
                    {
                        "name": "Sandro Junior Della Rovere"
                    },
                    {
                        "name": "Davide Basso"
                    },
                    {
                        "name": "Luca Bortolussi"
                    },
                    {
                        "name": "Mirjana Videnovic-Misic"
                    },
                    {
                        "name": "Husni Habal"
                    }
                ],
                "author_detail": {
                    "name": "Husni Habal"
                },
                "author": "Husni Habal",
                "arxiv_comment": "Published in Proceedings of the 21st International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD 2025). 4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05057v1",
                "updated": "2025-05-08T08:48:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    48,
                    17,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:48:17Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    48,
                    17,
                    3,
                    128,
                    0
                ],
                "title": "Towards Mitigating API Hallucination in Code Generated by LLMs with\n  Hierarchical Dependency Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mitigating API Hallucination in Code Generated by LLMs with\n  Hierarchical Dependency Aware"
                },
                "summary": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Zhongqi Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuchi Ma"
                },
                "author": "Yuchi Ma",
                "arxiv_comment": "Accepted by FSE 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.05467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05467v1",
                "updated": "2025-05-08T17:57:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    57,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:57:40Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    57,
                    40,
                    3,
                    128,
                    0
                ],
                "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant"
                },
                "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Bo Feng"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Ping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Huang"
                },
                "author": "Ping Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05465v1",
                "updated": "2025-05-08T17:56:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:56:57Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    57,
                    3,
                    128,
                    0
                ],
                "title": "ComPO: Preference Alignment via Comparison Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComPO: Preference Alignment via Comparison Oracles"
                },
                "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}."
                },
                "authors": [
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wotao Yin"
                    },
                    {
                        "name": "Tianyi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Lin"
                },
                "author": "Tianyi Lin",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05464v1",
                "updated": "2025-05-08T17:56:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:56:23Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    23,
                    3,
                    128,
                    0
                ],
                "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging"
                },
                "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."
                },
                "authors": [
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "arxiv_comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05453v1",
                "updated": "2025-05-08T17:44:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    44,
                    45,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:44:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    44,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Conversational Process Model Redesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Process Model Redesign"
                },
                "summary": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria."
                },
                "authors": [
                    {
                        "name": "Nataliia Klievtsova"
                    },
                    {
                        "name": "Timotheus Kampik"
                    },
                    {
                        "name": "Juergen Mangler"
                    },
                    {
                        "name": "Stefanie Rinderle-Ma"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Rinderle-Ma"
                },
                "author": "Stefanie Rinderle-Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05445v1",
                "updated": "2025-05-08T17:36:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    36,
                    36,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:36:36Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    36,
                    36,
                    3,
                    128,
                    0
                ],
                "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations"
                },
                "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18039v2",
                "updated": "2025-05-08T17:34:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    34,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-25T03:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    12,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Nuoqian Xiao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05441v1",
                "updated": "2025-05-08T17:31:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:31:28Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    28,
                    3,
                    128,
                    0
                ],
                "title": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based\n  Interaction in Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based\n  Interaction in Virtual Reality"
                },
                "summary": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments."
                },
                "authors": [
                    {
                        "name": "Xiyun Hu"
                    },
                    {
                        "name": "Dizhi Ma"
                    },
                    {
                        "name": "Fengming He"
                    },
                    {
                        "name": "Zhengzhe Zhu"
                    },
                    {
                        "name": "Shao-Kang Hsia"
                    },
                    {
                        "name": "Chenfei Zhu"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Karthik Ramani"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Ramani"
                },
                "author": "Karthik Ramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05440v1",
                "updated": "2025-05-08T17:31:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:31:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation"
                },
                "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation."
                },
                "authors": [
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Xavier Hu"
                    },
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05427v1",
                "updated": "2025-05-08T17:15:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    15,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:15:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    15,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for\n  High-Quality LLM Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-FineWeb: Efficient Data Filtering and Verification for\n  High-Quality LLM Training Data"
                },
                "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency."
                },
                "authors": [
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Peijun Tang"
                    },
                    {
                        "name": "Hongya Lyu"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "The datasets are available on\n  https://huggingface.co/datasets/openbmb/UltraFineWeb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05423v1",
                "updated": "2025-05-08T17:12:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    12,
                    56,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:12:56Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    12,
                    56,
                    3,
                    128,
                    0
                ],
                "title": "TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering"
                },
                "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Lieve Macken"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01876v2",
                "updated": "2025-05-08T17:10:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    10,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-26T15:12:29Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    12,
                    29,
                    2,
                    57,
                    0
                ],
                "title": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion\n  Models"
                },
                "summary": "Human-in-the-loop (HitL) robot deployment has gained significant attention in\nboth academia and industry as a semi-autonomous paradigm that enables human\noperators to intervene and adjust robot behaviors at deployment time, improving\nsuccess rates. However, continuous human monitoring and intervention can be\nhighly labor-intensive and impractical when deploying a large number of robots.\nTo address this limitation, we propose a method that allows diffusion policies\nto actively seek human assistance only when necessary, reducing reliance on\nconstant human oversight. To achieve this, we leverage the generative process\nof diffusion policies to compute an uncertainty-based metric based on which the\nautonomous agent can decide to request operator assistance at deployment time,\nwithout requiring any operator interaction during training. Additionally, we\nshow that the same method can be used for efficient data collection for\nfine-tuning diffusion policies in order to improve their autonomous\nperformance. Experimental results from simulated and real-world environments\ndemonstrate that our approach enhances policy performance during deployment for\na variety of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-in-the-loop (HitL) robot deployment has gained significant attention in\nboth academia and industry as a semi-autonomous paradigm that enables human\noperators to intervene and adjust robot behaviors at deployment time, improving\nsuccess rates. However, continuous human monitoring and intervention can be\nhighly labor-intensive and impractical when deploying a large number of robots.\nTo address this limitation, we propose a method that allows diffusion policies\nto actively seek human assistance only when necessary, reducing reliance on\nconstant human oversight. To achieve this, we leverage the generative process\nof diffusion policies to compute an uncertainty-based metric based on which the\nautonomous agent can decide to request operator assistance at deployment time,\nwithout requiring any operator interaction during training. Additionally, we\nshow that the same method can be used for efficient data collection for\nfine-tuning diffusion policies in order to improve their autonomous\nperformance. Experimental results from simulated and real-world environments\ndemonstrate that our approach enhances policy performance during deployment for\na variety of scenarios."
                },
                "authors": [
                    {
                        "name": "Zhanpeng He"
                    },
                    {
                        "name": "Yifeng Cao"
                    },
                    {
                        "name": "Matei Ciocarlie"
                    }
                ],
                "author_detail": {
                    "name": "Matei Ciocarlie"
                },
                "author": "Matei Ciocarlie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05505v2",
                "updated": "2025-05-08T16:52:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    52,
                    55,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-07T15:22:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    22,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "Correctness Coverage Evaluation for Medical Multiple-Choice Question\n  Answering Based on the Enhanced Conformal Prediction Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Coverage Evaluation for Medical Multiple-Choice Question\n  Answering Based on the Enhanced Conformal Prediction Framework"
                },
                "summary": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs."
                },
                "authors": [
                    {
                        "name": "Yusong Ke"
                    },
                    {
                        "name": "Hongru Lin"
                    },
                    {
                        "name": "Yuting Ruan"
                    },
                    {
                        "name": "Junya Tang"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "arxiv_comment": "Published by Mathematics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03989v2",
                "updated": "2025-05-08T16:52:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    52,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-06T21:53:44Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    53,
                    44,
                    1,
                    126,
                    0
                ],
                "title": "An alignment safety case sketch based on debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An alignment safety case sketch based on debate"
                },
                "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe."
                },
                "authors": [
                    {
                        "name": "Marie Davidsen Buhl"
                    },
                    {
                        "name": "Jacob Pfau"
                    },
                    {
                        "name": "Benjamin Hilton"
                    },
                    {
                        "name": "Geoffrey Irving"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Irving"
                },
                "author": "Geoffrey Irving",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05406v1",
                "updated": "2025-05-08T16:46:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    46,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:46:24Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    46,
                    24,
                    3,
                    128,
                    0
                ],
                "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than\n  Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than\n  Humans?"
                },
                "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting."
                },
                "authors": [
                    {
                        "name": "Valeria Pastorino"
                    },
                    {
                        "name": "Nafise Sadat Moosavi"
                    }
                ],
                "author_detail": {
                    "name": "Nafise Sadat Moosavi"
                },
                "author": "Nafise Sadat Moosavi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07074v3",
                "updated": "2025-05-08T16:40:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    40,
                    59,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-09T17:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning"
                },
                "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Yichuan Li"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Kyumin Lee"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02107v2",
                "updated": "2025-05-08T16:22:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    22,
                    26,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-02T20:11:54Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    20,
                    11,
                    54,
                    2,
                    92,
                    0
                ],
                "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining"
                },
                "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains."
                },
                "authors": [
                    {
                        "name": "Jeffrey Li"
                    },
                    {
                        "name": "Mohammadreza Armandpour"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Vaishaal Shankar"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    },
                    {
                        "name": "Samy Bengio"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Fartash Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Fartash Faghri"
                },
                "author": "Fartash Faghri",
                "arxiv_comment": "Code available at: https://github.com/apple/ml-tic-lm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05375v1",
                "updated": "2025-05-08T16:09:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:09:40Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    40,
                    3,
                    128,
                    0
                ],
                "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks"
                },
                "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN."
                },
                "authors": [
                    {
                        "name": "Kejie Zhao"
                    },
                    {
                        "name": "Wenjia Hua"
                    },
                    {
                        "name": "Aiersi Tuerhong"
                    },
                    {
                        "name": "Luziwei Leng"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Qinghua Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qinghua Guo"
                },
                "author": "Qinghua Guo",
                "arxiv_comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05374v1",
                "updated": "2025-05-08T16:09:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:09:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "OcularAge: A Comparative Study of Iris and Periocular Images for\n  Pediatric Age Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OcularAge: A Comparative Study of Iris and Periocular Images for\n  Pediatric Age Estimation"
                },
                "summary": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications."
                },
                "authors": [
                    {
                        "name": "Naveenkumar G Venkataswamy"
                    },
                    {
                        "name": "Poorna Ravi"
                    },
                    {
                        "name": "Stephanie Schuckers"
                    },
                    {
                        "name": "Masudul H. Imtiaz"
                    }
                ],
                "author_detail": {
                    "name": "Masudul H. Imtiaz"
                },
                "author": "Masudul H. Imtiaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05366v1",
                "updated": "2025-05-08T16:03:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    3,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T16:03:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    3,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "SDR-RDMA: Software-Defined Reliability Architecture for Planetary Scale\n  RDMA Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDR-RDMA: Software-Defined Reliability Architecture for Planetary Scale\n  RDMA Communication"
                },
                "summary": "RDMA is vital for efficient distributed training across datacenters, but\nmillisecond-scale latencies complicate the design of its reliability layer. We\nshow that depending on long-haul link characteristics, such as drop rate,\ndistance and bandwidth, the widely used Selective Repeat algorithm can be\ninefficient, warranting alternatives like Erasure Coding. To enable such\nalternatives on existing hardware, we propose SDR-RDMA, a software-defined\nreliability stack for RDMA. Its core is a lightweight SDR SDK that extends\nstandard point-to-point RDMA semantics -- fundamental to AI networking stacks\n-- with a receive buffer bitmap. SDR bitmap enables partial message completion\nto let applications implement custom reliability schemes tailored to specific\ndeployments, while preserving zero-copy RDMA benefits. By offloading the SDR\nbackend to NVIDIA's Data Path Accelerator (DPA), we achieve line-rate\nperformance, enabling efficient inter-datacenter communication and advancing\nreliability innovation for intra-datacenter training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDMA is vital for efficient distributed training across datacenters, but\nmillisecond-scale latencies complicate the design of its reliability layer. We\nshow that depending on long-haul link characteristics, such as drop rate,\ndistance and bandwidth, the widely used Selective Repeat algorithm can be\ninefficient, warranting alternatives like Erasure Coding. To enable such\nalternatives on existing hardware, we propose SDR-RDMA, a software-defined\nreliability stack for RDMA. Its core is a lightweight SDR SDK that extends\nstandard point-to-point RDMA semantics -- fundamental to AI networking stacks\n-- with a receive buffer bitmap. SDR bitmap enables partial message completion\nto let applications implement custom reliability schemes tailored to specific\ndeployments, while preserving zero-copy RDMA benefits. By offloading the SDR\nbackend to NVIDIA's Data Path Accelerator (DPA), we achieve line-rate\nperformance, enabling efficient inter-datacenter communication and advancing\nreliability innovation for intra-datacenter training."
                },
                "authors": [
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Siyuan Shen"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Tiancheng Chen"
                    },
                    {
                        "name": "Kenji Nakano"
                    },
                    {
                        "name": "Peter-Jan Gootzen"
                    },
                    {
                        "name": "Salvatore Di Girolamo"
                    },
                    {
                        "name": "Rami Nudelman"
                    },
                    {
                        "name": "Gil Bloch"
                    },
                    {
                        "name": "Sreevatsa Anantharamu"
                    },
                    {
                        "name": "Mahmoud Elhaddad"
                    },
                    {
                        "name": "Jithin Jose"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Scott Moe"
                    },
                    {
                        "name": "Konstantin Taranov"
                    },
                    {
                        "name": "Zhuolong Yu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Nicola Mazzoletti"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05360v1",
                "updated": "2025-05-08T15:53:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    53,
                    34,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:53:34Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    53,
                    34,
                    3,
                    128,
                    0
                ],
                "title": "DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning"
                },
                "summary": "We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD."
                },
                "authors": [
                    {
                        "name": "Wenru Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07944v2",
                "updated": "2025-05-08T15:48:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    48,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-12T07:06:38Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    6,
                    38,
                    2,
                    164,
                    0
                ],
                "title": "Enhancing Differential Testing With LLMs For Testing Deep Learning\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Differential Testing With LLMs For Testing Deep Learning\n  Libraries"
                },
                "summary": "Differential testing offers a promising strategy to alleviate the test oracle\nproblem by comparing the test results between alternative implementations.\nHowever, existing differential testing techniques for deep learning (DL)\nlibraries are limited by the key challenges of finding alternative\nimplementations (called counterparts) for a given API and subsequently\ngenerating diverse test inputs. To address the two challenges, this paper\nintroduces DLLens, an LLM-enhanced differential testing technique for DL\nlibraries. To address the first challenge, DLLens incorporates an LLM-based\ncounterpart synthesis workflow, with the insight that the counterpart of a\ngiven DL library API's computation could be successfully synthesized through\ncertain composition and adaptation of the APIs from another DL library. To\naddress the second challenge, DLLens incorporates a static analysis technique\nthat extracts the path constraints from the implementations of a given API and\nits counterpart to guide diverse test input generation. The extraction is\nfacilitated by LLM's knowledge of the concerned DL library and its upstream\nlibraries.\n  We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our\nevaluation shows that DLLens synthesizes counterparts for 1.84 times as many\nAPIs as those found by state-of-the-art techniques on these libraries.\nMoreover, under the same time budget, DLLens covers 7.23% more branches and\ndetects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly\nsampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 59 are confirmed by developers, including 46\nconfirmed as previously unknown bugs, and 10 of these previously unknown bugs\nhave been fixed in the latest version of TensorFlow and PyTorch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential testing offers a promising strategy to alleviate the test oracle\nproblem by comparing the test results between alternative implementations.\nHowever, existing differential testing techniques for deep learning (DL)\nlibraries are limited by the key challenges of finding alternative\nimplementations (called counterparts) for a given API and subsequently\ngenerating diverse test inputs. To address the two challenges, this paper\nintroduces DLLens, an LLM-enhanced differential testing technique for DL\nlibraries. To address the first challenge, DLLens incorporates an LLM-based\ncounterpart synthesis workflow, with the insight that the counterpart of a\ngiven DL library API's computation could be successfully synthesized through\ncertain composition and adaptation of the APIs from another DL library. To\naddress the second challenge, DLLens incorporates a static analysis technique\nthat extracts the path constraints from the implementations of a given API and\nits counterpart to guide diverse test input generation. The extraction is\nfacilitated by LLM's knowledge of the concerned DL library and its upstream\nlibraries.\n  We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our\nevaluation shows that DLLens synthesizes counterparts for 1.84 times as many\nAPIs as those found by state-of-the-art techniques on these libraries.\nMoreover, under the same time budget, DLLens covers 7.23% more branches and\ndetects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly\nsampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 59 are confirmed by developers, including 46\nconfirmed as previously unknown bugs, and 10 of these previously unknown bugs\nhave been fixed in the latest version of TensorFlow and PyTorch."
                },
                "authors": [
                    {
                        "name": "Meiziniu Li"
                    },
                    {
                        "name": "Dongze Li"
                    },
                    {
                        "name": "Jianmeng Liu"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yongqiang Tian"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "arxiv_comment": "This work has been accepted by ACM TOSEM. Manuscript under final\n  preparation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05351v1",
                "updated": "2025-05-08T15:41:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    41,
                    34,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:41:34Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    41,
                    34,
                    3,
                    128,
                    0
                ],
                "title": "Quantum-Aware Network Planning and Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Aware Network Planning and Integration"
                },
                "summary": "In order to broaden the adoption of highly-demanded quantum functionalities\nsuch as QKD, there is a need for having quantum signals coexist with classical\ntraffic over the same physical medium, typically optical fibers in\nalready-deployed networks. Beyond the experimental point-to-point\ndemonstrations of the past few years, efforts are now underway to integrate QKD\nat the network level: developing interfaces with the software-defined-network\necosystem; but also network planning tools that satisfy physical-layer\ncontraints jointly on the classical and quantum signals. We have found that in\ncertain situations, na\\\"ive network planning prioritizing quantum traffic\ndrastically degrades classical capacity, whereas a quantum-aware wavelength\nassignment heuristic allows coexistence with minimal impact on both capacities.\nMore such techniques will be required to enable widespread deployment of QKD\nand other future quantum functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to broaden the adoption of highly-demanded quantum functionalities\nsuch as QKD, there is a need for having quantum signals coexist with classical\ntraffic over the same physical medium, typically optical fibers in\nalready-deployed networks. Beyond the experimental point-to-point\ndemonstrations of the past few years, efforts are now underway to integrate QKD\nat the network level: developing interfaces with the software-defined-network\necosystem; but also network planning tools that satisfy physical-layer\ncontraints jointly on the classical and quantum signals. We have found that in\ncertain situations, na\\\"ive network planning prioritizing quantum traffic\ndrastically degrades classical capacity, whereas a quantum-aware wavelength\nassignment heuristic allows coexistence with minimal impact on both capacities.\nMore such techniques will be required to enable widespread deployment of QKD\nand other future quantum functionalities."
                },
                "authors": [
                    {
                        "name": "Cédric Ware"
                    },
                    {
                        "name": "Mounia Lourdiane"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lourdiane"
                },
                "author": "Mounia Lourdiane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05343v1",
                "updated": "2025-05-08T15:32:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    32,
                    4,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:32:04Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    32,
                    4,
                    3,
                    128,
                    0
                ],
                "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization"
                },
                "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Sooyoung Park"
                    },
                    {
                        "name": "Arda Senocak"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "arxiv_comment": "Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is\n  available at https://github.com/swimmiing/ACL-SSL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05335v1",
                "updated": "2025-05-08T15:27:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    27,
                    43,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:27:43Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    27,
                    43,
                    3,
                    128,
                    0
                ],
                "title": "FLAM: Frame-Wise Language-Audio Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAM: Frame-Wise Language-Audio Modeling"
                },
                "summary": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yusong Wu"
                    },
                    {
                        "name": "Christos Tsirigotis"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Cheng-Zhi Anna Huang"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Oriol Nieto"
                    },
                    {
                        "name": "Prem Seetharaman"
                    },
                    {
                        "name": "Justin Salamon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Salamon"
                },
                "author": "Justin Salamon",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05327v1",
                "updated": "2025-05-08T15:17:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    17,
                    37,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:17:37Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    17,
                    37,
                    3,
                    128,
                    0
                ],
                "title": "ICon: In-Context Contribution for Automatic Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICon: In-Context Contribution for Automatic Data Selection"
                },
                "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones."
                },
                "authors": [
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Linli Yao"
                    },
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05315v1",
                "updated": "2025-05-08T15:01:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    1,
                    6,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T15:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    1,
                    6,
                    3,
                    128,
                    0
                ],
                "title": "Scalable Chain of Thoughts via Elastic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Chain of Thoughts via Elastic Reasoning"
                },
                "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05298v1",
                "updated": "2025-05-08T14:41:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    41,
                    7,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:41:07Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    41,
                    7,
                    3,
                    128,
                    0
                ],
                "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with\n  Us by Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Reasonable Parrots: Why Large Language Models Should Argue with\n  Us by Design"
                },
                "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation."
                },
                "authors": [
                    {
                        "name": "Elena Musi"
                    },
                    {
                        "name": "Nadin Kokciyan"
                    },
                    {
                        "name": "Khalid Al-Khatib"
                    },
                    {
                        "name": "Davide Ceolin"
                    },
                    {
                        "name": "Emmanuelle Dietz"
                    },
                    {
                        "name": "Klara Gutekunst"
                    },
                    {
                        "name": "Annette Hautli-Janisz"
                    },
                    {
                        "name": "Cristian Manuel Santibañez Yañez"
                    },
                    {
                        "name": "Jodi Schneider"
                    },
                    {
                        "name": "Jonas Scholz"
                    },
                    {
                        "name": "Cor Steging"
                    },
                    {
                        "name": "Jacky Visser"
                    },
                    {
                        "name": "Henning Wachsmuth"
                    }
                ],
                "author_detail": {
                    "name": "Henning Wachsmuth"
                },
                "author": "Henning Wachsmuth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08478v2",
                "updated": "2025-05-08T14:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    34,
                    54,
                    3,
                    128,
                    0
                ],
                "published": "2024-12-11T15:45:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning"
                },
                "summary": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area."
                },
                "authors": [
                    {
                        "name": "Patrick Sattler"
                    },
                    {
                        "name": "Johannes Zirngibl"
                    },
                    {
                        "name": "Fahad Hilal"
                    },
                    {
                        "name": "Oliver Gasser"
                    },
                    {
                        "name": "Kevin Vermeulen"
                    },
                    {
                        "name": "Georg Carle"
                    },
                    {
                        "name": "Mattijs Jonker"
                    }
                ],
                "author_detail": {
                    "name": "Mattijs Jonker"
                },
                "author": "Mattijs Jonker",
                "arxiv_doi": "10.1145/3730977",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730977",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05295v1",
                "updated": "2025-05-08T14:34:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    34,
                    44,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:34:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    34,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Performance Estimation in Binary Classification Using Calibrated\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Estimation in Binary Classification Using Calibrated\n  Confidence"
                },
                "summary": "Model monitoring is a critical component of the machine learning lifecycle,\nsafeguarding against undetected drops in the model's performance after\ndeployment. Traditionally, performance monitoring has required access to ground\ntruth labels, which are not always readily available. This can result in\nunacceptable latency or render performance monitoring altogether impossible.\nRecently, methods designed to estimate the accuracy of classifier models\nwithout access to labels have shown promising results. However, there are\nvarious other metrics that might be more suitable for assessing model\nperformance in many cases. Until now, none of these important metrics has\nreceived similar interest from the scientific community. In this work, we\naddress this gap by presenting CBPE, a novel method that can estimate any\nbinary classification metric defined using the confusion matrix. In particular,\nwe choose four metrics from this large family: accuracy, precision, recall, and\nF$_1$, to demonstrate our method. CBPE treats the elements of the confusion\nmatrix as random variables and leverages calibrated confidence scores of the\nmodel to estimate their distributions. The desired metric is then also treated\nas a random variable, whose full probability distribution can be derived from\nthe estimated confusion matrix. CBPE is shown to produce estimates that come\nwith strong theoretical guarantees and valid confidence intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model monitoring is a critical component of the machine learning lifecycle,\nsafeguarding against undetected drops in the model's performance after\ndeployment. Traditionally, performance monitoring has required access to ground\ntruth labels, which are not always readily available. This can result in\nunacceptable latency or render performance monitoring altogether impossible.\nRecently, methods designed to estimate the accuracy of classifier models\nwithout access to labels have shown promising results. However, there are\nvarious other metrics that might be more suitable for assessing model\nperformance in many cases. Until now, none of these important metrics has\nreceived similar interest from the scientific community. In this work, we\naddress this gap by presenting CBPE, a novel method that can estimate any\nbinary classification metric defined using the confusion matrix. In particular,\nwe choose four metrics from this large family: accuracy, precision, recall, and\nF$_1$, to demonstrate our method. CBPE treats the elements of the confusion\nmatrix as random variables and leverages calibrated confidence scores of the\nmodel to estimate their distributions. The desired metric is then also treated\nas a random variable, whose full probability distribution can be derived from\nthe estimated confusion matrix. CBPE is shown to produce estimates that come\nwith strong theoretical guarantees and valid confidence intervals."
                },
                "authors": [
                    {
                        "name": "Juhani Kivimäki"
                    },
                    {
                        "name": "Jakub Białek"
                    },
                    {
                        "name": "Wojtek Kuberski"
                    },
                    {
                        "name": "Jukka K. Nurminen"
                    }
                ],
                "author_detail": {
                    "name": "Jukka K. Nurminen"
                },
                "author": "Jukka K. Nurminen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05288v1",
                "updated": "2025-05-08T14:29:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:29:11Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    11,
                    3,
                    128,
                    0
                ],
                "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes"
                },
                "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdelreheem"
                    },
                    {
                        "name": "Filippo Aleotti"
                    },
                    {
                        "name": "Jamie Watson"
                    },
                    {
                        "name": "Zawar Qureshi"
                    },
                    {
                        "name": "Abdelrahman Eldesokey"
                    },
                    {
                        "name": "Peter Wonka"
                    },
                    {
                        "name": "Gabriel Brostow"
                    },
                    {
                        "name": "Sara Vicente"
                    },
                    {
                        "name": "Guillermo Garcia-Hernando"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Garcia-Hernando"
                },
                "author": "Guillermo Garcia-Hernando",
                "arxiv_comment": "Tech report. Project page: https://nianticlabs.github.io/placeit3d/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05287v1",
                "updated": "2025-05-08T14:29:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:29:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous\n  Bimanual Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphologically Symmetric Reinforcement Learning for Ambidextrous\n  Bimanual Manipulation"
                },
                "summary": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Zechu Li"
                    },
                    {
                        "name": "Yufeng Jin"
                    },
                    {
                        "name": "Daniel Ordonez Apraez"
                    },
                    {
                        "name": "Claudio Semini"
                    },
                    {
                        "name": "Puze Liu"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Chalvatzaki"
                },
                "author": "Georgia Chalvatzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05286v1",
                "updated": "2025-05-08T14:28:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    28,
                    47,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:28:47Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    28,
                    47,
                    3,
                    128,
                    0
                ],
                "title": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow"
                },
                "summary": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow."
                },
                "authors": [
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05283v1",
                "updated": "2025-05-08T14:27:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    27,
                    45,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T14:27:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    27,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  CodeLLMs and Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  CodeLLMs and Agents"
                },
                "summary": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Kaixin Wang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Bin Shi"
                },
                "author": "Bin Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11039v2",
                "updated": "2025-05-08T14:24:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    24,
                    13,
                    3,
                    128,
                    0
                ],
                "published": "2024-12-15T03:35:00Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    35,
                    0,
                    6,
                    350,
                    0
                ],
                "title": "AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway\n  Analysis"
                },
                "summary": "Accurate anatomical labeling and analysis of the pulmonary structure and its\nsurrounding anatomy from thoracic CT is getting increasingly important for\nunderstanding the etilogy of abnormalities or supporting targetted therapy and\nearly interventions. Whilst lung and airway cell atlases have been attempted,\nthere is a lack of fine-grained morphological atlases that are clinically\ndeployable. In this work, we introduce AirMorph, a robust, end-to-end deep\nlearning pipeline enabling fully automatic and comprehensive airway anatomical\nlabeling at lobar, segmental, and subsegmental resolutions that can be used to\ncreate digital atlases of the lung. Evaluated across large-scale multi-center\ndatasets comprising diverse pulmonary conditions, the AirMorph consistently\noutperformed existing segmentation and labeling methods in terms of accuracy,\ntopological consistency, and completeness. To simplify clinical interpretation,\nwe further introduce a compact anatomical signature quantifying critical\nmorphological airway features, including stenosis, ectasia, tortuosity,\ndivergence, length, and complexity. When applied to various pulmonary diseases\nsuch as pulmonary fibrosis, emphysema, atelectasis, consolidation, and\nreticular opacities, it demonstrates strong discriminative power, revealing\ndisease-specific morphological patterns with high interpretability and\nexplainability. Additionally, AirMorph supports efficient automated branching\npattern analysis, potentially enhancing bronchoscopic navigation planning and\nprocedural safety, offering a valuable clinical tool for improved diagnosis,\ntargeted treatment, and personalized patient care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate anatomical labeling and analysis of the pulmonary structure and its\nsurrounding anatomy from thoracic CT is getting increasingly important for\nunderstanding the etilogy of abnormalities or supporting targetted therapy and\nearly interventions. Whilst lung and airway cell atlases have been attempted,\nthere is a lack of fine-grained morphological atlases that are clinically\ndeployable. In this work, we introduce AirMorph, a robust, end-to-end deep\nlearning pipeline enabling fully automatic and comprehensive airway anatomical\nlabeling at lobar, segmental, and subsegmental resolutions that can be used to\ncreate digital atlases of the lung. Evaluated across large-scale multi-center\ndatasets comprising diverse pulmonary conditions, the AirMorph consistently\noutperformed existing segmentation and labeling methods in terms of accuracy,\ntopological consistency, and completeness. To simplify clinical interpretation,\nwe further introduce a compact anatomical signature quantifying critical\nmorphological airway features, including stenosis, ectasia, tortuosity,\ndivergence, length, and complexity. When applied to various pulmonary diseases\nsuch as pulmonary fibrosis, emphysema, atelectasis, consolidation, and\nreticular opacities, it demonstrates strong discriminative power, revealing\ndisease-specific morphological patterns with high interpretability and\nexplainability. Additionally, AirMorph supports efficient automated branching\npattern analysis, potentially enhancing bronchoscopic navigation planning and\nprocedural safety, offering a valuable clinical tool for improved diagnosis,\ntargeted treatment, and personalized patient care."
                },
                "authors": [
                    {
                        "name": "Minghui Zhang"
                    },
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Fangfang Xie"
                    },
                    {
                        "name": "Yaoyu Liu"
                    },
                    {
                        "name": "Hanxiao Zhang"
                    },
                    {
                        "name": "Junyang Wu"
                    },
                    {
                        "name": "Chunxi Zhang"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jiayuan Sun"
                    },
                    {
                        "name": "Guang-Zhong Yang"
                    },
                    {
                        "name": "Yun Gu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Gu"
                },
                "author": "Yun Gu",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04728v2",
                "updated": "2025-05-08T13:42:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    42,
                    18,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-07T07:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "title": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models"
                },
                "summary": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks."
                },
                "authors": [
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Yuhuan Yuan"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Fuxiang Frank Xia"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ge Lin"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Accepted by TMLR2025 (32 pages, 6 figures)",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15236v2",
                "updated": "2025-05-08T13:35:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    35,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-20T00:00:56Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    0,
                    0,
                    56,
                    6,
                    294,
                    0
                ],
                "title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nadvancing natural language understanding and generation, enabling applications\nacross fields beyond healthcare, software engineering, and conversational\nsystems. Despite these advancements in the past few years, LLMs have shown\nconsiderable vulnerabilities, particularly to prompt injection and jailbreaking\nattacks. This review analyzes the state of research on these vulnerabilities\nand presents available defense strategies. We roughly categorize attack\napproaches into prompt-based, model-based, multimodal, and multilingual,\ncovering techniques such as adversarial prompting, backdoor injections, and\ncross-modality exploits. We also review various defense mechanisms, including\nprompt filtering, transformation, alignment techniques, multi-agent defenses,\nand self-regulation, evaluating their strengths and shortcomings. We also\ndiscuss key metrics and benchmarks used to assess LLM safety and robustness,\nnoting challenges like the quantification of attack success in interactive\ncontexts and biases in existing datasets. Identifying current research gaps, we\nsuggest future directions for resilient alignment strategies, advanced defenses\nagainst evolving attacks, automation of jailbreak detection, and consideration\nof ethical and societal impacts. This review emphasizes the need for continued\nresearch and cooperation within the AI community to enhance LLM security and\nensure their safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed artificial intelligence by\nadvancing natural language understanding and generation, enabling applications\nacross fields beyond healthcare, software engineering, and conversational\nsystems. Despite these advancements in the past few years, LLMs have shown\nconsiderable vulnerabilities, particularly to prompt injection and jailbreaking\nattacks. This review analyzes the state of research on these vulnerabilities\nand presents available defense strategies. We roughly categorize attack\napproaches into prompt-based, model-based, multimodal, and multilingual,\ncovering techniques such as adversarial prompting, backdoor injections, and\ncross-modality exploits. We also review various defense mechanisms, including\nprompt filtering, transformation, alignment techniques, multi-agent defenses,\nand self-regulation, evaluating their strengths and shortcomings. We also\ndiscuss key metrics and benchmarks used to assess LLM safety and robustness,\nnoting challenges like the quantification of attack success in interactive\ncontexts and biases in existing datasets. Identifying current research gaps, we\nsuggest future directions for resilient alignment strategies, advanced defenses\nagainst evolving attacks, automation of jailbreak detection, and consideration\nof ethical and societal impacts. This review emphasizes the need for continued\nresearch and cooperation within the AI community to enhance LLM security and\nensure their safe deployment."
                },
                "authors": [
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Lawrence K. Q. Yan"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Caitlyn Heqi Yin"
                },
                "author": "Caitlyn Heqi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05237v1",
                "updated": "2025-05-08T13:32:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    32,
                    9,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:32:09Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    32,
                    9,
                    3,
                    128,
                    0
                ],
                "title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\n  Learning"
                },
                "summary": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain"
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Hengrui Gu"
                    },
                    {
                        "name": "Hangting Ye"
                    },
                    {
                        "name": "Yiwei Dai"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05231v1",
                "updated": "2025-05-08T13:26:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    26,
                    9,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:26:09Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    26,
                    9,
                    3,
                    128,
                    0
                ],
                "title": "Adaptive Biased User Scheduling for Heterogeneous Wireless Federate\n  Learning Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Biased User Scheduling for Heterogeneous Wireless Federate\n  Learning Network"
                },
                "summary": "Federated Learning (FL) has revolutionized collaborative model training in\ndistributed networks, prioritizing data privacy and communication efficiency.\nThis paper investigates efficient deployment of FL in wireless heterogeneous\nnetworks, focusing on strategies to accelerate convergence despite stragglers.\nThe primary objective is to minimize long-term convergence wall-clock time\nthrough optimized user scheduling and resource allocation. While stragglers may\nintroduce delays in a single round, their inclusion can expedite subsequent\nrounds, particularly when they possess critical information. Moreover,\nbalancing single-round duration with the number of cumulative rounds,\ncompounded by dynamic training and transmission conditions, necessitates a\nnovel approach beyond conventional optimization solutions. To tackle these\nchallenges, convergence analysis with respect to adaptive and biased scheduling\nis derived. Then, by factoring in real-time system and statistical information,\nincluding diverse energy constraints and users' energy harvesting capabilities,\na deep reinforcement learning approach, empowered by proximal policy\noptimization, is employed to adaptively select user sets. For the scheduled\nusers, Lagrangian decomposition is applied to optimize local resource\nutilization, further enhancing system efficiency. Simulation results validate\nthe effectiveness and robustness of the proposed framework for various FL\ntasks, demonstrating reduced task time compared to existing benchmarks under\nvarious settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has revolutionized collaborative model training in\ndistributed networks, prioritizing data privacy and communication efficiency.\nThis paper investigates efficient deployment of FL in wireless heterogeneous\nnetworks, focusing on strategies to accelerate convergence despite stragglers.\nThe primary objective is to minimize long-term convergence wall-clock time\nthrough optimized user scheduling and resource allocation. While stragglers may\nintroduce delays in a single round, their inclusion can expedite subsequent\nrounds, particularly when they possess critical information. Moreover,\nbalancing single-round duration with the number of cumulative rounds,\ncompounded by dynamic training and transmission conditions, necessitates a\nnovel approach beyond conventional optimization solutions. To tackle these\nchallenges, convergence analysis with respect to adaptive and biased scheduling\nis derived. Then, by factoring in real-time system and statistical information,\nincluding diverse energy constraints and users' energy harvesting capabilities,\na deep reinforcement learning approach, empowered by proximal policy\noptimization, is employed to adaptively select user sets. For the scheduled\nusers, Lagrangian decomposition is applied to optimize local resource\nutilization, further enhancing system efficiency. Simulation results validate\nthe effectiveness and robustness of the proposed framework for various FL\ntasks, demonstrating reduced task time compared to existing benchmarks under\nvarious settings."
                },
                "authors": [
                    {
                        "name": "Changxiang Wu"
                    },
                    {
                        "name": "Yijing Ren"
                    },
                    {
                        "name": "Daniel K. C. So"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05225v1",
                "updated": "2025-05-08T13:16:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    16,
                    49,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:16:49Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    16,
                    49,
                    3,
                    128,
                    0
                ],
                "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation"
                },
                "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Wailing Ng"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jason Zhang"
                },
                "author": "Chen Jason Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00831v3",
                "updated": "2025-05-08T13:12:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    12,
                    24,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-01T19:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    44,
                    36,
                    3,
                    121,
                    0
                ],
                "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation"
                },
                "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."
                },
                "authors": [
                    {
                        "name": "Quang P. M. Pham"
                    },
                    {
                        "name": "Khoi T. N. Nguyen"
                    },
                    {
                        "name": "Nhi H. Doan"
                    },
                    {
                        "name": "Cuong A. Pham"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Dezhen Song"
                    }
                ],
                "author_detail": {
                    "name": "Dezhen Song"
                },
                "author": "Dezhen Song",
                "arxiv_comment": "Paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05215v1",
                "updated": "2025-05-08T13:09:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    9,
                    34,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:09:34Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    9,
                    34,
                    3,
                    128,
                    0
                ],
                "title": "Diffusion Model Quantization: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Model Quantization: A Review"
                },
                "summary": "Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization."
                },
                "authors": [
                    {
                        "name": "Qian Zeng"
                    },
                    {
                        "name": "Chenggong Hu"
                    },
                    {
                        "name": "Mingli Song"
                    },
                    {
                        "name": "Jie Song"
                    }
                ],
                "author_detail": {
                    "name": "Jie Song"
                },
                "author": "Jie Song",
                "arxiv_comment": "40 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05196v1",
                "updated": "2025-05-08T12:53:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    53,
                    42,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:53:42Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    53,
                    42,
                    3,
                    128,
                    0
                ],
                "title": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based\n  Retrieval-Augmented Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based\n  Retrieval-Augmented Recommender Systems"
                },
                "summary": "We present a systematic study of provider-side data poisoning in\nretrieval-augmented recommender systems (RAG-based). By modifying only a small\nfraction of tokens within item descriptions -- for instance, adding emotional\nkeywords or borrowing phrases from semantically related items -- an attacker\ncan significantly promote or demote targeted items. We formalize these attacks\nunder token-edit and semantic-similarity constraints, and we examine their\neffectiveness in both promotion (long-tail items) and demotion (short-head\nitems) scenarios. Our experiments on MovieLens, using two large language model\n(LLM) retrieval modules, show that even subtle attacks shift final rankings and\nitem exposures while eluding naive detection. The results underscore the\nvulnerability of RAG-based pipelines to small-scale metadata rewrites and\nemphasize the need for robust textual consistency checks and provenance\ntracking to thwart stealthy provider-side poisoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of provider-side data poisoning in\nretrieval-augmented recommender systems (RAG-based). By modifying only a small\nfraction of tokens within item descriptions -- for instance, adding emotional\nkeywords or borrowing phrases from semantically related items -- an attacker\ncan significantly promote or demote targeted items. We formalize these attacks\nunder token-edit and semantic-similarity constraints, and we examine their\neffectiveness in both promotion (long-tail items) and demotion (short-head\nitems) scenarios. Our experiments on MovieLens, using two large language model\n(LLM) retrieval modules, show that even subtle attacks shift final rankings and\nitem exposures while eluding naive detection. The results underscore the\nvulnerability of RAG-based pipelines to small-scale metadata rewrites and\nemphasize the need for robust textual consistency checks and provenance\ntracking to thwart stealthy provider-side poisoning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Nazary"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    },
                    {
                        "name": "Eugenio Di Sciascio"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Di Sciascio"
                },
                "author": "Eugenio Di Sciascio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23810v2",
                "updated": "2025-05-08T12:42:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    42,
                    12,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-31T07:44:14Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    44,
                    14,
                    0,
                    90,
                    0
                ],
                "title": "Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization"
                },
                "summary": "Radio-based localization in dynamic environments, such as urban and vehicular\nsettings, requires systems that efficiently adapt to varying signal conditions\nand environmental changes. Factors like multipath interference and obstructions\nintroduce different levels of complexity that affect the accuracy of the\nlocalization. Although generalized models offer broad applicability, they often\nstruggle to capture the nuances of specific environments, leading to suboptimal\nperformance in real-world deployments. In contrast, specialized models can be\ntailored to particular conditions, enabling more precise localization by\neffectively handling domain-specific variations, which also results in reduced\nexecution time and smaller model size. However, deploying multiple specialized\nmodels requires an efficient mechanism to select the most appropriate one for a\ngiven scenario. In this work, we develop an adaptive localization framework\nthat combines shallow attention-based models with a router/switching mechanism\nbased on a single-layer perceptron. This enables seamless transitions between\nspecialized localization models optimized for different conditions, balancing\naccuracy and computational complexity. We design three low-complex models\ntailored for distinct scenarios, and a router that dynamically selects the most\nsuitable model based on real-time input characteristics. The proposed framework\nis validated using real-world vehicle localization data collected from a\nmassive MIMO base station and compared to more general models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio-based localization in dynamic environments, such as urban and vehicular\nsettings, requires systems that efficiently adapt to varying signal conditions\nand environmental changes. Factors like multipath interference and obstructions\nintroduce different levels of complexity that affect the accuracy of the\nlocalization. Although generalized models offer broad applicability, they often\nstruggle to capture the nuances of specific environments, leading to suboptimal\nperformance in real-world deployments. In contrast, specialized models can be\ntailored to particular conditions, enabling more precise localization by\neffectively handling domain-specific variations, which also results in reduced\nexecution time and smaller model size. However, deploying multiple specialized\nmodels requires an efficient mechanism to select the most appropriate one for a\ngiven scenario. In this work, we develop an adaptive localization framework\nthat combines shallow attention-based models with a router/switching mechanism\nbased on a single-layer perceptron. This enables seamless transitions between\nspecialized localization models optimized for different conditions, balancing\naccuracy and computational complexity. We design three low-complex models\ntailored for distinct scenarios, and a router that dynamically selects the most\nsuitable model based on real-time input characteristics. The proposed framework\nis validated using real-world vehicle localization data collected from a\nmassive MIMO base station and compared to more general models."
                },
                "authors": [
                    {
                        "name": "Ilayda Yaman"
                    },
                    {
                        "name": "Guoda Tian"
                    },
                    {
                        "name": "Dino Pjanic"
                    },
                    {
                        "name": "Fredrik Tufvesson"
                    },
                    {
                        "name": "Ove Edfors"
                    },
                    {
                        "name": "Zhengya Zhang"
                    },
                    {
                        "name": "Liang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Liu"
                },
                "author": "Liang Liu",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05190v1",
                "updated": "2025-05-08T12:39:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    39,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:39:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    39,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "Revealing Weaknesses in Text Watermarking Through Self-Information\n  Rewrite Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Weaknesses in Text Watermarking Through Self-Information\n  Rewrite Attacks"
                },
                "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."
                },
                "authors": [
                    {
                        "name": "Yixin Cheng"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Yangming Li"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "arxiv_comment": "ICML 2025 Accpeted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05189v1",
                "updated": "2025-05-08T12:37:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    37,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:37:51Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    37,
                    51,
                    3,
                    128,
                    0
                ],
                "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models"
                },
                "summary": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}."
                },
                "authors": [
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jianchen Hu"
                    },
                    {
                        "name": "Meng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Zhang"
                },
                "author": "Meng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05177v1",
                "updated": "2025-05-08T12:28:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    28,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:28:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    28,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "MARK: Memory Augmented Refinement of Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARK: Memory Augmented Refinement of Knowledge"
                },
                "summary": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time."
                },
                "authors": [
                    {
                        "name": "Anish Ganguli"
                    },
                    {
                        "name": "Prabal Deb"
                    },
                    {
                        "name": "Debleena Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Debleena Banerjee"
                },
                "author": "Debleena Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20170v2",
                "updated": "2025-05-08T12:17:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    17,
                    22,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-27T15:07:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    7,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Re-evaluating Open-ended Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Open-ended Evaluation of Large Language Models"
                },
                "summary": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development."
                },
                "authors": [
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Ian Gemp"
                    },
                    {
                        "name": "Luke Marris"
                    },
                    {
                        "name": "Georgios Piliouras"
                    },
                    {
                        "name": "Nicolas Heess"
                    },
                    {
                        "name": "Marc Lanctot"
                    }
                ],
                "author_detail": {
                    "name": "Marc Lanctot"
                },
                "author": "Marc Lanctot",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05170v1",
                "updated": "2025-05-08T12:13:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    13,
                    16,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T12:13:16Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    12,
                    13,
                    16,
                    3,
                    128,
                    0
                ],
                "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dukawalla: Voice Interfaces for Small Businesses in Africa"
                },
                "summary": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights"
                },
                "authors": [
                    {
                        "name": "Elizabeth Ankrah"
                    },
                    {
                        "name": "Stephanie Nyairo"
                    },
                    {
                        "name": "Mercy Muchai"
                    },
                    {
                        "name": "Kagonya Awori"
                    },
                    {
                        "name": "Millicent Ochieng"
                    },
                    {
                        "name": "Mark Kariuki"
                    },
                    {
                        "name": "Jacki O'Neill"
                    }
                ],
                "author_detail": {
                    "name": "Jacki O'Neill"
                },
                "author": "Jacki O'Neill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15169v2",
                "updated": "2025-05-08T11:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    58,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-19T12:51:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    51,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Open-Source Large Language Models on Healthcare Text\n  Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Open-Source Large Language Models on Healthcare Text\n  Classification Tasks"
                },
                "summary": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts."
                },
                "authors": [
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Abeed Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Abeed Sarker"
                },
                "author": "Abeed Sarker",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05155v1",
                "updated": "2025-05-08T11:51:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    51,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:51:23Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    51,
                    23,
                    3,
                    128,
                    0
                ],
                "title": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data\n  Preparation via Federated Learning"
                },
                "summary": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Zhihao Zeng"
                    },
                    {
                        "name": "Ziquan Fang"
                    },
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v4",
                "updated": "2025-05-08T11:40:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    40,
                    1,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems"
                },
                "summary": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!"
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18635v2",
                "updated": "2025-05-08T10:58:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    58,
                    9,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-25T20:52:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    20,
                    52,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for\n  LLM and RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for\n  LLM and RAG Systems"
                },
                "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives."
                },
                "authors": [
                    {
                        "name": "Matthew Barker"
                    },
                    {
                        "name": "Andrew Bell"
                    },
                    {
                        "name": "Evan Thomas"
                    },
                    {
                        "name": "James Carr"
                    },
                    {
                        "name": "Thomas Andrews"
                    },
                    {
                        "name": "Umang Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Umang Bhatt"
                },
                "author": "Umang Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20, 68Q32, 90C29, 62P30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; G.1.6; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00654v3",
                "updated": "2025-05-08T10:52:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    52,
                    25,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-01T16:55:44Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    55,
                    44,
                    3,
                    121,
                    0
                ],
                "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Understanding: an Inherent Ambiguity Barrier"
                },
                "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."
                },
                "authors": [
                    {
                        "name": "Daniel N. Nissani"
                    }
                ],
                "author_detail": {
                    "name": "Daniel N. Nissani"
                },
                "arxiv_affiliation": "Nissensohn",
                "author": "Daniel N. Nissani",
                "arxiv_comment": "submitted to NEURAL COMPUTATION",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05122v1",
                "updated": "2025-05-08T10:51:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    51,
                    13,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:51:13Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    51,
                    13,
                    3,
                    128,
                    0
                ],
                "title": "Text2Cypher: Data Pruning using Hard Example Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher: Data Pruning using Hard Example Selection"
                },
                "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    }
                ],
                "author_detail": {
                    "name": "Makbule Gulcin Ozsoy"
                },
                "author": "Makbule Gulcin Ozsoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05111v1",
                "updated": "2025-05-08T10:24:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    24,
                    44,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:24:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    24,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Unveiling Language-Specific Features in Large Language Models via Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Language-Specific Features in Large Language Models via Sparse\n  Autoencoders"
                },
                "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs."
                },
                "authors": [
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yidan Zhang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14464v2",
                "updated": "2025-05-08T10:20:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    20,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-18T13:48:01Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    48,
                    1,
                    4,
                    292,
                    0
                ],
                "title": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning"
                },
                "summary": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Cecilia Mascolo"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "arxiv_comment": "Accepted at AHLI CHIL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05108v1",
                "updated": "2025-05-08T10:13:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    13,
                    53,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:13:53Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    13,
                    53,
                    3,
                    128,
                    0
                ],
                "title": "Multi-agent Embodied AI: Advances and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent Embodied AI: Advances and Future Directions"
                },
                "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field."
                },
                "authors": [
                    {
                        "name": "Zhaohan Feng"
                    },
                    {
                        "name": "Ruiqi Xue"
                    },
                    {
                        "name": "Lei Yuan"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Meiqin Liu"
                    },
                    {
                        "name": "Bingzhao Gao"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05103v1",
                "updated": "2025-05-08T10:04:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    4,
                    41,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T10:04:41Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    10,
                    4,
                    41,
                    3,
                    128,
                    0
                ],
                "title": "A Weighted Byzantine Fault Tolerance Consensus Driven Trusted Multiple\n  Large Language Models Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Weighted Byzantine Fault Tolerance Consensus Driven Trusted Multiple\n  Large Language Models Network"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of applications. However, individual LLMs often produce inconsistent,\nbiased, or hallucinated outputs due to limitations in their training corpora\nand model architectures. Recently, collaborative frameworks such as the\nMulti-LLM Network (MultiLLMN) have been introduced, enabling multiple LLMs to\ninteract and jointly respond to user queries. Nevertheless, MultiLLMN\narchitectures raise critical concerns regarding the reliability and security of\nthe generated content, particularly in open environments where malicious or\ncompromised LLMs may be present. Moreover, reliance on centralized coordination\nundermines system efficiency and introduces single points of failure. In this\npaper, we propose a novel Trusted MultiLLMN framework, driven by a Weighted\nByzantine Fault Tolerance (WBFT) blockchain consensus mechanism, to ensure the\nreliability, security, and efficiency of multi-LLM collaboration. In WBFT,\nvoting weights are adaptively assigned to each LLM based on its response\nquality and trustworthiness, incentivizing reliable behavior, and reducing the\nimpact of malicious nodes. Extensive simulations demonstrate that WBFT\nsignificantly improves both consensus security and efficiency compared to\nclassical and modern consensus mechanisms, particularly under wireless network\nconditions. Furthermore, our evaluations reveal that Trusted MultiLLMN\nsupported by WBFT can deliver higher-quality and more credible responses than\nboth single LLMs and conventional MultiLLMNs, thereby providing a promising\npath toward building robust, decentralized AI collaboration networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of applications. However, individual LLMs often produce inconsistent,\nbiased, or hallucinated outputs due to limitations in their training corpora\nand model architectures. Recently, collaborative frameworks such as the\nMulti-LLM Network (MultiLLMN) have been introduced, enabling multiple LLMs to\ninteract and jointly respond to user queries. Nevertheless, MultiLLMN\narchitectures raise critical concerns regarding the reliability and security of\nthe generated content, particularly in open environments where malicious or\ncompromised LLMs may be present. Moreover, reliance on centralized coordination\nundermines system efficiency and introduces single points of failure. In this\npaper, we propose a novel Trusted MultiLLMN framework, driven by a Weighted\nByzantine Fault Tolerance (WBFT) blockchain consensus mechanism, to ensure the\nreliability, security, and efficiency of multi-LLM collaboration. In WBFT,\nvoting weights are adaptively assigned to each LLM based on its response\nquality and trustworthiness, incentivizing reliable behavior, and reducing the\nimpact of malicious nodes. Extensive simulations demonstrate that WBFT\nsignificantly improves both consensus security and efficiency compared to\nclassical and modern consensus mechanisms, particularly under wireless network\nconditions. Furthermore, our evaluations reveal that Trusted MultiLLMN\nsupported by WBFT can deliver higher-quality and more credible responses than\nboth single LLMs and conventional MultiLLMNs, thereby providing a promising\npath toward building robust, decentralized AI collaboration networks."
                },
                "authors": [
                    {
                        "name": "Haoxiang Luo"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Schahram Dustdar"
                    }
                ],
                "author_detail": {
                    "name": "Schahram Dustdar"
                },
                "author": "Schahram Dustdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05098v1",
                "updated": "2025-05-08T09:52:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    52,
                    55,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T09:52:55Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    52,
                    55,
                    3,
                    128,
                    0
                ],
                "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Driver: Explainable Autonomous Driving with Vision-Language Models"
                },
                "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Binxiong Zheng"
                    },
                    {
                        "name": "Yufeng Hu"
                    },
                    {
                        "name": "Yingzhan Lin"
                    },
                    {
                        "name": "Zengfeng Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Zengfeng Zeng"
                },
                "author": "Zengfeng Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05086v1",
                "updated": "2025-05-08T09:34:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    34,
                    15,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T09:34:15Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    34,
                    15,
                    3,
                    128,
                    0
                ],
                "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning"
                },
                "summary": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks."
                },
                "authors": [
                    {
                        "name": "Le-Trung Nguyen"
                    },
                    {
                        "name": "Ael Quelennec"
                    },
                    {
                        "name": "Van-Tam Nguyen"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08292v2",
                "updated": "2025-05-08T09:33:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    33,
                    59,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-11T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    5,
                    42,
                    1,
                    70,
                    0
                ],
                "title": "Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges"
                },
                "summary": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Qingying Xiao"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Xiangyi Feng"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Bairui Zhang"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Jian Chang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21437v3",
                "updated": "2025-05-08T09:33:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    33,
                    46,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-30T08:52:32Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    52,
                    32,
                    2,
                    120,
                    0
                ],
                "title": "Identifying Critical Dependencies in Large-Scale Continuous Software\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Critical Dependencies in Large-Scale Continuous Software\n  Engineering"
                },
                "summary": "Continuous Software Engineering (CSE) is widely adopted in the industry,\nintegrating practices such as Continuous Integration and Continuous Deployment\n(CI/CD). Beyond technical aspects, CSE also encompasses business activities\nlike continuous planning, budgeting, and operational processes. Coordinating\nthese activities in large-scale product development involves multiple\nstakeholders, increasing complexity. This study aims to address this complexity\nby identifying and analyzing critical dependencies in large-scale CSE. Based on\n17 semi-structured interviews conducted at two Nordic fintech companies, our\npreliminary findings indicate that dependencies between software teams and\nsupport functions, as well as between software teams and external entities, are\nthe primary sources of delays and bottlenecks. As a next step, we plan to\nfurther refine our understanding of critical dependencies in large-scale CSE\nand explore coordination mechanisms that can better support software\ndevelopment teams in managing these challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Software Engineering (CSE) is widely adopted in the industry,\nintegrating practices such as Continuous Integration and Continuous Deployment\n(CI/CD). Beyond technical aspects, CSE also encompasses business activities\nlike continuous planning, budgeting, and operational processes. Coordinating\nthese activities in large-scale product development involves multiple\nstakeholders, increasing complexity. This study aims to address this complexity\nby identifying and analyzing critical dependencies in large-scale CSE. Based on\n17 semi-structured interviews conducted at two Nordic fintech companies, our\npreliminary findings indicate that dependencies between software teams and\nsupport functions, as well as between software teams and external entities, are\nthe primary sources of delays and bottlenecks. As a next step, we plan to\nfurther refine our understanding of critical dependencies in large-scale CSE\nand explore coordination mechanisms that can better support software\ndevelopment teams in managing these challenges."
                },
                "authors": [
                    {
                        "name": "Anastasiia Tkalich"
                    },
                    {
                        "name": "Eriks Klotins"
                    },
                    {
                        "name": "Nils Brede Moe"
                    }
                ],
                "author_detail": {
                    "name": "Nils Brede Moe"
                },
                "author": "Nils Brede Moe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09246v2",
                "updated": "2025-05-08T09:33:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    33,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-12T15:03:00Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    3,
                    0,
                    5,
                    102,
                    0
                ],
                "title": "Type-Constrained Code Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type-Constrained Code Generation with Language Models"
                },
                "summary": "Large language models (LLMs) have achieved notable success in code\ngeneration. However, they still frequently produce uncompilable output because\ntheir next-token inference procedure does not model formal aspects of code.\nAlthough constrained decoding is a promising approach to alleviate this issue,\nit has only been applied to handle either domain-specific languages or\nsyntactic features of general-purpose programming languages. However, LLMs\nfrequently generate code with typing errors, which are beyond the domain of\nsyntax and generally hard to adequately constrain. To address this challenge,\nwe introduce a type-constrained decoding approach that leverages type systems\nto guide code generation. For this purpose, we develop novel prefix automata\nand a search over inhabitable types, forming a sound approach to enforce\nwell-typedness on LLM-generated code. We formalize our approach on a\nfoundational simply-typed language and extend it to TypeScript to demonstrate\npracticality. Our evaluation on the HumanEval and MBPP datasets shows that our\napproach reduces compilation errors by more than half and significantly\nincreases functional correctness in code synthesis, translation, and repair\ntasks across LLMs of various sizes and model families, including\nstate-of-the-art open-weight models with more than 30B parameters. The results\ndemonstrate the generality and effectiveness of our approach in constraining\nLLM code generation with formal rules of type systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved notable success in code\ngeneration. However, they still frequently produce uncompilable output because\ntheir next-token inference procedure does not model formal aspects of code.\nAlthough constrained decoding is a promising approach to alleviate this issue,\nit has only been applied to handle either domain-specific languages or\nsyntactic features of general-purpose programming languages. However, LLMs\nfrequently generate code with typing errors, which are beyond the domain of\nsyntax and generally hard to adequately constrain. To address this challenge,\nwe introduce a type-constrained decoding approach that leverages type systems\nto guide code generation. For this purpose, we develop novel prefix automata\nand a search over inhabitable types, forming a sound approach to enforce\nwell-typedness on LLM-generated code. We formalize our approach on a\nfoundational simply-typed language and extend it to TypeScript to demonstrate\npracticality. Our evaluation on the HumanEval and MBPP datasets shows that our\napproach reduces compilation errors by more than half and significantly\nincreases functional correctness in code synthesis, translation, and repair\ntasks across LLMs of various sizes and model families, including\nstate-of-the-art open-weight models with more than 30B parameters. The results\ndemonstrate the generality and effectiveness of our approach in constraining\nLLM code generation with formal rules of type systems."
                },
                "authors": [
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_doi": "10.1145/3729274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.09246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12258v3",
                "updated": "2025-05-08T09:15:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    15,
                    15,
                    3,
                    128,
                    0
                ],
                "published": "2024-05-20T11:40:23Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    11,
                    40,
                    23,
                    0,
                    141,
                    0
                ],
                "title": "Scientific Hypothesis Generation by a Large Language Model: Laboratory\n  Validation in Breast Cancer Treatment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Hypothesis Generation by a Large Language Model: Laboratory\n  Validation in Breast Cancer Treatment"
                },
                "summary": "Large language models LLMs have transformed AI and achieved breakthrough\nperformance on a wide range of tasks In science the most interesting\napplication of LLMs is for hypothesis formation A feature of LLMs which results\nfrom their probabilistic structure is that the output text is not necessarily a\nvalid inference from the training text These are termed hallucinations and are\nharmful in many applications In science some hallucinations may be useful novel\nhypotheses whose validity may be tested by laboratory experiments Here we\nexperimentally test the application of LLMs as a source of scientific\nhypotheses using the domain of breast cancer treatment We applied the LLM GPT4\nto hypothesize novel synergistic pairs of FDA-approved noncancer drugs that\ntarget the MCF7 breast cancer cell line relative to the nontumorigenic breast\ncell line MCF10A In the first round of laboratory experiments GPT4 succeeded in\ndiscovering three drug combinations out of twelve tested with synergy scores\nabove the positive controls GPT4 then generated new combinations based on its\ninitial results this generated three more combinations with positive synergy\nscores out of four tested We conclude that LLMs are a valuable source of\nscientific hypotheses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models LLMs have transformed AI and achieved breakthrough\nperformance on a wide range of tasks In science the most interesting\napplication of LLMs is for hypothesis formation A feature of LLMs which results\nfrom their probabilistic structure is that the output text is not necessarily a\nvalid inference from the training text These are termed hallucinations and are\nharmful in many applications In science some hallucinations may be useful novel\nhypotheses whose validity may be tested by laboratory experiments Here we\nexperimentally test the application of LLMs as a source of scientific\nhypotheses using the domain of breast cancer treatment We applied the LLM GPT4\nto hypothesize novel synergistic pairs of FDA-approved noncancer drugs that\ntarget the MCF7 breast cancer cell line relative to the nontumorigenic breast\ncell line MCF10A In the first round of laboratory experiments GPT4 succeeded in\ndiscovering three drug combinations out of twelve tested with synergy scores\nabove the positive controls GPT4 then generated new combinations based on its\ninitial results this generated three more combinations with positive synergy\nscores out of four tested We conclude that LLMs are a valuable source of\nscientific hypotheses."
                },
                "authors": [
                    {
                        "name": "Abbi Abdel-Rehim"
                    },
                    {
                        "name": "Hector Zenil"
                    },
                    {
                        "name": "Oghenejokpeme Orhobor"
                    },
                    {
                        "name": "Marie Fisher"
                    },
                    {
                        "name": "Ross J. Collins"
                    },
                    {
                        "name": "Elizabeth Bourne"
                    },
                    {
                        "name": "Gareth W. Fearnley"
                    },
                    {
                        "name": "Emma Tate"
                    },
                    {
                        "name": "Holly X. Smith"
                    },
                    {
                        "name": "Larisa N. Soldatova"
                    },
                    {
                        "name": "Ross D. King"
                    }
                ],
                "author_detail": {
                    "name": "Ross D. King"
                },
                "author": "Ross D. King",
                "arxiv_comment": "12 pages, 6 tables, 1 figure. Supplementary information available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15699v2",
                "updated": "2025-05-08T09:12:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    12,
                    22,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-22T08:34:35Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    8,
                    34,
                    35,
                    1,
                    112,
                    0
                ],
                "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input\n  Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Embodied Agent Security: From Safety Benchmarks to Input\n  Moderation"
                },
                "summary": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Zihan Yan"
                    },
                    {
                        "name": "Weiyang Li"
                    },
                    {
                        "name": "Chuan Ma"
                    },
                    {
                        "name": "He Chen"
                    },
                    {
                        "name": "Tao Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xiang"
                },
                "author": "Tao Xiang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05070v1",
                "updated": "2025-05-08T09:06:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    6,
                    28,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T09:06:28Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    6,
                    28,
                    3,
                    128,
                    0
                ],
                "title": "Performance Evaluation of Large Language Models in Bangla Consumer\n  Health Query Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of Large Language Models in Bangla Consumer\n  Health Query Summarization"
                },
                "summary": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization."
                },
                "authors": [
                    {
                        "name": "Ajwad Abrar"
                    },
                    {
                        "name": "Farzana Tabassum"
                    },
                    {
                        "name": "Sabbir Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Sabbir Ahmed"
                },
                "author": "Sabbir Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21356v2",
                "updated": "2025-05-08T08:58:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    58,
                    12,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-30T06:30:48Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    30,
                    48,
                    2,
                    120,
                    0
                ],
                "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing"
                },
                "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."
                },
                "authors": [
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Zhongjie Duan"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yuze Zhao"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05064v1",
                "updated": "2025-05-08T08:56:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    56,
                    46,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:56:46Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    56,
                    46,
                    3,
                    128,
                    0
                ],
                "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterDrum: Watermarking for Data-centric Unlearning Metric"
                },
                "summary": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax."
                },
                "authors": [
                    {
                        "name": "Xinyang Lu"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "Bui Thi Cam Nhung"
                    },
                    {
                        "name": "Rachael Hwee Ling Sim"
                    },
                    {
                        "name": "Fanyu Wen"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05063v1",
                "updated": "2025-05-08T08:55:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    55,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:55:32Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    55,
                    32,
                    3,
                    128,
                    0
                ],
                "title": "CodeMixBench: Evaluating Large Language Models on Code Generation with\n  Code-Mixed Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMixBench: Evaluating Large Language Models on Code Generation with\n  Code-Mixed Prompts"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings."
                },
                "authors": [
                    {
                        "name": "Manik Sheokand"
                    },
                    {
                        "name": "Parth Sawant"
                    }
                ],
                "author_detail": {
                    "name": "Parth Sawant"
                },
                "author": "Parth Sawant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11055v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11055v3",
                "updated": "2025-05-08T08:51:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    51,
                    19,
                    3,
                    128,
                    0
                ],
                "published": "2024-09-17T10:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    31,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant"
                },
                "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove."
                },
                "authors": [
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Jinse Kwon"
                    },
                    {
                        "name": "Jihun Oh"
                    },
                    {
                        "name": "Yongin Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Yongin Kwon"
                },
                "author": "Yongin Kwon",
                "arxiv_comment": "Accepted in IJCAI 2025, 21 pages, 2 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11055v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11055v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05057v1",
                "updated": "2025-05-08T08:48:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    48,
                    17,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:48:17Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    48,
                    17,
                    3,
                    128,
                    0
                ],
                "title": "Towards Mitigating API Hallucination in Code Generated by LLMs with\n  Hierarchical Dependency Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mitigating API Hallucination in Code Generated by LLMs with\n  Hierarchical Dependency Aware"
                },
                "summary": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Programming Interfaces (APIs) are crucial in modern software\ndevelopment. Large Language Models (LLMs) assist in automated code generation\nbut often struggle with API hallucination, including invoking non-existent APIs\nand misusing existing ones in practical development scenarios. Existing studies\nresort to Retrieval-Augmented Generation (RAG) methods for mitigating the\nhallucination issue, but tend to fail since they generally ignore the\nstructural dependencies in practical projects and do not indeed validate\nwhether the generated APIs are available or not. To address these limitations,\nwe propose MARIN, a framework for mitigating API hallucination in code\ngenerated by LLMs with hierarchical dependency aware. MARIN consists of two\nphases: Hierarchical Dependency Mining, which analyzes local and global\ndependencies of the current function, aiming to supplement comprehensive\nproject context in LLMs input, and Dependency Constrained Decoding, which\nutilizes mined dependencies to adaptively constrain the generation process,\naiming to ensure the generated APIs align with the projects specifications. To\nfacilitate the evaluation of the degree of API hallucination, we introduce a\nnew benchmark APIHulBench and two new metrics including Micro Hallucination\nNumber (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six\nstate-of-the-art LLMs demonstrate that MARIN effectively reduces API\nhallucinations, achieving an average decrease of 67.52% in MiHN and 73.56% in\nMaHR compared to the RAG approach. Applied to Huaweis internal projects and two\nproprietary LLMs, MARIN achieves average decreases of 57.33% in MiHN and 59.41%\nin MaHR."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Zhongqi Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuchi Ma"
                },
                "author": "Yuchi Ma",
                "arxiv_comment": "Accepted by FSE 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19981v2",
                "updated": "2025-05-08T08:42:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    42,
                    15,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-28T16:56:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    56,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets"
                },
                "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Adam Younsi"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Salem Lahlou"
                    }
                ],
                "author_detail": {
                    "name": "Salem Lahlou"
                },
                "author": "Salem Lahlou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03961v2",
                "updated": "2025-05-08T08:29:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    29,
                    29,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-06T20:23:25Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    20,
                    23,
                    25,
                    1,
                    126,
                    0
                ],
                "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents\n  Collaborate and Compete",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Stories: Narrative Priming Shapes How LLM Agents\n  Collaborate and Compete"
                },
                "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."
                },
                "authors": [
                    {
                        "name": "Gerrit Großmann"
                    },
                    {
                        "name": "Larisa Ivanova"
                    },
                    {
                        "name": "Sai Leela Poduru"
                    },
                    {
                        "name": "Mohaddeseh Tabrizian"
                    },
                    {
                        "name": "Islam Mesabah"
                    },
                    {
                        "name": "David A. Selby"
                    },
                    {
                        "name": "Sebastian J. Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian J. Vollmer"
                },
                "author": "Sebastian J. Vollmer",
                "arxiv_comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; I.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03680v2",
                "updated": "2025-05-08T08:19:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    19,
                    47,
                    3,
                    128,
                    0
                ],
                "published": "2024-08-07T10:43:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    43,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "Smaller but Better: Self-Paced Knowledge Distillation for Lightweight\n  yet Effective LCMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller but Better: Self-Paced Knowledge Distillation for Lightweight\n  yet Effective LCMs"
                },
                "summary": "Large code models (LCMs) have remarkably advanced the field of code\ngeneration. Despite their impressive capabilities, they still face practical\ndeployment issues, such as high inference costs, limited accessibility of\nproprietary LCMs, and adaptability issues of ultra-large LCMs. These issues\nhighlight the critical need for more accessible, lightweight yet effective\nLCMs. Knowledge distillation (KD) offers a promising solution, which transfers\nthe programming capabilities of larger, advanced LCMs to smaller, less powerful\nLCMs. In this paper, we propose a novel Self-Paced knOwledge DistillAtion\nframework, named SODA, aiming at developing lightweight yet effective student\nLCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault\nKnowledge Delivery stage aims at improving the student models capability to\nrecognize errors while ensuring its basic programming skill during the\nknowledge transferring, which involves correctness-aware supervised learning\nand fault-aware contrastive learning methods. (2) Multi-View Feedback stage\naims at measuring the quality of results generated by the student model from\ntwo views, including model-based and static tool-based measurement, for\nidentifying the difficult questions. (3) Feedback-based Knowledge Update stage\naims at updating the student model adaptively by generating new questions at\ndifferent difficulty levels, in which the difficulty levels are categorized\nbased on the feedback in the second stage. Experimental results show that SODA\nimproves the student model by 65.96% in terms of average Pass@1, outperforming\nthe best baseline by 29.85%. Based on the SODA framework, we develop SodaCoder,\na series of lightweight yet effective LCMs, which outperform 15 LCMs with less\nthan or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on\nDeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large code models (LCMs) have remarkably advanced the field of code\ngeneration. Despite their impressive capabilities, they still face practical\ndeployment issues, such as high inference costs, limited accessibility of\nproprietary LCMs, and adaptability issues of ultra-large LCMs. These issues\nhighlight the critical need for more accessible, lightweight yet effective\nLCMs. Knowledge distillation (KD) offers a promising solution, which transfers\nthe programming capabilities of larger, advanced LCMs to smaller, less powerful\nLCMs. In this paper, we propose a novel Self-Paced knOwledge DistillAtion\nframework, named SODA, aiming at developing lightweight yet effective student\nLCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault\nKnowledge Delivery stage aims at improving the student models capability to\nrecognize errors while ensuring its basic programming skill during the\nknowledge transferring, which involves correctness-aware supervised learning\nand fault-aware contrastive learning methods. (2) Multi-View Feedback stage\naims at measuring the quality of results generated by the student model from\ntwo views, including model-based and static tool-based measurement, for\nidentifying the difficult questions. (3) Feedback-based Knowledge Update stage\naims at updating the student model adaptively by generating new questions at\ndifferent difficulty levels, in which the difficulty levels are categorized\nbased on the feedback in the second stage. Experimental results show that SODA\nimproves the student model by 65.96% in terms of average Pass@1, outperforming\nthe best baseline by 29.85%. Based on the SODA framework, we develop SodaCoder,\na series of lightweight yet effective LCMs, which outperform 15 LCMs with less\nthan or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on\nDeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1."
                },
                "authors": [
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Zhongqi Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "arxiv_comment": "Accepted by FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00762v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00762v4",
                "updated": "2025-05-08T08:07:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    7,
                    55,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-01T13:13:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    13,
                    13,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute"
                },
                "summary": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm."
                },
                "authors": [
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Zishuo Xun"
                    },
                    {
                        "name": "Bocheng Zhou"
                    },
                    {
                        "name": "Han Qi"
                    },
                    {
                        "name": "Hangfan Zhang"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Yuzhong Qu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Shuyue Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shuyue Hu"
                },
                "author": "Shuyue Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00762v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00762v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05031v1",
                "updated": "2025-05-08T08:06:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    6,
                    34,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T08:06:34Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    6,
                    34,
                    3,
                    128,
                    0
                ],
                "title": "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving\n  Cloud-Device Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving\n  Cloud-Device Collaboration"
                },
                "summary": "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Zhang-Yingyi/LSRP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)\nfor handling public user queries and on-device Small Language Models (SLMs) for\nprocessing private user data, collectively forming a powerful and\nprivacy-preserving solution. However, existing approaches often fail to fully\nleverage the scalable problem-solving capabilities of on-cloud LLMs while\nunderutilizing the advantage of on-device SLMs in accessing and processing\npersonalized data. This leads to two interconnected issues: 1) Limited\nutilization of the problem-solving capabilities of on-cloud LLMs, which fail to\nalign with personalized user-task needs, and 2) Inadequate integration of user\ndata into on-device SLM responses, resulting in mismatches in contextual user\ninformation.\n  In this paper, we propose a Leader-Subordinate Retrieval framework for\nPrivacy-preserving cloud-device collaboration (LSRP), a novel solution that\nbridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM\nthrough a dynamic selection of task-specific leader strategies named as\nuser-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the\ndata advantages of on-device SLMs through small model feedback Direct\nPreference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the\non-device SLM. Experiments on two datasets demonstrate that LSRP consistently\noutperforms state-of-the-art baselines, significantly improving question-answer\nrelevance and personalization, while preserving user privacy through efficient\non-device retrieval. Our code is available at:\nhttps://github.com/Zhang-Yingyi/LSRP."
                },
                "authors": [
                    {
                        "name": "Yingyi Zhang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Xianneng Li"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04180v2",
                "updated": "2025-05-08T07:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    51,
                    26,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T07:25:46Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    25,
                    46,
                    2,
                    127,
                    0
                ],
                "title": "Towards Large-scale Generative Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large-scale Generative Ranking"
                },
                "summary": "Generative recommendation has recently emerged as a promising paradigm in\ninformation retrieval. However, generative ranking systems are still\nunderstudied, particularly with respect to their effectiveness and feasibility\nin large-scale industrial settings. This paper investigates this topic at the\nranking stage of Xiaohongshu's Explore Feed, a recommender system that serves\nhundreds of millions of users. Specifically, we first examine how generative\nranking outperforms current industrial recommenders. Through theoretical and\nempirical analyses, we find that the primary improvement in effectiveness stems\nfrom the generative architecture, rather than the training paradigm. To\nfacilitate efficient deployment of generative ranking, we introduce GenRank, a\nnovel generative architecture for ranking. We validate the effectiveness and\nefficiency of our solution through online A/B experiments. The results show\nthat GenRank achieves significant improvements in user satisfaction with nearly\nequivalent computational resources compared to the existing production system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation has recently emerged as a promising paradigm in\ninformation retrieval. However, generative ranking systems are still\nunderstudied, particularly with respect to their effectiveness and feasibility\nin large-scale industrial settings. This paper investigates this topic at the\nranking stage of Xiaohongshu's Explore Feed, a recommender system that serves\nhundreds of millions of users. Specifically, we first examine how generative\nranking outperforms current industrial recommenders. Through theoretical and\nempirical analyses, we find that the primary improvement in effectiveness stems\nfrom the generative architecture, rather than the training paradigm. To\nfacilitate efficient deployment of generative ranking, we introduce GenRank, a\nnovel generative architecture for ranking. We validate the effectiveness and\nefficiency of our solution through online A/B experiments. The results show\nthat GenRank achieves significant improvements in user satisfaction with nearly\nequivalent computational resources compared to the existing production system."
                },
                "authors": [
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Xiong Cao"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Mingliang Qi"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Qingchang Han"
                    },
                    {
                        "name": "Yaowei Liu"
                    },
                    {
                        "name": "Zhaoyu Liu"
                    },
                    {
                        "name": "Xuefeng Yao"
                    },
                    {
                        "name": "Yuting Jia"
                    },
                    {
                        "name": "Leilei Ma"
                    },
                    {
                        "name": "Yinqi Zhang"
                    },
                    {
                        "name": "Taoyu Zhu"
                    },
                    {
                        "name": "Liujie Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Weihang Chen"
                    },
                    {
                        "name": "Min Zhu"
                    },
                    {
                        "name": "Ruiwen Xu"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12674v2",
                "updated": "2025-05-08T07:48:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    48,
                    59,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-18T09:25:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    25,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by\n  Animal Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by\n  Animal Learning"
                },
                "summary": "Despite recent advances in learning-based controllers for legged robots,\ndeployments in human-centric environments remain limited by safety concerns.\nMost of these approaches use position-based control, where policies output\ntarget joint angles that must be processed by a low-level controller (e.g., PD\nor impedance controllers) to compute joint torques. Although impressive results\nhave been achieved in controlled real-world scenarios, these methods often\nstruggle with compliance and adaptability when encountering environments or\ndisturbances unseen during training, potentially resulting in extreme or unsafe\nbehaviors. Inspired by how animals achieve smooth and adaptive movements by\ncontrolling muscle extension and contraction, torque-based policies offer a\npromising alternative by enabling precise and direct control of the actuators\nin torque space. In principle, this approach facilitates more effective\ninteractions with the environment, resulting in safer and more adaptable\nbehaviors. However, challenges such as a highly nonlinear state space and\ninefficient exploration during training have hindered their broader adoption.\nTo address these limitations, we propose SATA, a bio-inspired framework that\nmimics key biomechanical principles and adaptive learning mechanisms observed\nin animal locomotion. Our approach effectively addresses the inherent\nchallenges of learning torque-based policies by significantly improving\nearly-stage exploration, leading to high-performance final policies.\nRemarkably, our method achieves zero-shot sim-to-real transfer. Our\nexperimental results indicate that SATA demonstrates remarkable compliance and\nsafety, even in challenging environments such as soft/slippery terrain or\nnarrow passages, and under significant external disturbances, highlighting its\npotential for practical deployments in human-centric and safety-critical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in learning-based controllers for legged robots,\ndeployments in human-centric environments remain limited by safety concerns.\nMost of these approaches use position-based control, where policies output\ntarget joint angles that must be processed by a low-level controller (e.g., PD\nor impedance controllers) to compute joint torques. Although impressive results\nhave been achieved in controlled real-world scenarios, these methods often\nstruggle with compliance and adaptability when encountering environments or\ndisturbances unseen during training, potentially resulting in extreme or unsafe\nbehaviors. Inspired by how animals achieve smooth and adaptive movements by\ncontrolling muscle extension and contraction, torque-based policies offer a\npromising alternative by enabling precise and direct control of the actuators\nin torque space. In principle, this approach facilitates more effective\ninteractions with the environment, resulting in safer and more adaptable\nbehaviors. However, challenges such as a highly nonlinear state space and\ninefficient exploration during training have hindered their broader adoption.\nTo address these limitations, we propose SATA, a bio-inspired framework that\nmimics key biomechanical principles and adaptive learning mechanisms observed\nin animal locomotion. Our approach effectively addresses the inherent\nchallenges of learning torque-based policies by significantly improving\nearly-stage exploration, leading to high-performance final policies.\nRemarkably, our method achieves zero-shot sim-to-real transfer. Our\nexperimental results indicate that SATA demonstrates remarkable compliance and\nsafety, even in challenging environments such as soft/slippery terrain or\nnarrow passages, and under significant external disturbances, highlighting its\npotential for practical deployments in human-centric and safety-critical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Peizhuo Li"
                    },
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "Ge Sun"
                    },
                    {
                        "name": "Jin Cheng"
                    },
                    {
                        "name": "Xinrong Yang"
                    },
                    {
                        "name": "Guillaume Bellegarda"
                    },
                    {
                        "name": "Milad Shafiee"
                    },
                    {
                        "name": "Yuhong Cao"
                    },
                    {
                        "name": "Auke Ijspeert"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Sartoretti"
                },
                "author": "Guillaume Sartoretti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05017v1",
                "updated": "2025-05-08T07:43:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    43,
                    44,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T07:43:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    43,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Scalable Multi-Stage Influence Function for Large Language Models via\n  Eigenvalue-Corrected Kronecker-Factored Parameterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Multi-Stage Influence Function for Large Language Models via\n  Eigenvalue-Corrected Kronecker-Factored Parameterization"
                },
                "summary": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function."
                },
                "authors": [
                    {
                        "name": "Yuntai Bao"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Xinkui Zhao"
                    },
                    {
                        "name": "Jiang Zong"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "9 pages, accepted by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05016v1",
                "updated": "2025-05-08T07:43:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    43,
                    1,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T07:43:01Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    43,
                    1,
                    3,
                    128,
                    0
                ],
                "title": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based\n  Aggregation for Group Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based\n  Aggregation for Group Recommendations"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs."
                },
                "authors": [
                    {
                        "name": "Cedric Waterschoot"
                    },
                    {
                        "name": "Nava Tintarev"
                    },
                    {
                        "name": "Francesco Barile"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barile"
                },
                "author": "Francesco Barile",
                "arxiv_doi": "10.1145/3708319.3733659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708319.3733659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To be published in: Adjunct Proceedings of the 33rd ACM Conference on\n  User Modeling, Adaptation and Personalization (UMAP Adjunct '25), June\n  16--19, 2025, New York City, NY, USA Accepted at the 4th Workshop on Group\n  Modeling, Adaptation and Personalization (GMAP), co-located at UMAP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02082v2",
                "updated": "2025-05-08T07:37:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    37,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-04T12:21:16Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    12,
                    21,
                    16,
                    6,
                    124,
                    0
                ],
                "title": "Performance Characterization of Containers in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization of Containers in Edge Computing"
                },
                "summary": "Edge computing addresses critical limitations of cloud computing such as high\nlatency and network congestion by decentralizing processing from cloud to the\nedge. However, the need for software replication across heterogeneous edge\ndevices introduces dependency and portability challenges, driving the adoption\nof containerization technologies like Docker. While containers offer\nlightweight isolation and deployment advantages, they introduce new bottlenecks\nin edge environments, including cold-start delays, memory constraints, network\nthroughput variability, and inefficient IO handling when interfacing with\nembedded peripherals. This paper presents an empirical evaluation of Docker\ncontainers on resource-constrained edge devices, using Raspberry Pi as a\nrepresentative platform. We benchmark performance across diverse workloads,\nincluding microbenchmarks (CPU, memory, network profiling) and macrobenchmarks\n(AI inference, sensor IO operations), to quantify the overheads of\ncontainerization in real-world edge scenarios. Our testbed comprises physical\nRaspberry Pi nodes integrated with environmental sensors and camera modules,\nenabling measurements of latency, memory faults, IO throughput, and cold start\ndelays under varying loads. Key findings reveal trade-offs between container\nisolation and edge-specific resource limitations, with performance degradation\nobserved in IO heavy and latency sensitive tasks. We identify configuration\noptimizations to mitigate these issues, providing actionable insights for\ndeploying containers in edge environments while meeting real time and\nreliability requirements. This work advances the understanding of containerized\nedge computing by systematically evaluating its feasibility and pitfalls on\nlow-power embedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing addresses critical limitations of cloud computing such as high\nlatency and network congestion by decentralizing processing from cloud to the\nedge. However, the need for software replication across heterogeneous edge\ndevices introduces dependency and portability challenges, driving the adoption\nof containerization technologies like Docker. While containers offer\nlightweight isolation and deployment advantages, they introduce new bottlenecks\nin edge environments, including cold-start delays, memory constraints, network\nthroughput variability, and inefficient IO handling when interfacing with\nembedded peripherals. This paper presents an empirical evaluation of Docker\ncontainers on resource-constrained edge devices, using Raspberry Pi as a\nrepresentative platform. We benchmark performance across diverse workloads,\nincluding microbenchmarks (CPU, memory, network profiling) and macrobenchmarks\n(AI inference, sensor IO operations), to quantify the overheads of\ncontainerization in real-world edge scenarios. Our testbed comprises physical\nRaspberry Pi nodes integrated with environmental sensors and camera modules,\nenabling measurements of latency, memory faults, IO throughput, and cold start\ndelays under varying loads. Key findings reveal trade-offs between container\nisolation and edge-specific resource limitations, with performance degradation\nobserved in IO heavy and latency sensitive tasks. We identify configuration\noptimizations to mitigate these issues, providing actionable insights for\ndeploying containers in edge environments while meeting real time and\nreliability requirements. This work advances the understanding of containerized\nedge computing by systematically evaluating its feasibility and pitfalls on\nlow-power embedded systems."
                },
                "authors": [
                    {
                        "name": "Ragini Gupta"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03373v2",
                "updated": "2025-05-08T07:12:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    12,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-04T11:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    44,
                    24,
                    4,
                    94,
                    0
                ],
                "title": "An Efficient GPU-based Implementation for Noise Robust Sound Source\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient GPU-based Implementation for Noise Robust Sound Source\n  Localization"
                },
                "summary": "Robot audition, encompassing Sound Source Localization (SSL), Sound Source\nSeparation (SSS), and Automatic Speech Recognition (ASR), enables robots and\nsmart devices to acquire auditory capabilities similar to human hearing.\nDespite their wide applicability, processing multi-channel audio signals from\nmicrophone arrays in SSL involves computationally intensive matrix operations,\nwhich can hinder efficient deployment on Central Processing Units (CPUs),\nparticularly in embedded systems with limited CPU resources. This paper\nintroduces a GPU-based implementation of SSL for robot audition, utilizing the\nGeneralized Singular Value Decomposition-based Multiple Signal Classification\n(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an\nopen-source software suite. For a 60-channel microphone array, the proposed\nimplementation achieves significant performance improvements. On the Jetson AGX\nOrin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2\n64-bit CPUs, we observe speedups of 5648.7x for GSVD calculations and 10.7x for\nthe SSL module, while speedups of 4245.1x for GSVD calculation and 17.3x for\nthe entire SSL module on a server configured with an NVIDIA A100 GPU and AMD\nEPYC 7352 CPUs, making real-time processing feasible for large-scale microphone\narrays and providing ample capacity for real-time processing of potential\nsubsequent machine learning or deep learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot audition, encompassing Sound Source Localization (SSL), Sound Source\nSeparation (SSS), and Automatic Speech Recognition (ASR), enables robots and\nsmart devices to acquire auditory capabilities similar to human hearing.\nDespite their wide applicability, processing multi-channel audio signals from\nmicrophone arrays in SSL involves computationally intensive matrix operations,\nwhich can hinder efficient deployment on Central Processing Units (CPUs),\nparticularly in embedded systems with limited CPU resources. This paper\nintroduces a GPU-based implementation of SSL for robot audition, utilizing the\nGeneralized Singular Value Decomposition-based Multiple Signal Classification\n(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an\nopen-source software suite. For a 60-channel microphone array, the proposed\nimplementation achieves significant performance improvements. On the Jetson AGX\nOrin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2\n64-bit CPUs, we observe speedups of 5648.7x for GSVD calculations and 10.7x for\nthe SSL module, while speedups of 4245.1x for GSVD calculation and 17.3x for\nthe entire SSL module on a server configured with an NVIDIA A100 GPU and AMD\nEPYC 7352 CPUs, making real-time processing feasible for large-scale microphone\narrays and providing ample capacity for real-time processing of potential\nsubsequent machine learning or deep learning tasks."
                },
                "authors": [
                    {
                        "name": "Zirui Lin"
                    },
                    {
                        "name": "Masayuki Takigahira"
                    },
                    {
                        "name": "Naoya Terakado"
                    },
                    {
                        "name": "Haris Gulzar"
                    },
                    {
                        "name": "Monikka Roslianna Busto"
                    },
                    {
                        "name": "Takeharu Eda"
                    },
                    {
                        "name": "Katsutoshi Itoyama"
                    },
                    {
                        "name": "Kazuhiro Nakadai"
                    },
                    {
                        "name": "Hideharu Amano"
                    }
                ],
                "author_detail": {
                    "name": "Hideharu Amano"
                },
                "author": "Hideharu Amano",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08659v4",
                "updated": "2025-05-08T07:11:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    11,
                    30,
                    3,
                    128,
                    0
                ],
                "published": "2025-02-09T06:33:47Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    6,
                    33,
                    47,
                    6,
                    40,
                    0
                ],
                "title": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks"
                },
                "summary": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference."
                },
                "authors": [
                    {
                        "name": "Shuqi Shen"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Hui Zhong"
                    },
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Xinhu Zheng"
                    },
                    {
                        "name": "Hai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Yang"
                },
                "author": "Hai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14401v2",
                "updated": "2025-05-08T07:07:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    7,
                    6,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-19T20:38:09Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    20,
                    38,
                    9,
                    5,
                    109,
                    0
                ],
                "title": "LLM-Driven Usefulness Judgment for Web Search Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Usefulness Judgment for Web Search Evaluation"
                },
                "summary": "Evaluation is fundamental in optimizing search experiences and supporting\ndiverse user intents in Information Retrieval (IR). Traditional search\nevaluation methods primarily rely on relevance labels, which assess how well\nretrieved documents match a user's query. However, relevance alone fails to\ncapture a search system's effectiveness in helping users achieve their search\ngoals, making usefulness a critical evaluation criterion. In this paper, we\nexplore an alternative approach: LLM-generated usefulness labels, which\nincorporate both implicit and explicit user behavior signals to evaluate\ndocument usefulness. We propose Task-aware Rubric-based Usefulness Evaluation\n(TRUE), a rubric-driven evaluation method that employs iterative sampling and\nreasoning to model complex search behavior patterns. Our findings show that (i)\nLLMs can generate moderate usefulness labels by leveraging comprehensive search\nsession history incorporating personalization and contextual understanding, and\n(ii) fine-tuned LLMs improve usefulness judgments when provided with structured\nsearch session contexts. Additionally, we examine whether LLMs can distinguish\nbetween relevance and usefulness, particularly in cases where this divergence\nimpacts search success. We also conduct an ablation study to identify key\nmetrics for accurate usefulness label generation, optimizing for token\nefficiency and cost-effectiveness in real-world applications. This study\nadvances LLM-based usefulness evaluation by refining key user metrics,\nexploring LLM-generated label reliability, and ensuring feasibility for\nlarge-scale search systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation is fundamental in optimizing search experiences and supporting\ndiverse user intents in Information Retrieval (IR). Traditional search\nevaluation methods primarily rely on relevance labels, which assess how well\nretrieved documents match a user's query. However, relevance alone fails to\ncapture a search system's effectiveness in helping users achieve their search\ngoals, making usefulness a critical evaluation criterion. In this paper, we\nexplore an alternative approach: LLM-generated usefulness labels, which\nincorporate both implicit and explicit user behavior signals to evaluate\ndocument usefulness. We propose Task-aware Rubric-based Usefulness Evaluation\n(TRUE), a rubric-driven evaluation method that employs iterative sampling and\nreasoning to model complex search behavior patterns. Our findings show that (i)\nLLMs can generate moderate usefulness labels by leveraging comprehensive search\nsession history incorporating personalization and contextual understanding, and\n(ii) fine-tuned LLMs improve usefulness judgments when provided with structured\nsearch session contexts. Additionally, we examine whether LLMs can distinguish\nbetween relevance and usefulness, particularly in cases where this divergence\nimpacts search success. We also conduct an ablation study to identify key\nmetrics for accurate usefulness label generation, optimizing for token\nefficiency and cost-effectiveness in real-world applications. This study\nadvances LLM-based usefulness evaluation by refining key user metrics,\nexploring LLM-generated label reliability, and ensuring feasibility for\nlarge-scale search systems."
                },
                "authors": [
                    {
                        "name": "Mouly Dewan"
                    },
                    {
                        "name": "Jiqun Liu"
                    },
                    {
                        "name": "Aditya Gautam"
                    },
                    {
                        "name": "Chirag Shah"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Shah"
                },
                "author": "Chirag Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04993v1",
                "updated": "2025-05-08T06:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    59,
                    6,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T06:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    59,
                    6,
                    3,
                    128,
                    0
                ],
                "title": "Latent Preference Coding: Aligning Large Language Models via Discrete\n  Latent Codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Preference Coding: Aligning Large Language Models via Discrete\n  Latent Codes"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Gong"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06877v3",
                "updated": "2025-05-08T06:40:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    40,
                    2,
                    3,
                    128,
                    0
                ],
                "published": "2024-11-11T11:17:35Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    17,
                    35,
                    0,
                    316,
                    0
                ],
                "title": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?"
                },
                "summary": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant effort in manual annotations, which often makes it very expensive\nand time-consuming. Thus, test collections could become too small when the\nbudget is limited, which may lead to unstable evaluations. As a cheaper\nalternative, recent studies have proposed the use of large language models\n(LLMs) to completely replace human assessors. However, while LLMs seem to\nsomewhat correlate with human judgments, their predictions are not perfect and\noften show bias. Thus a complete replacement with LLMs is argued to be too\nrisky and not fully reliable. Thus, in this paper, we propose LLM-Assisted\nRelevance Assessments (LARA), an effective method to balance manual annotations\nwith LLM annotations, which helps to build a rich and reliable test collection\neven under a low budget. We use the LLM's predicted relevance probabilities to\nselect the most profitable documents to manually annotate under a budget\nconstraint. With theoretical reasoning, LARA effectively guides the human\nannotation process by actively learning to calibrate the LLM's predicted\nrelevance probabilities. Then, using the calibration model learned from the\nlimited manual annotations, LARA debiases the LLM predictions to annotate the\nremaining non-assessed data. Empirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad\nHoc, TREC Robust 2004, and TREC-COVID datasets show that LARA outperforms\nalternative solutions under almost any budget constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant effort in manual annotations, which often makes it very expensive\nand time-consuming. Thus, test collections could become too small when the\nbudget is limited, which may lead to unstable evaluations. As a cheaper\nalternative, recent studies have proposed the use of large language models\n(LLMs) to completely replace human assessors. However, while LLMs seem to\nsomewhat correlate with human judgments, their predictions are not perfect and\noften show bias. Thus a complete replacement with LLMs is argued to be too\nrisky and not fully reliable. Thus, in this paper, we propose LLM-Assisted\nRelevance Assessments (LARA), an effective method to balance manual annotations\nwith LLM annotations, which helps to build a rich and reliable test collection\neven under a low budget. We use the LLM's predicted relevance probabilities to\nselect the most profitable documents to manually annotate under a budget\nconstraint. With theoretical reasoning, LARA effectively guides the human\nannotation process by actively learning to calibrate the LLM's predicted\nrelevance probabilities. Then, using the calibration model learned from the\nlimited manual annotations, LARA debiases the LLM predictions to annotate the\nremaining non-assessed data. Empirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad\nHoc, TREC Robust 2004, and TREC-COVID datasets show that LARA outperforms\nalternative solutions under almost any budget constraint."
                },
                "authors": [
                    {
                        "name": "Rikiya Takehi"
                    },
                    {
                        "name": "Ellen M. Voorhees"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    },
                    {
                        "name": "Ian Soboroff"
                    }
                ],
                "author_detail": {
                    "name": "Ian Soboroff"
                },
                "author": "Ian Soboroff",
                "arxiv_doi": "10.1145/3726302.3729916",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729916",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.06877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages. Accepted at SIGIR 2025 (48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01259v2",
                "updated": "2025-05-08T06:40:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    40,
                    0,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-02T00:11:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    0,
                    11,
                    8,
                    2,
                    92,
                    0
                ],
                "title": "Facilitating Instructors-LLM Collaboration for Problem Design in\n  Introductory Programming Classrooms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Instructors-LLM Collaboration for Problem Design in\n  Introductory Programming Classrooms"
                },
                "summary": "Advancements in Large Language Models (LLMs), such as ChatGPT, offer\nsignificant opportunities to enhance instructional support in introductory\nprogramming courses. While extensive research has explored the effectiveness of\nLLMs in supporting student learning, limited studies have examined how these\nmodels can assist instructors in designing instructional activities. This work\ninvestigates how instructors' expertise in effective activity design can be\nintegrated with LLMs' ability to generate novel and targeted programming\nproblems, facilitating more effective activity creation for programming\nclassrooms. To achieve this, we employ a participatory design approach to\ndevelop an instructor-authoring tool that incorporates LLM support, fostering\ncollaboration between instructors and AI in generating programming exercises.\nThis tool also allows instructors to specify common student mistakes and\nmisconceptions, which informs the adaptive feedback generation process. We\nconduct case studies with three instructors, analyzing how they use our system\nto design programming problems for their introductory courses. Through these\ncase studies, we assess instructors' perceptions of the usefulness and\nlimitations of LLMs in authoring problem statements for instructional purposes.\nAdditionally, we compare the efficiency, quality, effectiveness, and coverage\nof designed activities when instructors create problems with and without\nstructured LLM prompting guidelines. Our findings provide insights into the\npotential of LLMs in enhancing instructor workflows and improving programming\neducation and provide guidelines for designing effective AI-assisted\nproblem-authoring interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in Large Language Models (LLMs), such as ChatGPT, offer\nsignificant opportunities to enhance instructional support in introductory\nprogramming courses. While extensive research has explored the effectiveness of\nLLMs in supporting student learning, limited studies have examined how these\nmodels can assist instructors in designing instructional activities. This work\ninvestigates how instructors' expertise in effective activity design can be\nintegrated with LLMs' ability to generate novel and targeted programming\nproblems, facilitating more effective activity creation for programming\nclassrooms. To achieve this, we employ a participatory design approach to\ndevelop an instructor-authoring tool that incorporates LLM support, fostering\ncollaboration between instructors and AI in generating programming exercises.\nThis tool also allows instructors to specify common student mistakes and\nmisconceptions, which informs the adaptive feedback generation process. We\nconduct case studies with three instructors, analyzing how they use our system\nto design programming problems for their introductory courses. Through these\ncase studies, we assess instructors' perceptions of the usefulness and\nlimitations of LLMs in authoring problem statements for instructional purposes.\nAdditionally, we compare the efficiency, quality, effectiveness, and coverage\nof designed activities when instructors create problems with and without\nstructured LLM prompting guidelines. Our findings provide insights into the\npotential of LLMs in enhancing instructor workflows and improving programming\neducation and provide guidelines for designing effective AI-assisted\nproblem-authoring interfaces."
                },
                "authors": [
                    {
                        "name": "Muntasir Hoq"
                    },
                    {
                        "name": "Jessica Vandenberg"
                    },
                    {
                        "name": "Shuyin Jiao"
                    },
                    {
                        "name": "Seung Lee"
                    },
                    {
                        "name": "Bradford Mott"
                    },
                    {
                        "name": "Narges Norouzi"
                    },
                    {
                        "name": "James Lester"
                    },
                    {
                        "name": "Bita Akram"
                    }
                ],
                "author_detail": {
                    "name": "Bita Akram"
                },
                "author": "Bita Akram",
                "arxiv_comment": "Accepted at CHI 2025 Workshop on Augmented Educators and AI: Shaping\n  the Future of Human and AI Cooperation in Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04977v1",
                "updated": "2025-05-08T06:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    30,
                    46,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T06:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    30,
                    46,
                    3,
                    128,
                    0
                ],
                "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChainMarks: Securing DNN Watermark with Cryptographic Chain"
                },
                "summary": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy."
                },
                "authors": [
                    {
                        "name": "Brian Choi"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Isabelle Choi"
                    },
                    {
                        "name": "Kun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Kun Sun"
                },
                "author": "Kun Sun",
                "arxiv_comment": "Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04594v2",
                "updated": "2025-05-08T06:18:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    18,
                    31,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T17:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    37,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"
                },
                "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Girish Chandar Ganesan"
                    },
                    {
                        "name": "Xiaoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Liu"
                },
                "author": "Xiaoming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09798v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09798v3",
                "updated": "2025-05-08T06:12:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    12,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-14T01:57:43Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    1,
                    57,
                    43,
                    0,
                    104,
                    0
                ],
                "title": "ReadMe.LLM: A Framework to Help LLMs Understand Your Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReadMe.LLM: A Framework to Help LLMs Understand Your Library"
                },
                "summary": "Large Language Models (LLMs) often struggle with code generation tasks\ninvolving niche software libraries. Existing code generation techniques with\nonly human-oriented documentation can fail -- even when the LLM has access to\nweb search and the library is documented online. To address this challenge, we\npropose ReadMe$.$LLM, LLM-oriented documentation for software libraries. By\nattaching the contents of ReadMe$.$LLM to a query, performance consistently\nimproves to near-perfect accuracy, with one case study demonstrating up to 100%\nsuccess across all tested models. We propose a software development lifecycle\nwhere LLM-specific documentation is maintained alongside traditional software\nupdates. In this study, we present two practical applications of the\nReadMe$.$LLM idea with diverse software libraries, highlighting that our\nproposed approach could generalize across programming domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with code generation tasks\ninvolving niche software libraries. Existing code generation techniques with\nonly human-oriented documentation can fail -- even when the LLM has access to\nweb search and the library is documented online. To address this challenge, we\npropose ReadMe$.$LLM, LLM-oriented documentation for software libraries. By\nattaching the contents of ReadMe$.$LLM to a query, performance consistently\nimproves to near-perfect accuracy, with one case study demonstrating up to 100%\nsuccess across all tested models. We propose a software development lifecycle\nwhere LLM-specific documentation is maintained alongside traditional software\nupdates. In this study, we present two practical applications of the\nReadMe$.$LLM idea with diverse software libraries, highlighting that our\nproposed approach could generalize across programming domains."
                },
                "authors": [
                    {
                        "name": "Sandya Wijaya"
                    },
                    {
                        "name": "Jacob Bolano"
                    },
                    {
                        "name": "Alejandro Gomez Soteres"
                    },
                    {
                        "name": "Shriyanshu Kode"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Anant Sahai"
                    }
                ],
                "author_detail": {
                    "name": "Anant Sahai"
                },
                "author": "Anant Sahai",
                "arxiv_comment": "15 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09798v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09798v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02309v2",
                "updated": "2025-05-08T05:55:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    55,
                    48,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-05T01:27:47Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    1,
                    27,
                    47,
                    0,
                    125,
                    0
                ],
                "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model\n  Compression Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model\n  Compression Techniques"
                },
                "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment."
                },
                "authors": [
                    {
                        "name": "Sanjay Surendranath Girija"
                    },
                    {
                        "name": "Shashank Kapoor"
                    },
                    {
                        "name": "Lakshit Arora"
                    },
                    {
                        "name": "Dipen Pradhan"
                    },
                    {
                        "name": "Aman Raj"
                    },
                    {
                        "name": "Ankit Shetgaonkar"
                    }
                ],
                "author_detail": {
                    "name": "Ankit Shetgaonkar"
                },
                "author": "Ankit Shetgaonkar",
                "arxiv_comment": "Accepted to IEEE COMPSAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03577v2",
                "updated": "2025-05-08T05:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    49,
                    30,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-04T16:30:54Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    16,
                    30,
                    54,
                    4,
                    278,
                    0
                ],
                "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for\n  Hallucination Mitigation in Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Twice Before You Answer: Memory-Space Visual Retracing for\n  Hallucination Mitigation in Multimodal Large Language Models"
                },
                "summary": "Despite their impressive capabilities, multimodal large language models\n(MLLMs) are prone to hallucinations, i.e., the generated content that is\nnonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in\nMLLMs often stem from the sensitivity of text decoder to visual tokens, leading\nto a phenomenon akin to \"amnesia\" about visual information. To address this\nissue, we propose MemVR, a novel decoding paradigm inspired by common\ncognition: when the memory of an image seen the moment before is forgotten,\npeople will look at it again for factual answers. Following this principle, we\ntreat visual tokens as supplementary evidence, re-injecting them into the MLLM\nthrough Feed Forward Network (FFN) as \"key-value memory\" at the middle trigger\nlayer. This \"look-twice\" mechanism occurs when the model exhibits high\nuncertainty during inference, effectively enhancing factual alignment.\nComprehensive experimental evaluations demonstrate that MemVR significantly\nmitigates hallucination across various MLLMs and excels in general benchmarks\nwithout incurring additional time overhead. The implementation is available\nfrom https://github.com/1zhou-Wang/MemVR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, multimodal large language models\n(MLLMs) are prone to hallucinations, i.e., the generated content that is\nnonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in\nMLLMs often stem from the sensitivity of text decoder to visual tokens, leading\nto a phenomenon akin to \"amnesia\" about visual information. To address this\nissue, we propose MemVR, a novel decoding paradigm inspired by common\ncognition: when the memory of an image seen the moment before is forgotten,\npeople will look at it again for factual answers. Following this principle, we\ntreat visual tokens as supplementary evidence, re-injecting them into the MLLM\nthrough Feed Forward Network (FFN) as \"key-value memory\" at the middle trigger\nlayer. This \"look-twice\" mechanism occurs when the model exhibits high\nuncertainty during inference, effectively enhancing factual alignment.\nComprehensive experimental evaluations demonstrate that MemVR significantly\nmitigates hallucination across various MLLMs and excels in general benchmarks\nwithout incurring additional time overhead. The implementation is available\nfrom https://github.com/1zhou-Wang/MemVR"
                },
                "authors": [
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Kening Zheng"
                    },
                    {
                        "name": "Sirui Huang"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Peijie Jiang"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Chang Tang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00540v2",
                "updated": "2025-05-08T05:47:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    47,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2023-08-01T13:36:33Z",
                "published_parsed": [
                    2023,
                    8,
                    1,
                    13,
                    36,
                    33,
                    1,
                    213,
                    0
                ],
                "title": "Compressed Private Aggregation for Scalable and Robust Federated\n  Learning over Massive Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Private Aggregation for Scalable and Robust Federated\n  Learning over Massive Networks"
                },
                "summary": "Federated learning (FL) is an emerging paradigm that allows a central server\nto train machine learning models using remote users' data. Despite its growing\npopularity, FL faces challenges in preserving the privacy of local datasets,\nits sensitivity to poisoning attacks by malicious users, and its communication\noverhead. The latter is additionally considerably dominant in large-scale\nnetworks. These limitations are often individually mitigated by local\ndifferential privacy (LDP) mechanisms, robust aggregation, compression, and\nuser selection techniques, which typically come at the cost of accuracy. In\nthis work, we present compressed private aggregation (CPA), that allows massive\ndeployments to simultaneously communicate at extremely low bit rates while\nachieving privacy, anonymity, and resilience to malicious users. CPA randomizes\na codebook for compressing the data into a few bits using nested lattice\nquantizers, while ensuring anonymity and robustness, with a subsequent\nperturbation to hold LDP. The proposed CPA is proven to result in FL\nconvergence in the same asymptotic rate as FL without privacy, compression, and\nrobustness considerations, while satisfying both anonymity and LDP\nrequirements. These analytical properties are empirically confirmed in a\nnumerical study, where we demonstrate the performance gains of CPA compared\nwith separate mechanisms for compression and privacy for training different\nimage classification models, as well as its robustness in mitigating the\nharmful effects of malicious users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is an emerging paradigm that allows a central server\nto train machine learning models using remote users' data. Despite its growing\npopularity, FL faces challenges in preserving the privacy of local datasets,\nits sensitivity to poisoning attacks by malicious users, and its communication\noverhead. The latter is additionally considerably dominant in large-scale\nnetworks. These limitations are often individually mitigated by local\ndifferential privacy (LDP) mechanisms, robust aggregation, compression, and\nuser selection techniques, which typically come at the cost of accuracy. In\nthis work, we present compressed private aggregation (CPA), that allows massive\ndeployments to simultaneously communicate at extremely low bit rates while\nachieving privacy, anonymity, and resilience to malicious users. CPA randomizes\na codebook for compressing the data into a few bits using nested lattice\nquantizers, while ensuring anonymity and robustness, with a subsequent\nperturbation to hold LDP. The proposed CPA is proven to result in FL\nconvergence in the same asymptotic rate as FL without privacy, compression, and\nrobustness considerations, while satisfying both anonymity and LDP\nrequirements. These analytical properties are empirically confirmed in a\nnumerical study, where we demonstrate the performance gains of CPA compared\nwith separate mechanisms for compression and privacy for training different\nimage classification models, as well as its robustness in mitigating the\nharmful effects of malicious users."
                },
                "authors": [
                    {
                        "name": "Natalie Lang"
                    },
                    {
                        "name": "Nir Shlezinger"
                    },
                    {
                        "name": "Rafael G. L. D'Oliveira"
                    },
                    {
                        "name": "Salim El Rouayheb"
                    }
                ],
                "author_detail": {
                    "name": "Salim El Rouayheb"
                },
                "author": "Salim El Rouayheb",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2208.10888",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04960v1",
                "updated": "2025-05-08T05:42:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    42,
                    22,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T05:42:22Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    42,
                    22,
                    3,
                    128,
                    0
                ],
                "title": "Learning Item Representations Directly from Multimodal Features for\n  Effective Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Item Representations Directly from Multimodal Features for\n  Effective Recommendation"
                },
                "summary": "Conventional multimodal recommender systems predominantly leverage Bayesian\nPersonalized Ranking (BPR) optimization to learn item representations by\namalgamating item identity (ID) embeddings with multimodal features.\nNevertheless, our empirical and theoretical findings unequivocally demonstrate\na pronounced optimization gradient bias in favor of acquiring representations\nfrom multimodal features over item ID embeddings. As a consequence, item ID\nembeddings frequently exhibit suboptimal characteristics despite the\nconvergence of multimodal feature parameters. Given the rich informational\ncontent inherent in multimodal features, in this paper, we propose a novel\nmodel (i.e., LIRDRec) that learns item representations directly from these\nfeatures to augment recommendation performance. Recognizing that features\nderived from each modality may capture disparate yet correlated aspects of\nitems, we propose a multimodal transformation mechanism, integrated with\nmodality-specific encoders, to effectively fuse features from all modalities.\nMoreover, to differentiate the influence of diverse modality types, we devise a\nprogressive weight copying fusion module within LIRDRec. This module\nincrementally learns the weight assigned to each modality in synthesizing the\nfinal user or item representations. Finally, we utilize the powerful visual\nunderstanding of Multimodal Large Language Models (MLLMs) to convert the item\nimages into texts and extract semantics embeddings upon the texts via LLMs.\nEmpirical evaluations conducted on five real-world datasets validate the\nsuperiority of our approach relative to competing baselines. It is worth noting\nthe proposed model, equipped with embeddings extracted from MLLMs and LLMs, can\nfurther improve the recommendation accuracy of NDCG@20 by an average of 4.21%\ncompared to the original embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional multimodal recommender systems predominantly leverage Bayesian\nPersonalized Ranking (BPR) optimization to learn item representations by\namalgamating item identity (ID) embeddings with multimodal features.\nNevertheless, our empirical and theoretical findings unequivocally demonstrate\na pronounced optimization gradient bias in favor of acquiring representations\nfrom multimodal features over item ID embeddings. As a consequence, item ID\nembeddings frequently exhibit suboptimal characteristics despite the\nconvergence of multimodal feature parameters. Given the rich informational\ncontent inherent in multimodal features, in this paper, we propose a novel\nmodel (i.e., LIRDRec) that learns item representations directly from these\nfeatures to augment recommendation performance. Recognizing that features\nderived from each modality may capture disparate yet correlated aspects of\nitems, we propose a multimodal transformation mechanism, integrated with\nmodality-specific encoders, to effectively fuse features from all modalities.\nMoreover, to differentiate the influence of diverse modality types, we devise a\nprogressive weight copying fusion module within LIRDRec. This module\nincrementally learns the weight assigned to each modality in synthesizing the\nfinal user or item representations. Finally, we utilize the powerful visual\nunderstanding of Multimodal Large Language Models (MLLMs) to convert the item\nimages into texts and extract semantics embeddings upon the texts via LLMs.\nEmpirical evaluations conducted on five real-world datasets validate the\nsuperiority of our approach relative to competing baselines. It is worth noting\nthe proposed model, equipped with embeddings extracted from MLLMs and LLMs, can\nfurther improve the recommendation accuracy of NDCG@20 by an average of 4.21%\ncompared to the original embeddings."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xiaoxiong Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zhiqi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Shen"
                },
                "author": "Zhiqi Shen",
                "arxiv_comment": "Code: https://github.com/enoche/LIRDRec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04955v1",
                "updated": "2025-05-08T05:32:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    32,
                    36,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T05:32:36Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    32,
                    36,
                    3,
                    128,
                    0
                ],
                "title": "Chain-of-Thought Tokens are Computer Program Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Tokens are Computer Program Variables"
                },
                "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables."
                },
                "authors": [
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02665v2",
                "updated": "2025-05-08T05:27:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    27,
                    18,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-05T14:14:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    14,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law"
                },
                "summary": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems."
                },
                "authors": [
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Wenkai Ji"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Shilian Chen"
                    },
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan Wu"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13199v3",
                "updated": "2025-05-08T05:10:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    10,
                    46,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-14T21:10:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    21,
                    10,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency,\n  and Ethics in Vision-Language Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency,\n  and Ethics in Vision-Language Tasks"
                },
                "summary": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework."
                },
                "authors": [
                    {
                        "name": "Mohammad Saleh"
                    },
                    {
                        "name": "Azadeh Tabatabaei"
                    }
                ],
                "author_detail": {
                    "name": "Azadeh Tabatabaei"
                },
                "author": "Azadeh Tabatabaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04948v1",
                "updated": "2025-05-08T05:01:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    1,
                    44,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T05:01:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    5,
                    1,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized\n  Recommendations"
                },
                "summary": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking."
                },
                "authors": [
                    {
                        "name": "Md Aminul Islam"
                    },
                    {
                        "name": "Ahmed Sayeed Faruk"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Sayeed Faruk"
                },
                "author": "Ahmed Sayeed Faruk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22688v2",
                "updated": "2025-05-08T04:56:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    4,
                    56,
                    5,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-05T09:47:02Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    9,
                    47,
                    2,
                    2,
                    64,
                    0
                ],
                "title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large\n  Language Models in Interactive Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large\n  Language Models in Interactive Code Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions,\noffering limited insight into their capabilities to generate code that strictly\nfollows users' instructions, especially in multi-turn interaction scenarios. In\nthis paper, we introduce CodeIF-Bench, a benchmark for evaluating LLMs'\ninstruction-following capabilities in interactive code generation.\nSpecifically, CodeIF-Bench incorporates nine types of verifiable instructions\naligned with the real-world software development requirements, which can be\nindependently and objectively validated through specified test cases,\nfacilitating the evaluation of instruction-following capability in multi-turn\ninteractions. We evaluate nine prominent LLMs using CodeIF-Bench, and the\nexperimental results reveal a significant disparity between their basic\nprogramming capability and instruction-following capability, particularly as\ntask complexity, context length, and the number of dialogue rounds increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions,\noffering limited insight into their capabilities to generate code that strictly\nfollows users' instructions, especially in multi-turn interaction scenarios. In\nthis paper, we introduce CodeIF-Bench, a benchmark for evaluating LLMs'\ninstruction-following capabilities in interactive code generation.\nSpecifically, CodeIF-Bench incorporates nine types of verifiable instructions\naligned with the real-world software development requirements, which can be\nindependently and objectively validated through specified test cases,\nfacilitating the evaluation of instruction-following capability in multi-turn\ninteractions. We evaluate nine prominent LLMs using CodeIF-Bench, and the\nexperimental results reveal a significant disparity between their basic\nprogramming capability and instruction-following capability, particularly as\ntask complexity, context length, and the number of dialogue rounds increase."
                },
                "authors": [
                    {
                        "name": "Peiding Wang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Minxiao Li"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "An Fu"
                    }
                ],
                "author_detail": {
                    "name": "An Fu"
                },
                "author": "An Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]