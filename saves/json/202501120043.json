[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v1",
                "updated": "2025-01-09T06:00:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v1",
                "updated": "2025-01-04T20:59:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04012v1",
                "updated": "2024-12-18T00:35:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T00:35:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "FlexCache: Flexible Approximate Cache System for Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexCache: Flexible Approximate Cache System for Video Diffusion"
                },
                "summary": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Henry Tian"
                    },
                    {
                        "name": "Tim Lu"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.05452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05452v1",
                "updated": "2025-01-09T18:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding"
                },
                "summary": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Minqian Liu"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "John Corring"
                    },
                    {
                        "name": "Yijuan Lu"
                    },
                    {
                        "name": "Jianwei Yang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Dinei Florencio"
                    },
                    {
                        "name": "Cha Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cha Zhang"
                },
                "author": "Cha Zhang",
                "arxiv_comment": "Project link: https://zeyofu.github.io/ReFocus/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05453v1",
                "updated": "2025-01-09T18:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "An Empirical Study of Autoregressive Pre-training from Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Autoregressive Pre-training from Videos"
                },
                "summary": "We empirically study autoregressive pre-training from videos. To perform our\nstudy, we construct a series of autoregressive video models, called Toto. We\ntreat videos as sequences of visual tokens and train transformer models to\nautoregressively predict future tokens. Our models are pre-trained on a diverse\ndataset of videos and images comprising over 1 trillion visual tokens. We\nexplore different architectural, training, and inference design choices. We\nevaluate the learned visual representations on a range of downstream tasks\nincluding image recognition, video classification, object tracking, and\nrobotics. Our results demonstrate that, despite minimal inductive biases,\nautoregressive pre-training leads to competitive performance across all\nbenchmarks. Finally, we find that scaling our video models results in similar\nscaling curves to those seen in language models, albeit with a different rate.\nMore details at https://brjathu.github.io/toto/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We empirically study autoregressive pre-training from videos. To perform our\nstudy, we construct a series of autoregressive video models, called Toto. We\ntreat videos as sequences of visual tokens and train transformer models to\nautoregressively predict future tokens. Our models are pre-trained on a diverse\ndataset of videos and images comprising over 1 trillion visual tokens. We\nexplore different architectural, training, and inference design choices. We\nevaluate the learned visual representations on a range of downstream tasks\nincluding image recognition, video classification, object tracking, and\nrobotics. Our results demonstrate that, despite minimal inductive biases,\nautoregressive pre-training leads to competitive performance across all\nbenchmarks. Finally, we find that scaling our video models results in similar\nscaling curves to those seen in language models, albeit with a different rate.\nMore details at https://brjathu.github.io/toto/"
                },
                "authors": [
                    {
                        "name": "Jathushan Rajasegaran"
                    },
                    {
                        "name": "Ilija Radosavovic"
                    },
                    {
                        "name": "Rahul Ravishankar"
                    },
                    {
                        "name": "Yossi Gandelsman"
                    },
                    {
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "name": "Jitendra Malik"
                    }
                ],
                "author_detail": {
                    "name": "Jitendra Malik"
                },
                "author": "Jitendra Malik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05450v1",
                "updated": "2025-01-09T18:59:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    56,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:59:56Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    56,
                    3,
                    9,
                    0
                ],
                "title": "Decentralized Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Diffusion Models"
                },
                "summary": "Large-scale AI model training divides work across thousands of GPUs, then\nsynchronizes gradients across them at each step. This incurs a significant\nnetwork burden that only centralized, monolithic clusters can support, driving\nup infrastructure costs and straining power systems. We propose Decentralized\nDiffusion Models, a scalable framework for distributing diffusion model\ntraining across independent clusters or datacenters by eliminating the\ndependence on a centralized, high-bandwidth networking fabric. Our method\ntrains a set of expert diffusion models over partitions of the dataset, each in\nfull isolation from one another. At inference time, the experts ensemble\nthrough a lightweight router. We show that the ensemble collectively optimizes\nthe same objective as a single model trained over the whole dataset. This means\nwe can divide the training burden among a number of \"compute islands,\" lowering\ninfrastructure costs and improving resilience to localized GPU failures.\nDecentralized diffusion models empower researchers to take advantage of\nsmaller, more cost-effective and more readily available compute like on-demand\nGPU nodes rather than central integrated systems. We conduct extensive\nexperiments on ImageNet and LAION Aesthetics, showing that decentralized\ndiffusion models FLOP-for-FLOP outperform standard diffusion models. We finally\nscale our approach to 24 billion parameters, demonstrating that high-quality\ndiffusion models can now be trained with just eight individual GPU nodes in\nless than a week.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale AI model training divides work across thousands of GPUs, then\nsynchronizes gradients across them at each step. This incurs a significant\nnetwork burden that only centralized, monolithic clusters can support, driving\nup infrastructure costs and straining power systems. We propose Decentralized\nDiffusion Models, a scalable framework for distributing diffusion model\ntraining across independent clusters or datacenters by eliminating the\ndependence on a centralized, high-bandwidth networking fabric. Our method\ntrains a set of expert diffusion models over partitions of the dataset, each in\nfull isolation from one another. At inference time, the experts ensemble\nthrough a lightweight router. We show that the ensemble collectively optimizes\nthe same objective as a single model trained over the whole dataset. This means\nwe can divide the training burden among a number of \"compute islands,\" lowering\ninfrastructure costs and improving resilience to localized GPU failures.\nDecentralized diffusion models empower researchers to take advantage of\nsmaller, more cost-effective and more readily available compute like on-demand\nGPU nodes rather than central integrated systems. We conduct extensive\nexperiments on ImageNet and LAION Aesthetics, showing that decentralized\ndiffusion models FLOP-for-FLOP outperform standard diffusion models. We finally\nscale our approach to 24 billion parameters, demonstrating that high-quality\ndiffusion models can now be trained with just eight individual GPU nodes in\nless than a week."
                },
                "authors": [
                    {
                        "name": "David McAllister"
                    },
                    {
                        "name": "Matthew Tancik"
                    },
                    {
                        "name": "Jiaming Song"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ],
                "author_detail": {
                    "name": "Angjoo Kanazawa"
                },
                "author": "Angjoo Kanazawa",
                "arxiv_comment": "Project webpage: https://decentralizeddiffusion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05443v1",
                "updated": "2025-01-09T18:55:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    55,
                    50,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:55:50Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    55,
                    50,
                    3,
                    9,
                    0
                ],
                "title": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models"
                },
                "summary": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it."
                },
                "authors": [
                    {
                        "name": "Jose A. Diaz-Garcia"
                    },
                    {
                        "name": "Joao Paulo Carvalho"
                    }
                ],
                "author_detail": {
                    "name": "Joao Paulo Carvalho"
                },
                "author": "Joao Paulo Carvalho",
                "arxiv_comment": "37 pages, under review in WIREs Data Mining and Knowledge Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05435v1",
                "updated": "2025-01-09T18:48:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    48,
                    35,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:48:35Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    48,
                    35,
                    3,
                    9,
                    0
                ],
                "title": "Neuro-Symbolic AI in 2024: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic AI in 2024: A Systematic Review"
                },
                "summary": "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems."
                },
                "authors": [
                    {
                        "name": "Brandon C. Colelough"
                    },
                    {
                        "name": "William Regli"
                    }
                ],
                "author_detail": {
                    "name": "William Regli"
                },
                "author": "William Regli",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05431v1",
                "updated": "2025-01-09T18:43:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    43,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:43:37Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    43,
                    37,
                    3,
                    9,
                    0
                ],
                "title": "A Disintegrating Rocky Planet with Prominent Comet-like Tails Around a\n  Bright Star",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Disintegrating Rocky Planet with Prominent Comet-like Tails Around a\n  Bright Star"
                },
                "summary": "We report the discovery of BD+05$\\,$4868$\\,$Ab, a transiting exoplanet\norbiting a bright ($V=10.16$) K-dwarf (TIC 466376085) with a period of 1.27\ndays. Observations from NASA's Transiting Exoplanet Survey Satellite (TESS)\nreveal variable transit depths and asymmetric transit profiles that are\ncharacteristic of comet-like tails formed by dusty effluents emanating from a\ndisintegrating planet. Unique to BD+05$\\,$4868$\\,$Ab is the presence of\nprominent dust tails in both the trailing and leading directions that\ncontribute to the extinction of starlight from the host star. By fitting the\nobserved transit profile and analytically modeling the drift of dust grains\nwithin both dust tails, we infer large grain sizes ($\\sim1-10\\,\\mu$m) and a\nmass loss rate of $10\\,M_{\\rm \\oplus}\\,$Gyr$^{-1}$, suggestive of a lunar-mass\nobject with a disintegration timescale of only several Myr. The host star is\nprobably older than the Sun and is accompanied by an M-dwarf companion at a\nprojected physical separation of 130 AU. The brightness of the host star,\ncombined with the planet's relatively deep transits ($0.8-2.0\\%$), presents\nBD+05$\\,$4868$\\,$Ab as a prime target for compositional studies of rocky\nexoplanets and investigations into the nature of catastrophically evaporating\nplanets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the discovery of BD+05$\\,$4868$\\,$Ab, a transiting exoplanet\norbiting a bright ($V=10.16$) K-dwarf (TIC 466376085) with a period of 1.27\ndays. Observations from NASA's Transiting Exoplanet Survey Satellite (TESS)\nreveal variable transit depths and asymmetric transit profiles that are\ncharacteristic of comet-like tails formed by dusty effluents emanating from a\ndisintegrating planet. Unique to BD+05$\\,$4868$\\,$Ab is the presence of\nprominent dust tails in both the trailing and leading directions that\ncontribute to the extinction of starlight from the host star. By fitting the\nobserved transit profile and analytically modeling the drift of dust grains\nwithin both dust tails, we infer large grain sizes ($\\sim1-10\\,\\mu$m) and a\nmass loss rate of $10\\,M_{\\rm \\oplus}\\,$Gyr$^{-1}$, suggestive of a lunar-mass\nobject with a disintegration timescale of only several Myr. The host star is\nprobably older than the Sun and is accompanied by an M-dwarf companion at a\nprojected physical separation of 130 AU. The brightness of the host star,\ncombined with the planet's relatively deep transits ($0.8-2.0\\%$), presents\nBD+05$\\,$4868$\\,$Ab as a prime target for compositional studies of rocky\nexoplanets and investigations into the nature of catastrophically evaporating\nplanets."
                },
                "authors": [
                    {
                        "name": "Marc Hon"
                    },
                    {
                        "name": "Saul Rappaport"
                    },
                    {
                        "name": "Avi Shporer"
                    },
                    {
                        "name": "Andrew Vanderburg"
                    },
                    {
                        "name": "Karen A. Collins"
                    },
                    {
                        "name": "Cristilyn N. Watkins"
                    },
                    {
                        "name": "Richard P. Schwarz"
                    },
                    {
                        "name": "Khalid Barkaoui"
                    },
                    {
                        "name": "Samuel W. Yee"
                    },
                    {
                        "name": "Joshua N. Winn"
                    },
                    {
                        "name": "Alex S. Polanski"
                    },
                    {
                        "name": "Emily A. Gilbert"
                    },
                    {
                        "name": "David R. Ciardi"
                    },
                    {
                        "name": "Jeroen Audenaert"
                    },
                    {
                        "name": "William Fong"
                    },
                    {
                        "name": "Jack Haviland"
                    },
                    {
                        "name": "Katharine Hesse"
                    },
                    {
                        "name": "Daniel Muthukrishna"
                    },
                    {
                        "name": "Glen Petitpas"
                    },
                    {
                        "name": "Ellie Hadjiyska Schmelzer"
                    },
                    {
                        "name": "Norio Narita"
                    },
                    {
                        "name": "Akihiko Fukui"
                    },
                    {
                        "name": "Sara Seager"
                    },
                    {
                        "name": "George R. Ricker"
                    }
                ],
                "author_detail": {
                    "name": "George R. Ricker"
                },
                "author": "George R. Ricker",
                "arxiv_comment": "24 pages, 17 figures. Submitted to AAS Journals",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08405v2",
                "updated": "2025-01-09T18:43:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    43,
                    18,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-10T22:38:26Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    22,
                    38,
                    26,
                    3,
                    284,
                    0
                ],
                "title": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning"
                },
                "summary": "Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT."
                },
                "authors": [
                    {
                        "name": "Muhammad Awais"
                    },
                    {
                        "name": "Ali Husain Salem Abdulla Alharthi"
                    },
                    {
                        "name": "Amandeep Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted at WACV, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2011.04168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2011.04168v4",
                "updated": "2025-01-09T18:37:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    37,
                    45,
                    3,
                    9,
                    0
                ],
                "published": "2020-11-09T03:28:22Z",
                "published_parsed": [
                    2020,
                    11,
                    9,
                    3,
                    28,
                    22,
                    0,
                    314,
                    0
                ],
                "title": "Likelihood Inference for Possibly Non-Stationary Processes via Adaptive\n  Overdifferencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood Inference for Possibly Non-Stationary Processes via Adaptive\n  Overdifferencing"
                },
                "summary": "We make an observation that facilitates exact likelihood-based inference for\nthe parameters of the popular ARFIMA model without requiring stationarity by\nallowing the upper bound $\\bar{d}$ for the memory parameter $d$ to exceed\n$0.5$: estimating the parameters of a single non-stationary ARFIMA model is\nequivalent to estimating the parameters of a sequence of stationary ARFIMA\nmodels. This allows for the use of existing methods for evaluating the\nlikelihood for an invertible and stationary ARFIMA model. This enables improved\ninference because many standard methods perform poorly when estimates are close\nto the boundary of the parameter space. It also allows us to leverage the\nwealth of likelihood approximations that have been introduced for estimating\nthe parameters of a stationary process. We explore how estimation of the memory\nparameter $d$ depends on the upper bound $\\bar{d}$ and introduce adaptive\nprocedures for choosing $\\bar{d}$. We show via simulation how our adaptive\nprocedures estimate the memory parameter well, relative to existing\nalternatives, when the true value is as large as 2.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We make an observation that facilitates exact likelihood-based inference for\nthe parameters of the popular ARFIMA model without requiring stationarity by\nallowing the upper bound $\\bar{d}$ for the memory parameter $d$ to exceed\n$0.5$: estimating the parameters of a single non-stationary ARFIMA model is\nequivalent to estimating the parameters of a sequence of stationary ARFIMA\nmodels. This allows for the use of existing methods for evaluating the\nlikelihood for an invertible and stationary ARFIMA model. This enables improved\ninference because many standard methods perform poorly when estimates are close\nto the boundary of the parameter space. It also allows us to leverage the\nwealth of likelihood approximations that have been introduced for estimating\nthe parameters of a stationary process. We explore how estimation of the memory\nparameter $d$ depends on the upper bound $\\bar{d}$ and introduce adaptive\nprocedures for choosing $\\bar{d}$. We show via simulation how our adaptive\nprocedures estimate the memory parameter well, relative to existing\nalternatives, when the true value is as large as 2.5."
                },
                "authors": [
                    {
                        "name": "Maryclare Griffin"
                    },
                    {
                        "name": "Gennady Samorodnitsky"
                    },
                    {
                        "name": "David S. Matteson"
                    }
                ],
                "author_detail": {
                    "name": "David S. Matteson"
                },
                "author": "David S. Matteson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2011.04168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2011.04168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05423v1",
                "updated": "2025-01-09T18:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    30,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    30,
                    14,
                    3,
                    9,
                    0
                ],
                "title": "Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese\n  Micro-bloggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese\n  Micro-bloggers"
                },
                "summary": "Studying public sentiment during crises is crucial for understanding how\nopinions and sentiments shift, resulting in polarized societies. We study\nWeibo, the most popular microblogging site in China, using posts made during\nthe outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19\nstage, the outbreak stage, and the early stage of epidemic prevention. We use\nLlama 3 8B, a Large Language Model, to analyze users' sentiments on the\nplatform by classifying them into positive, negative, sarcastic, and neutral\ncategories. Analyzing sentiment shifts on Weibo provides insights into how\nsocial events and government actions influence public opinion. This study\ncontributes to understanding the dynamics of social sentiments during health\ncrises, fulfilling a gap in sentiment analysis for Chinese platforms. By\nexamining these dynamics, we aim to offer valuable perspectives on digital\ncommunication's role in shaping society's responses during unprecedented global\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying public sentiment during crises is crucial for understanding how\nopinions and sentiments shift, resulting in polarized societies. We study\nWeibo, the most popular microblogging site in China, using posts made during\nthe outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19\nstage, the outbreak stage, and the early stage of epidemic prevention. We use\nLlama 3 8B, a Large Language Model, to analyze users' sentiments on the\nplatform by classifying them into positive, negative, sarcastic, and neutral\ncategories. Analyzing sentiment shifts on Weibo provides insights into how\nsocial events and government actions influence public opinion. This study\ncontributes to understanding the dynamics of social sentiments during health\ncrises, fulfilling a gap in sentiment analysis for Chinese platforms. By\nexamining these dynamics, we aim to offer valuable perspectives on digital\ncommunication's role in shaping society's responses during unprecedented global\nchallenges."
                },
                "authors": [
                    {
                        "name": "Jerry Chongyi Hu"
                    },
                    {
                        "name": "Mohammed Shahid Modi"
                    },
                    {
                        "name": "Boleslaw K. Szymanski"
                    }
                ],
                "author_detail": {
                    "name": "Boleslaw K. Szymanski"
                },
                "author": "Boleslaw K. Szymanski",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05396v1",
                "updated": "2025-01-09T17:42:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    42,
                    23,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T17:42:23Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    42,
                    23,
                    3,
                    9,
                    0
                ],
                "title": "FairCode: Evaluating Social Bias of LLMs in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairCode: Evaluating Social Bias of LLMs in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capability in code\ngeneration, drawing increasing attention to the evaluation of the quality and\nsafety of their outputs. However, research on bias in code generation remains\nlimited. Existing studies typically assess bias by applying malicious prompts\nor reapply tasks and dataset for discriminative models. Given that LLMs are\noften aligned with human values and that prior datasets are not fully optimized\nfor code-related tasks, there is a pressing need for benchmarks specifically\ndesigned for evaluating code models. In this study, we introduce FairCode, a\nnovel benchmark for evaluating bias in code generation. FairCode comprises two\ntasks: function implementation and test case generation, each evaluating social\nbias through diverse scenarios. Additionally, we propose a new metric,\nFairScore, to assess model performance on this benchmark. We conduct\nexperiments on widely used LLMs and provide a comprehensive analysis of the\nresults. The findings reveal that all tested LLMs exhibit bias. The code is\navailable at https://github.com/YongkDu/FairCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capability in code\ngeneration, drawing increasing attention to the evaluation of the quality and\nsafety of their outputs. However, research on bias in code generation remains\nlimited. Existing studies typically assess bias by applying malicious prompts\nor reapply tasks and dataset for discriminative models. Given that LLMs are\noften aligned with human values and that prior datasets are not fully optimized\nfor code-related tasks, there is a pressing need for benchmarks specifically\ndesigned for evaluating code models. In this study, we introduce FairCode, a\nnovel benchmark for evaluating bias in code generation. FairCode comprises two\ntasks: function implementation and test case generation, each evaluating social\nbias through diverse scenarios. Additionally, we propose a new metric,\nFairScore, to assess model performance on this benchmark. We conduct\nexperiments on widely used LLMs and provide a comprehensive analysis of the\nresults. The findings reveal that all tested LLMs exhibit bias. The code is\navailable at https://github.com/YongkDu/FairCode."
                },
                "authors": [
                    {
                        "name": "Yongkang Du"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11672v2",
                "updated": "2025-01-09T17:18:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    18,
                    12,
                    3,
                    9,
                    0
                ],
                "published": "2024-04-17T18:13:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    13,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"
                },
                "summary": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05382v1",
                "updated": "2025-01-09T17:11:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    11,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T17:11:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    11,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models"
                },
                "summary": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models."
                },
                "authors": [
                    {
                        "name": "Kristian G. Barman"
                    },
                    {
                        "name": "Sascha Caron"
                    },
                    {
                        "name": "Emily Sullivan"
                    },
                    {
                        "name": "Henk W. de Regt"
                    },
                    {
                        "name": "Roberto Ruiz de Austri"
                    },
                    {
                        "name": "Mieke Boon"
                    },
                    {
                        "name": "Michael Färber"
                    },
                    {
                        "name": "Stefan Fröse"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Andreas Ipp"
                    },
                    {
                        "name": "Rukshak Kapoor"
                    },
                    {
                        "name": "Gregor Kasieczka"
                    },
                    {
                        "name": "Daniel Kostić"
                    },
                    {
                        "name": "Michael Krämer"
                    },
                    {
                        "name": "Tobias Golling"
                    },
                    {
                        "name": "Luis G. Lopez"
                    },
                    {
                        "name": "Jesus Marco"
                    },
                    {
                        "name": "Sydney Otten"
                    },
                    {
                        "name": "Pawel Pawlowski"
                    },
                    {
                        "name": "Pietro Vischia"
                    },
                    {
                        "name": "Erik Weber"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05370v1",
                "updated": "2025-01-09T16:50:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    50,
                    16,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:50:16Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    50,
                    16,
                    3,
                    9,
                    0
                ],
                "title": "Accelerated Diffusion Models via Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Diffusion Models via Speculative Sampling"
                },
                "summary": "Speculative sampling is a popular technique for accelerating inference in\nLarge Language Models by generating candidate tokens using a fast draft model\nand accepting or rejecting them based on the target model's distribution. While\nspeculative sampling was previously limited to discrete sequences, we extend it\nto diffusion models, which generate samples via continuous, vector-valued\nMarkov chains. In this context, the target model is a high-quality but\ncomputationally expensive diffusion model. We propose various drafting\nstrategies, including a simple and effective approach that does not require\ntraining a draft model and is applicable out of the box to any diffusion model.\nOur experiments demonstrate significant generation speedup on various diffusion\nmodels, halving the number of function evaluations, while generating exact\nsamples from the target model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a popular technique for accelerating inference in\nLarge Language Models by generating candidate tokens using a fast draft model\nand accepting or rejecting them based on the target model's distribution. While\nspeculative sampling was previously limited to discrete sequences, we extend it\nto diffusion models, which generate samples via continuous, vector-valued\nMarkov chains. In this context, the target model is a high-quality but\ncomputationally expensive diffusion model. We propose various drafting\nstrategies, including a simple and effective approach that does not require\ntraining a draft model and is applicable out of the box to any diffusion model.\nOur experiments demonstrate significant generation speedup on various diffusion\nmodels, halving the number of function evaluations, while generating exact\nsamples from the target model."
                },
                "authors": [
                    {
                        "name": "Valentin De Bortoli"
                    },
                    {
                        "name": "Alexandre Galashov"
                    },
                    {
                        "name": "Arthur Gretton"
                    },
                    {
                        "name": "Arnaud Doucet"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Doucet"
                },
                "author": "Arnaud Doucet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17990v2",
                "updated": "2025-01-09T16:47:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    47,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-26T16:02:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models"
                },
                "summary": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Max Pellert"
                    },
                    {
                        "name": "David Garcia"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01428v3",
                "updated": "2025-01-09T16:41:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    41,
                    7,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-02T18:59:59Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    59,
                    3,
                    2,
                    0
                ],
                "title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models"
                },
                "summary": "In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding."
                },
                "authors": [
                    {
                        "name": "Zhangyang Qi"
                    },
                    {
                        "name": "Zhixiong Zhang"
                    },
                    {
                        "name": "Ye Fang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "Project page: https://gpt4scene.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20138v2",
                "updated": "2025-01-09T16:36:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    36,
                    26,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-28T12:54:06Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    12,
                    54,
                    6,
                    5,
                    363,
                    0
                ],
                "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TradingAgents: Multi-Agents LLM Financial Trading Framework"
                },
                "summary": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. More details on TradingAgents\nare available at https://TradingAgents-AI.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. More details on TradingAgents\nare available at https://TradingAgents-AI.github.io."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Edward Sun"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Multi-Agent AI in the Real World @ AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05338v1",
                "updated": "2025-01-09T16:09:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    9,
                    21,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:09:21Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    9,
                    21,
                    3,
                    9,
                    0
                ],
                "title": "Comparing latent inequality with ordinal data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing latent inequality with ordinal data"
                },
                "summary": "We propose new ways to compare two latent distributions when only ordinal\ndata are available and without imposing parametric assumptions on the\nunderlying continuous distributions. First, we contribute identification\nresults. We show how certain ordinal conditions provide evidence of\nbetween-group inequality, quantified by particular quantiles being higher in\none latent distribution than in the other. We also show how other ordinal\nconditions provide evidence of higher within-group inequality in one\ndistribution than in the other, quantified by particular interquantile ranges\nbeing wider in one latent distribution than in the other. Second, we propose an\n\"inner\" confidence set for the quantiles that are higher for the first latent\ndistribution. We also describe frequentist and Bayesian inference on features\nof the ordinal distributions relevant to our identification results. Our\ncontributions are illustrated by empirical examples with mental health and\ngeneral health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose new ways to compare two latent distributions when only ordinal\ndata are available and without imposing parametric assumptions on the\nunderlying continuous distributions. First, we contribute identification\nresults. We show how certain ordinal conditions provide evidence of\nbetween-group inequality, quantified by particular quantiles being higher in\none latent distribution than in the other. We also show how other ordinal\nconditions provide evidence of higher within-group inequality in one\ndistribution than in the other, quantified by particular interquantile ranges\nbeing wider in one latent distribution than in the other. Second, we propose an\n\"inner\" confidence set for the quantiles that are higher for the first latent\ndistribution. We also describe frequentist and Bayesian inference on features\nof the ordinal distributions relevant to our identification results. Our\ncontributions are illustrated by empirical examples with mental health and\ngeneral health."
                },
                "authors": [
                    {
                        "name": "David M. Kaplan"
                    },
                    {
                        "name": "Wei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhao"
                },
                "author": "Wei Zhao",
                "arxiv_doi": "10.1093/ectj/utac030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/ectj/utac030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted manuscript (post-peer review, pre-copyedited,\n  author-produced version)",
                "arxiv_journal_ref": "The Econometrics Journal 26 (2023) 189-214",
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05336v1",
                "updated": "2025-01-09T16:02:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    2,
                    51,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:02:51Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    2,
                    51,
                    3,
                    9,
                    0
                ],
                "title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction"
                },
                "summary": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model."
                },
                "authors": [
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "AAAI Alignment Track 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05323v1",
                "updated": "2025-01-09T15:48:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    48,
                    29,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:48:29Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    48,
                    29,
                    3,
                    9,
                    0
                ],
                "title": "Distributed Learning and Inference Systems: A Networking Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Learning and Inference Systems: A Networking Perspective"
                },
                "summary": "Machine learning models have achieved, and in some cases surpassed,\nhuman-level performance in various tasks, mainly through centralized training\nof static models and the use of large models stored in centralized clouds for\ninference. However, this centralized approach has several drawbacks, including\nprivacy concerns, high storage demands, a single point of failure, and\nsignificant computing requirements. These challenges have driven interest in\ndeveloping alternative decentralized and distributed methods for AI training\nand inference. Distribution introduces additional complexity, as it requires\nmanaging multiple moving parts. To address these complexities and fill a gap in\nthe development of distributed AI systems, this work proposes a novel\nframework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN).\nThe different components of DA-ITN and their functions are explored, and the\nassociated challenges and research areas are highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have achieved, and in some cases surpassed,\nhuman-level performance in various tasks, mainly through centralized training\nof static models and the use of large models stored in centralized clouds for\ninference. However, this centralized approach has several drawbacks, including\nprivacy concerns, high storage demands, a single point of failure, and\nsignificant computing requirements. These challenges have driven interest in\ndeveloping alternative decentralized and distributed methods for AI training\nand inference. Distribution introduces additional complexity, as it requires\nmanaging multiple moving parts. To address these complexities and fill a gap in\nthe development of distributed AI systems, this work proposes a novel\nframework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN).\nThe different components of DA-ITN and their functions are explored, and the\nassociated challenges and research areas are highlighted."
                },
                "authors": [
                    {
                        "name": "Hesham G. Moussa"
                    },
                    {
                        "name": "Arashmid Akhavain"
                    },
                    {
                        "name": "S. Maryam Hosseini"
                    },
                    {
                        "name": "Bill McCormick"
                    }
                ],
                "author_detail": {
                    "name": "Bill McCormick"
                },
                "author": "Bill McCormick",
                "arxiv_comment": "This paper has been submitted to IEEE Network magazine and is still\n  under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07614v3",
                "updated": "2025-01-09T15:48:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    48,
                    23,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-11T20:54:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    54,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching"
                },
                "summary": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/ ."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "Accepted by ICASSP 2025, camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05322v1",
                "updated": "2025-01-09T15:45:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    45,
                    28,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:45:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    45,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining\n  the Actions of Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining\n  the Actions of Autonomous Vehicles"
                },
                "summary": "Public distrust of self-driving cars is growing. Studies emphasize the need\nfor interpreting the behavior of these vehicles to passengers to promote trust\nin autonomous systems. Interpreters can enhance trust by improving transparency\nand reducing perceived risk. However, current solutions often lack a\nhuman-centric approach to integrating multimodal interpretations. This paper\nintroduces a novel Human-centered Multimodal Interpreter (HMI) system that\nleverages human preferences to provide visual, textual, and auditory feedback.\nThe system combines a visual interface with Bird's Eye View (BEV), map, and\ntext display, along with voice interaction using a fine-tuned large language\nmodel (LLM). Our user study, involving diverse participants, demonstrated that\nthe HMI system significantly boosts passenger trust in AVs, increasing average\ntrust levels by over 8%, with trust in ordinary environments rising by up to\n30%. These results underscore the potential of the HMI system to improve the\nacceptance and reliability of autonomous vehicles by providing clear,\nreal-time, and context-sensitive explanations of vehicle actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public distrust of self-driving cars is growing. Studies emphasize the need\nfor interpreting the behavior of these vehicles to passengers to promote trust\nin autonomous systems. Interpreters can enhance trust by improving transparency\nand reducing perceived risk. However, current solutions often lack a\nhuman-centric approach to integrating multimodal interpretations. This paper\nintroduces a novel Human-centered Multimodal Interpreter (HMI) system that\nleverages human preferences to provide visual, textual, and auditory feedback.\nThe system combines a visual interface with Bird's Eye View (BEV), map, and\ntext display, along with voice interaction using a fine-tuned large language\nmodel (LLM). Our user study, involving diverse participants, demonstrated that\nthe HMI system significantly boosts passenger trust in AVs, increasing average\ntrust levels by over 8%, with trust in ordinary environments rising by up to\n30%. These results underscore the potential of the HMI system to improve the\nacceptance and reliability of autonomous vehicles by providing clear,\nreal-time, and context-sensitive explanations of vehicle actions."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Ruiqi Chen"
                    },
                    {
                        "name": "Rishikesh Panda"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Shuyun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuyun Zhang"
                },
                "author": "Shuyun Zhang",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop HAVI\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15978v3",
                "updated": "2025-01-09T15:42:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    42,
                    56,
                    3,
                    9,
                    0
                ],
                "published": "2024-01-29T09:08:01Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    9,
                    8,
                    1,
                    0,
                    29,
                    0
                ],
                "title": "Multilevel Markov Chain Monte Carlo with likelihood scaling for Bayesian\n  inversion with high-resolution observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel Markov Chain Monte Carlo with likelihood scaling for Bayesian\n  inversion with high-resolution observations"
                },
                "summary": "We propose a multilevel Markov chain Monte Carlo (MCMC) method for the\nBayesian inference of random field parameters in PDEs using high-resolution\ndata. Compared to existing multilevel MCMC methods, we additionally consider\nlevel-dependent data resolution and introduce a suitable likelihood scaling to\nenable consistent cross-level comparisons. We theoretically show that this\napproach attains the same convergence rates as when using level-independent\ntreatment of data, but at significantly reduced computational cost. The\nconvergence analysis focuses on Lipschitz continuous transformations of\nGaussian random fields with Mat\\'ern covariance structure. These results are\nillustrated using numerical experiments for a 2D plane stress problem, where\nthe Young's modulus is estimated from discretisations of the displacement\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a multilevel Markov chain Monte Carlo (MCMC) method for the\nBayesian inference of random field parameters in PDEs using high-resolution\ndata. Compared to existing multilevel MCMC methods, we additionally consider\nlevel-dependent data resolution and introduce a suitable likelihood scaling to\nenable consistent cross-level comparisons. We theoretically show that this\napproach attains the same convergence rates as when using level-independent\ntreatment of data, but at significantly reduced computational cost. The\nconvergence analysis focuses on Lipschitz continuous transformations of\nGaussian random fields with Mat\\'ern covariance structure. These results are\nillustrated using numerical experiments for a 2D plane stress problem, where\nthe Young's modulus is estimated from discretisations of the displacement\nfield."
                },
                "authors": [
                    {
                        "name": "Pieter Vanmechelen"
                    },
                    {
                        "name": "Geert Lombaert"
                    },
                    {
                        "name": "Giovanni Samaey"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Samaey"
                },
                "author": "Giovanni Samaey",
                "arxiv_comment": "25 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 35R60, 65C40 (Primary), 62M05, 65C05, 65N30 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05313v1",
                "updated": "2025-01-09T15:29:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    29,
                    33,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:29:33Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    29,
                    33,
                    3,
                    9,
                    0
                ],
                "title": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing"
                },
                "summary": "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%."
                },
                "authors": [
                    {
                        "name": "Mengfan Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03982v2",
                "updated": "2025-01-09T15:27:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-07T18:39:28Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    39,
                    28,
                    1,
                    7,
                    0
                ],
                "title": "Sequentializing a Test: Anytime Validity is Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequentializing a Test: Anytime Validity is Free"
                },
                "summary": "An anytime valid sequential test permits us to peek at observations as they\narrive. This means we can stop, continue or adapt the testing process based on\nthe current data, without invalidating the inference. Given a maximum number of\nobservations $N$, one may believe that this benefit must be paid for in terms\nof power when compared to a conventional test that waits until all $N$\nobservations have arrived. Our key contribution is to show that this is false:\nfor any valid test based on $N$ observations, we derive an anytime valid\nsequential test that matches it after $N$ observations. In addition, we show\nthat the value of the sequential test before a rejection is attained can be\ndirectly used as a significance level for a subsequent test. We illustrate this\nfor the $z$-test. There, we find that the current state-of-the-art based on\nlog-optimal $e$-values can be obtained as a special limiting case that\nreplicates a $z$-test with level $\\alpha \\to 0$ as $N \\to \\infty$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An anytime valid sequential test permits us to peek at observations as they\narrive. This means we can stop, continue or adapt the testing process based on\nthe current data, without invalidating the inference. Given a maximum number of\nobservations $N$, one may believe that this benefit must be paid for in terms\nof power when compared to a conventional test that waits until all $N$\nobservations have arrived. Our key contribution is to show that this is false:\nfor any valid test based on $N$ observations, we derive an anytime valid\nsequential test that matches it after $N$ observations. In addition, we show\nthat the value of the sequential test before a rejection is attained can be\ndirectly used as a significance level for a subsequent test. We illustrate this\nfor the $z$-test. There, we find that the current state-of-the-art based on\nlog-optimal $e$-values can be obtained as a special limiting case that\nreplicates a $z$-test with level $\\alpha \\to 0$ as $N \\to \\infty$."
                },
                "authors": [
                    {
                        "name": "Nick W. Koning"
                    },
                    {
                        "name": "Sam van Meer"
                    }
                ],
                "author_detail": {
                    "name": "Sam van Meer"
                },
                "author": "Sam van Meer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16220v3",
                "updated": "2025-01-09T14:55:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    55,
                    29,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-18T10:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    56,
                    40,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Attention Graph Neural Networks for Inferring Gene Regulatory\n  Networks with Skewed Degree Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Attention Graph Neural Networks for Inferring Gene Regulatory\n  Networks with Skewed Degree Distribution"
                },
                "summary": "Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a\npivotal challenge in systems biology, and several innovative computational\nmethods have been introduced. However, most of these studies have not\nconsidered the skewed degree distribution of genes. Specifically, some genes\nmay regulate multiple target genes while some genes may be regulated by\nmultiple regulator genes. Such a skewed degree distribution issue significantly\ncomplicates the application of directed graph embedding methods. To tackle this\nissue, we propose the Cross-Attention Complex Dual Graph Embedding Model\n(XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture\nintricate gene interactions from gene expression profiles. Additionally, it\nuses a Dual Complex Graph Embedding approach to manage the skewed degree\ndistribution, thereby ensuring precise prediction of regulatory relationships\nand their directionality. Our model consistently outperforms existing\nstate-of-the-art methods across various datasets, underscoring its efficacy in\nelucidating complex gene regulatory mechanisms. Our codes used in this paper\nare publicly available at: https://github.com/kikixiong/XATGRN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a\npivotal challenge in systems biology, and several innovative computational\nmethods have been introduced. However, most of these studies have not\nconsidered the skewed degree distribution of genes. Specifically, some genes\nmay regulate multiple target genes while some genes may be regulated by\nmultiple regulator genes. Such a skewed degree distribution issue significantly\ncomplicates the application of directed graph embedding methods. To tackle this\nissue, we propose the Cross-Attention Complex Dual Graph Embedding Model\n(XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture\nintricate gene interactions from gene expression profiles. Additionally, it\nuses a Dual Complex Graph Embedding approach to manage the skewed degree\ndistribution, thereby ensuring precise prediction of regulatory relationships\nand their directionality. Our model consistently outperforms existing\nstate-of-the-art methods across various datasets, underscoring its efficacy in\nelucidating complex gene regulatory mechanisms. Our codes used in this paper\nare publicly available at: https://github.com/kikixiong/XATGRN."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiong"
                    },
                    {
                        "name": "Nan Yin"
                    },
                    {
                        "name": "Shiyang Liang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Duo Ai"
                    },
                    {
                        "name": "Fang Pan"
                    },
                    {
                        "name": "Jingjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingjie Wang"
                },
                "author": "Jingjie Wang",
                "arxiv_comment": "11 pages, 6 figures,1 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10747v2",
                "updated": "2025-01-09T14:35:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    35,
                    36,
                    3,
                    9,
                    0
                ],
                "published": "2024-07-15T14:20:09Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    20,
                    9,
                    0,
                    197,
                    0
                ],
                "title": "Codebook LLMs: Evaluating LLMs as Measurement Tools for Political\n  Science Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codebook LLMs: Evaluating LLMs as Measurement Tools for Political\n  Science Concepts"
                },
                "summary": "Codebooks -- documents that operationalize concepts and outline annotation\nprocedures -- are used almost universally by social scientists when coding\npolitical texts. To code these texts automatically, researchers are increasing\nturning to generative large language models (LLMs). However, there is limited\nempirical evidence on whether \"off-the-shelf\" LLMs faithfully follow real-world\ncodebook operationalizations and measure complex political constructs with\nsufficient accuracy. To address this, we gather and curate three real-world\npolitical science codebooks -- covering protest events, political violence and\nmanifestos -- along with their unstructured texts and human labels. We also\npropose a five-stage framework for codebook-LLM measurement: preparing a\ncodebook for both humans and LLMs, testing LLMs' basic capabilities on a\ncodebook, evaluating zero-shot measurement accuracy (i.e. off-the-shelf\nperformance), analyzing errors, and further (parameter-efficient) supervised\ntraining of LLMs. We provide an empirical demonstration of this framework using\nour three codebook datasets and several pretrained 7-12 billion open-weight\nLLMs. We find current open-weight LLMs have limitations in following codebooks\nzero-shot, but that supervised instruction tuning can substantially improve\nperformance. Rather than suggesting the \"best\" LLM, our contribution lies in\nour codebook datasets, evaluation framework, and guidance for applied\nresearchers who wish to implement their own codebook-LLM measurement projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codebooks -- documents that operationalize concepts and outline annotation\nprocedures -- are used almost universally by social scientists when coding\npolitical texts. To code these texts automatically, researchers are increasing\nturning to generative large language models (LLMs). However, there is limited\nempirical evidence on whether \"off-the-shelf\" LLMs faithfully follow real-world\ncodebook operationalizations and measure complex political constructs with\nsufficient accuracy. To address this, we gather and curate three real-world\npolitical science codebooks -- covering protest events, political violence and\nmanifestos -- along with their unstructured texts and human labels. We also\npropose a five-stage framework for codebook-LLM measurement: preparing a\ncodebook for both humans and LLMs, testing LLMs' basic capabilities on a\ncodebook, evaluating zero-shot measurement accuracy (i.e. off-the-shelf\nperformance), analyzing errors, and further (parameter-efficient) supervised\ntraining of LLMs. We provide an empirical demonstration of this framework using\nour three codebook datasets and several pretrained 7-12 billion open-weight\nLLMs. We find current open-weight LLMs have limitations in following codebooks\nzero-shot, but that supervised instruction tuning can substantially improve\nperformance. Rather than suggesting the \"best\" LLM, our contribution lies in\nour codebook datasets, evaluation framework, and guidance for applied\nresearchers who wish to implement their own codebook-LLM measurement projects."
                },
                "authors": [
                    {
                        "name": "Andrew Halterman"
                    },
                    {
                        "name": "Katherine A. Keith"
                    }
                ],
                "author_detail": {
                    "name": "Katherine A. Keith"
                },
                "author": "Katherine A. Keith",
                "arxiv_comment": "Version 2 (v1 Presented at PolMeth 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13426v2",
                "updated": "2025-01-09T14:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    33,
                    25,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-18T01:43:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    1,
                    43,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Safeguarding System Prompts for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding System Prompts for LLMs"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npropose PromptKeeper, a robust defense mechanism designed to safeguard system\nprompts. PromptKeeper tackles two core challenges: reliably detecting prompt\nleakage and mitigating side-channel vulnerabilities when leakage occurs. By\nframing detection as a hypothesis-testing problem, PromptKeeper effectively\nidentifies both explicit and subtle leakage. Upon detection, it regenerates\nresponses using a dummy prompt, ensuring that outputs remain indistinguishable\nfrom typical interactions when no leakage is present. PromptKeeper ensures\nrobust protection against prompt extraction attacks via either adversarial or\nregular queries, while preserving conversational capability and runtime\nefficiency during benign user interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npropose PromptKeeper, a robust defense mechanism designed to safeguard system\nprompts. PromptKeeper tackles two core challenges: reliably detecting prompt\nleakage and mitigating side-channel vulnerabilities when leakage occurs. By\nframing detection as a hypothesis-testing problem, PromptKeeper effectively\nidentifies both explicit and subtle leakage. Upon detection, it regenerates\nresponses using a dummy prompt, ensuring that outputs remain indistinguishable\nfrom typical interactions when no leakage is present. PromptKeeper ensures\nrobust protection against prompt extraction attacks via either adversarial or\nregular queries, while preserving conversational capability and runtime\nefficiency during benign user interactions."
                },
                "authors": [
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Zhihua Jin"
                    },
                    {
                        "name": "Guoliang He"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang He"
                },
                "author": "Guoliang He",
                "arxiv_comment": "15 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05838v2",
                "updated": "2025-01-09T14:04:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    4,
                    1,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-08T09:06:34Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    6,
                    34,
                    1,
                    282,
                    0
                ],
                "title": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite\n  Data Limit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite\n  Data Limit"
                },
                "summary": "One of the main challenges in optimal scaling of large language models (LLMs)\nis the prohibitive cost of hyperparameter tuning, particularly learning rate\n$\\eta$ and batch size $B$. While techniques like $\\mu$P (Yang et al., 2022)\nprovide scaling rules for optimal $\\eta$ transfer in the infinite model size\nlimit, the optimal scaling behavior in the infinite data size limit remains\nunknown. We fill in this gap by observing for the first time an intricate\ndependence of optimal $\\eta$ scaling on the pretraining token budget $T$, $B$\nand its relation to the critical batch size $B_\\mathrm{crit}$, which we measure\nto evolve as $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal\nbatch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed\nbecomes suboptimal over time even if learning rate is scaled optimally.\nSurprisingly, our results demonstrate that the observed optimal $\\eta$ and $B$\ndynamics are preserved with $\\mu$P model scaling, challenging the conventional\nview of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing\noptimality, we examine the sensitivity of loss to changes in learning rate,\nwhere we find the sensitivity to decrease with increase of $T$ and to remain\nconstant with $\\mu$P model scaling. We hope our results make the first step\ntowards a unified picture of the joint optimal data and model scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the main challenges in optimal scaling of large language models (LLMs)\nis the prohibitive cost of hyperparameter tuning, particularly learning rate\n$\\eta$ and batch size $B$. While techniques like $\\mu$P (Yang et al., 2022)\nprovide scaling rules for optimal $\\eta$ transfer in the infinite model size\nlimit, the optimal scaling behavior in the infinite data size limit remains\nunknown. We fill in this gap by observing for the first time an intricate\ndependence of optimal $\\eta$ scaling on the pretraining token budget $T$, $B$\nand its relation to the critical batch size $B_\\mathrm{crit}$, which we measure\nto evolve as $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal\nbatch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed\nbecomes suboptimal over time even if learning rate is scaled optimally.\nSurprisingly, our results demonstrate that the observed optimal $\\eta$ and $B$\ndynamics are preserved with $\\mu$P model scaling, challenging the conventional\nview of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing\noptimality, we examine the sensitivity of loss to changes in learning rate,\nwhere we find the sensitivity to decrease with increase of $T$ and to remain\nconstant with $\\mu$P model scaling. We hope our results make the first step\ntowards a unified picture of the joint optimal data and model scaling."
                },
                "authors": [
                    {
                        "name": "Oleg Filatov"
                    },
                    {
                        "name": "Jan Ebert"
                    },
                    {
                        "name": "Jiangtao Wang"
                    },
                    {
                        "name": "Stefan Kesselheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kesselheim"
                },
                "author": "Stefan Kesselheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05249v1",
                "updated": "2025-01-09T14:01:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    1,
                    15,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T14:01:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    1,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models"
                },
                "summary": "In recent years, tremendous success has been witnessed in Retrieval-Augmented\nGeneration (RAG), widely used to enhance Large Language Models (LLMs) in\ndomain-specific, knowledge-intensive, and privacy-sensitive tasks. However,\nattackers may steal those valuable RAGs and deploy or commercialize them,\nmaking it essential to detect Intellectual Property (IP) infringement. Most\nexisting ownership protection solutions, such as watermarks, are designed for\nrelational databases and texts. They cannot be directly applied to RAGs because\nrelational database watermarks require white-box access to detect IP\ninfringement, which is unrealistic for the knowledge base in RAGs. Meanwhile,\npost-processing by the adversary's deployed LLMs typically destructs text\nwatermark information. To address those problems, we propose a novel black-box\n\"knowledge watermark\" approach, named RAG-WM, to detect IP infringement of\nRAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark\nGenerator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark\ntexts based on watermark entity-relationship tuples and inject them into the\ntarget RAG. We evaluate RAG-WM across three domain-specific and two\nprivacy-sensitive tasks on four benchmark LLMs. Experimental results show that\nRAG-WM effectively detects the stolen RAGs in various deployed LLMs.\nFurthermore, RAG-WM is robust against paraphrasing, unrelated content removal,\nknowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising application in\ndetecting IP infringement of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, tremendous success has been witnessed in Retrieval-Augmented\nGeneration (RAG), widely used to enhance Large Language Models (LLMs) in\ndomain-specific, knowledge-intensive, and privacy-sensitive tasks. However,\nattackers may steal those valuable RAGs and deploy or commercialize them,\nmaking it essential to detect Intellectual Property (IP) infringement. Most\nexisting ownership protection solutions, such as watermarks, are designed for\nrelational databases and texts. They cannot be directly applied to RAGs because\nrelational database watermarks require white-box access to detect IP\ninfringement, which is unrealistic for the knowledge base in RAGs. Meanwhile,\npost-processing by the adversary's deployed LLMs typically destructs text\nwatermark information. To address those problems, we propose a novel black-box\n\"knowledge watermark\" approach, named RAG-WM, to detect IP infringement of\nRAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark\nGenerator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark\ntexts based on watermark entity-relationship tuples and inject them into the\ntarget RAG. We evaluate RAG-WM across three domain-specific and two\nprivacy-sensitive tasks on four benchmark LLMs. Experimental results show that\nRAG-WM effectively detects the stolen RAGs in various deployed LLMs.\nFurthermore, RAG-WM is robust against paraphrasing, unrelated content removal,\nknowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising application in\ndetecting IP infringement of RAG systems."
                },
                "authors": [
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Mengjie Sun"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Shengzhi Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05248v1",
                "updated": "2025-01-09T14:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T14:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback."
                },
                "authors": [
                    {
                        "name": "Laura Puccioni"
                    },
                    {
                        "name": "Alireza Farshin"
                    },
                    {
                        "name": "Mariano Scazzariello"
                    },
                    {
                        "name": "Changjie Wang"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2; I.2.6; D.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05247v1",
                "updated": "2025-01-09T13:57:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    57,
                    9,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:57:09Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    57,
                    9,
                    3,
                    9,
                    0
                ],
                "title": "Online Prompt and Solver Selection for Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Prompt and Solver Selection for Program Synthesis"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2\\% more queries than the best single solver and\nachieves results within 4\\% of the virtual best solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2\\% more queries than the best single solver and\nachieves results within 4\\% of the virtual best solver."
                },
                "authors": [
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Lewis Frampton"
                    },
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth Polgreen"
                },
                "author": "Elizabeth Polgreen",
                "arxiv_comment": "Accepted at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05246v1",
                "updated": "2025-01-09T13:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    54,
                    59,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    54,
                    59,
                    3,
                    9,
                    0
                ],
                "title": "Domain-Incremental Semantic Segmentation for Autonomous Driving under\n  Adverse Driving Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Incremental Semantic Segmentation for Autonomous Driving under\n  Adverse Driving Conditions"
                },
                "summary": "Semantic segmentation for autonomous driving is an even more challenging task\nwhen faced with adverse driving conditions. Standard models trained on data\nrecorded under ideal conditions show a deteriorated performance in unfavorable\nweather or illumination conditions. Fine-tuning on the new task or condition\nwould lead to overwriting the previously learned information resulting in\ncatastrophic forgetting. Adapting to the new conditions through traditional\ndomain adaption methods improves the performance on the target domain at the\nexpense of the source domain. Addressing these issues, we propose an\narchitecture-based domain-incremental learning approach called Progressive\nSemantic Segmentation (PSS). PSS is a task-agnostic, dynamically growing\ncollection of domain-specific segmentation models. The task of inferring the\ndomain and subsequently selecting the appropriate module for segmentation is\ncarried out using a collection of convolutional autoencoders. We extensively\nevaluate our proposed approach using several datasets at varying levels of\ngranularity in the categorization of adverse driving conditions. Furthermore,\nwe demonstrate the generalization of the proposed approach to similar and\nunseen domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation for autonomous driving is an even more challenging task\nwhen faced with adverse driving conditions. Standard models trained on data\nrecorded under ideal conditions show a deteriorated performance in unfavorable\nweather or illumination conditions. Fine-tuning on the new task or condition\nwould lead to overwriting the previously learned information resulting in\ncatastrophic forgetting. Adapting to the new conditions through traditional\ndomain adaption methods improves the performance on the target domain at the\nexpense of the source domain. Addressing these issues, we propose an\narchitecture-based domain-incremental learning approach called Progressive\nSemantic Segmentation (PSS). PSS is a task-agnostic, dynamically growing\ncollection of domain-specific segmentation models. The task of inferring the\ndomain and subsequently selecting the appropriate module for segmentation is\ncarried out using a collection of convolutional autoencoders. We extensively\nevaluate our proposed approach using several datasets at varying levels of\ngranularity in the categorization of adverse driving conditions. Furthermore,\nwe demonstrate the generalization of the proposed approach to similar and\nunseen domains."
                },
                "authors": [
                    {
                        "name": "Shishir Muralidhara"
                    },
                    {
                        "name": "René Schuster"
                    },
                    {
                        "name": "Didier Stricker"
                    }
                ],
                "author_detail": {
                    "name": "Didier Stricker"
                },
                "author": "Didier Stricker",
                "arxiv_comment": "Accepted at ICPRAM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05234v1",
                "updated": "2025-01-09T13:41:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    41,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:41:37Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    41,
                    37,
                    3,
                    9,
                    0
                ],
                "title": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs"
                },
                "summary": "This paper presents an approach for generating high-quality, same-language\nsubtitles for Estonian TV content. We fine-tune the Whisper model on\nhuman-generated Estonian subtitles and enhance it with iterative\npseudo-labeling and large language model (LLM) based post-editing. Our\nexperiments demonstrate notable subtitle quality improvement through\npseudo-labeling with an unlabeled dataset. We find that applying LLM-based\nediting at test time enhances subtitle accuracy, while its use during training\ndoes not yield further gains. This approach holds promise for creating subtitle\nquality close to human standard and could be extended to real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an approach for generating high-quality, same-language\nsubtitles for Estonian TV content. We fine-tune the Whisper model on\nhuman-generated Estonian subtitles and enhance it with iterative\npseudo-labeling and large language model (LLM) based post-editing. Our\nexperiments demonstrate notable subtitle quality improvement through\npseudo-labeling with an unlabeled dataset. We find that applying LLM-based\nediting at test time enhances subtitle accuracy, while its use during training\ndoes not yield further gains. This approach holds promise for creating subtitle\nquality close to human standard and could be extended to real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Artem Fedorchenko"
                    },
                    {
                        "name": "Tanel Alumäe"
                    }
                ],
                "author_detail": {
                    "name": "Tanel Alumäe"
                },
                "author": "Tanel Alumäe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05231v1",
                "updated": "2025-01-09T13:38:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    38,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:38:41Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    38,
                    41,
                    3,
                    9,
                    0
                ],
                "title": "Near-threshold dipole strength in {^{10}}Be with isoscalar character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-threshold dipole strength in {^{10}}Be with isoscalar character"
                },
                "summary": "Isoscalar dipole transitions are a distinctive fingerprint of cluster\nstructures. A {1^-} resonance at 7.27(10) MeV, located just below the\n{\\alpha}-emission threshold, has been observed in the deuteron inelastic\nscattering reactions off 10Be. The deformation lengths of the excited states in\n10Be below 9 MeV have been inferred from the differential cross sections using\ncoupled channel calculations. This observed {1^-} resonance has isoscalar\ncharacteristics and exhausts approximately 5{\\%}-15{\\%} of the isoscalar dipole\nenergy-weighted sum rule, providing evidence for pronounced {\\alpha} cluster\nstructure in 10Be. The Gamow coupled channel approach supports this\ninterpretation and suggests the near-threshold effect might be playing an\nimportant role in this excitation energy domain. The {\\alpha}+{\\alpha}+n+n\nfour-body calculation reproduces the observed enhanced dipole strength,\nimplying that the four-body cluster structure is essential to describe the\n{1^-} states in 10Be.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isoscalar dipole transitions are a distinctive fingerprint of cluster\nstructures. A {1^-} resonance at 7.27(10) MeV, located just below the\n{\\alpha}-emission threshold, has been observed in the deuteron inelastic\nscattering reactions off 10Be. The deformation lengths of the excited states in\n10Be below 9 MeV have been inferred from the differential cross sections using\ncoupled channel calculations. This observed {1^-} resonance has isoscalar\ncharacteristics and exhausts approximately 5{\\%}-15{\\%} of the isoscalar dipole\nenergy-weighted sum rule, providing evidence for pronounced {\\alpha} cluster\nstructure in 10Be. The Gamow coupled channel approach supports this\ninterpretation and suggests the near-threshold effect might be playing an\nimportant role in this excitation energy domain. The {\\alpha}+{\\alpha}+n+n\nfour-body calculation reproduces the observed enhanced dipole strength,\nimplying that the four-body cluster structure is essential to describe the\n{1^-} states in 10Be."
                },
                "authors": [
                    {
                        "name": "J. Chen"
                    },
                    {
                        "name": "Y. Ayyad"
                    },
                    {
                        "name": "D. Bazin"
                    },
                    {
                        "name": "W. Mittig"
                    },
                    {
                        "name": "M. Z. Serikow"
                    },
                    {
                        "name": "N. Keeley"
                    },
                    {
                        "name": "S. M. Wang"
                    },
                    {
                        "name": "B. Zhou"
                    },
                    {
                        "name": "J. C. Zamora"
                    },
                    {
                        "name": "S. Beceiro-Novo"
                    },
                    {
                        "name": "M. Cortesi"
                    },
                    {
                        "name": "M. DeNudt"
                    },
                    {
                        "name": "S. Heinitz"
                    },
                    {
                        "name": "S. Giraud"
                    },
                    {
                        "name": "P. Gueye"
                    },
                    {
                        "name": "C. R. Hoffman"
                    },
                    {
                        "name": "B. P. Kay"
                    },
                    {
                        "name": "E. A. Maugeri"
                    },
                    {
                        "name": "B. G. Monteagudo"
                    },
                    {
                        "name": "H. Li"
                    },
                    {
                        "name": "W. P. Liu"
                    },
                    {
                        "name": "A. Munoz"
                    },
                    {
                        "name": "F. Ndayisabye"
                    },
                    {
                        "name": "J. Pereira"
                    },
                    {
                        "name": "N. Rijal"
                    },
                    {
                        "name": "C. Santamaria"
                    },
                    {
                        "name": "D. Schumann"
                    },
                    {
                        "name": "N. Watwood"
                    },
                    {
                        "name": "G. Votta"
                    },
                    {
                        "name": "P. Yin"
                    },
                    {
                        "name": "C. X. Yuan"
                    },
                    {
                        "name": "Y. N. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Y. N. Zhang"
                },
                "author": "Y. N. Zhang",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05228v1",
                "updated": "2025-01-09T13:36:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    36,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:36:37Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    36,
                    37,
                    3,
                    9,
                    0
                ],
                "title": "Harnessing Large Language and Vision-Language Models for Robust\n  Out-of-Distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language and Vision-Language Models for Robust\n  Out-of-Distribution Detection"
                },
                "summary": "Out-of-distribution (OOD) detection has seen significant advancements with\nzero-shot approaches by leveraging the powerful Vision-Language Models (VLMs)\nsuch as CLIP. However, prior research works have predominantly focused on\nenhancing Far-OOD performance, while potentially compromising Near-OOD\nefficacy, as observed from our pilot study. To address this issue, we propose a\nnovel strategy to enhance zero-shot OOD detection performances for both Far-OOD\nand Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs)\nand VLMs. Our approach first exploit an LLM to generate superclasses of the ID\nlabels and their corresponding background descriptions followed by feature\nextraction using CLIP. We then isolate the core semantic features for ID data\nby subtracting background features from the superclass features. The refined\nrepresentation facilitates the selection of more appropriate negative labels\nfor OOD data from a comprehensive candidate label set of WordNet, thereby\nenhancing the performance of zero-shot OOD detection in both scenarios.\nFurthermore, we introduce novel few-shot prompt tuning and visual prompt tuning\nto adapt the proposed framework to better align with the target distribution.\nExperimental results demonstrate that the proposed approach consistently\noutperforms current state-of-the-art methods across multiple benchmarks, with\nan improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.\nAdditionally, our method exhibits superior robustness against covariate shift\nacross different domains, further highlighting its effectiveness in real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection has seen significant advancements with\nzero-shot approaches by leveraging the powerful Vision-Language Models (VLMs)\nsuch as CLIP. However, prior research works have predominantly focused on\nenhancing Far-OOD performance, while potentially compromising Near-OOD\nefficacy, as observed from our pilot study. To address this issue, we propose a\nnovel strategy to enhance zero-shot OOD detection performances for both Far-OOD\nand Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs)\nand VLMs. Our approach first exploit an LLM to generate superclasses of the ID\nlabels and their corresponding background descriptions followed by feature\nextraction using CLIP. We then isolate the core semantic features for ID data\nby subtracting background features from the superclass features. The refined\nrepresentation facilitates the selection of more appropriate negative labels\nfor OOD data from a comprehensive candidate label set of WordNet, thereby\nenhancing the performance of zero-shot OOD detection in both scenarios.\nFurthermore, we introduce novel few-shot prompt tuning and visual prompt tuning\nto adapt the proposed framework to better align with the target distribution.\nExperimental results demonstrate that the proposed approach consistently\noutperforms current state-of-the-art methods across multiple benchmarks, with\nan improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.\nAdditionally, our method exhibits superior robustness against covariate shift\nacross different domains, further highlighting its effectiveness in real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Pei-Kang Lee"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    },
                    {
                        "name": "Ja-Ling Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ja-Ling Wu"
                },
                "author": "Ja-Ling Wu",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05224v1",
                "updated": "2025-01-09T13:24:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    24,
                    11,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:24:11Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    24,
                    11,
                    3,
                    9,
                    0
                ],
                "title": "Leveraging Large Language Models for Zero-shot Lay Summarisation in\n  Biomedicine and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Zero-shot Lay Summarisation in\n  Biomedicine and Beyond"
                },
                "summary": "In this work, we explore the application of Large Language Models to\nzero-shot Lay Summarisation. We propose a novel two-stage framework for Lay\nSummarisation based on real-life processes, and find that summaries generated\nwith this method are increasingly preferred by human judges for larger models.\nTo help establish best practices for employing LLMs in zero-shot settings, we\nalso assess the ability of LLMs as judges, finding that they are able to\nreplicate the preferences of human judges. Finally, we take the initial steps\ntowards Lay Summarisation for Natural Language Processing (NLP) articles,\nfinding that LLMs are able to generalise to this new domain, and further\nhighlighting the greater utility of summaries generated by our proposed\napproach via an in-depth human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the application of Large Language Models to\nzero-shot Lay Summarisation. We propose a novel two-stage framework for Lay\nSummarisation based on real-life processes, and find that summaries generated\nwith this method are increasingly preferred by human judges for larger models.\nTo help establish best practices for employing LLMs in zero-shot settings, we\nalso assess the ability of LLMs as judges, finding that they are able to\nreplicate the preferences of human judges. Finally, we take the initial steps\ntowards Lay Summarisation for Natural Language Processing (NLP) articles,\nfinding that LLMs are able to generalise to this new domain, and further\nhighlighting the greater utility of summaries generated by our proposed\napproach via an in-depth human evaluation."
                },
                "authors": [
                    {
                        "name": "Tomas Goldsack"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03963v2",
                "updated": "2025-01-09T13:12:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    12,
                    25,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-06T01:08:58Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    8,
                    58,
                    4,
                    250,
                    0
                ],
                "title": "The Arizona Molecular ISM Survey with the SMT: Variations in the\n  CO(2-1)/CO(1-0) Line Ratio Across the Galaxy Population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arizona Molecular ISM Survey with the SMT: Variations in the\n  CO(2-1)/CO(1-0) Line Ratio Across the Galaxy Population"
                },
                "summary": "The J=1$\\rightarrow$0 spectral line of carbon monoxide (CO(1-0)) is the\ncanonical tracer of molecular gas. However, CO(2-1) is frequently used in its\nplace, following the assumption that the higher energy line can be used to\ninfer the CO(1-0) luminosity and molecular gas mass. The use of CO(2-1) depends\non a knowledge of the ratio between CO(2-1) and CO(1-0) luminosities, r21. Here\nwe present galaxy-integrated r21 measurements for 122 galaxies spanning stellar\nmasses from 10$^9$ to 10$^{11.5}$ M$_\\odot$ and star formation rates (SFRs)\nfrom 0.08 to 35 M$_\\odot$/yr. We find strong trends between r21 and SFR, SFR\nsurface density, star formation efficiency, and distance from the star\nformation main sequence (SFMS). We show that the assumption of a constant r21\ncan introduce biases into the molecular gas trends in galaxy population studies\nand demonstrate how this affects the recovery of important galaxy scaling\nrelations, including the Kennicutt-Schmidt law and the relation between SFMS\noffset and star formation efficiency. We provide a prescription which accounts\nfor variations in r21 as a function of SFR and can be used to convert between\nCO(2-1) and CO(1-0) when only one line is available. Our prescription matches\nvariations in r21 for both AMISS and literature samples and can be used to\nderive more accurate gas masses from CO(2-1) observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The J=1$\\rightarrow$0 spectral line of carbon monoxide (CO(1-0)) is the\ncanonical tracer of molecular gas. However, CO(2-1) is frequently used in its\nplace, following the assumption that the higher energy line can be used to\ninfer the CO(1-0) luminosity and molecular gas mass. The use of CO(2-1) depends\non a knowledge of the ratio between CO(2-1) and CO(1-0) luminosities, r21. Here\nwe present galaxy-integrated r21 measurements for 122 galaxies spanning stellar\nmasses from 10$^9$ to 10$^{11.5}$ M$_\\odot$ and star formation rates (SFRs)\nfrom 0.08 to 35 M$_\\odot$/yr. We find strong trends between r21 and SFR, SFR\nsurface density, star formation efficiency, and distance from the star\nformation main sequence (SFMS). We show that the assumption of a constant r21\ncan introduce biases into the molecular gas trends in galaxy population studies\nand demonstrate how this affects the recovery of important galaxy scaling\nrelations, including the Kennicutt-Schmidt law and the relation between SFMS\noffset and star formation efficiency. We provide a prescription which accounts\nfor variations in r21 as a function of SFR and can be used to convert between\nCO(2-1) and CO(1-0) when only one line is available. Our prescription matches\nvariations in r21 for both AMISS and literature samples and can be used to\nderive more accurate gas masses from CO(2-1) observations."
                },
                "authors": [
                    {
                        "name": "Ryan P. Keenan"
                    },
                    {
                        "name": "Daniel P. Marrone"
                    },
                    {
                        "name": "Garrett K. Keating"
                    }
                ],
                "author_detail": {
                    "name": "Garrett K. Keating"
                },
                "author": "Garrett K. Keating",
                "arxiv_doi": "10.3847/1538-4357/ada361",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ada361",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted version, in press at ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05207v1",
                "updated": "2025-01-09T12:57:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    57,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T12:57:41Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    57,
                    41,
                    3,
                    9,
                    0
                ],
                "title": "CoDe: Communication Delay-Tolerant Multi-Agent Collaboration via Dual\n  Alignment of Intent and Timeliness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDe: Communication Delay-Tolerant Multi-Agent Collaboration via Dual\n  Alignment of Intent and Timeliness"
                },
                "summary": "Communication has been widely employed to enhance multi-agent collaboration.\nPrevious research has typically assumed delay-free communication, a strong\nassumption that is challenging to meet in practice. However, real-world agents\nsuffer from channel delays, receiving messages sent at different time points,\ntermed {\\it{Asynchronous Communication}}, leading to cognitive biases and\nbreakdowns in collaboration. This paper first defines two communication delay\nsettings in MARL and emphasizes their harm to collaboration. To handle the\nabove delays, this paper proposes a novel framework, Communication\nDelay-tolerant Multi-Agent Collaboration (CoDe). At first, CoDe learns an\nintent representation as messages through future action inference, reflecting\nthe stable future behavioral trends of the agents. Then, CoDe devises a dual\nalignment mechanism of intent and timeliness to strengthen the fusion process\nof asynchronous messages. In this way, agents can extract the long-term intent\nof others, even from delayed messages, and selectively utilize the most recent\nmessages that are relevant to their intent. Experimental results demonstrate\nthat CoDe outperforms baseline algorithms in three MARL benchmarks without\ndelay and exhibits robustness under fixed and time-varying delays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication has been widely employed to enhance multi-agent collaboration.\nPrevious research has typically assumed delay-free communication, a strong\nassumption that is challenging to meet in practice. However, real-world agents\nsuffer from channel delays, receiving messages sent at different time points,\ntermed {\\it{Asynchronous Communication}}, leading to cognitive biases and\nbreakdowns in collaboration. This paper first defines two communication delay\nsettings in MARL and emphasizes their harm to collaboration. To handle the\nabove delays, this paper proposes a novel framework, Communication\nDelay-tolerant Multi-Agent Collaboration (CoDe). At first, CoDe learns an\nintent representation as messages through future action inference, reflecting\nthe stable future behavioral trends of the agents. Then, CoDe devises a dual\nalignment mechanism of intent and timeliness to strengthen the fusion process\nof asynchronous messages. In this way, agents can extract the long-term intent\nof others, even from delayed messages, and selectively utilize the most recent\nmessages that are relevant to their intent. Experimental results demonstrate\nthat CoDe outperforms baseline algorithms in three MARL benchmarks without\ndelay and exhibits robustness under fixed and time-varying delays."
                },
                "authors": [
                    {
                        "name": "Shoucheng Song"
                    },
                    {
                        "name": "Youfang Lin"
                    },
                    {
                        "name": "Sheng Han"
                    },
                    {
                        "name": "Chang Yao"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Kai Lv"
                    }
                ],
                "author_detail": {
                    "name": "Kai Lv"
                },
                "author": "Kai Lv",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05197v1",
                "updated": "2025-01-09T12:48:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    48,
                    15,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T12:48:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    48,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "An Algorithmic Approach for Causal Health Equity: A Look at Race\n  Differentials in Intensive Care Unit (ICU) Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Algorithmic Approach for Causal Health Equity: A Look at Race\n  Differentials in Intensive Care Unit (ICU) Outcomes"
                },
                "summary": "The new era of large-scale data collection and analysis presents an\nopportunity for diagnosing and understanding the causes of health inequities.\nIn this study, we describe a framework for systematically analyzing health\ndisparities using causal inference. The framework is illustrated by\ninvestigating racial and ethnic disparities in intensive care unit (ICU)\noutcome between majority and minority groups in Australia (Indigenous vs.\nNon-Indigenous) and the United States (African-American vs. White). We\ndemonstrate that commonly used statistical measures for quantifying inequity\nare insufficient, and focus on attributing the observed disparity to the causal\nmechanisms that generate it. We find that minority patients are younger at\nadmission, have worse chronic health, are more likely to be admitted for urgent\nand non-elective reasons, and have higher illness severity. At the same time,\nhowever, we find a protective direct effect of belonging to a minority group,\nwith minority patients showing improved survival compared to their majority\ncounterparts, with all other variables kept equal. We demonstrate that this\nprotective effect is related to the increased probability of being admitted to\nICU, with minority patients having an increased risk of ICU admission. We also\nfind that minority patients, while showing improved survival, are more likely\nto be readmitted to ICU. Thus, due to worse access to primary health care,\nminority patients are more likely to end up in ICU for preventable conditions,\ncausing a reduction in the mortality rates and creating an effect that appears\nto be protective. Since the baseline risk of ICU admission may serve as proxy\nfor lack of access to primary care, we developed the Indigenous Intensive Care\nEquity (IICE) Radar, a monitoring system for tracking the over-utilization of\nICU resources by the Indigenous population of Australia across geographical\nareas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new era of large-scale data collection and analysis presents an\nopportunity for diagnosing and understanding the causes of health inequities.\nIn this study, we describe a framework for systematically analyzing health\ndisparities using causal inference. The framework is illustrated by\ninvestigating racial and ethnic disparities in intensive care unit (ICU)\noutcome between majority and minority groups in Australia (Indigenous vs.\nNon-Indigenous) and the United States (African-American vs. White). We\ndemonstrate that commonly used statistical measures for quantifying inequity\nare insufficient, and focus on attributing the observed disparity to the causal\nmechanisms that generate it. We find that minority patients are younger at\nadmission, have worse chronic health, are more likely to be admitted for urgent\nand non-elective reasons, and have higher illness severity. At the same time,\nhowever, we find a protective direct effect of belonging to a minority group,\nwith minority patients showing improved survival compared to their majority\ncounterparts, with all other variables kept equal. We demonstrate that this\nprotective effect is related to the increased probability of being admitted to\nICU, with minority patients having an increased risk of ICU admission. We also\nfind that minority patients, while showing improved survival, are more likely\nto be readmitted to ICU. Thus, due to worse access to primary health care,\nminority patients are more likely to end up in ICU for preventable conditions,\ncausing a reduction in the mortality rates and creating an effect that appears\nto be protective. Since the baseline risk of ICU admission may serve as proxy\nfor lack of access to primary care, we developed the Indigenous Intensive Care\nEquity (IICE) Radar, a monitoring system for tracking the over-utilization of\nICU resources by the Indigenous population of Australia across geographical\nareas."
                },
                "authors": [
                    {
                        "name": "Drago Plecko"
                    },
                    {
                        "name": "Paul Secombe"
                    },
                    {
                        "name": "Andrea Clarke"
                    },
                    {
                        "name": "Amelia Fiske"
                    },
                    {
                        "name": "Samarra Toby"
                    },
                    {
                        "name": "Donisha Duff"
                    },
                    {
                        "name": "David Pilcher"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Rinaldo Bellomo"
                    },
                    {
                        "name": "Elias Bareinboim"
                    }
                ],
                "author_detail": {
                    "name": "Elias Bareinboim"
                },
                "author": "Elias Bareinboim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09094v2",
                "updated": "2025-01-09T12:38:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    38,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-12T09:22:04Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    4,
                    3,
                    347,
                    0
                ],
                "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion"
                },
                "summary": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihai Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Min Peng"
                    }
                ],
                "author_detail": {
                    "name": "Min Peng"
                },
                "author": "Min Peng",
                "arxiv_comment": "COLING 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01192v2",
                "updated": "2025-01-09T12:33:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    33,
                    13,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-02T10:55:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education"
                },
                "summary": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly childhood science literacy gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly childhood science literacy gap."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Amin Alibakhshi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Alibakhshi"
                },
                "author": "Amin Alibakhshi",
                "arxiv_comment": "CHI late-breaking work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15361v2",
                "updated": "2025-01-09T12:18:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    18,
                    26,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-19T19:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    19,
                    47,
                    35,
                    3,
                    354,
                    0
                ],
                "title": "Spatiotemporally Coherent Probabilistic Generation of Weather from\n  Climate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporally Coherent Probabilistic Generation of Weather from\n  Climate"
                },
                "summary": "Local climate information is crucial for impact assessment and\ndecision-making, yet coarse global climate simulations cannot capture\nsmall-scale phenomena. Current statistical downscaling methods infer these\nphenomena as temporally decoupled spatial patches. However, to preserve\nphysical properties, estimating spatio-temporally coherent high-resolution\nweather dynamics for multiple variables across long time horizons is crucial.\nWe present a novel generative approach that uses a score-based diffusion model\ntrained on high-resolution reanalysis data to capture the statistical\nproperties of local weather dynamics. After training, we condition on coarse\nclimate model data to generate weather patterns consistent with the aggregate\ninformation. As this inference task is inherently uncertain, we leverage the\nprobabilistic nature of diffusion models and sample multiple trajectories. We\nevaluate our approach with high-resolution reanalysis information before\napplying it to the climate model downscaling task. We then demonstrate that the\nmodel generates spatially and temporally coherent weather dynamics that align\nwith global climate output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local climate information is crucial for impact assessment and\ndecision-making, yet coarse global climate simulations cannot capture\nsmall-scale phenomena. Current statistical downscaling methods infer these\nphenomena as temporally decoupled spatial patches. However, to preserve\nphysical properties, estimating spatio-temporally coherent high-resolution\nweather dynamics for multiple variables across long time horizons is crucial.\nWe present a novel generative approach that uses a score-based diffusion model\ntrained on high-resolution reanalysis data to capture the statistical\nproperties of local weather dynamics. After training, we condition on coarse\nclimate model data to generate weather patterns consistent with the aggregate\ninformation. As this inference task is inherently uncertain, we leverage the\nprobabilistic nature of diffusion models and sample multiple trajectories. We\nevaluate our approach with high-resolution reanalysis information before\napplying it to the climate model downscaling task. We then demonstrate that the\nmodel generates spatially and temporally coherent weather dynamics that align\nwith global climate output."
                },
                "authors": [
                    {
                        "name": "Jonathan Schmidt"
                    },
                    {
                        "name": "Luca Schmidt"
                    },
                    {
                        "name": "Felix Strnad"
                    },
                    {
                        "name": "Nicole Ludwig"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "15 pages, 6 figures, additional supplementary text and figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05179v1",
                "updated": "2025-01-09T11:57:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    57,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:57:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    57,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration"
                },
                "summary": "Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the ``commander'' of the entire token compression process,\ndirecting the allocation of retention ratios and the specific compression for\neach crop. In this way, redundant tokens are eliminated while important local\ndetails are adaptively preserved to the highest extent feasible. Empirical\nresults across 10 benchmarks reveal that GlobalCom$^2$ achieves an optimal\nbalance between performance and efficiency, and consistently outperforms\nstate-of-the-art token compression methods with LLaVA-NeXT-7B/13B models. Our\ncode is released at \\url{https://github.com/xuyang-liu16/GlobalCom2}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the ``commander'' of the entire token compression process,\ndirecting the allocation of retention ratios and the specific compression for\neach crop. In this way, redundant tokens are eliminated while important local\ndetails are adaptively preserved to the highest extent feasible. Empirical\nresults across 10 benchmarks reveal that GlobalCom$^2$ achieves an optimal\nbalance between performance and efficiency, and consistently outperforms\nstate-of-the-art token compression methods with LLaVA-NeXT-7B/13B models. Our\ncode is released at \\url{https://github.com/xuyang-liu16/GlobalCom2}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Yingyao Wang"
                    },
                    {
                        "name": "Jiale Yuan"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Honggang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Chen"
                },
                "author": "Honggang Chen",
                "arxiv_comment": "Our code is released at\n  \\url{https://github.com/xuyang-liu16/GlobalCom2}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05177v1",
                "updated": "2025-01-09T11:52:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    52,
                    54,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:52:54Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    52,
                    54,
                    3,
                    9,
                    0
                ],
                "title": "FaceMe: Robust Blind Face Restoration with Personal Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceMe: Robust Blind Face Restoration with Personal Identification"
                },
                "summary": "Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness."
                },
                "authors": [
                    {
                        "name": "Siyu Liu"
                    },
                    {
                        "name": "Zheng-Peng Duan"
                    },
                    {
                        "name": "Jia OuYang"
                    },
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Hyunhee Park"
                    },
                    {
                        "name": "Zikun Liu"
                    },
                    {
                        "name": "Chun-Le Guo"
                    },
                    {
                        "name": "Chongyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongyi Li"
                },
                "author": "Chongyi Li",
                "arxiv_comment": "To appear at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05171v1",
                "updated": "2025-01-09T11:45:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:45:05Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "title": "Emergence of human-like polarization among large language model agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of human-like polarization among large language model agents"
                },
                "summary": "Rapid advances in large language models (LLMs) have empowered autonomous\nagents to establish social relationships, communicate, and form shared and\ndiverging opinions on political issues. Our understanding of their collective\nbehaviours and underlying mechanisms remains incomplete, however, posing\nunexpected risks to human society. In this paper, we simulate a networked\nsystem involving thousands of large language model agents, discovering their\nsocial interactions, guided through LLM conversation, result in human-like\npolarization. We discover that these agents spontaneously develop their own\nsocial network with human-like properties, including homophilic clustering, but\nalso shape their collective opinions through mechanisms observed in the real\nworld, including the echo chamber effect. Similarities between humans and LLM\nagents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise\nconcerns about their capacity to amplify societal polarization, but also hold\nthe potential to serve as a valuable testbed for identifying plausible\nstrategies to mitigate polarization and its consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in large language models (LLMs) have empowered autonomous\nagents to establish social relationships, communicate, and form shared and\ndiverging opinions on political issues. Our understanding of their collective\nbehaviours and underlying mechanisms remains incomplete, however, posing\nunexpected risks to human society. In this paper, we simulate a networked\nsystem involving thousands of large language model agents, discovering their\nsocial interactions, guided through LLM conversation, result in human-like\npolarization. We discover that these agents spontaneously develop their own\nsocial network with human-like properties, including homophilic clustering, but\nalso shape their collective opinions through mechanisms observed in the real\nworld, including the echo chamber effect. Similarities between humans and LLM\nagents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise\nconcerns about their capacity to amplify societal polarization, but also hold\nthe potential to serve as a valuable testbed for identifying plausible\nstrategies to mitigate polarization and its consequences."
                },
                "authors": [
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Zhihong Lu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Fernando P. Santos"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "James Evans"
                    }
                ],
                "author_detail": {
                    "name": "James Evans"
                },
                "author": "James Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11120v2",
                "updated": "2025-01-09T11:39:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    39,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-15T08:51:14Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    8,
                    51,
                    14,
                    6,
                    350,
                    0
                ],
                "title": "Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning (RL) often encounters delayed and sparse feedback in\nreal-world applications, even with only episodic rewards. Previous approaches\nhave made some progress in reward redistribution for credit assignment but\nstill face challenges, including training difficulties due to redundancy and\nambiguous attributions stemming from overlooking the multifaceted nature of\nmission performance evaluation. Hopefully, Large Language Model (LLM)\nencompasses fruitful decision-making knowledge and provides a plausible tool\nfor reward redistribution. Even so, deploying LLM in this case is non-trivial\ndue to the misalignment between linguistic knowledge and the symbolic form\nrequirement, together with inherent randomness and hallucinations in inference.\nTo tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based\ndecision-making framework, to improve credit assignment. Key to LaRe is the\nconcept of the Latent Reward, which works as a multi-dimensional performance\nevaluation, enabling more interpretable goal attainment from various\nperspectives and facilitating more effective reward redistribution. We examine\nthat semantically generated code from LLM can bridge linguistic knowledge and\nsymbolic latent rewards, as it is executable for symbolic objects. Meanwhile,\nwe design latent reward self-verification to increase the stability and\nreliability of LLM inference. Theoretically, reward-irrelevant redundancy\nelimination in the latent reward benefits RL performance from more accurate\nreward estimation. Extensive experimental results witness that LaRe (i)\nachieves superior temporal credit assignment to SOTA methods, (ii) excels in\nallocating contributions among multiple agents, and (iii) outperforms policies\ntrained with ground truth rewards for certain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) often encounters delayed and sparse feedback in\nreal-world applications, even with only episodic rewards. Previous approaches\nhave made some progress in reward redistribution for credit assignment but\nstill face challenges, including training difficulties due to redundancy and\nambiguous attributions stemming from overlooking the multifaceted nature of\nmission performance evaluation. Hopefully, Large Language Model (LLM)\nencompasses fruitful decision-making knowledge and provides a plausible tool\nfor reward redistribution. Even so, deploying LLM in this case is non-trivial\ndue to the misalignment between linguistic knowledge and the symbolic form\nrequirement, together with inherent randomness and hallucinations in inference.\nTo tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based\ndecision-making framework, to improve credit assignment. Key to LaRe is the\nconcept of the Latent Reward, which works as a multi-dimensional performance\nevaluation, enabling more interpretable goal attainment from various\nperspectives and facilitating more effective reward redistribution. We examine\nthat semantically generated code from LLM can bridge linguistic knowledge and\nsymbolic latent rewards, as it is executable for symbolic objects. Meanwhile,\nwe design latent reward self-verification to increase the stability and\nreliability of LLM inference. Theoretically, reward-irrelevant redundancy\nelimination in the latent reward benefits RL performance from more accurate\nreward estimation. Extensive experimental results witness that LaRe (i)\nachieves superior temporal credit assignment to SOTA methods, (ii) excels in\nallocating contributions among multiple agents, and (iii) outperforms policies\ntrained with ground truth rewards for certain tasks."
                },
                "authors": [
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Yuhang Jiang"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Cheems Wang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05165v1",
                "updated": "2025-01-09T11:38:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    38,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:38:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    38,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in\n  Secure Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in\n  Secure Software Engineering"
                },
                "summary": "Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Esposito"
                },
                "author": "Matteo Esposito",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05155v1",
                "updated": "2025-01-09T11:19:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    19,
                    40,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:19:40Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    19,
                    40,
                    3,
                    9,
                    0
                ],
                "title": "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier"
                },
                "summary": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works."
                },
                "authors": [
                    {
                        "name": "Yufei Shang"
                    },
                    {
                        "name": "Yanrong Guo"
                    },
                    {
                        "name": "Shijie Hao"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02185v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02185v3",
                "updated": "2025-01-09T11:18:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    18,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2023-05-03T15:29:22Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    15,
                    29,
                    22,
                    2,
                    123,
                    0
                ],
                "title": "Doubly Robust Uniform Confidence Bands for Group-Time Conditional\n  Average Treatment Effects in Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Robust Uniform Confidence Bands for Group-Time Conditional\n  Average Treatment Effects in Difference-in-Differences"
                },
                "summary": "We consider a panel data analysis to examine the heterogeneity in treatment\neffects with respect to a pre-treatment covariate of interest in the staggered\ndifference-in-differences setting of Callaway and Sant'Anna (2021). Under\nstandard identification conditions, a doubly robust estimand conditional on the\ncovariate identifies the group-time conditional average treatment effect given\nthe covariate. Focusing on the case of a continuous covariate, we propose a\nthree-step estimation procedure based on nonparametric local polynomial\nregressions and parametric estimation methods. Using uniformly valid\ndistributional approximation results for empirical processes and multiplier\nbootstrapping, we develop doubly robust inference methods to construct uniform\nconfidence bands for the group-time conditional average treatment effect\nfunction and a variety of useful summary parameters. The accompanying R package\ndidhetero allows for easy implementation of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a panel data analysis to examine the heterogeneity in treatment\neffects with respect to a pre-treatment covariate of interest in the staggered\ndifference-in-differences setting of Callaway and Sant'Anna (2021). Under\nstandard identification conditions, a doubly robust estimand conditional on the\ncovariate identifies the group-time conditional average treatment effect given\nthe covariate. Focusing on the case of a continuous covariate, we propose a\nthree-step estimation procedure based on nonparametric local polynomial\nregressions and parametric estimation methods. Using uniformly valid\ndistributional approximation results for empirical processes and multiplier\nbootstrapping, we develop doubly robust inference methods to construct uniform\nconfidence bands for the group-time conditional average treatment effect\nfunction and a variety of useful summary parameters. The accompanying R package\ndidhetero allows for easy implementation of our methods."
                },
                "authors": [
                    {
                        "name": "Shunsuke Imai"
                    },
                    {
                        "name": "Lei Qin"
                    },
                    {
                        "name": "Takahide Yanagi"
                    }
                ],
                "author_detail": {
                    "name": "Takahide Yanagi"
                },
                "author": "Takahide Yanagi",
                "arxiv_comment": "The supplementary appendix and the accompanying R package can be\n  found on the authors' websites",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02185v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02185v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07066v2",
                "updated": "2025-01-09T11:11:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    11,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-11T15:30:16Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training"
                },
                "summary": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}."
                },
                "authors": [
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Leonardo Lucio Custode"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06396v3",
                "updated": "2025-01-09T11:04:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    4,
                    42,
                    3,
                    9,
                    0
                ],
                "published": "2023-09-12T16:59:22Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    16,
                    59,
                    22,
                    1,
                    255,
                    0
                ],
                "title": "On extensions of number fields with given quadratic algebras and\n  cohomology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On extensions of number fields with given quadratic algebras and\n  cohomology"
                },
                "summary": "We introduce a criterion on the presentation of finitely presented pro-$p$\ngroups which allows us to compute their cohomology groups and infer quotients\nof mild groups of cohomological dimension strictly larger than two, from\n(non-free) mild groups. We interpret these groups as Galois groups over\n$p$-rational fields with prescribed ramification and splitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a criterion on the presentation of finitely presented pro-$p$\ngroups which allows us to compute their cohomology groups and infer quotients\nof mild groups of cohomological dimension strictly larger than two, from\n(non-free) mild groups. We interpret these groups as Galois groups over\n$p$-rational fields with prescribed ramification and splitting."
                },
                "authors": [
                    {
                        "name": "Oussama Hamza"
                    }
                ],
                "author_detail": {
                    "name": "Oussama Hamza"
                },
                "author": "Oussama Hamza",
                "arxiv_comment": "18 pages, Final version, To appear in Manuscripta Mathematica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.06396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "12G10, 20J05, 20F05, 20F40, 17A45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04530v2",
                "updated": "2025-01-09T10:54:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    54,
                    4,
                    3,
                    9,
                    0
                ],
                "published": "2024-07-05T14:18:07Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    14,
                    18,
                    7,
                    4,
                    187,
                    0
                ],
                "title": "A spatial-correlated multitask linear mixed-effects model for imaging\n  genetics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A spatial-correlated multitask linear mixed-effects model for imaging\n  genetics"
                },
                "summary": "Imaging genetics aims to uncover the hidden relationship between imaging\nquantitative traits (QTs) and genetic markers (e.g. single nucleotide\npolymorphism (SNP)), and brings valuable insights into the pathogenesis of\ncomplex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's\nDisease). However, most linear models in imaging genetics didn't explicitly\nmodel the inner relationship among QTs, which might miss some potential\nefficiency gains from information borrowing across brain regions. In this work,\nwe developed a novel Bayesian regression framework for identifying significant\nassociations between QTs and genetic markers while explicitly modeling spatial\ndependency between QTs, with the main contributions as follows. Firstly, we\ndeveloped a spatial-correlated multitask linear mixed-effects model (LMM) to\naccount for dependencies between QTs. We incorporated a population-level mixed\neffects term into the model, taking full advantage of the dependent structure\nof brain imaging-derived QTs. Secondly, we implemented the model in the\nBayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to\nachieve the model inference. Further, we incorporated the MCMC samples with the\nCauchy combination test (CCT) to examine the association between SNPs and QTs,\nwhich avoided computationally intractable multi-test issues. The simulation\nstudies indicated improved power of our proposed model compared to classic\nmodels where inner dependencies of QTs were not modeled. We also applied the\nnew spatial model to an imaging dataset obtained from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imaging genetics aims to uncover the hidden relationship between imaging\nquantitative traits (QTs) and genetic markers (e.g. single nucleotide\npolymorphism (SNP)), and brings valuable insights into the pathogenesis of\ncomplex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's\nDisease). However, most linear models in imaging genetics didn't explicitly\nmodel the inner relationship among QTs, which might miss some potential\nefficiency gains from information borrowing across brain regions. In this work,\nwe developed a novel Bayesian regression framework for identifying significant\nassociations between QTs and genetic markers while explicitly modeling spatial\ndependency between QTs, with the main contributions as follows. Firstly, we\ndeveloped a spatial-correlated multitask linear mixed-effects model (LMM) to\naccount for dependencies between QTs. We incorporated a population-level mixed\neffects term into the model, taking full advantage of the dependent structure\nof brain imaging-derived QTs. Secondly, we implemented the model in the\nBayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to\nachieve the model inference. Further, we incorporated the MCMC samples with the\nCauchy combination test (CCT) to examine the association between SNPs and QTs,\nwhich avoided computationally intractable multi-test issues. The simulation\nstudies indicated improved power of our proposed model compared to classic\nmodels where inner dependencies of QTs were not modeled. We also applied the\nnew spatial model to an imaging dataset obtained from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database."
                },
                "authors": [
                    {
                        "name": "Zhibin Pu"
                    },
                    {
                        "name": "Shufei Ge"
                    }
                ],
                "author_detail": {
                    "name": "Shufei Ge"
                },
                "author": "Shufei Ge",
                "arxiv_comment": "32 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08 (Primary) 62J05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00778v2",
                "updated": "2025-01-09T10:47:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    47,
                    35,
                    3,
                    9,
                    0
                ],
                "published": "2024-06-02T15:35:45Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    15,
                    35,
                    45,
                    6,
                    154,
                    0
                ],
                "title": "Bayesian Joint Additive Factor Models for Multiview Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Joint Additive Factor Models for Multiview Learning"
                },
                "summary": "It is increasingly common in a wide variety of applied settings to collect\ndata of multiple different types on the same set of samples. Our particular\nfocus in this article is on studying relationships between such multiview\nfeatures and responses. A motivating application arises in the context of\nprecision medicine where multi-omics data are collected to correlate with\nclinical outcomes. It is of interest to infer dependence within and across\nviews while combining multimodal information to improve the prediction of\noutcomes. The signal-to-noise ratio can vary substantially across views,\nmotivating more nuanced statistical tools beyond standard late and early\nfusion. This challenge comes with the need to preserve interpretability, select\nfeatures, and obtain accurate uncertainty quantification. We propose a joint\nadditive factor regression model (JAFAR) with a structured additive design,\naccounting for shared and view-specific components. We ensure identifiability\nvia a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide\nan efficient implementation via a partially collapsed Gibbs sampler and extend\nour approach to allow flexible feature and outcome distributions. Prediction of\ntime-to-labor onset from immunome, metabolome, and proteome data illustrates\nperformance gains against state-of-the-art competitors. Our open-source\nsoftware (R package) is available at https://github.com/niccoloanceschi/jafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is increasingly common in a wide variety of applied settings to collect\ndata of multiple different types on the same set of samples. Our particular\nfocus in this article is on studying relationships between such multiview\nfeatures and responses. A motivating application arises in the context of\nprecision medicine where multi-omics data are collected to correlate with\nclinical outcomes. It is of interest to infer dependence within and across\nviews while combining multimodal information to improve the prediction of\noutcomes. The signal-to-noise ratio can vary substantially across views,\nmotivating more nuanced statistical tools beyond standard late and early\nfusion. This challenge comes with the need to preserve interpretability, select\nfeatures, and obtain accurate uncertainty quantification. We propose a joint\nadditive factor regression model (JAFAR) with a structured additive design,\naccounting for shared and view-specific components. We ensure identifiability\nvia a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide\nan efficient implementation via a partially collapsed Gibbs sampler and extend\nour approach to allow flexible feature and outcome distributions. Prediction of\ntime-to-labor onset from immunome, metabolome, and proteome data illustrates\nperformance gains against state-of-the-art competitors. Our open-source\nsoftware (R package) is available at https://github.com/niccoloanceschi/jafar."
                },
                "authors": [
                    {
                        "name": "Niccolo Anceschi"
                    },
                    {
                        "name": "Federico Ferrari"
                    },
                    {
                        "name": "David B. Dunson"
                    },
                    {
                        "name": "Himel Mallick"
                    }
                ],
                "author_detail": {
                    "name": "Himel Mallick"
                },
                "author": "Himel Mallick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16222v2",
                "updated": "2025-01-09T10:27:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    27,
                    10,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-18T12:54:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    54,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "A matheuristic approach for an integrated lot-sizing and scheduling\n  problem with a period-based learning effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A matheuristic approach for an integrated lot-sizing and scheduling\n  problem with a period-based learning effect"
                },
                "summary": "This research investigates a multi-product capacitated lot-sizing and\nscheduling problem incorporating a novel learning effect, namely the\nperiod-based learning effect. This is inspired by a real case in a core\nanalysis laboratory under a job shop setting. Accordingly, a Mixed-Integer\nLinear Programming (MILP) model is extended based on the big-bucket\nformulation, optimizing the total tardiness and overtime costs. Given the\ncomplexity of the problem, a cutting plane method is employed to simplify the\nmodel. Afterward, three matheuristic methods based on the rolling horizon\napproach are devised, incorporating two lower bounds and a local search\nheuristic. Furthermore, a post-processing approach is implemented to\nincorporate lot-streaming possibility. Computational experiments demonstrate:\n1) the simplified model performs effectively in terms of both solution quality\nand computational time; and 2) although the model encounters challenges with\nlarge-scale instances, the proposed matheuristic methods achieve satisfactory\noutcomes; and 3) it can be inferred that the complexity of the models and\nsolution methods are independent of the learning effect; however, the value of\nlearning effect may impact the performance of the lower bounds; 4) in\nmanufacturing settings, where the lot-streaming is possible, incorporating\npost-processing can drastically improve the objective function; 5) the impact\nof the period-based learning effect in the results is significant, and the\nmodel's sensitivity to time-based parameters (e.g., learning rate) is more than\ncost-based ones (e.g., tardiness cost).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates a multi-product capacitated lot-sizing and\nscheduling problem incorporating a novel learning effect, namely the\nperiod-based learning effect. This is inspired by a real case in a core\nanalysis laboratory under a job shop setting. Accordingly, a Mixed-Integer\nLinear Programming (MILP) model is extended based on the big-bucket\nformulation, optimizing the total tardiness and overtime costs. Given the\ncomplexity of the problem, a cutting plane method is employed to simplify the\nmodel. Afterward, three matheuristic methods based on the rolling horizon\napproach are devised, incorporating two lower bounds and a local search\nheuristic. Furthermore, a post-processing approach is implemented to\nincorporate lot-streaming possibility. Computational experiments demonstrate:\n1) the simplified model performs effectively in terms of both solution quality\nand computational time; and 2) although the model encounters challenges with\nlarge-scale instances, the proposed matheuristic methods achieve satisfactory\noutcomes; and 3) it can be inferred that the complexity of the models and\nsolution methods are independent of the learning effect; however, the value of\nlearning effect may impact the performance of the lower bounds; 4) in\nmanufacturing settings, where the lot-streaming is possible, incorporating\npost-processing can drastically improve the objective function; 5) the impact\nof the period-based learning effect in the results is significant, and the\nmodel's sensitivity to time-based parameters (e.g., learning rate) is more than\ncost-based ones (e.g., tardiness cost)."
                },
                "authors": [
                    {
                        "name": "Mohammad Rohaninejad"
                    },
                    {
                        "name": "Behdin Vahedi-Nouri"
                    },
                    {
                        "name": "Reza Tavakkoli-Moghaddam"
                    },
                    {
                        "name": "Zdeněk Hanzálek"
                    }
                ],
                "author_detail": {
                    "name": "Zdeněk Hanzálek"
                },
                "author": "Zdeněk Hanzálek",
                "arxiv_doi": "10.1016/j.eswa.2024.126234",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2024.126234",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.16222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05120v1",
                "updated": "2025-01-09T10:22:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    22,
                    35,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T10:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    22,
                    35,
                    3,
                    9,
                    0
                ],
                "title": "Improving the U-Net Configuration for Automated Delineation of Head and\n  Neck Cancer on MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the U-Net Configuration for Automated Delineation of Head and\n  Neck Cancer on MRI"
                },
                "summary": "Tumor volume segmentation on MRI is a challenging and time-consuming process\nthat is performed manually in typical clinical settings. This work presents an\napproach to automated delineation of head and neck tumors on MRI scans,\ndeveloped in the context of the MICCAI Head and Neck Tumor Segmentation for\nMR-Guided Applications (HNTS-MRG) 2024 Challenge. Rather than designing a new,\ntask-specific convolutional neural network, the focus of this research was to\npropose improvements to the configuration commonly used in medical segmentation\ntasks, relying solely on the traditional U-Net architecture. The empirical\nresults presented in this article suggest the superiority of patch-wise\nnormalization used for both training and sliding window inference. They also\nindicate that the performance of segmentation models can be enhanced by\napplying a scheduled data augmentation policy during training. Finally, it is\nshown that a small improvement in quality can be achieved by using Gaussian\nweighting to combine predictions for individual patches during sliding window\ninference. The model with the best configuration obtained an aggregated Dice\nSimilarity Coefficient (DSCagg) of 0.749 in Task 1 and 0.710 in Task 2 on five\ncross-validation folds. The ensemble of five models (one best model per\nvalidation fold) showed consistent results on a private test set of 50 patients\nwith an DSCagg of 0.752 in Task 1 and 0.718 in Task 2 (team name:\nandrei.iantsen). The source code and model weights are freely available at\nwww.github.com/iantsen/hntsmrg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor volume segmentation on MRI is a challenging and time-consuming process\nthat is performed manually in typical clinical settings. This work presents an\napproach to automated delineation of head and neck tumors on MRI scans,\ndeveloped in the context of the MICCAI Head and Neck Tumor Segmentation for\nMR-Guided Applications (HNTS-MRG) 2024 Challenge. Rather than designing a new,\ntask-specific convolutional neural network, the focus of this research was to\npropose improvements to the configuration commonly used in medical segmentation\ntasks, relying solely on the traditional U-Net architecture. The empirical\nresults presented in this article suggest the superiority of patch-wise\nnormalization used for both training and sliding window inference. They also\nindicate that the performance of segmentation models can be enhanced by\napplying a scheduled data augmentation policy during training. Finally, it is\nshown that a small improvement in quality can be achieved by using Gaussian\nweighting to combine predictions for individual patches during sliding window\ninference. The model with the best configuration obtained an aggregated Dice\nSimilarity Coefficient (DSCagg) of 0.749 in Task 1 and 0.710 in Task 2 on five\ncross-validation folds. The ensemble of five models (one best model per\nvalidation fold) showed consistent results on a private test set of 50 patients\nwith an DSCagg of 0.752 in Task 1 and 0.718 in Task 2 (team name:\nandrei.iantsen). The source code and model weights are freely available at\nwww.github.com/iantsen/hntsmrg."
                },
                "authors": [
                    {
                        "name": "Andrei Iantsen"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Iantsen"
                },
                "author": "Andrei Iantsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14677v2",
                "updated": "2025-01-09T10:00:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    0,
                    2,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-18T17:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts"
                },
                "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld."
                },
                "authors": [
                    {
                        "name": "German Gritsai"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Andrey Grabovoy"
                    },
                    {
                        "name": "Yury Chekhovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Chekhovich"
                },
                "author": "Yury Chekhovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09726v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09726v4",
                "updated": "2025-01-09T09:47:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    47,
                    26,
                    3,
                    9,
                    0
                ],
                "published": "2024-03-12T20:52:03Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    20,
                    52,
                    3,
                    1,
                    72,
                    0
                ],
                "title": "Quantile balancing inverse probability weighting for non-probability\n  samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile balancing inverse probability weighting for non-probability\n  samples"
                },
                "summary": "The use of non-probability data sources for statistical purposes and for\nofficial statistics has become increasingly popular in recent years. However,\nstatistical inference based on non-probability samples is made more difficult\nby nature of their biasedness and lack of representativity. In this paper we\npropose quantile balancing inverse probability weighting estimator (QBIPW) for\nnon-probability samples. We apply the idea of Harms and Duchesne (2006)\nallowing the use of quantile information in the estimation process to reproduce\nknown totals and the distribution of auxiliary variables. We discuss the\nestimation of the QBIPW probabilities and its variance. Our simulation study\nhas demonstrated that the proposed estimators are robust against model\nmis-specification and, as a result, help to reduce bias and mean squared error.\nFinally, we applied the proposed methods to estimate the share of job vacancies\naimed at Ukrainian workers in Poland using an integrated set of administrative\nand survey data about job vacancies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of non-probability data sources for statistical purposes and for\nofficial statistics has become increasingly popular in recent years. However,\nstatistical inference based on non-probability samples is made more difficult\nby nature of their biasedness and lack of representativity. In this paper we\npropose quantile balancing inverse probability weighting estimator (QBIPW) for\nnon-probability samples. We apply the idea of Harms and Duchesne (2006)\nallowing the use of quantile information in the estimation process to reproduce\nknown totals and the distribution of auxiliary variables. We discuss the\nestimation of the QBIPW probabilities and its variance. Our simulation study\nhas demonstrated that the proposed estimators are robust against model\nmis-specification and, as a result, help to reduce bias and mean squared error.\nFinally, we applied the proposed methods to estimate the share of job vacancies\naimed at Ukrainian workers in Poland using an integrated set of administrative\nand survey data about job vacancies."
                },
                "authors": [
                    {
                        "name": "Maciej Beręsewicz"
                    },
                    {
                        "name": "Marcin Szymkowiak"
                    },
                    {
                        "name": "Piotr Chlebicki"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Chlebicki"
                },
                "author": "Piotr Chlebicki",
                "arxiv_comment": "Accepted to the Survey Methodology journal (publication: December\n  2025, 51(2))",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09726v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09726v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23541v2",
                "updated": "2025-01-09T09:29:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    29,
                    8,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-31T01:03:34Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    1,
                    3,
                    34,
                    3,
                    305,
                    0
                ],
                "title": "Inferring cosmology from gravitational waves using non-parametric\n  detector-frame mass distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring cosmology from gravitational waves using non-parametric\n  detector-frame mass distribution"
                },
                "summary": "The challenge of understanding the Universe's dynamics, particularly the\nHubble tension, requires precise measurements of the Hubble constant. Building\nupon the existing spectral-siren method, which capitalizes on population\ninformation from gravitational-wave sources, this paper explores an alternative\nway to analyze the population data to obtain the cosmological parameters in\n$\\Lambda$CDM. We demonstrated how non-parametric methods, which are flexible\nmodels that can be used to agnostically reconstruct arbitrary probability\ndensities, can be incorporated into this framework and leverage the\ndetector-frame mass distribution to infer the cosmological parameters. We\ntested our method with mock data and applied it to $70$ binary black hole\nmergers from the third gravitational-wave transient catalog of the\nLIGO-Virgo-KAGRA Collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of understanding the Universe's dynamics, particularly the\nHubble tension, requires precise measurements of the Hubble constant. Building\nupon the existing spectral-siren method, which capitalizes on population\ninformation from gravitational-wave sources, this paper explores an alternative\nway to analyze the population data to obtain the cosmological parameters in\n$\\Lambda$CDM. We demonstrated how non-parametric methods, which are flexible\nmodels that can be used to agnostically reconstruct arbitrary probability\ndensities, can be incorporated into this framework and leverage the\ndetector-frame mass distribution to infer the cosmological parameters. We\ntested our method with mock data and applied it to $70$ binary black hole\nmergers from the third gravitational-wave transient catalog of the\nLIGO-Virgo-KAGRA Collaboration."
                },
                "authors": [
                    {
                        "name": "Thomas C. K. Ng"
                    },
                    {
                        "name": "Stefano Rinaldi"
                    },
                    {
                        "name": "Otto A. Hannuksela"
                    }
                ],
                "author_detail": {
                    "name": "Otto A. Hannuksela"
                },
                "author": "Otto A. Hannuksela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05097v1",
                "updated": "2025-01-09T09:25:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    25,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T09:25:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    25,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "A 1Mb mixed-precision quantized encoder for image classification and\n  patch-based compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 1Mb mixed-precision quantized encoder for image classification and\n  patch-based compression"
                },
                "summary": "Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even if Application-Specific Integrated Circuits (ASIC) have proven to be a\nrelevant choice for integrating inference at the edge, they are often limited\nin terms of applicability. In this paper, we demonstrate that an ASIC neural\nnetwork accelerator dedicated to image processing can be applied to multiple\ntasks of different levels: image classification and compression, while\nrequiring a very limited hardware. The key component is a reconfigurable,\nmixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and\nactivation quantizations combined with convolutional layer structural pruning\nto lower hardware-related constraints (memory and computing). We introduce an\nautomatic adaptation of linear symmetric quantizer scaling factors to perform\nquantized levels equalization, aiming at stabilizing quinary and ternary\nweights training. In addition, a proposed layer-shared Bit-Shift Normalization\nsignificantly simplifies the implementation of the hardware-expensive Batch\nNormalization. For a specific configuration in which the encoder design only\nrequires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides,\nwe also show that this quantized encoder can be used to compress image\npatch-by-patch while the reconstruction can performed remotely, by a dedicated\nfull-frame decoder. This solution typically enables an end-to-end compression\nalmost without any block artifacts, outperforming patch-based state-of-the-art\ntechniques employing a patch-constant bitrate."
                },
                "authors": [
                    {
                        "name": "Van Thien Nguyen"
                    },
                    {
                        "name": "William Guicquero"
                    },
                    {
                        "name": "Gilles Sicard"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Sicard"
                },
                "author": "Gilles Sicard",
                "arxiv_doi": "10.1109/TCSVT.2022.3145024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCSVT.2022.3145024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)",
                "arxiv_journal_ref": "vol. 32, no. 8, pp. 5581-5594, Aug. 2022",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05091v1",
                "updated": "2025-01-09T09:15:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    15,
                    7,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T09:15:07Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    15,
                    7,
                    3,
                    9,
                    0
                ],
                "title": "ResPanDiff: Diffusion Model with Disentangled Modulations for Image\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResPanDiff: Diffusion Model with Disentangled Modulations for Image\n  Fusion"
                },
                "summary": "The implementation of diffusion-based pansharpening task is predominantly\nconstrained by its slow inference speed, which results from numerous sampling\nsteps. Despite the existing techniques aiming to accelerate sampling, they\noften compromise performance when fusing multi-source images. To ease this\nlimitation, we introduce a novel and efficient diffusion model named Diffusion\nModel for Pansharpening by Inferring Residual Inference (ResPanDiff), which\nsignificantly reduces the number of diffusion steps without sacrificing the\nperformance to tackle pansharpening task. In ResPanDiff, we innovatively\npropose a Markov chain that transits from noisy residuals to the residuals\nbetween the LRMS and HRMS images, thereby reducing the number of sampling steps\nand enhancing performance. Additionally, we design the latent space to help\nmodel extract more features at the encoding stage, Shallow\nCond-Injection~(SC-I) to help model fetch cond-injected hidden features with\nhigher dimensions, and loss functions to give a better guidance for the\nresidual generation task. enabling the model to achieve superior performance in\nresidual generation. Furthermore, experimental evaluations on pansharpening\ndatasets demonstrate that the proposed method achieves superior outcomes\ncompared to recent state-of-the-art~(SOTA) techniques, requiring only 15\nsampling steps, which reduces over $90\\%$ step compared with the benchmark\ndiffusion models. Our experiments also include thorough discussions and\nablation studies to underscore the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of diffusion-based pansharpening task is predominantly\nconstrained by its slow inference speed, which results from numerous sampling\nsteps. Despite the existing techniques aiming to accelerate sampling, they\noften compromise performance when fusing multi-source images. To ease this\nlimitation, we introduce a novel and efficient diffusion model named Diffusion\nModel for Pansharpening by Inferring Residual Inference (ResPanDiff), which\nsignificantly reduces the number of diffusion steps without sacrificing the\nperformance to tackle pansharpening task. In ResPanDiff, we innovatively\npropose a Markov chain that transits from noisy residuals to the residuals\nbetween the LRMS and HRMS images, thereby reducing the number of sampling steps\nand enhancing performance. Additionally, we design the latent space to help\nmodel extract more features at the encoding stage, Shallow\nCond-Injection~(SC-I) to help model fetch cond-injected hidden features with\nhigher dimensions, and loss functions to give a better guidance for the\nresidual generation task. enabling the model to achieve superior performance in\nresidual generation. Furthermore, experimental evaluations on pansharpening\ndatasets demonstrate that the proposed method achieves superior outcomes\ncompared to recent state-of-the-art~(SOTA) techniques, requiring only 15\nsampling steps, which reduces over $90\\%$ step compared with the benchmark\ndiffusion models. Our experiments also include thorough discussions and\nablation studies to underscore the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Shiqi Cao"
                    },
                    {
                        "name": "Liangjian Deng"
                    },
                    {
                        "name": "Shangqi Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shangqi Deng"
                },
                "author": "Shangqi Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03700v2",
                "updated": "2025-01-09T09:12:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    12,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2023-12-06T18:59:19Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    18,
                    59,
                    19,
                    2,
                    340,
                    0
                ],
                "title": "OneLLM: One Framework to Align All Modalities with Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneLLM: One Framework to Align All Modalities with Language"
                },
                "summary": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM"
                },
                "authors": [
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Kaixiong Gong"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Accepted by CVPR 2024. Code: https://github.com/csuhan/OneLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05079v1",
                "updated": "2025-01-09T09:01:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    1,
                    4,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T09:01:04Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    1,
                    4,
                    3,
                    9,
                    0
                ],
                "title": "Multimodal-to-Text Prompt Engineering in Large Language Models Using\n  Feature Embeddings for GNSS Interference Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal-to-Text Prompt Engineering in Large Language Models Using\n  Feature Embeddings for GNSS Interference Characterization"
                },
                "summary": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks."
                },
                "authors": [
                    {
                        "name": "Harshith Manjunath"
                    },
                    {
                        "name": "Lucas Heublein"
                    },
                    {
                        "name": "Tobias Feigl"
                    },
                    {
                        "name": "Felix Ott"
                    }
                ],
                "author_detail": {
                    "name": "Felix Ott"
                },
                "author": "Felix Ott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1; H.5; I.4.9; I.4.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05078v1",
                "updated": "2025-01-09T09:00:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    0,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T09:00:32Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    0,
                    32,
                    3,
                    9,
                    0
                ],
                "title": "Analyzing Memorization in Large Language Models through the Lens of\n  Model Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Memorization in Large Language Models through the Lens of\n  Model Attribution"
                },
                "summary": "Large Language Models (LLMs) are prevalent in modern applications but often\nmemorize training data, leading to privacy breaches and copyright issues.\nExisting research has mainly focused on posthoc analyses, such as extracting\nmemorized content or developing memorization metrics, without exploring the\nunderlying architectural factors that contribute to memorization. In this work,\nwe investigate memorization from an architectural lens by analyzing how\nattention modules at different layers impact its memorization and\ngeneralization performance. Using attribution techniques, we systematically\nintervene in the LLM architecture by bypassing attention modules at specific\nblocks while keeping other components like layer normalization and MLP\ntransformations intact. We provide theorems analyzing our intervention\nmechanism from a mathematical view, bounding the difference in layer outputs\nwith and without our attributions. Our theoretical and empirical analyses\nreveal that attention modules in deeper transformer blocks are primarily\nresponsible for memorization, whereas earlier blocks are crucial for the models\ngeneralization and reasoning capabilities. We validate our findings through\ncomprehensive experiments on different LLM families (Pythia and GPTNeo) and\nfive benchmark datasets. Our insights offer a practical approach to mitigate\nmemorization in LLMs while preserving their performance, contributing to safer\nand more ethical deployment in real world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prevalent in modern applications but often\nmemorize training data, leading to privacy breaches and copyright issues.\nExisting research has mainly focused on posthoc analyses, such as extracting\nmemorized content or developing memorization metrics, without exploring the\nunderlying architectural factors that contribute to memorization. In this work,\nwe investigate memorization from an architectural lens by analyzing how\nattention modules at different layers impact its memorization and\ngeneralization performance. Using attribution techniques, we systematically\nintervene in the LLM architecture by bypassing attention modules at specific\nblocks while keeping other components like layer normalization and MLP\ntransformations intact. We provide theorems analyzing our intervention\nmechanism from a mathematical view, bounding the difference in layer outputs\nwith and without our attributions. Our theoretical and empirical analyses\nreveal that attention modules in deeper transformer blocks are primarily\nresponsible for memorization, whereas earlier blocks are crucial for the models\ngeneralization and reasoning capabilities. We validate our findings through\ncomprehensive experiments on different LLM families (Pythia and GPTNeo) and\nfive benchmark datasets. Our insights offer a practical approach to mitigate\nmemorization in LLMs while preserving their performance, contributing to safer\nand more ethical deployment in real world applications."
                },
                "authors": [
                    {
                        "name": "Tarun Ram Menta"
                    },
                    {
                        "name": "Susmit Agrawal"
                    },
                    {
                        "name": "Chirag Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Agarwal"
                },
                "author": "Chirag Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05075v1",
                "updated": "2025-01-09T08:59:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    59,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:59:14Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    59,
                    14,
                    3,
                    9,
                    0
                ],
                "title": "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model"
                },
                "summary": "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance."
                },
                "authors": [
                    {
                        "name": "Shuo Tong"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Runyuan Guo"
                    },
                    {
                        "name": "Xueqiong Tian"
                    },
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Youmin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youmin Zhang"
                },
                "author": "Youmin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14503v2",
                "updated": "2025-01-09T08:55:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    55,
                    7,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-21T08:31:06Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    31,
                    6,
                    3,
                    326,
                    0
                ],
                "title": "Planning-Driven Programming: A Large Language Model Programming Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning-Driven Programming: A Large Language Model Programming Workflow"
                },
                "summary": "The strong performance of large language models (LLMs) raises extensive\ndiscussion on their application to code generation. Recent research suggests\ncontinuous program refinements through visible tests to improve code generation\naccuracy in LLMs. However, these methods suffer from LLMs' inefficiency and\nlimited reasoning capacity. In this work, we propose an LLM programming\nworkflow (LPW) designed to improve both initial code generation and subsequent\nrefinements within a structured two-phase workflow. Specifically, the solution\ngeneration phase formulates a solution plan, which is then verified through\nvisible tests to specify the intended natural language solution. Subsequently,\nthe code implementation phase drafts an initial code according to the solution\nplan and its verification. If the generated code fails the visible tests, the\nplan verification serves as the intended solution to consistently inform the\nrefinement process for correcting bugs. Compared to state-of-the-art methods\nacross various existing LLMs, LPW significantly improves the Pass@1 accuracy by\nup to 16.4% on well-established text-to-code generation benchmarks. LPW also\nsets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8%\non MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using\nGPT-4o as the backbone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong performance of large language models (LLMs) raises extensive\ndiscussion on their application to code generation. Recent research suggests\ncontinuous program refinements through visible tests to improve code generation\naccuracy in LLMs. However, these methods suffer from LLMs' inefficiency and\nlimited reasoning capacity. In this work, we propose an LLM programming\nworkflow (LPW) designed to improve both initial code generation and subsequent\nrefinements within a structured two-phase workflow. Specifically, the solution\ngeneration phase formulates a solution plan, which is then verified through\nvisible tests to specify the intended natural language solution. Subsequently,\nthe code implementation phase drafts an initial code according to the solution\nplan and its verification. If the generated code fails the visible tests, the\nplan verification serves as the intended solution to consistently inform the\nrefinement process for correcting bugs. Compared to state-of-the-art methods\nacross various existing LLMs, LPW significantly improves the Pass@1 accuracy by\nup to 16.4% on well-established text-to-code generation benchmarks. LPW also\nsets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8%\non MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using\nGPT-4o as the backbone."
                },
                "authors": [
                    {
                        "name": "Chao Lei"
                    },
                    {
                        "name": "Yanchuan Chang"
                    },
                    {
                        "name": "Nir Lipovetzky"
                    },
                    {
                        "name": "Krista A. Ehinger"
                    }
                ],
                "author_detail": {
                    "name": "Krista A. Ehinger"
                },
                "author": "Krista A. Ehinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05072v1",
                "updated": "2025-01-09T08:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    54,
                    19,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    54,
                    19,
                    3,
                    9,
                    0
                ],
                "title": "A Flexible and Scalable Framework for Video Moment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible and Scalable Framework for Video Moment Search"
                },
                "summary": "Video moment search, the process of finding relevant moments in a video\ncorpus to match a user's query, is crucial for various applications. Existing\nsolutions, however, often assume a single perfect matching moment, struggle\nwith inefficient inference, and have limitations with hour-long videos. This\npaper introduces a flexible and scalable framework for retrieving a ranked list\nof moments from collection of videos in any length to match a text query, a\ntask termed Ranked Video Moment Retrieval (RVMR). Our framework, called\nSegment-Proposal-Ranking (SPR), simplifies the search process into three\nindependent stages: segment retrieval, proposal generation, and moment\nrefinement with re-ranking. Specifically, videos are divided into equal-length\nsegments with precomputed embeddings indexed offline, allowing efficient\nretrieval regardless of video length. For scalable online retrieval, both\nsegments and queries are projected into a shared feature space to enable\napproximate nearest neighbor (ANN) search. Retrieved segments are then merged\ninto coarse-grained moment proposals. Then a refinement and re-ranking module\nis designed to reorder and adjust timestamps of the coarse-grained proposals.\nEvaluations on the TVR-Ranking dataset demonstrate that our framework achieves\nstate-of-the-art performance with significant reductions in computational cost\nand processing time. The flexible design also allows for independent\nimprovements to each stage, making SPR highly adaptable for large-scale\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video moment search, the process of finding relevant moments in a video\ncorpus to match a user's query, is crucial for various applications. Existing\nsolutions, however, often assume a single perfect matching moment, struggle\nwith inefficient inference, and have limitations with hour-long videos. This\npaper introduces a flexible and scalable framework for retrieving a ranked list\nof moments from collection of videos in any length to match a text query, a\ntask termed Ranked Video Moment Retrieval (RVMR). Our framework, called\nSegment-Proposal-Ranking (SPR), simplifies the search process into three\nindependent stages: segment retrieval, proposal generation, and moment\nrefinement with re-ranking. Specifically, videos are divided into equal-length\nsegments with precomputed embeddings indexed offline, allowing efficient\nretrieval regardless of video length. For scalable online retrieval, both\nsegments and queries are projected into a shared feature space to enable\napproximate nearest neighbor (ANN) search. Retrieved segments are then merged\ninto coarse-grained moment proposals. Then a refinement and re-ranking module\nis designed to reorder and adjust timestamps of the coarse-grained proposals.\nEvaluations on the TVR-Ranking dataset demonstrate that our framework achieves\nstate-of-the-art performance with significant reductions in computational cost\nand processing time. The flexible design also allows for independent\nimprovements to each stage, making SPR highly adaptable for large-scale\napplications."
                },
                "authors": [
                    {
                        "name": "Chongzhi Zhang"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05068v1",
                "updated": "2025-01-09T08:44:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription"
                },
                "summary": "Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm."
                },
                "authors": [
                    {
                        "name": "Hounsu Kim"
                    },
                    {
                        "name": "Taegyun Kwon"
                    },
                    {
                        "name": "Juhan Nam"
                    }
                ],
                "author_detail": {
                    "name": "Juhan Nam"
                },
                "author": "Juhan Nam",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05057v1",
                "updated": "2025-01-09T08:28:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    28,
                    16,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:28:16Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    28,
                    16,
                    3,
                    9,
                    0
                ],
                "title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with\n  Large Language Models"
                },
                "summary": "Recent advancements in reinforcement learning (RL) demonstrate the\nsignificant potential in autonomous driving. Despite this promise, challenges\nsuch as the manual design of reward functions and low sample efficiency in\ncomplex environments continue to impede the development of safe and effective\ndriving policies. To tackle these issues, we introduce LearningFlow, an\ninnovative automated policy learning workflow tailored to urban driving. This\nframework leverages the collaboration of multiple large language model (LLM)\nagents throughout the RL training process. LearningFlow includes a curriculum\nsequence generation process and a reward generation process, which work in\ntandem to guide the RL policy by generating tailored training curricula and\nreward functions. Particularly, each process is supported by an analysis agent\nthat evaluates training progress and provides critical insights to the\ngeneration agent. Through the collaborative efforts of these LLM agents,\nLearningFlow automates policy learning across a series of complex driving\ntasks, and it significantly reduces the reliance on manual reward function\ndesign while enhancing sample efficiency. Comprehensive experiments are\nconducted in the high-fidelity CARLA simulator, along with comparisons with\nother existing methods, to demonstrate the efficacy of our proposed approach.\nThe results demonstrate that LearningFlow excels in generating rewards and\ncurricula. It also achieves superior performance and robust generalization\nacross various driving tasks, as well as commendable adaptation to different RL\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reinforcement learning (RL) demonstrate the\nsignificant potential in autonomous driving. Despite this promise, challenges\nsuch as the manual design of reward functions and low sample efficiency in\ncomplex environments continue to impede the development of safe and effective\ndriving policies. To tackle these issues, we introduce LearningFlow, an\ninnovative automated policy learning workflow tailored to urban driving. This\nframework leverages the collaboration of multiple large language model (LLM)\nagents throughout the RL training process. LearningFlow includes a curriculum\nsequence generation process and a reward generation process, which work in\ntandem to guide the RL policy by generating tailored training curricula and\nreward functions. Particularly, each process is supported by an analysis agent\nthat evaluates training progress and provides critical insights to the\ngeneration agent. Through the collaborative efforts of these LLM agents,\nLearningFlow automates policy learning across a series of complex driving\ntasks, and it significantly reduces the reliance on manual reward function\ndesign while enhancing sample efficiency. Comprehensive experiments are\nconducted in the high-fidelity CARLA simulator, along with comparisons with\nother existing methods, to demonstrate the efficacy of our proposed approach.\nThe results demonstrate that LearningFlow excels in generating rewards and\ncurricula. It also achieves superior performance and robust generalization\nacross various driving tasks, as well as commendable adaptation to different RL\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Zengqi Peng"
                    },
                    {
                        "name": "Yubin Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Lei Zheng"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05053v1",
                "updated": "2025-01-09T08:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    24,
                    10,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:24:10Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    24,
                    10,
                    3,
                    9,
                    0
                ],
                "title": "TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated\n  Learning"
                },
                "summary": "Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning is a computing paradigm that enhances privacy by enabling\nmultiple parties to collaboratively train a machine learning model without\nrevealing personal data. However, current research indicates that traditional\nfederated learning platforms are unable to ensure privacy due to privacy leaks\ncaused by the interchange of gradients. To achieve privacy-preserving federated\nlearning, integrating secure aggregation mechanisms is essential.\nUnfortunately, existing solutions are vulnerable to recently demonstrated\ninference attacks such as the disaggregation attack. This paper proposes\nTAPFed, an approach for achieving privacy-preserving federated learning in the\ncontext of multiple decentralized aggregators with malicious actors. TAPFed\nuses a proposed threshold functional encryption scheme and allows for a certain\nnumber of malicious aggregators while maintaining security and privacy. We\nprovide formal security and privacy analyses of TAPFed and compare it to\nvarious baselines through experimental evaluation. Our results show that TAPFed\noffers equivalent performance in terms of model quality compared to\nstate-of-the-art approaches while reducing transmission overhead by 29%-45%\nacross different model training scenarios. Most importantly, TAPFed can defend\nagainst recently demonstrated inference attacks caused by curious aggregators,\nwhich the majority of existing approaches are susceptible to."
                },
                "authors": [
                    {
                        "name": "Runhua Xu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "James B. D. Joshi"
                    },
                    {
                        "name": "Shuai Ma"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "arxiv_doi": "10.1109/TDSC.2024.3350206",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TDSC.2024.3350206",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The paper has been published in IEEE TDSC",
                "arxiv_journal_ref": "in IEEE Transactions on Dependable and Secure Computing, vol. 21,\n  no. 5, pp. 4309-4323, Sept.-Oct. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05051v1",
                "updated": "2025-01-09T08:20:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    20,
                    42,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:20:42Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    20,
                    42,
                    3,
                    9,
                    0
                ],
                "title": "On the Generalizability of Transformer Models to Code Completions of\n  Different Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalizability of Transformer Models to Code Completions of\n  Different Lengths"
                },
                "summary": "The programming landscape is nowadays being reshaped by the advent of Large\nLanguage Models (LLMs) able to automate code-related tasks related to code\nimplementation (e.g., code completion) and comprehension (e.g., code\nsummarization). Such a paradigm shift comes with a number of implications\nrelated to how software will be written, maintained, and evolved. Also, these\nLLMs are extremely expensive to train, posing questions on their sustainability\nover time. Given their training cost, their ability to generalize, namely their\nability to work on task instances different from those on which they have been\ntrained, is an aspect worth being investigated. Previous work already showed\nthat transformer models can successfully support code completion in a\ncross-project setting. However, it is unclear whether LLM are able to\ngeneralize to inputs having lengths not seen during training. For example, it\nis known that training a model on short instances allows to substantially\nreduce the training cost. However, the extent to which such a model would\nprovide good performance on sequences having lengths not seen during training\nis not known. Many recent works in Natural Language Processing (NLP) tackled\nthis problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To\nassess if these solutions extend to encoder-decoder LLMs usually adopted in the\ncode-related tasks, we present a large empirical study evaluating this\ngeneralization property of these and other encoding schemes proposed in the\nliterature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these\nsolutions successfully generalize to unseen lengths and that the only safe\nsolution is to ensure the representativeness in the training set of all lengths\nlikely to be encountered at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The programming landscape is nowadays being reshaped by the advent of Large\nLanguage Models (LLMs) able to automate code-related tasks related to code\nimplementation (e.g., code completion) and comprehension (e.g., code\nsummarization). Such a paradigm shift comes with a number of implications\nrelated to how software will be written, maintained, and evolved. Also, these\nLLMs are extremely expensive to train, posing questions on their sustainability\nover time. Given their training cost, their ability to generalize, namely their\nability to work on task instances different from those on which they have been\ntrained, is an aspect worth being investigated. Previous work already showed\nthat transformer models can successfully support code completion in a\ncross-project setting. However, it is unclear whether LLM are able to\ngeneralize to inputs having lengths not seen during training. For example, it\nis known that training a model on short instances allows to substantially\nreduce the training cost. However, the extent to which such a model would\nprovide good performance on sequences having lengths not seen during training\nis not known. Many recent works in Natural Language Processing (NLP) tackled\nthis problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To\nassess if these solutions extend to encoder-decoder LLMs usually adopted in the\ncode-related tasks, we present a large empirical study evaluating this\ngeneralization property of these and other encoding schemes proposed in the\nliterature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these\nsolutions successfully generalize to unseen lengths and that the only safe\nsolution is to ensure the representativeness in the training set of all lengths\nlikely to be encountered at inference time."
                },
                "authors": [
                    {
                        "name": "Nathan Cooper"
                    },
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Gabriele Bavota"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    }
                ],
                "author_detail": {
                    "name": "Denys Poshyvanyk"
                },
                "author": "Denys Poshyvanyk",
                "arxiv_comment": "Accepted for publication at ICSME 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10440v3",
                "updated": "2025-01-09T07:58:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    58,
                    20,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-15T18:58:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    58,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
                },
                "summary": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities, particularly through inference-time scaling, as illustrated by\nmodels such as OpenAI's o1. However, current Vision-Language Models (VLMs)\noften struggle to perform systematic and structured reasoning, especially when\nhandling complex visual question-answering tasks. In this work, we introduce\nLLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning.\nUnlike chain-of-thought prompting, LLaVA-CoT independently engages in\nsequential stages of summarization, visual interpretation, logical reasoning,\nand conclusion generation. This structured approach enables LLaVA-CoT to\nachieve marked improvements in precision on reasoning-intensive tasks. To\naccomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples\nfrom various visual question answering sources and providing structured\nreasoning annotations. Besides, we propose an inference-time stage-level beam\nsearch method, which enables effective inference-time scaling. Remarkably, with\nonly 100k training samples and a simple yet effective inference time scaling\nmethod, LLaVA-CoT not only outperforms its base model by 7.4% on a wide range\nof multimodal reasoning benchmarks, but also surpasses the performance of\nlarger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and\nLlama-3.2-90B-Vision-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities, particularly through inference-time scaling, as illustrated by\nmodels such as OpenAI's o1. However, current Vision-Language Models (VLMs)\noften struggle to perform systematic and structured reasoning, especially when\nhandling complex visual question-answering tasks. In this work, we introduce\nLLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning.\nUnlike chain-of-thought prompting, LLaVA-CoT independently engages in\nsequential stages of summarization, visual interpretation, logical reasoning,\nand conclusion generation. This structured approach enables LLaVA-CoT to\nachieve marked improvements in precision on reasoning-intensive tasks. To\naccomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples\nfrom various visual question answering sources and providing structured\nreasoning annotations. Besides, we propose an inference-time stage-level beam\nsearch method, which enables effective inference-time scaling. Remarkably, with\nonly 100k training samples and a simple yet effective inference time scaling\nmethod, LLaVA-CoT not only outperforms its base model by 7.4% on a wide range\nof multimodal reasoning benchmarks, but also surpasses the performance of\nlarger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and\nLlama-3.2-90B-Vision-Instruct."
                },
                "authors": [
                    {
                        "name": "Guowei Xu"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05040v1",
                "updated": "2025-01-09T07:54:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    54,
                    24,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:54:24Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    54,
                    24,
                    3,
                    9,
                    0
                ],
                "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source LLM designed to effectively and efficiently resolve GitHub\nissues. SWE-Fixer comprises two essential modules: a code file retrieval module\nand a code editing module. The retrieval module employs BM25 along with a\nlightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently,\nthe code editing module utilizes the other LLM model to generate patches for\nthe identified files. Then, to mitigate the lack of publicly available\ndatasets, we compile an extensive dataset that includes 110K GitHub issues\nalong with their corresponding patches, and train the two modules of SWE-Fixer\nseparately. We assess our approach on the SWE-Bench Lite and Verified\nbenchmarks, achieving state-of-the-art performance among open-source models\nwith scores of 23.3% and 30.2%, respectively. These outcomes highlight the\nefficacy of our approach. We will make our model, dataset, and code publicly\navailable at https://github.com/InternLM/SWE-Fixer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source LLM designed to effectively and efficiently resolve GitHub\nissues. SWE-Fixer comprises two essential modules: a code file retrieval module\nand a code editing module. The retrieval module employs BM25 along with a\nlightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently,\nthe code editing module utilizes the other LLM model to generate patches for\nthe identified files. Then, to mitigate the lack of publicly available\ndatasets, we compile an extensive dataset that includes 110K GitHub issues\nalong with their corresponding patches, and train the two modules of SWE-Fixer\nseparately. We assess our approach on the SWE-Bench Lite and Verified\nbenchmarks, achieving state-of-the-art performance among open-source models\nwith scores of 23.3% and 30.2%, respectively. These outcomes highlight the\nefficacy of our approach. We will make our model, dataset, and code publicly\navailable at https://github.com/InternLM/SWE-Fixer."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Our code, data, and model will be released at\n  https://github.com/InternLM/SWE-Fixer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05032v1",
                "updated": "2025-01-09T07:44:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "Enhancing Human-Like Responses in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Human-Like Responses in Large Language Models"
                },
                "summary": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes."
                },
                "authors": [
                    {
                        "name": "Ethem Yağız Çalık"
                    },
                    {
                        "name": "Talha Rüzgar Akkuş"
                    }
                ],
                "author_detail": {
                    "name": "Talha Rüzgar Akkuş"
                },
                "author": "Talha Rüzgar Akkuş",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05030v1",
                "updated": "2025-01-09T07:41:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    41,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:41:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    41,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "A General Retrieval-Augmented Generation Framework for Multimodal\n  Case-Based Reasoning Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Retrieval-Augmented Generation Framework for Multimodal\n  Case-Based Reasoning Applications"
                },
                "summary": "Case-based reasoning (CBR) is an experience-based approach to problem\nsolving, where a repository of solved cases is adapted to solve new cases.\nRecent research shows that Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages\nof the CBR pipeline by retrieving similar cases and using them as additional\ncontext to an LLM query. Most studies have focused on text-only applications,\nhowever, in many real-world problems the components of a case are multimodal.\nIn this paper we present MCBR-RAG, a general RAG framework for multimodal CBR\napplications. The MCBR-RAG framework converts non-text case components into\ntext-based representations, allowing it to: 1) learn application-specific\nlatent representations that can be indexed for retrieval, and 2) enrich the\nquery provided to the LLM by incorporating all case components for better\ncontext. We demonstrate MCBR-RAG's effectiveness through experiments conducted\non a simplified Math-24 application and a more complex Backgammon application.\nOur empirical results show that MCBR-RAG improves generation quality compared\nto a baseline LLM with no contextual information provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case-based reasoning (CBR) is an experience-based approach to problem\nsolving, where a repository of solved cases is adapted to solve new cases.\nRecent research shows that Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages\nof the CBR pipeline by retrieving similar cases and using them as additional\ncontext to an LLM query. Most studies have focused on text-only applications,\nhowever, in many real-world problems the components of a case are multimodal.\nIn this paper we present MCBR-RAG, a general RAG framework for multimodal CBR\napplications. The MCBR-RAG framework converts non-text case components into\ntext-based representations, allowing it to: 1) learn application-specific\nlatent representations that can be indexed for retrieval, and 2) enrich the\nquery provided to the LLM by incorporating all case components for better\ncontext. We demonstrate MCBR-RAG's effectiveness through experiments conducted\non a simplified Math-24 application and a more complex Backgammon application.\nOur empirical results show that MCBR-RAG improves generation quality compared\nto a baseline LLM with no contextual information provided."
                },
                "authors": [
                    {
                        "name": "Ofir Marom"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Marom"
                },
                "author": "Ofir Marom",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01973v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01973v3",
                "updated": "2025-01-09T07:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    26,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-28T02:28:19Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    2,
                    28,
                    19,
                    5,
                    363,
                    0
                ],
                "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models"
                },
                "summary": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles."
                },
                "authors": [
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Jia Qing Yap"
                    },
                    {
                        "name": "Andrea Wong"
                    },
                    {
                        "name": "Adriana Crespo"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Zhiyuan Yin"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Ryan Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Ye"
                },
                "author": "Ryan Ye",
                "arxiv_comment": "Di Jin and Xing Liu contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01973v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01973v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05017v1",
                "updated": "2025-01-09T07:18:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    18,
                    48,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    18,
                    48,
                    3,
                    9,
                    0
                ],
                "title": "Continuous Knowledge-Preserving Decomposition for Few-Shot Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Knowledge-Preserving Decomposition for Few-Shot Continual\n  Learning"
                },
                "summary": "Few-shot class-incremental learning (FSCIL) involves learning new classes\nfrom limited data while retaining prior knowledge, and often results in\ncatastrophic forgetting. Existing methods either freeze backbone networks to\npreserve knowledge, which limits adaptability, or rely on additional modules or\nprompts, introducing inference overhead. To this end, we propose Continuous\nKnowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that\ndecomposes a model's weights into two parts: one that compacts existing\nknowledge (knowledge-sensitive components) and another that carries redundant\ncapacity to accommodate new abilities (redundant-capacity components). The\ndecomposition is guided by a covariance matrix from replay samples, ensuring\nprincipal components align with classification abilities. During adaptation, we\nfreeze the knowledge-sensitive components and only adapt the redundant-capacity\ncomponents, fostering plasticity while minimizing interference without changing\nthe architecture or increasing overhead. Additionally, CKPD introduces an\nadaptive layer selection strategy to identify layers with redundant capacity,\ndynamically allocating adapters. Experiments on multiple benchmarks show that\nCKPD-FSCIL outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot class-incremental learning (FSCIL) involves learning new classes\nfrom limited data while retaining prior knowledge, and often results in\ncatastrophic forgetting. Existing methods either freeze backbone networks to\npreserve knowledge, which limits adaptability, or rely on additional modules or\nprompts, introducing inference overhead. To this end, we propose Continuous\nKnowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that\ndecomposes a model's weights into two parts: one that compacts existing\nknowledge (knowledge-sensitive components) and another that carries redundant\ncapacity to accommodate new abilities (redundant-capacity components). The\ndecomposition is guided by a covariance matrix from replay samples, ensuring\nprincipal components align with classification abilities. During adaptation, we\nfreeze the knowledge-sensitive components and only adapt the redundant-capacity\ncomponents, fostering plasticity while minimizing interference without changing\nthe architecture or increasing overhead. Additionally, CKPD introduces an\nadaptive layer selection strategy to identify layers with redundant capacity,\ndynamically allocating adapters. Experiments on multiple benchmarks show that\nCKPD-FSCIL outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xiaojie Li"
                    },
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Code: https://github.com/xiaojieli0903/CKPD-FSCIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05007v1",
                "updated": "2025-01-09T07:05:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    5,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:05:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    5,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "Quantum-enhanced causal discovery for a small number of samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-enhanced causal discovery for a small number of samples"
                },
                "summary": "The discovery of causal relationships from observed data has attracted\nsignificant interest from disciplines such as economics, social sciences,\nepidemiology, and biology. In practical applications, considerable knowledge of\nthe underlying systems is often unavailable, and real data are often associated\nwith nonlinear causal structures, which make the direct use of most\nconventional causality analysis methods difficult. This study proposes a novel\nquantum Peter-Clark (qPC) algorithm for causal discovery that does not assume\nany underlying model structures. Based on the independence conditional tests in\na class of reproducing kernel Hilbert spaces characterized by quantum circuits,\nthe proposed qPC algorithm can explore causal relationships from the observed\ndata drawn from arbitrary distributions. We conducted systematic experiments on\nfundamental graph parts of causal structures, demonstrating that the qPC\nalgorithm exhibits a significantly better performance, particularly with\nsmaller sample sizes compared to its classical counterpart. Furthermore, we\nproposed a novel optimization approach based on Kernel Target Alignment (KTA)\nfor determining hyperparameters of quantum kernels. This method effectively\nreduced the risk of false positives in causal discovery, enabling more reliable\ninference. Our theoretical and experimental results demonstrate that the\nproposed quantum algorithm can empower classical algorithms for robust and\naccurate inference in causal discovery, supporting them in regimes where\nclassical algorithms typically fail. Additionally, the effectiveness of this\nmethod was validated using the Boston Housing dataset as a real-world\napplication. These findings demonstrate the new potential of quantum\ncircuit-based causal discovery methods in addressing practical challenges,\nparticularly in small-sample scenarios where traditional approaches have shown\nlimitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of causal relationships from observed data has attracted\nsignificant interest from disciplines such as economics, social sciences,\nepidemiology, and biology. In practical applications, considerable knowledge of\nthe underlying systems is often unavailable, and real data are often associated\nwith nonlinear causal structures, which make the direct use of most\nconventional causality analysis methods difficult. This study proposes a novel\nquantum Peter-Clark (qPC) algorithm for causal discovery that does not assume\nany underlying model structures. Based on the independence conditional tests in\na class of reproducing kernel Hilbert spaces characterized by quantum circuits,\nthe proposed qPC algorithm can explore causal relationships from the observed\ndata drawn from arbitrary distributions. We conducted systematic experiments on\nfundamental graph parts of causal structures, demonstrating that the qPC\nalgorithm exhibits a significantly better performance, particularly with\nsmaller sample sizes compared to its classical counterpart. Furthermore, we\nproposed a novel optimization approach based on Kernel Target Alignment (KTA)\nfor determining hyperparameters of quantum kernels. This method effectively\nreduced the risk of false positives in causal discovery, enabling more reliable\ninference. Our theoretical and experimental results demonstrate that the\nproposed quantum algorithm can empower classical algorithms for robust and\naccurate inference in causal discovery, supporting them in regimes where\nclassical algorithms typically fail. Additionally, the effectiveness of this\nmethod was validated using the Boston Housing dataset as a real-world\napplication. These findings demonstrate the new potential of quantum\ncircuit-based causal discovery methods in addressing practical challenges,\nparticularly in small-sample scenarios where traditional approaches have shown\nlimitations."
                },
                "authors": [
                    {
                        "name": "Yota Maeda"
                    },
                    {
                        "name": "Ken Arai"
                    },
                    {
                        "name": "Yu Tanaka"
                    },
                    {
                        "name": "Yu Terada"
                    },
                    {
                        "name": "Hiroshi Ueno"
                    },
                    {
                        "name": "Hiroyuki Tezuka"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Tezuka"
                },
                "author": "Hiroyuki Tezuka",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04919v2",
                "updated": "2025-01-09T07:00:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    0,
                    24,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-09T08:01:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    1,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost."
                },
                "authors": [
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Yingxia Shao"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-41136-3}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10517v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10517v5",
                "updated": "2025-01-09T06:41:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    41,
                    46,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-20T03:35:28Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    3,
                    35,
                    28,
                    1,
                    233,
                    0
                ],
                "title": "Integrating Multi-Modal Input Token Mixer Into Mamba-Based Decision\n  Models: Decision MetaMamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Multi-Modal Input Token Mixer Into Mamba-Based Decision\n  Models: Decision MetaMamba"
                },
                "summary": "Sequence modeling with State Space models (SSMs) has demonstrated performance\nsurpassing that of Transformers in various tasks, raising expectations for\ntheir potential to outperform the Decision Transformer and its enhanced\nvariants in offline reinforcement learning (RL). However, decision models based\non Mamba, a state-of-the-art SSM, failed to achieve superior performance\ncompared to these enhanced Decision Transformers. We hypothesize that this\nlimitation arises from information loss during the selective scanning phase. To\naddress this, we propose the Decision MetaMamba (DMM), which augments Mamba\nwith a token mixer in its input layer. This mixer explicitly accounts for the\nmultimodal nature of offline RL inputs, comprising state, action, and\nreturn-to-go. The DMM demonstrates improved performance while significantly\nreducing parameter count compared to prior models. Notably, similar performance\ngains were achieved using a simple linear token mixer, emphasizing the\nimportance of preserving information from proximate time steps rather than the\nspecific design of the token mixer itself. This novel modification to Mamba's\ninput layer represents a departure from conventional timestamp-based encoding\napproaches used in Transformers. By enhancing performance of Mamba in offline\nRL, characterized by memory efficiency and fast inference, this work opens new\navenues for its broader application in future RL research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence modeling with State Space models (SSMs) has demonstrated performance\nsurpassing that of Transformers in various tasks, raising expectations for\ntheir potential to outperform the Decision Transformer and its enhanced\nvariants in offline reinforcement learning (RL). However, decision models based\non Mamba, a state-of-the-art SSM, failed to achieve superior performance\ncompared to these enhanced Decision Transformers. We hypothesize that this\nlimitation arises from information loss during the selective scanning phase. To\naddress this, we propose the Decision MetaMamba (DMM), which augments Mamba\nwith a token mixer in its input layer. This mixer explicitly accounts for the\nmultimodal nature of offline RL inputs, comprising state, action, and\nreturn-to-go. The DMM demonstrates improved performance while significantly\nreducing parameter count compared to prior models. Notably, similar performance\ngains were achieved using a simple linear token mixer, emphasizing the\nimportance of preserving information from proximate time steps rather than the\nspecific design of the token mixer itself. This novel modification to Mamba's\ninput layer represents a departure from conventional timestamp-based encoding\napproaches used in Transformers. By enhancing performance of Mamba in offline\nRL, characterized by memory efficiency and fast inference, this work opens new\navenues for its broader application in future RL research."
                },
                "authors": [
                    {
                        "name": "Wall Kim"
                    }
                ],
                "author_detail": {
                    "name": "Wall Kim"
                },
                "author": "Wall Kim",
                "arxiv_comment": "We have decided to withdraw this manuscript as we believe that the\n  work requires significant improvements and further research to ensure its\n  quality and impact. We are currently pursuing a more comprehensive approach\n  to address the limitations of the current submission and plan to resubmit an\n  improved version in the future",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10517v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10517v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14368v2",
                "updated": "2025-01-09T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    2,
                    11,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-18T10:53:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    53,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic"
                },
                "summary": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Vishnu Nandam"
                    },
                    {
                        "name": "Justin Turnau"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Linsey Pang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "8 pages, 4 figures, accepted to SDM25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 90B20, 90C27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.9; H.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v1",
                "updated": "2025-01-09T06:00:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04985v1",
                "updated": "2025-01-09T06:00:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    8,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:08Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    8,
                    3,
                    9,
                    0
                ],
                "title": "SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and\n  Commercial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and\n  Commercial LLMs"
                },
                "summary": "The increasing threat of SMS spam, driven by evolving adversarial techniques\nand concept drift, calls for more robust and adaptive detection methods. In\nthis paper, we evaluate the potential of large language models (LLMs), both\nopen-source and commercial, for SMS spam detection, comparing their performance\nacross zero-shot, few-shot, fine-tuning, and chain-of-thought prompting\napproaches. Using a comprehensive dataset of SMS messages, we assess the spam\ndetection capabilities of prominent LLMs such as GPT-4, DeepSeek, LLAMA-2, and\nMixtral. Our findings reveal that while zero-shot learning provides\nconvenience, it is unreliable for effective spam detection. Few-shot learning,\nparticularly with carefully selected examples, improves detection but exhibits\nvariability across models. Fine-tuning emerges as the most effective strategy,\nwith Mixtral achieving 98.6% accuracy and a balanced false positive and false\nnegative rate below 2%, meeting the criteria for robust spam detection.\nFurthermore, we explore the resilience of these models to adversarial attacks,\nfinding that fine-tuning significantly enhances robustness against both\nperceptible and imperceptible manipulations. Lastly, we investigate the impact\nof concept drift and demonstrate that fine-tuned LLMs, especially when combined\nwith few-shot learning, can mitigate its effects, maintaining high performance\neven on evolving spam datasets. This study highlights the importance of\nfine-tuning and tailored learning strategies to deploy LLMs effectively for\nreal-world SMS spam detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing threat of SMS spam, driven by evolving adversarial techniques\nand concept drift, calls for more robust and adaptive detection methods. In\nthis paper, we evaluate the potential of large language models (LLMs), both\nopen-source and commercial, for SMS spam detection, comparing their performance\nacross zero-shot, few-shot, fine-tuning, and chain-of-thought prompting\napproaches. Using a comprehensive dataset of SMS messages, we assess the spam\ndetection capabilities of prominent LLMs such as GPT-4, DeepSeek, LLAMA-2, and\nMixtral. Our findings reveal that while zero-shot learning provides\nconvenience, it is unreliable for effective spam detection. Few-shot learning,\nparticularly with carefully selected examples, improves detection but exhibits\nvariability across models. Fine-tuning emerges as the most effective strategy,\nwith Mixtral achieving 98.6% accuracy and a balanced false positive and false\nnegative rate below 2%, meeting the criteria for robust spam detection.\nFurthermore, we explore the resilience of these models to adversarial attacks,\nfinding that fine-tuning significantly enhances robustness against both\nperceptible and imperceptible manipulations. Lastly, we investigate the impact\nof concept drift and demonstrate that fine-tuned LLMs, especially when combined\nwith few-shot learning, can mitigate its effects, maintaining high performance\neven on evolving spam datasets. This study highlights the importance of\nfine-tuning and tailored learning strategies to deploy LLMs effectively for\nreal-world SMS spam detection"
                },
                "authors": [
                    {
                        "name": "Muhammad Salman"
                    },
                    {
                        "name": "Muhammad Ikram"
                    },
                    {
                        "name": "Nardine Basta"
                    },
                    {
                        "name": "Mohamed Ali Kaafar"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Ali Kaafar"
                },
                "author": "Mohamed Ali Kaafar",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04983v1",
                "updated": "2025-01-09T05:54:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    54,
                    1,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T05:54:01Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    54,
                    1,
                    3,
                    9,
                    0
                ],
                "title": "Collective inference of the truth of propositions from crowd probability\n  judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective inference of the truth of propositions from crowd probability\n  judgments"
                },
                "summary": "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty."
                },
                "authors": [
                    {
                        "name": "Patrick Stinson"
                    },
                    {
                        "name": "Jasper van den Bosch"
                    },
                    {
                        "name": "Trenton Jerde"
                    },
                    {
                        "name": "Nikolaus Kriegeskorte"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Kriegeskorte"
                },
                "author": "Nikolaus Kriegeskorte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01145v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01145v5",
                "updated": "2025-01-09T05:14:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    14,
                    56,
                    3,
                    9,
                    0
                ],
                "published": "2024-01-02T10:55:01Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    10,
                    55,
                    1,
                    1,
                    2,
                    0
                ],
                "title": "HAAQI-Net: A Non-intrusive Neural Music Audio Quality Assessment Model\n  for Hearing Aids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAAQI-Net: A Non-intrusive Neural Music Audio Quality Assessment Model\n  for Hearing Aids"
                },
                "summary": "This paper introduces HAAQI-Net, a non-intrusive deep learning-based music\naudio quality assessment model for hearing aid users. Unlike traditional\nmethods like the Hearing Aid Audio Quality Index (HAAQI) that require intrusive\nreference signal comparisons, HAAQI-Net offers a more accessible and\ncomputationally efficient alternative. By utilizing a Bidirectional Long\nShort-Term Memory (BLSTM) architecture with attention mechanisms and features\nextracted from the pre-trained BEATs model, it can predict HAAQI scores\ndirectly from music audio clips and hearing loss patterns. Experimental results\ndemonstrate HAAQI-Net's effectiveness, achieving a Linear Correlation\nCoefficient (LCC) of 0.9368 , a Spearman's Rank Correlation Coefficient (SRCC)\nof 0.9486 , and a Mean Squared Error (MSE) of 0.0064 and inference time\nsignificantly reduces from 62.52 to 2.54 seconds. To address computational\noverhead, a knowledge distillation strategy was applied, reducing parameters by\n75.85% and inference time by 96.46%, while maintaining strong performance (LCC:\n0.9071 , SRCC: 0.9307 , MSE: 0.0091 ). To expand its capabilities, HAAQI-Net\nwas adapted to predict subjective human scores like the Mean Opinion Score\n(MOS) through fine-tuning. This adaptation significantly improved prediction\naccuracy, validated through statistical analysis. Furthermore, the robustness\nof HAAQI-Net was evaluated under varying Sound Pressure Level (SPL) conditions,\nrevealing optimal performance at a reference SPL of 65 dB, with accuracy\ngradually decreasing as SPL deviated from this point. The advancements in\nsubjective score prediction, SPL robustness, and computational efficiency\nposition HAAQI-Net as a scalable solution for music audio quality assessment in\nhearing aid applications, contributing to efficient and accurate models in\naudio signal processing and hearing aid technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces HAAQI-Net, a non-intrusive deep learning-based music\naudio quality assessment model for hearing aid users. Unlike traditional\nmethods like the Hearing Aid Audio Quality Index (HAAQI) that require intrusive\nreference signal comparisons, HAAQI-Net offers a more accessible and\ncomputationally efficient alternative. By utilizing a Bidirectional Long\nShort-Term Memory (BLSTM) architecture with attention mechanisms and features\nextracted from the pre-trained BEATs model, it can predict HAAQI scores\ndirectly from music audio clips and hearing loss patterns. Experimental results\ndemonstrate HAAQI-Net's effectiveness, achieving a Linear Correlation\nCoefficient (LCC) of 0.9368 , a Spearman's Rank Correlation Coefficient (SRCC)\nof 0.9486 , and a Mean Squared Error (MSE) of 0.0064 and inference time\nsignificantly reduces from 62.52 to 2.54 seconds. To address computational\noverhead, a knowledge distillation strategy was applied, reducing parameters by\n75.85% and inference time by 96.46%, while maintaining strong performance (LCC:\n0.9071 , SRCC: 0.9307 , MSE: 0.0091 ). To expand its capabilities, HAAQI-Net\nwas adapted to predict subjective human scores like the Mean Opinion Score\n(MOS) through fine-tuning. This adaptation significantly improved prediction\naccuracy, validated through statistical analysis. Furthermore, the robustness\nof HAAQI-Net was evaluated under varying Sound Pressure Level (SPL) conditions,\nrevealing optimal performance at a reference SPL of 65 dB, with accuracy\ngradually decreasing as SPL deviated from this point. The advancements in\nsubjective score prediction, SPL robustness, and computational efficiency\nposition HAAQI-Net as a scalable solution for music audio quality assessment in\nhearing aid applications, contributing to efficient and accurate models in\naudio signal processing and hearing aid technology."
                },
                "authors": [
                    {
                        "name": "Dyah A. M. G. Wisnu"
                    },
                    {
                        "name": "Stefano Rini"
                    },
                    {
                        "name": "Ryandhimas E. Zezario"
                    },
                    {
                        "name": "Hsin-Min Wang"
                    },
                    {
                        "name": "Yu Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tsao"
                },
                "author": "Yu Tsao",
                "arxiv_comment": "Accepted by IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01145v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01145v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04975v1",
                "updated": "2025-01-09T05:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    12,
                    38,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T05:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    12,
                    38,
                    3,
                    9,
                    0
                ],
                "title": "V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer"
                },
                "summary": "Concept Bottleneck Models (CBMs) offer inherent interpretability by initially\ntranslating images into human-comprehensible concepts, followed by a linear\ncombination of these concepts for classification. However, the annotation of\nconcepts for visual recognition tasks requires extensive expert knowledge and\nlabor, constraining the broad adoption of CBMs. Recent approaches have\nleveraged the knowledge of large language models to construct concept\nbottlenecks, with multimodal models like CLIP subsequently mapping image\nfeatures into the concept feature space for classification. Despite this, the\nconcepts produced by language models can be verbose and may introduce\nnon-visual attributes, which hurts accuracy and interpretability. In this\nstudy, we investigate to avoid these issues by constructing CBMs directly from\nmultimodal models. To this end, we adopt common words as base concept\nvocabulary and leverage auxiliary unlabeled images to construct a\nVision-to-Concept (V2C) tokenizer that can explicitly quantize images into\ntheir most relevant visual concepts, thus creating a vision-oriented concept\nbottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM\nwhich is training efficient and interpretable with high accuracy. Our V2C-CBM\nhas matched or outperformed LLM-supervised CBMs on various visual\nclassification benchmarks, validating the efficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Bottleneck Models (CBMs) offer inherent interpretability by initially\ntranslating images into human-comprehensible concepts, followed by a linear\ncombination of these concepts for classification. However, the annotation of\nconcepts for visual recognition tasks requires extensive expert knowledge and\nlabor, constraining the broad adoption of CBMs. Recent approaches have\nleveraged the knowledge of large language models to construct concept\nbottlenecks, with multimodal models like CLIP subsequently mapping image\nfeatures into the concept feature space for classification. Despite this, the\nconcepts produced by language models can be verbose and may introduce\nnon-visual attributes, which hurts accuracy and interpretability. In this\nstudy, we investigate to avoid these issues by constructing CBMs directly from\nmultimodal models. To this end, we adopt common words as base concept\nvocabulary and leverage auxiliary unlabeled images to construct a\nVision-to-Concept (V2C) tokenizer that can explicitly quantize images into\ntheir most relevant visual concepts, thus creating a vision-oriented concept\nbottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM\nwhich is training efficient and interpretable with high accuracy. Our V2C-CBM\nhas matched or outperformed LLM-supervised CBMs on various visual\nclassification benchmarks, validating the efficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Hangzhou He"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Xinliang Zhang"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Yanye Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanye Lu"
                },
                "author": "Yanye Lu",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02795v2",
                "updated": "2025-01-09T04:50:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    50,
                    16,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-06T06:29:55Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    29,
                    55,
                    0,
                    6,
                    0
                ],
                "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Yan"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuhao Fu"
                    },
                    {
                        "name": "Baoyi He"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Yining Di"
                    },
                    {
                        "name": "Chunlin Ji"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04961v1",
                "updated": "2025-01-09T04:26:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    26,
                    15,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T04:26:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    26,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Domain-adaptive Post-training for Financial LLMs"
                },
                "summary": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain-adaptive post-training of LLMs for the finance\ndomain. Our approach begins by identifying the core capabilities required for\nthe target domain and designing a comprehensive evaluation suite aligned with\nthese needs. We then analyze the effectiveness of key post-training stages,\nincluding continual pretraining, instruction tuning, and preference alignment.\nBuilding on these insights, we propose an effective training recipe centered on\na novel preference data distillation method, which leverages process signals\nfrom a generative reward model. The resulting model, Llama-Fin, achieves\nstate-of-the-art performance across a wide range of financial tasks. Our\nanalysis also highlights how each post-training stage contributes to distinct\ncapabilities, uncovering specific challenges and effective solutions, providing\nvaluable insights for domain adaptation of LLMs. Project page:\nhttps://github.com/SalesforceAIResearch/FinDap",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain-adaptive post-training of LLMs for the finance\ndomain. Our approach begins by identifying the core capabilities required for\nthe target domain and designing a comprehensive evaluation suite aligned with\nthese needs. We then analyze the effectiveness of key post-training stages,\nincluding continual pretraining, instruction tuning, and preference alignment.\nBuilding on these insights, we propose an effective training recipe centered on\na novel preference data distillation method, which leverages process signals\nfrom a generative reward model. The resulting model, Llama-Fin, achieves\nstate-of-the-art performance across a wide range of financial tasks. Our\nanalysis also highlights how each post-training stage contributes to distinct\ncapabilities, uncovering specific challenges and effective solutions, providing\nvaluable insights for domain adaptation of LLMs. Project page:\nhttps://github.com/SalesforceAIResearch/FinDap"
                },
                "authors": [
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07074v2",
                "updated": "2025-01-09T04:25:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    25,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-09T17:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning"
                },
                "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Yichuan Li"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Kyumin Lee"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.10168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.10168v3",
                "updated": "2025-01-09T04:13:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    13,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2023-07-19T17:54:43Z",
                "published_parsed": [
                    2023,
                    7,
                    19,
                    17,
                    54,
                    43,
                    2,
                    200,
                    0
                ],
                "title": "LLMs as Workers in Human-Computational Algorithms? Replicating\n  Crowdsourcing Pipelines with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Workers in Human-Computational Algorithms? Replicating\n  Crowdsourcing Pipelines with LLMs"
                },
                "summary": "LLMs have shown promise in replicating human-like behavior in crowdsourcing\ntasks that were previously thought to be exclusive to human abilities. However,\ncurrent efforts focus mainly on simple atomic tasks. We explore whether LLMs\ncan replicate more complex crowdsourcing pipelines. We find that modern LLMs\ncan simulate some of crowdworkers' abilities in these ``human computation\nalgorithms,'' but the level of success is variable and influenced by\nrequesters' understanding of LLM capabilities, the specific skills required for\nsub-tasks, and the optimal interaction modality for performing these sub-tasks.\nWe reflect on human and LLMs' different sensitivities to instructions, stress\nthe importance of enabling human-facing safeguards for LLMs, and discuss the\npotential of training humans and LLMs with complementary skill sets. Crucially,\nwe show that replicating crowdsourcing pipelines offers a valuable platform to\ninvestigate 1) the relative LLM strengths on different tasks (by\ncross-comparing their performances on sub-tasks) and 2) LLMs' potential in\ncomplex tasks, where they can complete part of the tasks while leaving others\nto humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown promise in replicating human-like behavior in crowdsourcing\ntasks that were previously thought to be exclusive to human abilities. However,\ncurrent efforts focus mainly on simple atomic tasks. We explore whether LLMs\ncan replicate more complex crowdsourcing pipelines. We find that modern LLMs\ncan simulate some of crowdworkers' abilities in these ``human computation\nalgorithms,'' but the level of success is variable and influenced by\nrequesters' understanding of LLM capabilities, the specific skills required for\nsub-tasks, and the optimal interaction modality for performing these sub-tasks.\nWe reflect on human and LLMs' different sensitivities to instructions, stress\nthe importance of enabling human-facing safeguards for LLMs, and discuss the\npotential of training humans and LLMs with complementary skill sets. Crucially,\nwe show that replicating crowdsourcing pipelines offers a valuable platform to\ninvestigate 1) the relative LLM strengths on different tasks (by\ncross-comparing their performances on sub-tasks) and 2) LLMs' potential in\ncomplex tasks, where they can complete part of the tasks while leaving others\nto humans."
                },
                "authors": [
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Haiyi Zhu"
                    },
                    {
                        "name": "Maya Albayrak"
                    },
                    {
                        "name": "Alexis Axon"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Wenxing Deng"
                    },
                    {
                        "name": "Ziqi Ding"
                    },
                    {
                        "name": "Bill Guo"
                    },
                    {
                        "name": "Sireesh Gururaja"
                    },
                    {
                        "name": "Tzu-Sheng Kuo"
                    },
                    {
                        "name": "Jenny T. Liang"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Ihita Mandal"
                    },
                    {
                        "name": "Jeremiah Milbauer"
                    },
                    {
                        "name": "Xiaolin Ni"
                    },
                    {
                        "name": "Namrata Padmanabhan"
                    },
                    {
                        "name": "Subhashini Ramkumar"
                    },
                    {
                        "name": "Alexis Sudjianto"
                    },
                    {
                        "name": "Jordan Taylor"
                    },
                    {
                        "name": "Ying-Jui Tseng"
                    },
                    {
                        "name": "Patricia Vaidos"
                    },
                    {
                        "name": "Zhijin Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chenyang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyang Yang"
                },
                "author": "Chenyang Yang",
                "arxiv_doi": "10.1145/3706599.3706690",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3706690",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.10168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.10168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "CHI 2025 Case Study Track",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04947v1",
                "updated": "2025-01-09T03:50:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    50,
                    0,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T03:50:00Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    50,
                    0,
                    3,
                    9,
                    0
                ],
                "title": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene\n  Recognition in Built Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene\n  Recognition in Built Environments"
                },
                "summary": "In assistive robotics serving people with disabilities (PWD), accurate place\nrecognition in built environments is crucial to ensure that robots navigate and\ninteract safely within diverse indoor spaces. Language interfaces, particularly\nthose powered by Large Language Models (LLM) and Vision Language Models (VLM),\nhold significant promise in this context, as they can interpret visual scenes\nand correlate them with semantic information. However, such interfaces are also\nknown for their hallucinated predictions. In addition, language instructions\nprovided by humans can also be ambiguous and lack precise details about\nspecific locations, objects, or actions, exacerbating the hallucination issue.\nIn this work, we introduce Seeing with Partial Certainty (SwPC) - a framework\ndesigned to measure and align uncertainty in VLM-based place recognition,\nenabling the model to recognize when it lacks confidence and seek assistance\nwhen necessary. This framework is built on the theory of conformal prediction\nto provide statistical guarantees on place recognition while minimizing\nrequests for human help in complex indoor environment settings. Through\nexperiments on the widely used richly-annotated scene dataset Matterport3D, we\nshow that SwPC significantly increases the success rate and decreases the\namount of human intervention required relative to the prior art. SwPC can be\nutilized with any VLMs directly without requiring model fine-tuning, offering a\npromising, lightweight approach to uncertainty modeling that complements and\nscales alongside the expanding capabilities of foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In assistive robotics serving people with disabilities (PWD), accurate place\nrecognition in built environments is crucial to ensure that robots navigate and\ninteract safely within diverse indoor spaces. Language interfaces, particularly\nthose powered by Large Language Models (LLM) and Vision Language Models (VLM),\nhold significant promise in this context, as they can interpret visual scenes\nand correlate them with semantic information. However, such interfaces are also\nknown for their hallucinated predictions. In addition, language instructions\nprovided by humans can also be ambiguous and lack precise details about\nspecific locations, objects, or actions, exacerbating the hallucination issue.\nIn this work, we introduce Seeing with Partial Certainty (SwPC) - a framework\ndesigned to measure and align uncertainty in VLM-based place recognition,\nenabling the model to recognize when it lacks confidence and seek assistance\nwhen necessary. This framework is built on the theory of conformal prediction\nto provide statistical guarantees on place recognition while minimizing\nrequests for human help in complex indoor environment settings. Through\nexperiments on the widely used richly-annotated scene dataset Matterport3D, we\nshow that SwPC significantly increases the success rate and decreases the\namount of human intervention required relative to the prior art. SwPC can be\nutilized with any VLMs directly without requiring model fine-tuning, offering a\npromising, lightweight approach to uncertainty modeling that complements and\nscales alongside the expanding capabilities of foundational models."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Vineet Kamat"
                    },
                    {
                        "name": "Carol Menassa"
                    }
                ],
                "author_detail": {
                    "name": "Carol Menassa"
                },
                "author": "Carol Menassa",
                "arxiv_comment": "10 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04945v1",
                "updated": "2025-01-09T03:34:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    34,
                    7,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T03:34:07Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    34,
                    7,
                    3,
                    9,
                    0
                ],
                "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models"
                },
                "summary": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints."
                },
                "authors": [
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00358v2",
                "updated": "2025-01-09T03:25:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    25,
                    24,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-31T09:22:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    22,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and\n  Embodied Sensors Enables Dynamic Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and\n  Embodied Sensors Enables Dynamic Scene Understanding"
                },
                "summary": "This paper investigates the problem of understanding dynamic 3D scenes from\negocentric observations, a key challenge in robotics and embodied AI. Unlike\nprior studies that explored this as long-form video understanding and utilized\negocentric video only, we instead propose an LLM-based agent, Embodied\nVideoAgent, which constructs scene memory from both egocentric video and\nembodied sensory inputs (e.g. depth and pose sensing). We further introduce a\nVLM-based approach to automatically update the memory when actions or\nactivities over objects are perceived. Embodied VideoAgent attains significant\nadvantages over counterparts in challenging reasoning and planning tasks in 3D\nscenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on\nEnvQA. We have also demonstrated its potential in various embodied AI tasks\nincluding generating embodied interactions and perception for robot\nmanipulation. The code and demo will be made public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the problem of understanding dynamic 3D scenes from\negocentric observations, a key challenge in robotics and embodied AI. Unlike\nprior studies that explored this as long-form video understanding and utilized\negocentric video only, we instead propose an LLM-based agent, Embodied\nVideoAgent, which constructs scene memory from both egocentric video and\nembodied sensory inputs (e.g. depth and pose sensing). We further introduce a\nVLM-based approach to automatically update the memory when actions or\nactivities over objects are perceived. Embodied VideoAgent attains significant\nadvantages over counterparts in challenging reasoning and planning tasks in 3D\nscenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on\nEnvQA. We have also demonstrated its potential in various embodied AI tasks\nincluding generating embodied interactions and perception for robot\nmanipulation. The code and demo will be made public."
                },
                "authors": [
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Rongpeng Su"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Rujie Wu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "project page: https://embodied-videoagent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15594v3",
                "updated": "2025-01-09T03:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    8,
                    17,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-23T16:03:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    16,
                    3,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "A Survey on LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-as-a-Judge"
                },
                "summary": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Jiawei Gu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Xuehao Zhai"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "arxiv_comment": "Corrected typos & more discussion on reasoning models 33 pages, 9\n  figures. arXiv admin note: text overlap with arXiv:2310.05470 by other\n  authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04934v1",
                "updated": "2025-01-09T02:52:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    52,
                    30,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T02:52:30Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    52,
                    30,
                    3,
                    9,
                    0
                ],
                "title": "Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel\n  Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel\n  Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images"
                },
                "summary": "Existing Weakly-Supervised Change Detection (WSCD) methods often encounter\nthe problem of \"instance lumping\" under scene-level supervision, particularly\nin scenarios with a dense distribution of changed instances (i.e., changed\nobjects). In these scenarios, unchanged pixels between changed instances are\nalso mistakenly identified as changed, causing multiple changes to be\nmistakenly viewed as one. In practical applications, this issue prevents the\naccurate quantification of the number of changes. To address this issue, we\npropose a Dense Instance Separation (DISep) method as a plug-and-play solution,\nrefining pixel features from a unified instance perspective under scene-level\nsupervision. Specifically, our DISep comprises a three-step iterative training\nprocess: 1) Instance Localization: We locate instance candidate regions for\nchanged pixels using high-pass class activation maps. 2) Instance Retrieval: We\nidentify and group these changed pixels into different instance IDs through\nconnectivity searching. Then, based on the assigned instance IDs, we extract\ncorresponding pixel-level features on a per-instance basis. 3) Instance\nSeparation: We introduce a separation loss to enforce intra-instance pixel\nconsistency in the embedding space, thereby ensuring separable instance feature\nrepresentations. The proposed DISep adds only minimal training cost and no\ninference cost. It can be seamlessly integrated to enhance existing WSCD\nmethods. We achieve state-of-the-art performance by enhancing {three\nTransformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD,\nDSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to\nimprove fully-supervised change detection methods. Code is available at\nhttps://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Weakly-Supervised Change Detection (WSCD) methods often encounter\nthe problem of \"instance lumping\" under scene-level supervision, particularly\nin scenarios with a dense distribution of changed instances (i.e., changed\nobjects). In these scenarios, unchanged pixels between changed instances are\nalso mistakenly identified as changed, causing multiple changes to be\nmistakenly viewed as one. In practical applications, this issue prevents the\naccurate quantification of the number of changes. To address this issue, we\npropose a Dense Instance Separation (DISep) method as a plug-and-play solution,\nrefining pixel features from a unified instance perspective under scene-level\nsupervision. Specifically, our DISep comprises a three-step iterative training\nprocess: 1) Instance Localization: We locate instance candidate regions for\nchanged pixels using high-pass class activation maps. 2) Instance Retrieval: We\nidentify and group these changed pixels into different instance IDs through\nconnectivity searching. Then, based on the assigned instance IDs, we extract\ncorresponding pixel-level features on a per-instance basis. 3) Instance\nSeparation: We introduce a separation loss to enforce intra-instance pixel\nconsistency in the embedding space, thereby ensuring separable instance feature\nrepresentations. The proposed DISep adds only minimal training cost and no\ninference cost. It can be seamlessly integrated to enhance existing WSCD\nmethods. We achieve state-of-the-art performance by enhancing {three\nTransformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD,\nDSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to\nimprove fully-supervised change detection methods. Code is available at\nhttps://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection."
                },
                "authors": [
                    {
                        "name": "Zhenghui Zhao"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Lixiang Ru"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Hongruixuan Chen"
                    },
                    {
                        "name": "Cuiqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cuiqun Chen"
                },
                "author": "Cuiqun Chen",
                "arxiv_comment": "Accepted by ISPRS Journal of Photogrammetry and Remote Sensing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04932v1",
                "updated": "2025-01-09T02:47:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    47,
                    50,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T02:47:50Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    47,
                    50,
                    3,
                    9,
                    0
                ],
                "title": "The Catalogue of Virtual Early-Type Galaxies from IllustrisTNG:\n  Validation and Real Observation Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Catalogue of Virtual Early-Type Galaxies from IllustrisTNG:\n  Validation and Real Observation Consistency"
                },
                "summary": "Early-type galaxies (ETGs) are reference systems to understand galaxy\nformation and evolution processes. The physics of their collapse and internal\ndynamics are codified in well-known scaling relations. Cosmological\nhydrodynamical simulations play an important role, providing insights into the\n3D distribution of matter and galaxy formation mechanisms, as well as\nvalidating methods to infer the properties of real objects. In this work, we\npresent the closest-to-reality sample of ETGs from the IllustrisTNG100-1\nsimulation, dubbed \"virtual-ETGs,\" based on an observational-like algorithm\nthat combines standard projected and three-dimensional galaxy structural\nparameters. We extract 2D photometric information by projecting the galaxies'\nlight into three planes and modeling them via S\\'ersic profiles. Aperture\nvelocity dispersions, corrected for softened central dynamics, are calculated\nalong the line-of-sight orthogonal to the photometric projection plane. Central\nmass density profiles assume a power-law model, while 3D masses remain\nunmodified from the IllustrisTNG catalogue. The final catalogue includes\n$10121$ galaxies at redshifts $z \\leq 0.1$. By comparing the virtual properties\nwith observations, we find that the virtual-ETG scaling relations (e.g.,\nsize-mass, size-central surface brightness, and Faber-Jackson), central density\nslopes, and scaling relations among total density slopes and galaxy structural\nparameters are generally consistent with observations. We make the virtual-ETG\npublicly available for galaxy formation studies and plan to use this sample as\na training set for machine learning tools to infer galaxy properties in future\nimaging and spectroscopic surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-type galaxies (ETGs) are reference systems to understand galaxy\nformation and evolution processes. The physics of their collapse and internal\ndynamics are codified in well-known scaling relations. Cosmological\nhydrodynamical simulations play an important role, providing insights into the\n3D distribution of matter and galaxy formation mechanisms, as well as\nvalidating methods to infer the properties of real objects. In this work, we\npresent the closest-to-reality sample of ETGs from the IllustrisTNG100-1\nsimulation, dubbed \"virtual-ETGs,\" based on an observational-like algorithm\nthat combines standard projected and three-dimensional galaxy structural\nparameters. We extract 2D photometric information by projecting the galaxies'\nlight into three planes and modeling them via S\\'ersic profiles. Aperture\nvelocity dispersions, corrected for softened central dynamics, are calculated\nalong the line-of-sight orthogonal to the photometric projection plane. Central\nmass density profiles assume a power-law model, while 3D masses remain\nunmodified from the IllustrisTNG catalogue. The final catalogue includes\n$10121$ galaxies at redshifts $z \\leq 0.1$. By comparing the virtual properties\nwith observations, we find that the virtual-ETG scaling relations (e.g.,\nsize-mass, size-central surface brightness, and Faber-Jackson), central density\nslopes, and scaling relations among total density slopes and galaxy structural\nparameters are generally consistent with observations. We make the virtual-ETG\npublicly available for galaxy formation studies and plan to use this sample as\na training set for machine learning tools to infer galaxy properties in future\nimaging and spectroscopic surveys."
                },
                "authors": [
                    {
                        "name": "Pedro de Araujo Ferreira"
                    },
                    {
                        "name": "Nicola R. Napolitano"
                    },
                    {
                        "name": "Luciano Casarini"
                    },
                    {
                        "name": "Crescenzo Tortora"
                    },
                    {
                        "name": "Rodrigo von Marttens"
                    },
                    {
                        "name": "Sirui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Sirui Wu"
                },
                "author": "Sirui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04928v1",
                "updated": "2025-01-09T02:36:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    36,
                    21,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T02:36:21Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    36,
                    21,
                    3,
                    9,
                    0
                ],
                "title": "Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference\n  from Product Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference\n  from Product Images"
                },
                "summary": "Computer-aided design (CAD) tools empower designers to design and modify 3D\nmodels through a series of CAD operations, commonly referred to as a CAD\nsequence. In scenarios where digital CAD files are not accessible, reverse\nengineering (RE) has been used to reconstruct 3D CAD models. Recent advances\nhave seen the rise of data-driven approaches for RE, with a primary focus on\nconverting 3D data, such as point clouds, into 3D models in boundary\nrepresentation (B-rep) format. However, obtaining 3D data poses significant\nchallenges, and B-rep models do not reveal knowledge about the 3D modeling\nprocess of designs. To this end, our research introduces a novel data-driven\napproach with an Image2CADSeq neural network model. This model aims to reverse\nengineer CAD models by processing images as input and generating CAD sequences.\nThese sequences can then be translated into B-rep models using a solid modeling\nkernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify\nindividual steps of model creation, providing a deeper understanding of the\nconstruction process of CAD models. To quantitatively and rigorously evaluate\nthe predictive performance of the Image2CADSeq model, we have developed a\nmulti-level evaluation framework for model assessment. The model was trained on\na specially synthesized dataset, and various network architectures were\nexplored to optimize the performance. The experimental and validation results\nshow great potential for the model in generating CAD sequences from 2D image\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-aided design (CAD) tools empower designers to design and modify 3D\nmodels through a series of CAD operations, commonly referred to as a CAD\nsequence. In scenarios where digital CAD files are not accessible, reverse\nengineering (RE) has been used to reconstruct 3D CAD models. Recent advances\nhave seen the rise of data-driven approaches for RE, with a primary focus on\nconverting 3D data, such as point clouds, into 3D models in boundary\nrepresentation (B-rep) format. However, obtaining 3D data poses significant\nchallenges, and B-rep models do not reveal knowledge about the 3D modeling\nprocess of designs. To this end, our research introduces a novel data-driven\napproach with an Image2CADSeq neural network model. This model aims to reverse\nengineer CAD models by processing images as input and generating CAD sequences.\nThese sequences can then be translated into B-rep models using a solid modeling\nkernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify\nindividual steps of model creation, providing a deeper understanding of the\nconstruction process of CAD models. To quantitatively and rigorously evaluate\nthe predictive performance of the Image2CADSeq model, we have developed a\nmulti-level evaluation framework for model assessment. The model was trained on\na specially synthesized dataset, and various network architectures were\nexplored to optimize the performance. The experimental and validation results\nshow great potential for the model in generating CAD sequences from 2D image\ndata."
                },
                "authors": [
                    {
                        "name": "Xingang Li"
                    },
                    {
                        "name": "Zhenghui Sha"
                    }
                ],
                "author_detail": {
                    "name": "Zhenghui Sha"
                },
                "author": "Zhenghui Sha",
                "arxiv_comment": "20 pages, 10 figures, and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17052v2",
                "updated": "2025-01-09T02:33:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    33,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-22T15:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    5,
                    30,
                    6,
                    357,
                    0
                ],
                "title": "ViLBias: A Comprehensive Framework for Bias Detection through Linguistic\n  and Visual Cues , presenting Annotation Strategies, Evaluation, and Key\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLBias: A Comprehensive Framework for Bias Detection through Linguistic\n  and Visual Cues , presenting Annotation Strategies, Evaluation, and Key\n  Challenges"
                },
                "summary": "The integration of Large Language Models (LLMs) and Vision-Language Models\n(VLMs) opens new avenues for addressing complex challenges in multimodal\ncontent analysis, particularly in biased news detection. This study introduces\nVLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect\nlinguistic and visual biases in news content. We present a multimodal dataset\ncomprising textual content and corresponding images from diverse news sources.\nWe propose a hybrid annotation framework that combines LLM-based annotations\nwith human review to ensure high-quality labeling while reducing costs and\nenhancing scalability. Our evaluation compares the performance of\nstate-of-the-art SLMs and LLMs for both modalities (text and images) and the\nresults reveal that while SLMs are computationally efficient, LLMs demonstrate\nsuperior accuracy in identifying subtle framing and text-visual\ninconsistencies. Furthermore, empirical analysis shows that incorporating\nvisual cues alongside textual data improves bias detection accuracy by 3 to 5%.\nThis study provides a comprehensive exploration of LLMs, SLMs, and VLMs as\ntools for detecting multimodal biases in news content and highlights their\nrespective strengths, limitations, and potential for future applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) and Vision-Language Models\n(VLMs) opens new avenues for addressing complex challenges in multimodal\ncontent analysis, particularly in biased news detection. This study introduces\nVLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect\nlinguistic and visual biases in news content. We present a multimodal dataset\ncomprising textual content and corresponding images from diverse news sources.\nWe propose a hybrid annotation framework that combines LLM-based annotations\nwith human review to ensure high-quality labeling while reducing costs and\nenhancing scalability. Our evaluation compares the performance of\nstate-of-the-art SLMs and LLMs for both modalities (text and images) and the\nresults reveal that while SLMs are computationally efficient, LLMs demonstrate\nsuperior accuracy in identifying subtle framing and text-visual\ninconsistencies. Furthermore, empirical analysis shows that incorporating\nvisual cues alongside textual data improves bias detection accuracy by 3 to 5%.\nThis study provides a comprehensive exploration of LLMs, SLMs, and VLMs as\ntools for detecting multimodal biases in news content and highlights their\nrespective strengths, limitations, and potential for future applications"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Caesar Saleh"
                    },
                    {
                        "name": "Emrul Hasan"
                    },
                    {
                        "name": "Franklin Ogidi"
                    },
                    {
                        "name": "Maximus Powers"
                    },
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Roya Javadi"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Reza Khazaie"
                },
                "author": "Vahid Reza Khazaie",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04323v2",
                "updated": "2025-01-09T02:33:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    33,
                    4,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T07:47:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models"
                },
                "summary": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance."
                },
                "authors": [
                    {
                        "name": "Haonan Shi"
                    },
                    {
                        "name": "Tu Ouyang"
                    },
                    {
                        "name": "An Wang"
                    }
                ],
                "author_detail": {
                    "name": "An Wang"
                },
                "author": "An Wang",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04927v1",
                "updated": "2025-01-09T02:32:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    32,
                    40,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T02:32:40Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    32,
                    40,
                    3,
                    9,
                    0
                ],
                "title": "Investigating Numerical Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Numerical Translation with Large Language Models"
                },
                "summary": "The inaccurate translation of numbers can lead to significant security\nissues, ranging from financial setbacks to medical inaccuracies. While large\nlanguage models (LLMs) have made significant advancements in machine\ntranslation, their capacity for translating numbers has not been thoroughly\nexplored. This study focuses on evaluating the reliability of LLM-based machine\ntranslation systems when handling numerical data. In order to systematically\ntest the numerical translation capabilities of currently open source LLMs, we\nhave constructed a numerical translation dataset between Chinese and English\nbased on real business data, encompassing ten types of numerical translation.\nExperiments on the dataset indicate that errors in numerical translation are a\ncommon issue, with most open-source LLMs faltering when faced with our test\nscenarios. Especially when it comes to numerical types involving large units\nlike ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can\nhave error rates as high as 20%. Finally, we introduce three potential\nstrategies to mitigate the numerical mistranslations for large units.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inaccurate translation of numbers can lead to significant security\nissues, ranging from financial setbacks to medical inaccuracies. While large\nlanguage models (LLMs) have made significant advancements in machine\ntranslation, their capacity for translating numbers has not been thoroughly\nexplored. This study focuses on evaluating the reliability of LLM-based machine\ntranslation systems when handling numerical data. In order to systematically\ntest the numerical translation capabilities of currently open source LLMs, we\nhave constructed a numerical translation dataset between Chinese and English\nbased on real business data, encompassing ten types of numerical translation.\nExperiments on the dataset indicate that errors in numerical translation are a\ncommon issue, with most open-source LLMs faltering when faced with our test\nscenarios. Especially when it comes to numerical types involving large units\nlike ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can\nhave error rates as high as 20%. Finally, we introduce three potential\nstrategies to mitigate the numerical mistranslations for large units."
                },
                "authors": [
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Yanqing Zhao"
                    },
                    {
                        "name": "Weidong Zhang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08275v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08275v4",
                "updated": "2025-01-09T02:31:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    31,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2023-10-12T12:24:52Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    12,
                    24,
                    52,
                    3,
                    285,
                    0
                ],
                "title": "Harnessing the Power of LLM to Support Binary Taint Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Power of LLM to Support Binary Taint Analysis"
                },
                "summary": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs."
                },
                "authors": [
                    {
                        "name": "Puzhuo Liu"
                    },
                    {
                        "name": "Chengnian Sun"
                    },
                    {
                        "name": "Yaowen Zheng"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Yuncheng Wang"
                    },
                    {
                        "name": "Zhenyang Xu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "arxiv_doi": "10.1145/3711816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08275v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08275v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "36 pages,16 figures",
                "arxiv_journal_ref": "TOSEM 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.05452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05452v1",
                "updated": "2025-01-09T18:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding"
                },
                "summary": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Minqian Liu"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "John Corring"
                    },
                    {
                        "name": "Yijuan Lu"
                    },
                    {
                        "name": "Jianwei Yang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Dinei Florencio"
                    },
                    {
                        "name": "Cha Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cha Zhang"
                },
                "author": "Cha Zhang",
                "arxiv_comment": "Project link: https://zeyofu.github.io/ReFocus/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05443v1",
                "updated": "2025-01-09T18:55:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    55,
                    50,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:55:50Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    55,
                    50,
                    3,
                    9,
                    0
                ],
                "title": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models"
                },
                "summary": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it."
                },
                "authors": [
                    {
                        "name": "Jose A. Diaz-Garcia"
                    },
                    {
                        "name": "Joao Paulo Carvalho"
                    }
                ],
                "author_detail": {
                    "name": "Joao Paulo Carvalho"
                },
                "author": "Joao Paulo Carvalho",
                "arxiv_comment": "37 pages, under review in WIREs Data Mining and Knowledge Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08405v2",
                "updated": "2025-01-09T18:43:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    43,
                    18,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-10T22:38:26Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    22,
                    38,
                    26,
                    3,
                    284,
                    0
                ],
                "title": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning"
                },
                "summary": "Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT."
                },
                "authors": [
                    {
                        "name": "Muhammad Awais"
                    },
                    {
                        "name": "Ali Husain Salem Abdulla Alharthi"
                    },
                    {
                        "name": "Amandeep Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted at WACV, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05423v1",
                "updated": "2025-01-09T18:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    30,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    30,
                    14,
                    3,
                    9,
                    0
                ],
                "title": "Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese\n  Micro-bloggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese\n  Micro-bloggers"
                },
                "summary": "Studying public sentiment during crises is crucial for understanding how\nopinions and sentiments shift, resulting in polarized societies. We study\nWeibo, the most popular microblogging site in China, using posts made during\nthe outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19\nstage, the outbreak stage, and the early stage of epidemic prevention. We use\nLlama 3 8B, a Large Language Model, to analyze users' sentiments on the\nplatform by classifying them into positive, negative, sarcastic, and neutral\ncategories. Analyzing sentiment shifts on Weibo provides insights into how\nsocial events and government actions influence public opinion. This study\ncontributes to understanding the dynamics of social sentiments during health\ncrises, fulfilling a gap in sentiment analysis for Chinese platforms. By\nexamining these dynamics, we aim to offer valuable perspectives on digital\ncommunication's role in shaping society's responses during unprecedented global\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying public sentiment during crises is crucial for understanding how\nopinions and sentiments shift, resulting in polarized societies. We study\nWeibo, the most popular microblogging site in China, using posts made during\nthe outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19\nstage, the outbreak stage, and the early stage of epidemic prevention. We use\nLlama 3 8B, a Large Language Model, to analyze users' sentiments on the\nplatform by classifying them into positive, negative, sarcastic, and neutral\ncategories. Analyzing sentiment shifts on Weibo provides insights into how\nsocial events and government actions influence public opinion. This study\ncontributes to understanding the dynamics of social sentiments during health\ncrises, fulfilling a gap in sentiment analysis for Chinese platforms. By\nexamining these dynamics, we aim to offer valuable perspectives on digital\ncommunication's role in shaping society's responses during unprecedented global\nchallenges."
                },
                "authors": [
                    {
                        "name": "Jerry Chongyi Hu"
                    },
                    {
                        "name": "Mohammed Shahid Modi"
                    },
                    {
                        "name": "Boleslaw K. Szymanski"
                    }
                ],
                "author_detail": {
                    "name": "Boleslaw K. Szymanski"
                },
                "author": "Boleslaw K. Szymanski",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05396v1",
                "updated": "2025-01-09T17:42:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    42,
                    23,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T17:42:23Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    42,
                    23,
                    3,
                    9,
                    0
                ],
                "title": "FairCode: Evaluating Social Bias of LLMs in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairCode: Evaluating Social Bias of LLMs in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capability in code\ngeneration, drawing increasing attention to the evaluation of the quality and\nsafety of their outputs. However, research on bias in code generation remains\nlimited. Existing studies typically assess bias by applying malicious prompts\nor reapply tasks and dataset for discriminative models. Given that LLMs are\noften aligned with human values and that prior datasets are not fully optimized\nfor code-related tasks, there is a pressing need for benchmarks specifically\ndesigned for evaluating code models. In this study, we introduce FairCode, a\nnovel benchmark for evaluating bias in code generation. FairCode comprises two\ntasks: function implementation and test case generation, each evaluating social\nbias through diverse scenarios. Additionally, we propose a new metric,\nFairScore, to assess model performance on this benchmark. We conduct\nexperiments on widely used LLMs and provide a comprehensive analysis of the\nresults. The findings reveal that all tested LLMs exhibit bias. The code is\navailable at https://github.com/YongkDu/FairCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capability in code\ngeneration, drawing increasing attention to the evaluation of the quality and\nsafety of their outputs. However, research on bias in code generation remains\nlimited. Existing studies typically assess bias by applying malicious prompts\nor reapply tasks and dataset for discriminative models. Given that LLMs are\noften aligned with human values and that prior datasets are not fully optimized\nfor code-related tasks, there is a pressing need for benchmarks specifically\ndesigned for evaluating code models. In this study, we introduce FairCode, a\nnovel benchmark for evaluating bias in code generation. FairCode comprises two\ntasks: function implementation and test case generation, each evaluating social\nbias through diverse scenarios. Additionally, we propose a new metric,\nFairScore, to assess model performance on this benchmark. We conduct\nexperiments on widely used LLMs and provide a comprehensive analysis of the\nresults. The findings reveal that all tested LLMs exhibit bias. The code is\navailable at https://github.com/YongkDu/FairCode."
                },
                "authors": [
                    {
                        "name": "Yongkang Du"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11672v2",
                "updated": "2025-01-09T17:18:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    18,
                    12,
                    3,
                    9,
                    0
                ],
                "published": "2024-04-17T18:13:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    13,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"
                },
                "summary": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05382v1",
                "updated": "2025-01-09T17:11:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    11,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T17:11:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    11,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models"
                },
                "summary": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models."
                },
                "authors": [
                    {
                        "name": "Kristian G. Barman"
                    },
                    {
                        "name": "Sascha Caron"
                    },
                    {
                        "name": "Emily Sullivan"
                    },
                    {
                        "name": "Henk W. de Regt"
                    },
                    {
                        "name": "Roberto Ruiz de Austri"
                    },
                    {
                        "name": "Mieke Boon"
                    },
                    {
                        "name": "Michael Färber"
                    },
                    {
                        "name": "Stefan Fröse"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Andreas Ipp"
                    },
                    {
                        "name": "Rukshak Kapoor"
                    },
                    {
                        "name": "Gregor Kasieczka"
                    },
                    {
                        "name": "Daniel Kostić"
                    },
                    {
                        "name": "Michael Krämer"
                    },
                    {
                        "name": "Tobias Golling"
                    },
                    {
                        "name": "Luis G. Lopez"
                    },
                    {
                        "name": "Jesus Marco"
                    },
                    {
                        "name": "Sydney Otten"
                    },
                    {
                        "name": "Pawel Pawlowski"
                    },
                    {
                        "name": "Pietro Vischia"
                    },
                    {
                        "name": "Erik Weber"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00846v2",
                "updated": "2025-01-09T16:55:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    55,
                    55,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-01T18:01:23Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    1,
                    23,
                    3,
                    214,
                    0
                ],
                "title": "Occupation-aware planning method for robotic monitoring missions in\n  dynamic environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupation-aware planning method for robotic monitoring missions in\n  dynamic environments"
                },
                "summary": "This paper presents a method for robotic monitoring missions in the presence\nof moving obstacles. Although the scenario map is known, the robot lacks\ninformation about the movement of dynamic obstacles during the monitoring\nmission. Numerous local planners have been developed in recent years for\nnavigating highly dynamic environments. However, the absence of a global\nplanner for these environments can result in unavoidable collisions or the\ninability to successfully complete missions in densely populated areas, such as\na scenario monitoring in our case. This work addresses the development and\nevaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas),\naimed at enhancing the deployment of robots in such challenging conditions. The\nrobot plans and executes the mission using the proposed two-step approach. The\nfirst step involves selecting the observation goal based on the environment's\ndistribution and estimated monitoring costs. In the second step, the robot\nidentifies areas with moving obstacles and obtains paths avoiding densely\noccupied dynamic regions based on their occupation. Quantitative and\nqualitative results based on simulations and on real-world experimentation,\nconfirm that the proposed method allows the robot to effectively monitor most\nof the environment while avoiding densely occupied dynamic areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method for robotic monitoring missions in the presence\nof moving obstacles. Although the scenario map is known, the robot lacks\ninformation about the movement of dynamic obstacles during the monitoring\nmission. Numerous local planners have been developed in recent years for\nnavigating highly dynamic environments. However, the absence of a global\nplanner for these environments can result in unavoidable collisions or the\ninability to successfully complete missions in densely populated areas, such as\na scenario monitoring in our case. This work addresses the development and\nevaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas),\naimed at enhancing the deployment of robots in such challenging conditions. The\nrobot plans and executes the mission using the proposed two-step approach. The\nfirst step involves selecting the observation goal based on the environment's\ndistribution and estimated monitoring costs. In the second step, the robot\nidentifies areas with moving obstacles and obtains paths avoiding densely\noccupied dynamic regions based on their occupation. Quantitative and\nqualitative results based on simulations and on real-world experimentation,\nconfirm that the proposed method allows the robot to effectively monitor most\nof the environment while avoiding densely occupied dynamic areas."
                },
                "authors": [
                    {
                        "name": "Yaroslav Marchukov"
                    },
                    {
                        "name": "Luis Montano"
                    }
                ],
                "author_detail": {
                    "name": "Luis Montano"
                },
                "author": "Luis Montano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17990v2",
                "updated": "2025-01-09T16:47:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    47,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-26T16:02:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models"
                },
                "summary": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Max Pellert"
                    },
                    {
                        "name": "David Garcia"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05361v1",
                "updated": "2025-01-09T16:44:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    44,
                    53,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:44:53Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    44,
                    53,
                    3,
                    9,
                    0
                ],
                "title": "No-Regret Linear Bandits under Gap-Adjusted Misspecification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-Regret Linear Bandits under Gap-Adjusted Misspecification"
                },
                "summary": "This work studies linear bandits under a new notion of gap-adjusted\nmisspecification and is an extension of Liu et al. (2023). When the underlying\nreward function is not linear, existing linear bandits work usually relies on a\nuniform misspecification parameter $\\epsilon$ that measures the sup-norm error\nof the best linear approximation. This results in an unavoidable linear regret\nwhenever $\\epsilon > 0$. We propose a more natural model of misspecification\nwhich only requires the approximation error at each input $x$ to be\nproportional to the suboptimality gap at $x$. It captures the intuition that,\nfor optimization problems, near-optimal regions should matter more and we can\ntolerate larger approximation errors in suboptimal regions.\n  Quite surprisingly, we show that the classical LinUCB algorithm -- designed\nfor the realizable case -- is automatically robust against such\n$\\rho$-gap-adjusted misspecification with parameter $\\rho$ diminishing at\n$O(1/(d \\sqrt{\\log T}))$. It achieves a near-optimal $O(\\sqrt{T})$ regret for\nproblems that the best-known regret is almost linear in time horizon $T$. We\nfurther advance this frontier by presenting a novel phased elimination-based\nalgorithm whose gap-adjusted misspecification parameter $\\rho = O(1/\\sqrt{d})$\ndoes not scale with $T$. This algorithm attains optimal $O(\\sqrt{T})$ regret\nand is deployment-efficient, requiring only $\\log T$ batches of exploration. It\nalso enjoys an adaptive $O(\\log T)$ regret when a constant suboptimality gap\nexists. Technically, our proof relies on a novel self-bounding argument that\nbounds the part of the regret due to misspecification by the regret itself, and\na new inductive lemma that limits the misspecification error within the\nsuboptimality gap for all valid actions in each batch selected by G-optimal\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies linear bandits under a new notion of gap-adjusted\nmisspecification and is an extension of Liu et al. (2023). When the underlying\nreward function is not linear, existing linear bandits work usually relies on a\nuniform misspecification parameter $\\epsilon$ that measures the sup-norm error\nof the best linear approximation. This results in an unavoidable linear regret\nwhenever $\\epsilon > 0$. We propose a more natural model of misspecification\nwhich only requires the approximation error at each input $x$ to be\nproportional to the suboptimality gap at $x$. It captures the intuition that,\nfor optimization problems, near-optimal regions should matter more and we can\ntolerate larger approximation errors in suboptimal regions.\n  Quite surprisingly, we show that the classical LinUCB algorithm -- designed\nfor the realizable case -- is automatically robust against such\n$\\rho$-gap-adjusted misspecification with parameter $\\rho$ diminishing at\n$O(1/(d \\sqrt{\\log T}))$. It achieves a near-optimal $O(\\sqrt{T})$ regret for\nproblems that the best-known regret is almost linear in time horizon $T$. We\nfurther advance this frontier by presenting a novel phased elimination-based\nalgorithm whose gap-adjusted misspecification parameter $\\rho = O(1/\\sqrt{d})$\ndoes not scale with $T$. This algorithm attains optimal $O(\\sqrt{T})$ regret\nand is deployment-efficient, requiring only $\\log T$ batches of exploration. It\nalso enjoys an adaptive $O(\\log T)$ regret when a constant suboptimality gap\nexists. Technically, our proof relies on a novel self-bounding argument that\nbounds the part of the regret due to misspecification by the regret itself, and\na new inductive lemma that limits the misspecification error within the\nsuboptimality gap for all valid actions in each batch selected by G-optimal\ndesign."
                },
                "authors": [
                    {
                        "name": "Chong Liu"
                    },
                    {
                        "name": "Dan Qiao"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Yu-Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiang Wang"
                },
                "author": "Yu-Xiang Wang",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2302.13252",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20138v2",
                "updated": "2025-01-09T16:36:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    36,
                    26,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-28T12:54:06Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    12,
                    54,
                    6,
                    5,
                    363,
                    0
                ],
                "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TradingAgents: Multi-Agents LLM Financial Trading Framework"
                },
                "summary": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. More details on TradingAgents\nare available at https://TradingAgents-AI.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. More details on TradingAgents\nare available at https://TradingAgents-AI.github.io."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Edward Sun"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Multi-Agent AI in the Real World @ AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04733v2",
                "updated": "2025-01-09T16:30:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    30,
                    53,
                    3,
                    9,
                    0
                ],
                "published": "2024-06-07T08:32:30Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    32,
                    30,
                    4,
                    159,
                    0
                ],
                "title": "Unsupervised representation learning with Hebbian synaptic and\n  structural plasticity in brain-like feedforward neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised representation learning with Hebbian synaptic and\n  structural plasticity in brain-like feedforward neural networks"
                },
                "summary": "Neural networks that can capture key principles underlying brain computation\noffer exciting new opportunities for developing artificial intelligence and\nbrain-like computing algorithms. Such networks remain biologically plausible\nwhile leveraging localized forms of synaptic learning rules and modular network\narchitecture found in the neocortex. Compared to backprop-driven deep learning\napproches, they provide more suitable models for deployment of neuromorphic\nhardware and have greater potential for scalability on large-scale computing\nclusters. The development of such brain-like neural networks depends on having\na learning procedure that can build effective internal representations from\ndata. In this work, we introduce and evaluate a brain-like neural network model\ncapable of unsupervised representation learning. It builds on the Bayesian\nConfidence Propagation Neural Network (BCPNN), which has earlier been\nimplemented as abstract as well as biophyscially detailed recurrent attractor\nneural networks explaining various cortical associative memory phenomena. Here\nwe developed a feedforward BCPNN model to perform representation learning by\nincorporating a range of brain-like attributes derived from neocortical\ncircuits such as cortical columns, divisive normalization, Hebbian synaptic\nplasticity, structural plasticity, sparse activity, and sparse patchy\nconnectivity. The model was tested on a diverse set of popular machine learning\nbenchmarks: grayscale images (MNIST, F-MNIST), RGB natural images (SVHN,\nCIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER). The performance of\nthe model when using a linear classifier to predict the class labels fared\ncompetitively with conventional multi-layer perceptrons and other\nstate-of-the-art brain-like neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks that can capture key principles underlying brain computation\noffer exciting new opportunities for developing artificial intelligence and\nbrain-like computing algorithms. Such networks remain biologically plausible\nwhile leveraging localized forms of synaptic learning rules and modular network\narchitecture found in the neocortex. Compared to backprop-driven deep learning\napproches, they provide more suitable models for deployment of neuromorphic\nhardware and have greater potential for scalability on large-scale computing\nclusters. The development of such brain-like neural networks depends on having\na learning procedure that can build effective internal representations from\ndata. In this work, we introduce and evaluate a brain-like neural network model\ncapable of unsupervised representation learning. It builds on the Bayesian\nConfidence Propagation Neural Network (BCPNN), which has earlier been\nimplemented as abstract as well as biophyscially detailed recurrent attractor\nneural networks explaining various cortical associative memory phenomena. Here\nwe developed a feedforward BCPNN model to perform representation learning by\nincorporating a range of brain-like attributes derived from neocortical\ncircuits such as cortical columns, divisive normalization, Hebbian synaptic\nplasticity, structural plasticity, sparse activity, and sparse patchy\nconnectivity. The model was tested on a diverse set of popular machine learning\nbenchmarks: grayscale images (MNIST, F-MNIST), RGB natural images (SVHN,\nCIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER). The performance of\nthe model when using a linear classifier to predict the class labels fared\ncompetitively with conventional multi-layer perceptrons and other\nstate-of-the-art brain-like neural networks."
                },
                "authors": [
                    {
                        "name": "Naresh Ravichandran"
                    },
                    {
                        "name": "Anders Lansner"
                    },
                    {
                        "name": "Pawel Herman"
                    }
                ],
                "author_detail": {
                    "name": "Pawel Herman"
                },
                "author": "Pawel Herman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05339v1",
                "updated": "2025-01-09T16:10:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    10,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:10:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    10,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with\n  Hardware-Software Co-Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with\n  Hardware-Software Co-Exploration"
                },
                "summary": "The co-design of neural network architectures, quantization precisions, and\nhardware accelerators offers a promising approach to achieving an optimal\nbalance between performance and efficiency, particularly for model deployment\non resource-constrained edge devices. In this work, we propose the JAQ\nFramework, which jointly optimizes the three critical dimensions. However,\neffectively automating the design process across the vast search space of those\nthree dimensions poses significant challenges, especially when pursuing\nextremely low-bit quantization. Specifical, the primary challenges include: (1)\nMemory overhead in software-side: Low-precision quantization-aware training can\nlead to significant memory usage due to storing large intermediate features and\nlatent weights for back-propagation, potentially causing memory exhaustion. (2)\nSearch time-consuming in hardware-side: The discrete nature of hardware\nparameters and the complex interplay between compiler optimizations and\nindividual operators make the accelerator search time-consuming. To address\nthese issues, JAQ mitigates the memory overhead through a channel-wise sparse\nquantization (CSQ) scheme, selectively applying quantization to the most\nsensitive components of the model during optimization. Additionally, JAQ\ndesigns BatchTile, which employs a hardware generation network to encode all\npossible tiling modes, thereby speeding up the search for the optimal compiler\nmapping strategy. Extensive experiments demonstrate the effectiveness of JAQ,\nachieving approximately 7% higher Top-1 accuracy on ImageNet compared to\nprevious methods and reducing the hardware search time per iteration to 0.15\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-design of neural network architectures, quantization precisions, and\nhardware accelerators offers a promising approach to achieving an optimal\nbalance between performance and efficiency, particularly for model deployment\non resource-constrained edge devices. In this work, we propose the JAQ\nFramework, which jointly optimizes the three critical dimensions. However,\neffectively automating the design process across the vast search space of those\nthree dimensions poses significant challenges, especially when pursuing\nextremely low-bit quantization. Specifical, the primary challenges include: (1)\nMemory overhead in software-side: Low-precision quantization-aware training can\nlead to significant memory usage due to storing large intermediate features and\nlatent weights for back-propagation, potentially causing memory exhaustion. (2)\nSearch time-consuming in hardware-side: The discrete nature of hardware\nparameters and the complex interplay between compiler optimizations and\nindividual operators make the accelerator search time-consuming. To address\nthese issues, JAQ mitigates the memory overhead through a channel-wise sparse\nquantization (CSQ) scheme, selectively applying quantization to the most\nsensitive components of the model during optimization. Additionally, JAQ\ndesigns BatchTile, which employs a hardware generation network to encode all\npossible tiling modes, thereby speeding up the search for the optimal compiler\nmapping strategy. Extensive experiments demonstrate the effectiveness of JAQ,\nachieving approximately 7% higher Top-1 accuracy on ImageNet compared to\nprevious methods and reducing the hardware search time per iteration to 0.15\nseconds."
                },
                "authors": [
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Yijian Qin"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Tongtong Feng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xun Guan"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05336v1",
                "updated": "2025-01-09T16:02:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    2,
                    51,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:02:51Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    2,
                    51,
                    3,
                    9,
                    0
                ],
                "title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction"
                },
                "summary": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model."
                },
                "authors": [
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "AAAI Alignment Track 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05329v1",
                "updated": "2025-01-09T15:55:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    55,
                    8,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:55:08Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    55,
                    8,
                    3,
                    9,
                    0
                ],
                "title": "Knowledge Transfer in Model-Based Reinforcement Learning Agents for\n  Efficient Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Transfer in Model-Based Reinforcement Learning Agents for\n  Efficient Multi-Task Learning"
                },
                "summary": "We propose an efficient knowledge transfer approach for model-based\nreinforcement learning, addressing the challenge of deploying large world\nmodels in resource-constrained environments. Our method distills a\nhigh-capacity multi-task agent (317M parameters) into a compact 1M parameter\nmodel, achieving state-of-the-art performance on the MT30 benchmark with a\nnormalized score of 28.45, a substantial improvement over the original 1M\nparameter model's score of 18.93. This demonstrates the ability of our\ndistillation technique to consolidate complex multi-task knowledge effectively.\nAdditionally, we apply FP16 post-training quantization, reducing the model size\nby 50% while maintaining performance. Our work bridges the gap between the\npower of large models and practical deployment constraints, offering a scalable\nsolution for efficient and accessible multi-task reinforcement learning in\nrobotics and other resource-limited domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an efficient knowledge transfer approach for model-based\nreinforcement learning, addressing the challenge of deploying large world\nmodels in resource-constrained environments. Our method distills a\nhigh-capacity multi-task agent (317M parameters) into a compact 1M parameter\nmodel, achieving state-of-the-art performance on the MT30 benchmark with a\nnormalized score of 28.45, a substantial improvement over the original 1M\nparameter model's score of 18.93. This demonstrates the ability of our\ndistillation technique to consolidate complex multi-task knowledge effectively.\nAdditionally, we apply FP16 post-training quantization, reducing the model size\nby 50% while maintaining performance. Our work bridges the gap between the\npower of large models and practical deployment constraints, offering a scalable\nsolution for efficient and accessible multi-task reinforcement learning in\nrobotics and other resource-limited domains."
                },
                "authors": [
                    {
                        "name": "Dmytro Kuzmenko"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Preprint of an extended abstract accepted to AAMAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05322v1",
                "updated": "2025-01-09T15:45:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    45,
                    28,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:45:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    45,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining\n  the Actions of Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining\n  the Actions of Autonomous Vehicles"
                },
                "summary": "Public distrust of self-driving cars is growing. Studies emphasize the need\nfor interpreting the behavior of these vehicles to passengers to promote trust\nin autonomous systems. Interpreters can enhance trust by improving transparency\nand reducing perceived risk. However, current solutions often lack a\nhuman-centric approach to integrating multimodal interpretations. This paper\nintroduces a novel Human-centered Multimodal Interpreter (HMI) system that\nleverages human preferences to provide visual, textual, and auditory feedback.\nThe system combines a visual interface with Bird's Eye View (BEV), map, and\ntext display, along with voice interaction using a fine-tuned large language\nmodel (LLM). Our user study, involving diverse participants, demonstrated that\nthe HMI system significantly boosts passenger trust in AVs, increasing average\ntrust levels by over 8%, with trust in ordinary environments rising by up to\n30%. These results underscore the potential of the HMI system to improve the\nacceptance and reliability of autonomous vehicles by providing clear,\nreal-time, and context-sensitive explanations of vehicle actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public distrust of self-driving cars is growing. Studies emphasize the need\nfor interpreting the behavior of these vehicles to passengers to promote trust\nin autonomous systems. Interpreters can enhance trust by improving transparency\nand reducing perceived risk. However, current solutions often lack a\nhuman-centric approach to integrating multimodal interpretations. This paper\nintroduces a novel Human-centered Multimodal Interpreter (HMI) system that\nleverages human preferences to provide visual, textual, and auditory feedback.\nThe system combines a visual interface with Bird's Eye View (BEV), map, and\ntext display, along with voice interaction using a fine-tuned large language\nmodel (LLM). Our user study, involving diverse participants, demonstrated that\nthe HMI system significantly boosts passenger trust in AVs, increasing average\ntrust levels by over 8%, with trust in ordinary environments rising by up to\n30%. These results underscore the potential of the HMI system to improve the\nacceptance and reliability of autonomous vehicles by providing clear,\nreal-time, and context-sensitive explanations of vehicle actions."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Ruiqi Chen"
                    },
                    {
                        "name": "Rishikesh Panda"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Shuyun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuyun Zhang"
                },
                "author": "Shuyun Zhang",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop HAVI\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05313v1",
                "updated": "2025-01-09T15:29:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    29,
                    33,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:29:33Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    29,
                    33,
                    3,
                    9,
                    0
                ],
                "title": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing"
                },
                "summary": "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%."
                },
                "authors": [
                    {
                        "name": "Mengfan Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05295v1",
                "updated": "2025-01-09T14:57:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    57,
                    19,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T14:57:19Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    57,
                    19,
                    3,
                    9,
                    0
                ],
                "title": "GaussDB-Global: A Geographically Distributed Database System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussDB-Global: A Geographically Distributed Database System"
                },
                "summary": "Geographically distributed database systems use remote replication to protect\nagainst regional failures. These systems are sensitive to severe latency\npenalties caused by centralized transaction management, remote access to\nsharded data, and log shipping over long distances. To tackle these issues, we\npresent GaussDB-Global, a sharded geographically distributed database system\nwith asynchronous replication, for OLTP applications. To tackle the transaction\nmanagement bottleneck, we take a decentralized approach using synchronized\nclocks. Our system can seamlessly transition between centralized and\ndecentralized transaction management, providing efficient fault tolerance and\nstreamlining deployment. To alleviate the remote read and log shipping issues,\nwe support reads on asynchronous replicas with strong consistency, tunable\nfreshness guarantees, and dynamic load balancing. Our experimental results on a\ngeographically distributed cluster show that our approach provides up to 14x\nhigher read throughput, and 50% more TPC-C throughput compared to our baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geographically distributed database systems use remote replication to protect\nagainst regional failures. These systems are sensitive to severe latency\npenalties caused by centralized transaction management, remote access to\nsharded data, and log shipping over long distances. To tackle these issues, we\npresent GaussDB-Global, a sharded geographically distributed database system\nwith asynchronous replication, for OLTP applications. To tackle the transaction\nmanagement bottleneck, we take a decentralized approach using synchronized\nclocks. Our system can seamlessly transition between centralized and\ndecentralized transaction management, providing efficient fault tolerance and\nstreamlining deployment. To alleviate the remote read and log shipping issues,\nwe support reads on asynchronous replicas with strong consistency, tunable\nfreshness guarantees, and dynamic load balancing. Our experimental results on a\ngeographically distributed cluster show that our approach provides up to 14x\nhigher read throughput, and 50% more TPC-C throughput compared to our baseline."
                },
                "authors": [
                    {
                        "name": "Puya Memarzia"
                    },
                    {
                        "name": "Huaxin Zhang"
                    },
                    {
                        "name": "Kelvin Ho"
                    },
                    {
                        "name": "Ronen Grosman"
                    },
                    {
                        "name": "Jiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Wang"
                },
                "author": "Jiang Wang",
                "arxiv_doi": "10.1109/ICDE60146.2024.0038",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.0038",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 11 figures, published in ICDE 2024",
                "arxiv_journal_ref": "2024 IEEE 40th International Conference on Data Engineering\n  (ICDE), Utrecht, Netherlands, 2024, pp. 5111-5118",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2; H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10747v2",
                "updated": "2025-01-09T14:35:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    35,
                    36,
                    3,
                    9,
                    0
                ],
                "published": "2024-07-15T14:20:09Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    20,
                    9,
                    0,
                    197,
                    0
                ],
                "title": "Codebook LLMs: Evaluating LLMs as Measurement Tools for Political\n  Science Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codebook LLMs: Evaluating LLMs as Measurement Tools for Political\n  Science Concepts"
                },
                "summary": "Codebooks -- documents that operationalize concepts and outline annotation\nprocedures -- are used almost universally by social scientists when coding\npolitical texts. To code these texts automatically, researchers are increasing\nturning to generative large language models (LLMs). However, there is limited\nempirical evidence on whether \"off-the-shelf\" LLMs faithfully follow real-world\ncodebook operationalizations and measure complex political constructs with\nsufficient accuracy. To address this, we gather and curate three real-world\npolitical science codebooks -- covering protest events, political violence and\nmanifestos -- along with their unstructured texts and human labels. We also\npropose a five-stage framework for codebook-LLM measurement: preparing a\ncodebook for both humans and LLMs, testing LLMs' basic capabilities on a\ncodebook, evaluating zero-shot measurement accuracy (i.e. off-the-shelf\nperformance), analyzing errors, and further (parameter-efficient) supervised\ntraining of LLMs. We provide an empirical demonstration of this framework using\nour three codebook datasets and several pretrained 7-12 billion open-weight\nLLMs. We find current open-weight LLMs have limitations in following codebooks\nzero-shot, but that supervised instruction tuning can substantially improve\nperformance. Rather than suggesting the \"best\" LLM, our contribution lies in\nour codebook datasets, evaluation framework, and guidance for applied\nresearchers who wish to implement their own codebook-LLM measurement projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codebooks -- documents that operationalize concepts and outline annotation\nprocedures -- are used almost universally by social scientists when coding\npolitical texts. To code these texts automatically, researchers are increasing\nturning to generative large language models (LLMs). However, there is limited\nempirical evidence on whether \"off-the-shelf\" LLMs faithfully follow real-world\ncodebook operationalizations and measure complex political constructs with\nsufficient accuracy. To address this, we gather and curate three real-world\npolitical science codebooks -- covering protest events, political violence and\nmanifestos -- along with their unstructured texts and human labels. We also\npropose a five-stage framework for codebook-LLM measurement: preparing a\ncodebook for both humans and LLMs, testing LLMs' basic capabilities on a\ncodebook, evaluating zero-shot measurement accuracy (i.e. off-the-shelf\nperformance), analyzing errors, and further (parameter-efficient) supervised\ntraining of LLMs. We provide an empirical demonstration of this framework using\nour three codebook datasets and several pretrained 7-12 billion open-weight\nLLMs. We find current open-weight LLMs have limitations in following codebooks\nzero-shot, but that supervised instruction tuning can substantially improve\nperformance. Rather than suggesting the \"best\" LLM, our contribution lies in\nour codebook datasets, evaluation framework, and guidance for applied\nresearchers who wish to implement their own codebook-LLM measurement projects."
                },
                "authors": [
                    {
                        "name": "Andrew Halterman"
                    },
                    {
                        "name": "Katherine A. Keith"
                    }
                ],
                "author_detail": {
                    "name": "Katherine A. Keith"
                },
                "author": "Katherine A. Keith",
                "arxiv_comment": "Version 2 (v1 Presented at PolMeth 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13426v2",
                "updated": "2025-01-09T14:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    33,
                    25,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-18T01:43:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    1,
                    43,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Safeguarding System Prompts for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding System Prompts for LLMs"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npropose PromptKeeper, a robust defense mechanism designed to safeguard system\nprompts. PromptKeeper tackles two core challenges: reliably detecting prompt\nleakage and mitigating side-channel vulnerabilities when leakage occurs. By\nframing detection as a hypothesis-testing problem, PromptKeeper effectively\nidentifies both explicit and subtle leakage. Upon detection, it regenerates\nresponses using a dummy prompt, ensuring that outputs remain indistinguishable\nfrom typical interactions when no leakage is present. PromptKeeper ensures\nrobust protection against prompt extraction attacks via either adversarial or\nregular queries, while preserving conversational capability and runtime\nefficiency during benign user interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npropose PromptKeeper, a robust defense mechanism designed to safeguard system\nprompts. PromptKeeper tackles two core challenges: reliably detecting prompt\nleakage and mitigating side-channel vulnerabilities when leakage occurs. By\nframing detection as a hypothesis-testing problem, PromptKeeper effectively\nidentifies both explicit and subtle leakage. Upon detection, it regenerates\nresponses using a dummy prompt, ensuring that outputs remain indistinguishable\nfrom typical interactions when no leakage is present. PromptKeeper ensures\nrobust protection against prompt extraction attacks via either adversarial or\nregular queries, while preserving conversational capability and runtime\nefficiency during benign user interactions."
                },
                "authors": [
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Zhihua Jin"
                    },
                    {
                        "name": "Guoliang He"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang He"
                },
                "author": "Guoliang He",
                "arxiv_comment": "15 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04572v2",
                "updated": "2025-01-09T14:30:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    30,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T15:42:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    42,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "Regret Analysis: a control perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regret Analysis: a control perspective"
                },
                "summary": "Online learning and model reference adaptive control have many interesting\nintersections. One area where they differ however is in how the algorithms are\nanalyzed and what objective or metric is used to discriminate \"good\" algorithms\nfrom \"bad\" algorithms. In adaptive control there are usually two objectives: 1)\nprove that all time varying parameters/states of the system are bounded, and 2)\nthat the instantaneous error between the adaptively controlled system and a\nreference system converges to zero over time (or at least a compact set). For\nonline learning the performance of algorithms is often characterized by the\nregret the algorithm incurs. Regret is defined as the cumulative loss (cost)\nover time from the online algorithm minus the cumulative loss (cost) of the\nsingle optimal fixed parameter choice in hindsight. Another significant\ndifference between the two areas of research is with regard to the assumptions\nmade in order to obtain said results. Adaptive control makes assumptions about\nthe input-output properties of the control problem and derives solutions for a\nfixed error model or optimization task. In the online learning literature\nresults are derived for classes of loss functions (i.e. convex) while a priori\nassuming that all time varying parameters are bounded, which for many\noptimization tasks is not unrealistic, but is a non starter in control\napplications. In this work we discuss these differences in detail through the\nregret based analysis of gradient descent for convex functions and the control\nbased analysis of a streaming regression problem. We close with a discussion\nabout the newly defined paradigm of online adaptive control and ask the\nfollowing question \"Are regret optimal control strategies deployable?\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning and model reference adaptive control have many interesting\nintersections. One area where they differ however is in how the algorithms are\nanalyzed and what objective or metric is used to discriminate \"good\" algorithms\nfrom \"bad\" algorithms. In adaptive control there are usually two objectives: 1)\nprove that all time varying parameters/states of the system are bounded, and 2)\nthat the instantaneous error between the adaptively controlled system and a\nreference system converges to zero over time (or at least a compact set). For\nonline learning the performance of algorithms is often characterized by the\nregret the algorithm incurs. Regret is defined as the cumulative loss (cost)\nover time from the online algorithm minus the cumulative loss (cost) of the\nsingle optimal fixed parameter choice in hindsight. Another significant\ndifference between the two areas of research is with regard to the assumptions\nmade in order to obtain said results. Adaptive control makes assumptions about\nthe input-output properties of the control problem and derives solutions for a\nfixed error model or optimization task. In the online learning literature\nresults are derived for classes of loss functions (i.e. convex) while a priori\nassuming that all time varying parameters are bounded, which for many\noptimization tasks is not unrealistic, but is a non starter in control\napplications. In this work we discuss these differences in detail through the\nregret based analysis of gradient descent for convex functions and the control\nbased analysis of a streaming regression problem. We close with a discussion\nabout the newly defined paradigm of online adaptive control and ask the\nfollowing question \"Are regret optimal control strategies deployable?\""
                },
                "authors": [
                    {
                        "name": "Travis E. Gibson"
                    },
                    {
                        "name": "Sawal Acharya"
                    }
                ],
                "author_detail": {
                    "name": "Sawal Acharya"
                },
                "author": "Sawal Acharya",
                "arxiv_comment": "10 pages no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05838v2",
                "updated": "2025-01-09T14:04:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    4,
                    1,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-08T09:06:34Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    6,
                    34,
                    1,
                    282,
                    0
                ],
                "title": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite\n  Data Limit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite\n  Data Limit"
                },
                "summary": "One of the main challenges in optimal scaling of large language models (LLMs)\nis the prohibitive cost of hyperparameter tuning, particularly learning rate\n$\\eta$ and batch size $B$. While techniques like $\\mu$P (Yang et al., 2022)\nprovide scaling rules for optimal $\\eta$ transfer in the infinite model size\nlimit, the optimal scaling behavior in the infinite data size limit remains\nunknown. We fill in this gap by observing for the first time an intricate\ndependence of optimal $\\eta$ scaling on the pretraining token budget $T$, $B$\nand its relation to the critical batch size $B_\\mathrm{crit}$, which we measure\nto evolve as $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal\nbatch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed\nbecomes suboptimal over time even if learning rate is scaled optimally.\nSurprisingly, our results demonstrate that the observed optimal $\\eta$ and $B$\ndynamics are preserved with $\\mu$P model scaling, challenging the conventional\nview of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing\noptimality, we examine the sensitivity of loss to changes in learning rate,\nwhere we find the sensitivity to decrease with increase of $T$ and to remain\nconstant with $\\mu$P model scaling. We hope our results make the first step\ntowards a unified picture of the joint optimal data and model scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the main challenges in optimal scaling of large language models (LLMs)\nis the prohibitive cost of hyperparameter tuning, particularly learning rate\n$\\eta$ and batch size $B$. While techniques like $\\mu$P (Yang et al., 2022)\nprovide scaling rules for optimal $\\eta$ transfer in the infinite model size\nlimit, the optimal scaling behavior in the infinite data size limit remains\nunknown. We fill in this gap by observing for the first time an intricate\ndependence of optimal $\\eta$ scaling on the pretraining token budget $T$, $B$\nand its relation to the critical batch size $B_\\mathrm{crit}$, which we measure\nto evolve as $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal\nbatch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed\nbecomes suboptimal over time even if learning rate is scaled optimally.\nSurprisingly, our results demonstrate that the observed optimal $\\eta$ and $B$\ndynamics are preserved with $\\mu$P model scaling, challenging the conventional\nview of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing\noptimality, we examine the sensitivity of loss to changes in learning rate,\nwhere we find the sensitivity to decrease with increase of $T$ and to remain\nconstant with $\\mu$P model scaling. We hope our results make the first step\ntowards a unified picture of the joint optimal data and model scaling."
                },
                "authors": [
                    {
                        "name": "Oleg Filatov"
                    },
                    {
                        "name": "Jan Ebert"
                    },
                    {
                        "name": "Jiangtao Wang"
                    },
                    {
                        "name": "Stefan Kesselheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kesselheim"
                },
                "author": "Stefan Kesselheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05249v1",
                "updated": "2025-01-09T14:01:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    1,
                    15,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T14:01:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    1,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-WM: An Efficient Black-Box Watermarking Approach for\n  Retrieval-Augmented Generation of Large Language Models"
                },
                "summary": "In recent years, tremendous success has been witnessed in Retrieval-Augmented\nGeneration (RAG), widely used to enhance Large Language Models (LLMs) in\ndomain-specific, knowledge-intensive, and privacy-sensitive tasks. However,\nattackers may steal those valuable RAGs and deploy or commercialize them,\nmaking it essential to detect Intellectual Property (IP) infringement. Most\nexisting ownership protection solutions, such as watermarks, are designed for\nrelational databases and texts. They cannot be directly applied to RAGs because\nrelational database watermarks require white-box access to detect IP\ninfringement, which is unrealistic for the knowledge base in RAGs. Meanwhile,\npost-processing by the adversary's deployed LLMs typically destructs text\nwatermark information. To address those problems, we propose a novel black-box\n\"knowledge watermark\" approach, named RAG-WM, to detect IP infringement of\nRAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark\nGenerator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark\ntexts based on watermark entity-relationship tuples and inject them into the\ntarget RAG. We evaluate RAG-WM across three domain-specific and two\nprivacy-sensitive tasks on four benchmark LLMs. Experimental results show that\nRAG-WM effectively detects the stolen RAGs in various deployed LLMs.\nFurthermore, RAG-WM is robust against paraphrasing, unrelated content removal,\nknowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising application in\ndetecting IP infringement of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, tremendous success has been witnessed in Retrieval-Augmented\nGeneration (RAG), widely used to enhance Large Language Models (LLMs) in\ndomain-specific, knowledge-intensive, and privacy-sensitive tasks. However,\nattackers may steal those valuable RAGs and deploy or commercialize them,\nmaking it essential to detect Intellectual Property (IP) infringement. Most\nexisting ownership protection solutions, such as watermarks, are designed for\nrelational databases and texts. They cannot be directly applied to RAGs because\nrelational database watermarks require white-box access to detect IP\ninfringement, which is unrealistic for the knowledge base in RAGs. Meanwhile,\npost-processing by the adversary's deployed LLMs typically destructs text\nwatermark information. To address those problems, we propose a novel black-box\n\"knowledge watermark\" approach, named RAG-WM, to detect IP infringement of\nRAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark\nGenerator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark\ntexts based on watermark entity-relationship tuples and inject them into the\ntarget RAG. We evaluate RAG-WM across three domain-specific and two\nprivacy-sensitive tasks on four benchmark LLMs. Experimental results show that\nRAG-WM effectively detects the stolen RAGs in various deployed LLMs.\nFurthermore, RAG-WM is robust against paraphrasing, unrelated content removal,\nknowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising application in\ndetecting IP infringement of RAG systems."
                },
                "authors": [
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Mengjie Sun"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Shengzhi Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05248v1",
                "updated": "2025-01-09T14:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T14:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback."
                },
                "authors": [
                    {
                        "name": "Laura Puccioni"
                    },
                    {
                        "name": "Alireza Farshin"
                    },
                    {
                        "name": "Mariano Scazzariello"
                    },
                    {
                        "name": "Changjie Wang"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.2; I.2.6; D.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05247v1",
                "updated": "2025-01-09T13:57:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    57,
                    9,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:57:09Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    57,
                    9,
                    3,
                    9,
                    0
                ],
                "title": "Online Prompt and Solver Selection for Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Prompt and Solver Selection for Program Synthesis"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2\\% more queries than the best single solver and\nachieves results within 4\\% of the virtual best solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2\\% more queries than the best single solver and\nachieves results within 4\\% of the virtual best solver."
                },
                "authors": [
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Lewis Frampton"
                    },
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth Polgreen"
                },
                "author": "Elizabeth Polgreen",
                "arxiv_comment": "Accepted at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05244v1",
                "updated": "2025-01-09T13:52:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    52,
                    30,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:52:30Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    52,
                    30,
                    3,
                    9,
                    0
                ],
                "title": "Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast\n  Fourier Transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast\n  Fourier Transforms"
                },
                "summary": "Non-line-of-Sight (NLOS) imaging systems collect light at a diffuse relay\nsurface and input this measurement into computational algorithms that output a\n3D volumetric reconstruction. These algorithms utilize the Fast Fourier\nTransform (FFT) to accelerate the reconstruction process but require both input\nand output to be sampled spatially with uniform grids. However, the geometry of\nNLOS imaging inherently results in non-uniform sampling on the relay surface\nwhen using multi-pixel detector arrays, even though such arrays significantly\nreduce acquisition times. Furthermore, using these arrays increases the data\nrate required for sensor readout, posing challenges for real-world deployment.\nIn this work, we utilize the phasor field framework to demonstrate that\nexisting NLOS imaging setups typically oversample the relay surface spatially,\nexplaining why the measurement can be compressed without significantly\nsacrificing reconstruction quality. This enables us to utilize the Non-Uniform\nFast Fourier Transform (NUFFT) to reconstruct from sparse measurements acquired\nfrom irregularly sampled relay surfaces of arbitrary shapes. Furthermore, we\nutilize the NUFFT to reconstruct at arbitrary locations in the hidden volume,\nensuring flexible sampling schemes for both the input and output. Finally, we\nutilize the Scaled Fast Fourier Transform (SFFT) to reconstruct larger volumes\nwithout increasing the number of samples stored in memory. All algorithms\nintroduced in this paper preserve the computational complexity of FFT-based\nmethods, ensuring scalability for practical NLOS imaging applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-line-of-Sight (NLOS) imaging systems collect light at a diffuse relay\nsurface and input this measurement into computational algorithms that output a\n3D volumetric reconstruction. These algorithms utilize the Fast Fourier\nTransform (FFT) to accelerate the reconstruction process but require both input\nand output to be sampled spatially with uniform grids. However, the geometry of\nNLOS imaging inherently results in non-uniform sampling on the relay surface\nwhen using multi-pixel detector arrays, even though such arrays significantly\nreduce acquisition times. Furthermore, using these arrays increases the data\nrate required for sensor readout, posing challenges for real-world deployment.\nIn this work, we utilize the phasor field framework to demonstrate that\nexisting NLOS imaging setups typically oversample the relay surface spatially,\nexplaining why the measurement can be compressed without significantly\nsacrificing reconstruction quality. This enables us to utilize the Non-Uniform\nFast Fourier Transform (NUFFT) to reconstruct from sparse measurements acquired\nfrom irregularly sampled relay surfaces of arbitrary shapes. Furthermore, we\nutilize the NUFFT to reconstruct at arbitrary locations in the hidden volume,\nensuring flexible sampling schemes for both the input and output. Finally, we\nutilize the Scaled Fast Fourier Transform (SFFT) to reconstruct larger volumes\nwithout increasing the number of samples stored in memory. All algorithms\nintroduced in this paper preserve the computational complexity of FFT-based\nmethods, ensuring scalability for practical NLOS imaging applications."
                },
                "authors": [
                    {
                        "name": "Talha Sultan"
                    },
                    {
                        "name": "Alex Bocchieri"
                    },
                    {
                        "name": "Chaoying Gu"
                    },
                    {
                        "name": "Xiaochun Liu"
                    },
                    {
                        "name": "Pavel Polynkin"
                    },
                    {
                        "name": "Andreas Velten"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Velten"
                },
                "author": "Andreas Velten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05234v1",
                "updated": "2025-01-09T13:41:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    41,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:41:37Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    41,
                    37,
                    3,
                    9,
                    0
                ],
                "title": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs"
                },
                "summary": "This paper presents an approach for generating high-quality, same-language\nsubtitles for Estonian TV content. We fine-tune the Whisper model on\nhuman-generated Estonian subtitles and enhance it with iterative\npseudo-labeling and large language model (LLM) based post-editing. Our\nexperiments demonstrate notable subtitle quality improvement through\npseudo-labeling with an unlabeled dataset. We find that applying LLM-based\nediting at test time enhances subtitle accuracy, while its use during training\ndoes not yield further gains. This approach holds promise for creating subtitle\nquality close to human standard and could be extended to real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an approach for generating high-quality, same-language\nsubtitles for Estonian TV content. We fine-tune the Whisper model on\nhuman-generated Estonian subtitles and enhance it with iterative\npseudo-labeling and large language model (LLM) based post-editing. Our\nexperiments demonstrate notable subtitle quality improvement through\npseudo-labeling with an unlabeled dataset. We find that applying LLM-based\nediting at test time enhances subtitle accuracy, while its use during training\ndoes not yield further gains. This approach holds promise for creating subtitle\nquality close to human standard and could be extended to real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Artem Fedorchenko"
                    },
                    {
                        "name": "Tanel Alumäe"
                    }
                ],
                "author_detail": {
                    "name": "Tanel Alumäe"
                },
                "author": "Tanel Alumäe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05228v1",
                "updated": "2025-01-09T13:36:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    36,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:36:37Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    36,
                    37,
                    3,
                    9,
                    0
                ],
                "title": "Harnessing Large Language and Vision-Language Models for Robust\n  Out-of-Distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language and Vision-Language Models for Robust\n  Out-of-Distribution Detection"
                },
                "summary": "Out-of-distribution (OOD) detection has seen significant advancements with\nzero-shot approaches by leveraging the powerful Vision-Language Models (VLMs)\nsuch as CLIP. However, prior research works have predominantly focused on\nenhancing Far-OOD performance, while potentially compromising Near-OOD\nefficacy, as observed from our pilot study. To address this issue, we propose a\nnovel strategy to enhance zero-shot OOD detection performances for both Far-OOD\nand Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs)\nand VLMs. Our approach first exploit an LLM to generate superclasses of the ID\nlabels and their corresponding background descriptions followed by feature\nextraction using CLIP. We then isolate the core semantic features for ID data\nby subtracting background features from the superclass features. The refined\nrepresentation facilitates the selection of more appropriate negative labels\nfor OOD data from a comprehensive candidate label set of WordNet, thereby\nenhancing the performance of zero-shot OOD detection in both scenarios.\nFurthermore, we introduce novel few-shot prompt tuning and visual prompt tuning\nto adapt the proposed framework to better align with the target distribution.\nExperimental results demonstrate that the proposed approach consistently\noutperforms current state-of-the-art methods across multiple benchmarks, with\nan improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.\nAdditionally, our method exhibits superior robustness against covariate shift\nacross different domains, further highlighting its effectiveness in real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection has seen significant advancements with\nzero-shot approaches by leveraging the powerful Vision-Language Models (VLMs)\nsuch as CLIP. However, prior research works have predominantly focused on\nenhancing Far-OOD performance, while potentially compromising Near-OOD\nefficacy, as observed from our pilot study. To address this issue, we propose a\nnovel strategy to enhance zero-shot OOD detection performances for both Far-OOD\nand Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs)\nand VLMs. Our approach first exploit an LLM to generate superclasses of the ID\nlabels and their corresponding background descriptions followed by feature\nextraction using CLIP. We then isolate the core semantic features for ID data\nby subtracting background features from the superclass features. The refined\nrepresentation facilitates the selection of more appropriate negative labels\nfor OOD data from a comprehensive candidate label set of WordNet, thereby\nenhancing the performance of zero-shot OOD detection in both scenarios.\nFurthermore, we introduce novel few-shot prompt tuning and visual prompt tuning\nto adapt the proposed framework to better align with the target distribution.\nExperimental results demonstrate that the proposed approach consistently\noutperforms current state-of-the-art methods across multiple benchmarks, with\nan improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95.\nAdditionally, our method exhibits superior robustness against covariate shift\nacross different domains, further highlighting its effectiveness in real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Pei-Kang Lee"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    },
                    {
                        "name": "Ja-Ling Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ja-Ling Wu"
                },
                "author": "Ja-Ling Wu",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05224v1",
                "updated": "2025-01-09T13:24:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    24,
                    11,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T13:24:11Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    24,
                    11,
                    3,
                    9,
                    0
                ],
                "title": "Leveraging Large Language Models for Zero-shot Lay Summarisation in\n  Biomedicine and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Zero-shot Lay Summarisation in\n  Biomedicine and Beyond"
                },
                "summary": "In this work, we explore the application of Large Language Models to\nzero-shot Lay Summarisation. We propose a novel two-stage framework for Lay\nSummarisation based on real-life processes, and find that summaries generated\nwith this method are increasingly preferred by human judges for larger models.\nTo help establish best practices for employing LLMs in zero-shot settings, we\nalso assess the ability of LLMs as judges, finding that they are able to\nreplicate the preferences of human judges. Finally, we take the initial steps\ntowards Lay Summarisation for Natural Language Processing (NLP) articles,\nfinding that LLMs are able to generalise to this new domain, and further\nhighlighting the greater utility of summaries generated by our proposed\napproach via an in-depth human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the application of Large Language Models to\nzero-shot Lay Summarisation. We propose a novel two-stage framework for Lay\nSummarisation based on real-life processes, and find that summaries generated\nwith this method are increasingly preferred by human judges for larger models.\nTo help establish best practices for employing LLMs in zero-shot settings, we\nalso assess the ability of LLMs as judges, finding that they are able to\nreplicate the preferences of human judges. Finally, we take the initial steps\ntowards Lay Summarisation for Natural Language Processing (NLP) articles,\nfinding that LLMs are able to generalise to this new domain, and further\nhighlighting the greater utility of summaries generated by our proposed\napproach via an in-depth human evaluation."
                },
                "authors": [
                    {
                        "name": "Tomas Goldsack"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09094v2",
                "updated": "2025-01-09T12:38:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    38,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-12T09:22:04Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    22,
                    4,
                    3,
                    347,
                    0
                ],
                "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filter-then-Generate: Large Language Models with Structure-Text Adapter\n  for Knowledge Graph Completion"
                },
                "summary": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a \\textit{filter-then-generate} paradigm and formulate the KGC task\ninto a multiple-choice question format. In this way, we can harness the\ncapability of LLMs while mitigating the issue casused by hallucinations.\nMoreover, we devise a flexible ego-graph serialization prompt and employ a\nstructure-text adapter to couple structure and text information in a\ncontextualized manner. Experimental results demonstrate that FtG achieves\nsubstantial performance gain compared to existing state-of-the-art methods. The\ninstruction dataset and code are available at\n\\url{https://github.com/LB0828/FtG}."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihai Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Min Peng"
                    }
                ],
                "author_detail": {
                    "name": "Min Peng"
                },
                "author": "Min Peng",
                "arxiv_comment": "COLING 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05195v1",
                "updated": "2025-01-09T12:33:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    33,
                    46,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T12:33:46Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    33,
                    46,
                    3,
                    9,
                    0
                ],
                "title": "HipyrNet: Hypernet-Guided Feature Pyramid network for mixed-exposure\n  correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HipyrNet: Hypernet-Guided Feature Pyramid network for mixed-exposure\n  correction"
                },
                "summary": "Recent advancements in image translation for enhancing mixed-exposure images\nhave demonstrated the transformative potential of deep learning algorithms.\nHowever, addressing extreme exposure variations in images remains a significant\nchallenge due to the inherent complexity and contrast inconsistencies across\nregions. Current methods often struggle to adapt effectively to these\nvariations, resulting in suboptimal performance. In this work, we propose\nHipyrNet, a novel approach that integrates a HyperNetwork within a Laplacian\nPyramid-based framework to tackle the challenges of mixed-exposure image\nenhancement. The inclusion of a HyperNetwork allows the model to adapt to these\nexposure variations. HyperNetworks dynamically generates weights for another\nnetwork, allowing dynamic changes during deployment. In our model, the\nHyperNetwork employed is used to predict optimal kernels for Feature Pyramid\ndecomposition, which enables a tailored and adaptive decomposition process for\neach input image. Our enhanced translational network incorporates multiscale\ndecomposition and reconstruction, leveraging dynamic kernel prediction to\ncapture and manipulate features across varying scales. Extensive experiments\ndemonstrate that HipyrNet outperforms existing methods, particularly in\nscenarios with extreme exposure variations, achieving superior results in both\nqualitative and quantitative evaluations. Our approach sets a new benchmark for\nmixed-exposure image enhancement, paving the way for future research in\nadaptive image translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image translation for enhancing mixed-exposure images\nhave demonstrated the transformative potential of deep learning algorithms.\nHowever, addressing extreme exposure variations in images remains a significant\nchallenge due to the inherent complexity and contrast inconsistencies across\nregions. Current methods often struggle to adapt effectively to these\nvariations, resulting in suboptimal performance. In this work, we propose\nHipyrNet, a novel approach that integrates a HyperNetwork within a Laplacian\nPyramid-based framework to tackle the challenges of mixed-exposure image\nenhancement. The inclusion of a HyperNetwork allows the model to adapt to these\nexposure variations. HyperNetworks dynamically generates weights for another\nnetwork, allowing dynamic changes during deployment. In our model, the\nHyperNetwork employed is used to predict optimal kernels for Feature Pyramid\ndecomposition, which enables a tailored and adaptive decomposition process for\neach input image. Our enhanced translational network incorporates multiscale\ndecomposition and reconstruction, leveraging dynamic kernel prediction to\ncapture and manipulate features across varying scales. Extensive experiments\ndemonstrate that HipyrNet outperforms existing methods, particularly in\nscenarios with extreme exposure variations, achieving superior results in both\nqualitative and quantitative evaluations. Our approach sets a new benchmark for\nmixed-exposure image enhancement, paving the way for future research in\nadaptive image translation."
                },
                "authors": [
                    {
                        "name": "Shaurya Singh Rathore"
                    },
                    {
                        "name": "Aravind Shenoy"
                    },
                    {
                        "name": "Krish Didwania"
                    },
                    {
                        "name": "Aditya Kasliwal"
                    },
                    {
                        "name": "Ujjwal Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ujjwal Verma"
                },
                "author": "Ujjwal Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01192v2",
                "updated": "2025-01-09T12:33:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    33,
                    13,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-02T10:55:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education"
                },
                "summary": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly childhood science literacy gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly childhood science literacy gap."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Amin Alibakhshi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Alibakhshi"
                },
                "author": "Amin Alibakhshi",
                "arxiv_comment": "CHI late-breaking work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07627v2",
                "updated": "2025-01-09T12:26:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    12,
                    26,
                    49,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-10T16:07:34Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    7,
                    34,
                    1,
                    345,
                    0
                ],
                "title": "Terabit-class coherent communications enabled by an integrated photonics\n  erbium doped amplifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terabit-class coherent communications enabled by an integrated photonics\n  erbium doped amplifier"
                },
                "summary": "Coherent technologies have revolutionized optical communications, driving the\ncapacity per fiber to multi-terabit per second (Tb/s) in combination with\nwavelength division multiplexing (WDM). With an ever-increasing deployment\ndensity of coherent systems, the demand for highly integrated WDM coherent\ntransceivers has been rising. While tremendous progress has been made on\nsilicon photonics compatible high-speed modulation and photodetection on chip,\na solution for monolithically integrable amplifier with high gain and output\npower remains a challenge. Recently, an erbium doped waveguide amplifier based\non ultra-low loss silicon nitride waveguides has demonstrated gain and output\npower levels potentially suitable for Terabit class coherent communications.\nHere, we demonstrate a WDM coherent system enabled by this integrated photonic\namplification solution. The system uses the waveguide amplifier as a booster\namplifier of 16 WDM signals each carrying a net data rate of 1.6 Tb/s,\nachieving 25.6-Tb/s net capacity over 81-km fiber transmission. Our results\nhighlight a fully integrated solution for highly parallel coherent transceivers\nincluding amplification, that has the potential to transform future optical\ncommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent technologies have revolutionized optical communications, driving the\ncapacity per fiber to multi-terabit per second (Tb/s) in combination with\nwavelength division multiplexing (WDM). With an ever-increasing deployment\ndensity of coherent systems, the demand for highly integrated WDM coherent\ntransceivers has been rising. While tremendous progress has been made on\nsilicon photonics compatible high-speed modulation and photodetection on chip,\na solution for monolithically integrable amplifier with high gain and output\npower remains a challenge. Recently, an erbium doped waveguide amplifier based\non ultra-low loss silicon nitride waveguides has demonstrated gain and output\npower levels potentially suitable for Terabit class coherent communications.\nHere, we demonstrate a WDM coherent system enabled by this integrated photonic\namplification solution. The system uses the waveguide amplifier as a booster\namplifier of 16 WDM signals each carrying a net data rate of 1.6 Tb/s,\nachieving 25.6-Tb/s net capacity over 81-km fiber transmission. Our results\nhighlight a fully integrated solution for highly parallel coherent transceivers\nincluding amplification, that has the potential to transform future optical\ncommunications."
                },
                "authors": [
                    {
                        "name": "Di Che"
                    },
                    {
                        "name": "Stefano Grillanda"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zheru Qiu"
                    },
                    {
                        "name": "Xinru Ji"
                    },
                    {
                        "name": "Gregory Raybon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Kwangwoong Kim"
                    },
                    {
                        "name": "Tobias J. Kippenberg"
                    },
                    {
                        "name": "Andrea Blanco-Redondo"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Blanco-Redondo"
                },
                "author": "Andrea Blanco-Redondo",
                "arxiv_comment": "Added acknowledgements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05171v1",
                "updated": "2025-01-09T11:45:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:45:05Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "title": "Emergence of human-like polarization among large language model agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergence of human-like polarization among large language model agents"
                },
                "summary": "Rapid advances in large language models (LLMs) have empowered autonomous\nagents to establish social relationships, communicate, and form shared and\ndiverging opinions on political issues. Our understanding of their collective\nbehaviours and underlying mechanisms remains incomplete, however, posing\nunexpected risks to human society. In this paper, we simulate a networked\nsystem involving thousands of large language model agents, discovering their\nsocial interactions, guided through LLM conversation, result in human-like\npolarization. We discover that these agents spontaneously develop their own\nsocial network with human-like properties, including homophilic clustering, but\nalso shape their collective opinions through mechanisms observed in the real\nworld, including the echo chamber effect. Similarities between humans and LLM\nagents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise\nconcerns about their capacity to amplify societal polarization, but also hold\nthe potential to serve as a valuable testbed for identifying plausible\nstrategies to mitigate polarization and its consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in large language models (LLMs) have empowered autonomous\nagents to establish social relationships, communicate, and form shared and\ndiverging opinions on political issues. Our understanding of their collective\nbehaviours and underlying mechanisms remains incomplete, however, posing\nunexpected risks to human society. In this paper, we simulate a networked\nsystem involving thousands of large language model agents, discovering their\nsocial interactions, guided through LLM conversation, result in human-like\npolarization. We discover that these agents spontaneously develop their own\nsocial network with human-like properties, including homophilic clustering, but\nalso shape their collective opinions through mechanisms observed in the real\nworld, including the echo chamber effect. Similarities between humans and LLM\nagents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise\nconcerns about their capacity to amplify societal polarization, but also hold\nthe potential to serve as a valuable testbed for identifying plausible\nstrategies to mitigate polarization and its consequences."
                },
                "authors": [
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Zhihong Lu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Fernando P. Santos"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "James Evans"
                    }
                ],
                "author_detail": {
                    "name": "James Evans"
                },
                "author": "James Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11120v2",
                "updated": "2025-01-09T11:39:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    39,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-15T08:51:14Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    8,
                    51,
                    14,
                    6,
                    350,
                    0
                ],
                "title": "Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning (RL) often encounters delayed and sparse feedback in\nreal-world applications, even with only episodic rewards. Previous approaches\nhave made some progress in reward redistribution for credit assignment but\nstill face challenges, including training difficulties due to redundancy and\nambiguous attributions stemming from overlooking the multifaceted nature of\nmission performance evaluation. Hopefully, Large Language Model (LLM)\nencompasses fruitful decision-making knowledge and provides a plausible tool\nfor reward redistribution. Even so, deploying LLM in this case is non-trivial\ndue to the misalignment between linguistic knowledge and the symbolic form\nrequirement, together with inherent randomness and hallucinations in inference.\nTo tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based\ndecision-making framework, to improve credit assignment. Key to LaRe is the\nconcept of the Latent Reward, which works as a multi-dimensional performance\nevaluation, enabling more interpretable goal attainment from various\nperspectives and facilitating more effective reward redistribution. We examine\nthat semantically generated code from LLM can bridge linguistic knowledge and\nsymbolic latent rewards, as it is executable for symbolic objects. Meanwhile,\nwe design latent reward self-verification to increase the stability and\nreliability of LLM inference. Theoretically, reward-irrelevant redundancy\nelimination in the latent reward benefits RL performance from more accurate\nreward estimation. Extensive experimental results witness that LaRe (i)\nachieves superior temporal credit assignment to SOTA methods, (ii) excels in\nallocating contributions among multiple agents, and (iii) outperforms policies\ntrained with ground truth rewards for certain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) often encounters delayed and sparse feedback in\nreal-world applications, even with only episodic rewards. Previous approaches\nhave made some progress in reward redistribution for credit assignment but\nstill face challenges, including training difficulties due to redundancy and\nambiguous attributions stemming from overlooking the multifaceted nature of\nmission performance evaluation. Hopefully, Large Language Model (LLM)\nencompasses fruitful decision-making knowledge and provides a plausible tool\nfor reward redistribution. Even so, deploying LLM in this case is non-trivial\ndue to the misalignment between linguistic knowledge and the symbolic form\nrequirement, together with inherent randomness and hallucinations in inference.\nTo tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based\ndecision-making framework, to improve credit assignment. Key to LaRe is the\nconcept of the Latent Reward, which works as a multi-dimensional performance\nevaluation, enabling more interpretable goal attainment from various\nperspectives and facilitating more effective reward redistribution. We examine\nthat semantically generated code from LLM can bridge linguistic knowledge and\nsymbolic latent rewards, as it is executable for symbolic objects. Meanwhile,\nwe design latent reward self-verification to increase the stability and\nreliability of LLM inference. Theoretically, reward-irrelevant redundancy\nelimination in the latent reward benefits RL performance from more accurate\nreward estimation. Extensive experimental results witness that LaRe (i)\nachieves superior temporal credit assignment to SOTA methods, (ii) excels in\nallocating contributions among multiple agents, and (iii) outperforms policies\ntrained with ground truth rewards for certain tasks."
                },
                "authors": [
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Yuhang Jiang"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Cheems Wang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05165v1",
                "updated": "2025-01-09T11:38:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    38,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:38:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    38,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in\n  Secure Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in\n  Secure Software Engineering"
                },
                "summary": "Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Developing secure and reliable software remains a key challenge in\nsoftware engineering (SE). The ever-evolving technological landscape offers\nboth opportunities and threats, creating a dynamic space where chaos and order\ncompete. Secure software engineering (SSE) must continuously address\nvulnerabilities that endanger software systems and carry broader socio-economic\nrisks, such as compromising critical national infrastructure and causing\nsignificant financial losses. Researchers and practitioners have explored\nmethodologies like Static Application Security Testing Tools (SASTTs) and\nartificial intelligence (AI) approaches, including machine learning (ML) and\nlarge language models (LLMs), to detect and mitigate these vulnerabilities.\nEach method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing\ndomain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as\nevaluating effort-aware metrics, analyzing SASTTs, conducting method-level\nanalysis, and leveraging evidence-based techniques like systematic dataset\nreviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for\nidentifying vulnerabilities, gaps in SASTT coverage of vulnerability types,\nweak relationships among vulnerability severity scores, improved defect\nprediction accuracy using just-in-time modeling, and threats posed by untouched\nmethods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance\nof contextual knowledge in improving AI-driven vulnerability and defect\nprediction. The comprehensive analysis advances effective prediction models,\nbenefiting both researchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Esposito"
                },
                "author": "Matteo Esposito",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04239v2",
                "updated": "2025-01-09T11:38:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    38,
                    45,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T02:32:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    32,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Dynamic Localisation of Spatial-Temporal Graph Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Localisation of Spatial-Temporal Graph Neural Network"
                },
                "summary": "Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings."
                },
                "authors": [
                    {
                        "name": "Wenying Duan"
                    },
                    {
                        "name": "Shujun Guo"
                    },
                    {
                        "name": "Wei huang"
                    },
                    {
                        "name": "Hong Rao"
                    },
                    {
                        "name": "Xiaoxi He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxi He"
                },
                "author": "Xiaoxi He",
                "arxiv_comment": "This paper was accepted by KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05155v1",
                "updated": "2025-01-09T11:19:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    19,
                    40,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T11:19:40Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    19,
                    40,
                    3,
                    9,
                    0
                ],
                "title": "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Relation Extraction via Adaptive Document-Relation\n  Cross-Mapping and Concept Unique Identifier"
                },
                "summary": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify\nrelations between biomedical entities within extensive texts, serving as a\ncrucial subfield of biomedical text mining. Existing Bio-RE methods struggle\nwith cross-sentence inference, which is essential for capturing relations\nspanning multiple sentences. Moreover, previous methods often overlook the\nincompleteness of documents and lack the integration of external knowledge,\nlimiting contextual richness. Besides, the scarcity of annotated data further\nhampers model training. Recent advancements in large language models (LLMs)\nhave inspired us to explore all the above issues for document-level Bio-RE.\nSpecifically, we propose a document-level Bio-RE framework via LLM Adaptive\nDocument-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique\nIdentifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the\nIteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In\nthis way, Bio-RE task-specific synthetic data can be generated by guiding\nChatGPT to focus on entity relations and iteratively refining synthetic data.\nNext, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes\nmappings across different documents and relations, enhancing the model's\ncontextual understanding and cross-sentence inference capabilities. Finally,\nduring the inference, a biomedical-specific RAG approach, named CUI RAG, is\ndesigned to leverage CUIs as indexes for entities, narrowing the retrieval\nscope and enriching the relevant document contexts. Experiments conducted on\nthree Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art\nperformance of our proposed method by comparing it with other related works."
                },
                "authors": [
                    {
                        "name": "Yufei Shang"
                    },
                    {
                        "name": "Yanrong Guo"
                    },
                    {
                        "name": "Shijie Hao"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07066v2",
                "updated": "2025-01-09T11:11:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    11,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-11T15:30:16Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training"
                },
                "summary": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}."
                },
                "authors": [
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Leonardo Lucio Custode"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02321v2",
                "updated": "2025-01-09T10:58:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    58,
                    2,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-04T15:59:33Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    15,
                    59,
                    33,
                    5,
                    4,
                    0
                ],
                "title": "KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe\n  and 3D to 1D Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe\n  and 3D to 1D Knowledge Distillation"
                },
                "summary": "Artificial intelligence has achieved notable results in sign language\nrecognition and translation. However, relatively few efforts have been made to\nsignificantly improve the quality of life for the 72 million hearing-impaired\npeople worldwide. Sign language translation models, relying on video inputs,\ninvolves with large parameter sizes, making it time-consuming and\ncomputationally intensive to be deployed. This directly contributes to the\nscarcity of human-centered technology in this field. Additionally, the lack of\ndatasets in sign language translation hampers research progress in this area.\nTo address these, we first propose a cross-modal multi-knowledge distillation\ntechnique from 3D to 1D and a novel end-to-end pre-training text correction\nframework. Compared to other pre-trained models, our framework achieves\nsignificant advancements in correcting text output errors. Our model achieves a\ndecrease in Word Error Rate (WER) of at least 1.4% on PHOENIX14 and PHOENIX14T\ndatasets compared to the state-of-the-art CorrNet. Additionally, the TensorFlow\nLite (TFLite) quantized model size is reduced to 12.93 MB, making it the\nsmallest, fastest, and most accurate model to date. We have also collected and\nreleased extensive Chinese sign language datasets, and developed a specialized\ntraining vocabulary. To address the lack of research on data augmentation for\nlandmark data, we have designed comparative experiments on various augmentation\nmethods. Moreover, we performed a simulated deployment and prediction of our\nmodel on Intel platform CPUs and assessed the feasibility of deploying the\nmodel on other platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence has achieved notable results in sign language\nrecognition and translation. However, relatively few efforts have been made to\nsignificantly improve the quality of life for the 72 million hearing-impaired\npeople worldwide. Sign language translation models, relying on video inputs,\ninvolves with large parameter sizes, making it time-consuming and\ncomputationally intensive to be deployed. This directly contributes to the\nscarcity of human-centered technology in this field. Additionally, the lack of\ndatasets in sign language translation hampers research progress in this area.\nTo address these, we first propose a cross-modal multi-knowledge distillation\ntechnique from 3D to 1D and a novel end-to-end pre-training text correction\nframework. Compared to other pre-trained models, our framework achieves\nsignificant advancements in correcting text output errors. Our model achieves a\ndecrease in Word Error Rate (WER) of at least 1.4% on PHOENIX14 and PHOENIX14T\ndatasets compared to the state-of-the-art CorrNet. Additionally, the TensorFlow\nLite (TFLite) quantized model size is reduced to 12.93 MB, making it the\nsmallest, fastest, and most accurate model to date. We have also collected and\nreleased extensive Chinese sign language datasets, and developed a specialized\ntraining vocabulary. To address the lack of research on data augmentation for\nlandmark data, we have designed comparative experiments on various augmentation\nmethods. Moreover, we performed a simulated deployment and prediction of our\nmodel on Intel platform CPUs and assessed the feasibility of deploying the\nmodel on other platforms."
                },
                "authors": [
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Bolin Ren"
                    },
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Changyuan Liu"
                    },
                    {
                        "name": "Zhengyong Jiang"
                    },
                    {
                        "name": "Kang Dang"
                    },
                    {
                        "name": "Jionglong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jionglong Su"
                },
                "author": "Jionglong Su",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05129v1",
                "updated": "2025-01-09T10:32:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    32,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T10:32:41Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    32,
                    41,
                    3,
                    9,
                    0
                ],
                "title": "A Framework for Devising, Evaluating and Fine-tuning Indoor Tracking\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Devising, Evaluating and Fine-tuning Indoor Tracking\n  Algorithms"
                },
                "summary": "In recent years, we have observed a growing interest in Indoor Tracking\nSystems (ITS) for providing location-based services indoors. This is due to the\nlimitations of Global Navigation and Satellite Systems, which do not operate in\nnon-line-of-sight environments. Depending on their architecture, ITS can rely\non expensive infrastructure, accumulate errors, or be challenging to evaluate\nin real-life environments. Building an ITS is a complex process that involves\ndevising, evaluating and fine-tuning tracking algorithms. This process is not\nyet standard, as researchers use different types of equipment, deployment\nenvironments, and evaluation metrics. Therefore, it is challenging for\nresearchers to build novel tracking algorithms and for the research community\nto reproduce the experiments.\n  To address these challenges, we propose MobiXIM, a framework that provides a\nset of tools for devising, evaluating and fine-tuning tracking algorithms in a\nstructured manner. For devising tracking algorithms, MobiXIM introduces a novel\nplugin architecture, allowing researchers to collaborate and extend existing\nalgorithms. We assess our framework by building an ITS encompassing the key\nelements of wireless, inertial, and collaborative ITS. The proposed ITS\nachieves a positioning accuracy of 4 m, which is an improvement of up to 33%\ncompared to a baseline Pedestrian Dead Reckoning algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, we have observed a growing interest in Indoor Tracking\nSystems (ITS) for providing location-based services indoors. This is due to the\nlimitations of Global Navigation and Satellite Systems, which do not operate in\nnon-line-of-sight environments. Depending on their architecture, ITS can rely\non expensive infrastructure, accumulate errors, or be challenging to evaluate\nin real-life environments. Building an ITS is a complex process that involves\ndevising, evaluating and fine-tuning tracking algorithms. This process is not\nyet standard, as researchers use different types of equipment, deployment\nenvironments, and evaluation metrics. Therefore, it is challenging for\nresearchers to build novel tracking algorithms and for the research community\nto reproduce the experiments.\n  To address these challenges, we propose MobiXIM, a framework that provides a\nset of tools for devising, evaluating and fine-tuning tracking algorithms in a\nstructured manner. For devising tracking algorithms, MobiXIM introduces a novel\nplugin architecture, allowing researchers to collaborate and extend existing\nalgorithms. We assess our framework by building an ITS encompassing the key\nelements of wireless, inertial, and collaborative ITS. The proposed ITS\nachieves a positioning accuracy of 4 m, which is an improvement of up to 33%\ncompared to a baseline Pedestrian Dead Reckoning algorithm."
                },
                "authors": [
                    {
                        "name": "Alpha Diallo"
                    },
                    {
                        "name": "Benoit Garbinato"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Garbinato"
                },
                "author": "Benoit Garbinato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14677v2",
                "updated": "2025-01-09T10:00:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    10,
                    0,
                    2,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-18T17:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts"
                },
                "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld."
                },
                "authors": [
                    {
                        "name": "German Gritsai"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Andrey Grabovoy"
                    },
                    {
                        "name": "Yury Chekhovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Chekhovich"
                },
                "author": "Yury Chekhovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09352v2",
                "updated": "2025-01-09T09:24:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    24,
                    40,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-12T03:36:52Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    3,
                    36,
                    52,
                    5,
                    286,
                    0
                ],
                "title": "LogLM: From Task-based to Instruction-based Automated Log Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogLM: From Task-based to Instruction-based Automated Log Analysis"
                },
                "summary": "Automatic log analysis is essential for the efficient Operation and\nMaintenance (O&M) of software systems, providing critical insights into system\nbehaviors. However, existing approaches mostly treat log analysis as training a\nmodel to perform an isolated task ( e.g., anomaly detection, log parsing, etc.)\nusing task-specific log-label pairs. These task-based approaches are inflexible\nin generalizing to complex scenarios, depend on task-specific training data,\nand cost significantly when deploying multiple models. In this paper, we\npropose an instruction-based training approach that transforms log-label pairs\nfrom multiple tasks and domains into a unified format of instruction-response\npairs. Our trained model, LogLM, can follow complex user instructions and\ngeneralize better across different tasks, thereby increasing flexibility and\nreducing the dependence on task-specific training data. By integrating major\nlog analysis tasks into a single model, our approach also relieves model\ndeployment burden. Experimentally, LogLM outperforms existing approaches across\nfive log analysis capabilities, and exhibits strong generalization abilities on\ncomplex instructions and unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic log analysis is essential for the efficient Operation and\nMaintenance (O&M) of software systems, providing critical insights into system\nbehaviors. However, existing approaches mostly treat log analysis as training a\nmodel to perform an isolated task ( e.g., anomaly detection, log parsing, etc.)\nusing task-specific log-label pairs. These task-based approaches are inflexible\nin generalizing to complex scenarios, depend on task-specific training data,\nand cost significantly when deploying multiple models. In this paper, we\npropose an instruction-based training approach that transforms log-label pairs\nfrom multiple tasks and domains into a unified format of instruction-response\npairs. Our trained model, LogLM, can follow complex user instructions and\ngeneralize better across different tasks, thereby increasing flexibility and\nreducing the dependence on task-specific training data. By integrating major\nlog analysis tasks into a single model, our approach also relieves model\ndeployment burden. Experimentally, LogLM outperforms existing approaches across\nfive log analysis capabilities, and exhibits strong generalization abilities on\ncomplex instructions and unseen tasks."
                },
                "authors": [
                    {
                        "name": "Yilun Liu"
                    },
                    {
                        "name": "Yuhe Ji"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Minggui He"
                    },
                    {
                        "name": "Weibin Meng"
                    },
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Yuming Xie"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "Accepted by ICSE 2025 (SEIP Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03700v2",
                "updated": "2025-01-09T09:12:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    12,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2023-12-06T18:59:19Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    18,
                    59,
                    19,
                    2,
                    340,
                    0
                ],
                "title": "OneLLM: One Framework to Align All Modalities with Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneLLM: One Framework to Align All Modalities with Language"
                },
                "summary": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM"
                },
                "authors": [
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Kaixiong Gong"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Accepted by CVPR 2024. Code: https://github.com/csuhan/OneLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05079v1",
                "updated": "2025-01-09T09:01:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    1,
                    4,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T09:01:04Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    1,
                    4,
                    3,
                    9,
                    0
                ],
                "title": "Multimodal-to-Text Prompt Engineering in Large Language Models Using\n  Feature Embeddings for GNSS Interference Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal-to-Text Prompt Engineering in Large Language Models Using\n  Feature Embeddings for GNSS Interference Characterization"
                },
                "summary": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks."
                },
                "authors": [
                    {
                        "name": "Harshith Manjunath"
                    },
                    {
                        "name": "Lucas Heublein"
                    },
                    {
                        "name": "Tobias Feigl"
                    },
                    {
                        "name": "Felix Ott"
                    }
                ],
                "author_detail": {
                    "name": "Felix Ott"
                },
                "author": "Felix Ott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1; H.5; I.4.9; I.4.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05078v1",
                "updated": "2025-01-09T09:00:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    0,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T09:00:32Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    0,
                    32,
                    3,
                    9,
                    0
                ],
                "title": "Analyzing Memorization in Large Language Models through the Lens of\n  Model Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Memorization in Large Language Models through the Lens of\n  Model Attribution"
                },
                "summary": "Large Language Models (LLMs) are prevalent in modern applications but often\nmemorize training data, leading to privacy breaches and copyright issues.\nExisting research has mainly focused on posthoc analyses, such as extracting\nmemorized content or developing memorization metrics, without exploring the\nunderlying architectural factors that contribute to memorization. In this work,\nwe investigate memorization from an architectural lens by analyzing how\nattention modules at different layers impact its memorization and\ngeneralization performance. Using attribution techniques, we systematically\nintervene in the LLM architecture by bypassing attention modules at specific\nblocks while keeping other components like layer normalization and MLP\ntransformations intact. We provide theorems analyzing our intervention\nmechanism from a mathematical view, bounding the difference in layer outputs\nwith and without our attributions. Our theoretical and empirical analyses\nreveal that attention modules in deeper transformer blocks are primarily\nresponsible for memorization, whereas earlier blocks are crucial for the models\ngeneralization and reasoning capabilities. We validate our findings through\ncomprehensive experiments on different LLM families (Pythia and GPTNeo) and\nfive benchmark datasets. Our insights offer a practical approach to mitigate\nmemorization in LLMs while preserving their performance, contributing to safer\nand more ethical deployment in real world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prevalent in modern applications but often\nmemorize training data, leading to privacy breaches and copyright issues.\nExisting research has mainly focused on posthoc analyses, such as extracting\nmemorized content or developing memorization metrics, without exploring the\nunderlying architectural factors that contribute to memorization. In this work,\nwe investigate memorization from an architectural lens by analyzing how\nattention modules at different layers impact its memorization and\ngeneralization performance. Using attribution techniques, we systematically\nintervene in the LLM architecture by bypassing attention modules at specific\nblocks while keeping other components like layer normalization and MLP\ntransformations intact. We provide theorems analyzing our intervention\nmechanism from a mathematical view, bounding the difference in layer outputs\nwith and without our attributions. Our theoretical and empirical analyses\nreveal that attention modules in deeper transformer blocks are primarily\nresponsible for memorization, whereas earlier blocks are crucial for the models\ngeneralization and reasoning capabilities. We validate our findings through\ncomprehensive experiments on different LLM families (Pythia and GPTNeo) and\nfive benchmark datasets. Our insights offer a practical approach to mitigate\nmemorization in LLMs while preserving their performance, contributing to safer\nand more ethical deployment in real world applications."
                },
                "authors": [
                    {
                        "name": "Tarun Ram Menta"
                    },
                    {
                        "name": "Susmit Agrawal"
                    },
                    {
                        "name": "Chirag Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Agarwal"
                },
                "author": "Chirag Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05075v1",
                "updated": "2025-01-09T08:59:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    59,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:59:14Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    59,
                    14,
                    3,
                    9,
                    0
                ],
                "title": "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model"
                },
                "summary": "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance."
                },
                "authors": [
                    {
                        "name": "Shuo Tong"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Runyuan Guo"
                    },
                    {
                        "name": "Xueqiong Tian"
                    },
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Youmin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youmin Zhang"
                },
                "author": "Youmin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14503v2",
                "updated": "2025-01-09T08:55:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    55,
                    7,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-21T08:31:06Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    31,
                    6,
                    3,
                    326,
                    0
                ],
                "title": "Planning-Driven Programming: A Large Language Model Programming Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning-Driven Programming: A Large Language Model Programming Workflow"
                },
                "summary": "The strong performance of large language models (LLMs) raises extensive\ndiscussion on their application to code generation. Recent research suggests\ncontinuous program refinements through visible tests to improve code generation\naccuracy in LLMs. However, these methods suffer from LLMs' inefficiency and\nlimited reasoning capacity. In this work, we propose an LLM programming\nworkflow (LPW) designed to improve both initial code generation and subsequent\nrefinements within a structured two-phase workflow. Specifically, the solution\ngeneration phase formulates a solution plan, which is then verified through\nvisible tests to specify the intended natural language solution. Subsequently,\nthe code implementation phase drafts an initial code according to the solution\nplan and its verification. If the generated code fails the visible tests, the\nplan verification serves as the intended solution to consistently inform the\nrefinement process for correcting bugs. Compared to state-of-the-art methods\nacross various existing LLMs, LPW significantly improves the Pass@1 accuracy by\nup to 16.4% on well-established text-to-code generation benchmarks. LPW also\nsets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8%\non MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using\nGPT-4o as the backbone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong performance of large language models (LLMs) raises extensive\ndiscussion on their application to code generation. Recent research suggests\ncontinuous program refinements through visible tests to improve code generation\naccuracy in LLMs. However, these methods suffer from LLMs' inefficiency and\nlimited reasoning capacity. In this work, we propose an LLM programming\nworkflow (LPW) designed to improve both initial code generation and subsequent\nrefinements within a structured two-phase workflow. Specifically, the solution\ngeneration phase formulates a solution plan, which is then verified through\nvisible tests to specify the intended natural language solution. Subsequently,\nthe code implementation phase drafts an initial code according to the solution\nplan and its verification. If the generated code fails the visible tests, the\nplan verification serves as the intended solution to consistently inform the\nrefinement process for correcting bugs. Compared to state-of-the-art methods\nacross various existing LLMs, LPW significantly improves the Pass@1 accuracy by\nup to 16.4% on well-established text-to-code generation benchmarks. LPW also\nsets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8%\non MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using\nGPT-4o as the backbone."
                },
                "authors": [
                    {
                        "name": "Chao Lei"
                    },
                    {
                        "name": "Yanchuan Chang"
                    },
                    {
                        "name": "Nir Lipovetzky"
                    },
                    {
                        "name": "Krista A. Ehinger"
                    }
                ],
                "author_detail": {
                    "name": "Krista A. Ehinger"
                },
                "author": "Krista A. Ehinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05057v1",
                "updated": "2025-01-09T08:28:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    28,
                    16,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:28:16Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    28,
                    16,
                    3,
                    9,
                    0
                ],
                "title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with\n  Large Language Models"
                },
                "summary": "Recent advancements in reinforcement learning (RL) demonstrate the\nsignificant potential in autonomous driving. Despite this promise, challenges\nsuch as the manual design of reward functions and low sample efficiency in\ncomplex environments continue to impede the development of safe and effective\ndriving policies. To tackle these issues, we introduce LearningFlow, an\ninnovative automated policy learning workflow tailored to urban driving. This\nframework leverages the collaboration of multiple large language model (LLM)\nagents throughout the RL training process. LearningFlow includes a curriculum\nsequence generation process and a reward generation process, which work in\ntandem to guide the RL policy by generating tailored training curricula and\nreward functions. Particularly, each process is supported by an analysis agent\nthat evaluates training progress and provides critical insights to the\ngeneration agent. Through the collaborative efforts of these LLM agents,\nLearningFlow automates policy learning across a series of complex driving\ntasks, and it significantly reduces the reliance on manual reward function\ndesign while enhancing sample efficiency. Comprehensive experiments are\nconducted in the high-fidelity CARLA simulator, along with comparisons with\nother existing methods, to demonstrate the efficacy of our proposed approach.\nThe results demonstrate that LearningFlow excels in generating rewards and\ncurricula. It also achieves superior performance and robust generalization\nacross various driving tasks, as well as commendable adaptation to different RL\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reinforcement learning (RL) demonstrate the\nsignificant potential in autonomous driving. Despite this promise, challenges\nsuch as the manual design of reward functions and low sample efficiency in\ncomplex environments continue to impede the development of safe and effective\ndriving policies. To tackle these issues, we introduce LearningFlow, an\ninnovative automated policy learning workflow tailored to urban driving. This\nframework leverages the collaboration of multiple large language model (LLM)\nagents throughout the RL training process. LearningFlow includes a curriculum\nsequence generation process and a reward generation process, which work in\ntandem to guide the RL policy by generating tailored training curricula and\nreward functions. Particularly, each process is supported by an analysis agent\nthat evaluates training progress and provides critical insights to the\ngeneration agent. Through the collaborative efforts of these LLM agents,\nLearningFlow automates policy learning across a series of complex driving\ntasks, and it significantly reduces the reliance on manual reward function\ndesign while enhancing sample efficiency. Comprehensive experiments are\nconducted in the high-fidelity CARLA simulator, along with comparisons with\nother existing methods, to demonstrate the efficacy of our proposed approach.\nThe results demonstrate that LearningFlow excels in generating rewards and\ncurricula. It also achieves superior performance and robust generalization\nacross various driving tasks, as well as commendable adaptation to different RL\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Zengqi Peng"
                    },
                    {
                        "name": "Yubin Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Lei Zheng"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05051v1",
                "updated": "2025-01-09T08:20:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    20,
                    42,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T08:20:42Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    20,
                    42,
                    3,
                    9,
                    0
                ],
                "title": "On the Generalizability of Transformer Models to Code Completions of\n  Different Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalizability of Transformer Models to Code Completions of\n  Different Lengths"
                },
                "summary": "The programming landscape is nowadays being reshaped by the advent of Large\nLanguage Models (LLMs) able to automate code-related tasks related to code\nimplementation (e.g., code completion) and comprehension (e.g., code\nsummarization). Such a paradigm shift comes with a number of implications\nrelated to how software will be written, maintained, and evolved. Also, these\nLLMs are extremely expensive to train, posing questions on their sustainability\nover time. Given their training cost, their ability to generalize, namely their\nability to work on task instances different from those on which they have been\ntrained, is an aspect worth being investigated. Previous work already showed\nthat transformer models can successfully support code completion in a\ncross-project setting. However, it is unclear whether LLM are able to\ngeneralize to inputs having lengths not seen during training. For example, it\nis known that training a model on short instances allows to substantially\nreduce the training cost. However, the extent to which such a model would\nprovide good performance on sequences having lengths not seen during training\nis not known. Many recent works in Natural Language Processing (NLP) tackled\nthis problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To\nassess if these solutions extend to encoder-decoder LLMs usually adopted in the\ncode-related tasks, we present a large empirical study evaluating this\ngeneralization property of these and other encoding schemes proposed in the\nliterature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these\nsolutions successfully generalize to unseen lengths and that the only safe\nsolution is to ensure the representativeness in the training set of all lengths\nlikely to be encountered at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The programming landscape is nowadays being reshaped by the advent of Large\nLanguage Models (LLMs) able to automate code-related tasks related to code\nimplementation (e.g., code completion) and comprehension (e.g., code\nsummarization). Such a paradigm shift comes with a number of implications\nrelated to how software will be written, maintained, and evolved. Also, these\nLLMs are extremely expensive to train, posing questions on their sustainability\nover time. Given their training cost, their ability to generalize, namely their\nability to work on task instances different from those on which they have been\ntrained, is an aspect worth being investigated. Previous work already showed\nthat transformer models can successfully support code completion in a\ncross-project setting. However, it is unclear whether LLM are able to\ngeneralize to inputs having lengths not seen during training. For example, it\nis known that training a model on short instances allows to substantially\nreduce the training cost. However, the extent to which such a model would\nprovide good performance on sequences having lengths not seen during training\nis not known. Many recent works in Natural Language Processing (NLP) tackled\nthis problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To\nassess if these solutions extend to encoder-decoder LLMs usually adopted in the\ncode-related tasks, we present a large empirical study evaluating this\ngeneralization property of these and other encoding schemes proposed in the\nliterature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these\nsolutions successfully generalize to unseen lengths and that the only safe\nsolution is to ensure the representativeness in the training set of all lengths\nlikely to be encountered at inference time."
                },
                "authors": [
                    {
                        "name": "Nathan Cooper"
                    },
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Gabriele Bavota"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    }
                ],
                "author_detail": {
                    "name": "Denys Poshyvanyk"
                },
                "author": "Denys Poshyvanyk",
                "arxiv_comment": "Accepted for publication at ICSME 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05040v1",
                "updated": "2025-01-09T07:54:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    54,
                    24,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:54:24Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    54,
                    24,
                    3,
                    9,
                    0
                ],
                "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source LLM designed to effectively and efficiently resolve GitHub\nissues. SWE-Fixer comprises two essential modules: a code file retrieval module\nand a code editing module. The retrieval module employs BM25 along with a\nlightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently,\nthe code editing module utilizes the other LLM model to generate patches for\nthe identified files. Then, to mitigate the lack of publicly available\ndatasets, we compile an extensive dataset that includes 110K GitHub issues\nalong with their corresponding patches, and train the two modules of SWE-Fixer\nseparately. We assess our approach on the SWE-Bench Lite and Verified\nbenchmarks, achieving state-of-the-art performance among open-source models\nwith scores of 23.3% and 30.2%, respectively. These outcomes highlight the\nefficacy of our approach. We will make our model, dataset, and code publicly\navailable at https://github.com/InternLM/SWE-Fixer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source LLM designed to effectively and efficiently resolve GitHub\nissues. SWE-Fixer comprises two essential modules: a code file retrieval module\nand a code editing module. The retrieval module employs BM25 along with a\nlightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently,\nthe code editing module utilizes the other LLM model to generate patches for\nthe identified files. Then, to mitigate the lack of publicly available\ndatasets, we compile an extensive dataset that includes 110K GitHub issues\nalong with their corresponding patches, and train the two modules of SWE-Fixer\nseparately. We assess our approach on the SWE-Bench Lite and Verified\nbenchmarks, achieving state-of-the-art performance among open-source models\nwith scores of 23.3% and 30.2%, respectively. These outcomes highlight the\nefficacy of our approach. We will make our model, dataset, and code publicly\navailable at https://github.com/InternLM/SWE-Fixer."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Our code, data, and model will be released at\n  https://github.com/InternLM/SWE-Fixer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05032v1",
                "updated": "2025-01-09T07:44:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "Enhancing Human-Like Responses in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Human-Like Responses in Large Language Models"
                },
                "summary": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes."
                },
                "authors": [
                    {
                        "name": "Ethem Yağız Çalık"
                    },
                    {
                        "name": "Talha Rüzgar Akkuş"
                    }
                ],
                "author_detail": {
                    "name": "Talha Rüzgar Akkuş"
                },
                "author": "Talha Rüzgar Akkuş",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05030v1",
                "updated": "2025-01-09T07:41:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    41,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T07:41:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    41,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "A General Retrieval-Augmented Generation Framework for Multimodal\n  Case-Based Reasoning Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Retrieval-Augmented Generation Framework for Multimodal\n  Case-Based Reasoning Applications"
                },
                "summary": "Case-based reasoning (CBR) is an experience-based approach to problem\nsolving, where a repository of solved cases is adapted to solve new cases.\nRecent research shows that Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages\nof the CBR pipeline by retrieving similar cases and using them as additional\ncontext to an LLM query. Most studies have focused on text-only applications,\nhowever, in many real-world problems the components of a case are multimodal.\nIn this paper we present MCBR-RAG, a general RAG framework for multimodal CBR\napplications. The MCBR-RAG framework converts non-text case components into\ntext-based representations, allowing it to: 1) learn application-specific\nlatent representations that can be indexed for retrieval, and 2) enrich the\nquery provided to the LLM by incorporating all case components for better\ncontext. We demonstrate MCBR-RAG's effectiveness through experiments conducted\non a simplified Math-24 application and a more complex Backgammon application.\nOur empirical results show that MCBR-RAG improves generation quality compared\nto a baseline LLM with no contextual information provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case-based reasoning (CBR) is an experience-based approach to problem\nsolving, where a repository of solved cases is adapted to solve new cases.\nRecent research shows that Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages\nof the CBR pipeline by retrieving similar cases and using them as additional\ncontext to an LLM query. Most studies have focused on text-only applications,\nhowever, in many real-world problems the components of a case are multimodal.\nIn this paper we present MCBR-RAG, a general RAG framework for multimodal CBR\napplications. The MCBR-RAG framework converts non-text case components into\ntext-based representations, allowing it to: 1) learn application-specific\nlatent representations that can be indexed for retrieval, and 2) enrich the\nquery provided to the LLM by incorporating all case components for better\ncontext. We demonstrate MCBR-RAG's effectiveness through experiments conducted\non a simplified Math-24 application and a more complex Backgammon application.\nOur empirical results show that MCBR-RAG improves generation quality compared\nto a baseline LLM with no contextual information provided."
                },
                "authors": [
                    {
                        "name": "Ofir Marom"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Marom"
                },
                "author": "Ofir Marom",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01973v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01973v3",
                "updated": "2025-01-09T07:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    26,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-28T02:28:19Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    2,
                    28,
                    19,
                    5,
                    363,
                    0
                ],
                "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models"
                },
                "summary": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles."
                },
                "authors": [
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Jia Qing Yap"
                    },
                    {
                        "name": "Andrea Wong"
                    },
                    {
                        "name": "Adriana Crespo"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Zhiyuan Yin"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Ryan Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Ye"
                },
                "author": "Ryan Ye",
                "arxiv_comment": "Di Jin and Xing Liu contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01973v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01973v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04919v2",
                "updated": "2025-01-09T07:00:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    0,
                    24,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-09T08:01:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    1,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost."
                },
                "authors": [
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Yingxia Shao"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-41136-3}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14368v2",
                "updated": "2025-01-09T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    2,
                    11,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-18T10:53:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    53,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic"
                },
                "summary": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Vishnu Nandam"
                    },
                    {
                        "name": "Justin Turnau"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Linsey Pang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "8 pages, 4 figures, accepted to SDM25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 90B20, 90C27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.9; H.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v1",
                "updated": "2025-01-09T06:00:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04985v1",
                "updated": "2025-01-09T06:00:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    8,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:08Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    8,
                    3,
                    9,
                    0
                ],
                "title": "SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and\n  Commercial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and\n  Commercial LLMs"
                },
                "summary": "The increasing threat of SMS spam, driven by evolving adversarial techniques\nand concept drift, calls for more robust and adaptive detection methods. In\nthis paper, we evaluate the potential of large language models (LLMs), both\nopen-source and commercial, for SMS spam detection, comparing their performance\nacross zero-shot, few-shot, fine-tuning, and chain-of-thought prompting\napproaches. Using a comprehensive dataset of SMS messages, we assess the spam\ndetection capabilities of prominent LLMs such as GPT-4, DeepSeek, LLAMA-2, and\nMixtral. Our findings reveal that while zero-shot learning provides\nconvenience, it is unreliable for effective spam detection. Few-shot learning,\nparticularly with carefully selected examples, improves detection but exhibits\nvariability across models. Fine-tuning emerges as the most effective strategy,\nwith Mixtral achieving 98.6% accuracy and a balanced false positive and false\nnegative rate below 2%, meeting the criteria for robust spam detection.\nFurthermore, we explore the resilience of these models to adversarial attacks,\nfinding that fine-tuning significantly enhances robustness against both\nperceptible and imperceptible manipulations. Lastly, we investigate the impact\nof concept drift and demonstrate that fine-tuned LLMs, especially when combined\nwith few-shot learning, can mitigate its effects, maintaining high performance\neven on evolving spam datasets. This study highlights the importance of\nfine-tuning and tailored learning strategies to deploy LLMs effectively for\nreal-world SMS spam detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing threat of SMS spam, driven by evolving adversarial techniques\nand concept drift, calls for more robust and adaptive detection methods. In\nthis paper, we evaluate the potential of large language models (LLMs), both\nopen-source and commercial, for SMS spam detection, comparing their performance\nacross zero-shot, few-shot, fine-tuning, and chain-of-thought prompting\napproaches. Using a comprehensive dataset of SMS messages, we assess the spam\ndetection capabilities of prominent LLMs such as GPT-4, DeepSeek, LLAMA-2, and\nMixtral. Our findings reveal that while zero-shot learning provides\nconvenience, it is unreliable for effective spam detection. Few-shot learning,\nparticularly with carefully selected examples, improves detection but exhibits\nvariability across models. Fine-tuning emerges as the most effective strategy,\nwith Mixtral achieving 98.6% accuracy and a balanced false positive and false\nnegative rate below 2%, meeting the criteria for robust spam detection.\nFurthermore, we explore the resilience of these models to adversarial attacks,\nfinding that fine-tuning significantly enhances robustness against both\nperceptible and imperceptible manipulations. Lastly, we investigate the impact\nof concept drift and demonstrate that fine-tuned LLMs, especially when combined\nwith few-shot learning, can mitigate its effects, maintaining high performance\neven on evolving spam datasets. This study highlights the importance of\nfine-tuning and tailored learning strategies to deploy LLMs effectively for\nreal-world SMS spam detection"
                },
                "authors": [
                    {
                        "name": "Muhammad Salman"
                    },
                    {
                        "name": "Muhammad Ikram"
                    },
                    {
                        "name": "Nardine Basta"
                    },
                    {
                        "name": "Mohamed Ali Kaafar"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Ali Kaafar"
                },
                "author": "Mohamed Ali Kaafar",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04976v1",
                "updated": "2025-01-09T05:15:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    15,
                    55,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T05:15:55Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    15,
                    55,
                    3,
                    9,
                    0
                ],
                "title": "On the Diagnosis of Flaky Job Failures: Understanding and Prioritizing\n  Failure Categories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Diagnosis of Flaky Job Failures: Understanding and Prioritizing\n  Failure Categories"
                },
                "summary": "The continuous delivery of modern software requires the execution of many\nautomated pipeline jobs. These jobs ensure the frequent release of new software\nversions while detecting code problems at an early stage. For TELUS, our\nindustrial partner in the telecommunications field, reliable job execution is\ncrucial to minimize wasted time and streamline Continuous Deployment (CD). In\nthis context, flaky job failures are one of the main issues hindering CD. Prior\nstudies proposed techniques based on machine learning to automate the detection\nof flaky jobs. While valuable, these solutions are insufficient to address the\nwaste associated with the diagnosis of flaky failures, which remain largely\nunexplored due to the wide range of underlying causes. This study examines\n4,511 flaky job failures at TELUS to identify the different categories of flaky\nfailures that we prioritize based on Recency, Frequency, and Monetary (RFM)\nmeasures. We identified 46 flaky failure categories that we analyzed using\nclustering and RFM measures to determine 14 priority categories for future\nautomated diagnosis and repair research. Our findings also provide valuable\ninsights into the evolution and impact of these categories. The identification\nand prioritization of flaky failure categories using RFM analysis introduce a\nnovel approach that can be used in other contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous delivery of modern software requires the execution of many\nautomated pipeline jobs. These jobs ensure the frequent release of new software\nversions while detecting code problems at an early stage. For TELUS, our\nindustrial partner in the telecommunications field, reliable job execution is\ncrucial to minimize wasted time and streamline Continuous Deployment (CD). In\nthis context, flaky job failures are one of the main issues hindering CD. Prior\nstudies proposed techniques based on machine learning to automate the detection\nof flaky jobs. While valuable, these solutions are insufficient to address the\nwaste associated with the diagnosis of flaky failures, which remain largely\nunexplored due to the wide range of underlying causes. This study examines\n4,511 flaky job failures at TELUS to identify the different categories of flaky\nfailures that we prioritize based on Recency, Frequency, and Monetary (RFM)\nmeasures. We identified 46 flaky failure categories that we analyzed using\nclustering and RFM measures to determine 14 priority categories for future\nautomated diagnosis and repair research. Our findings also provide valuable\ninsights into the evolution and impact of these categories. The identification\nand prioritization of flaky failure categories using RFM analysis introduce a\nnovel approach that can be used in other contexts."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "This paper has been accepted at the 47th International Conference on\n  Software Engineering: Software Engineering in Practice 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04975v1",
                "updated": "2025-01-09T05:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    12,
                    38,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T05:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    5,
                    12,
                    38,
                    3,
                    9,
                    0
                ],
                "title": "V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer"
                },
                "summary": "Concept Bottleneck Models (CBMs) offer inherent interpretability by initially\ntranslating images into human-comprehensible concepts, followed by a linear\ncombination of these concepts for classification. However, the annotation of\nconcepts for visual recognition tasks requires extensive expert knowledge and\nlabor, constraining the broad adoption of CBMs. Recent approaches have\nleveraged the knowledge of large language models to construct concept\nbottlenecks, with multimodal models like CLIP subsequently mapping image\nfeatures into the concept feature space for classification. Despite this, the\nconcepts produced by language models can be verbose and may introduce\nnon-visual attributes, which hurts accuracy and interpretability. In this\nstudy, we investigate to avoid these issues by constructing CBMs directly from\nmultimodal models. To this end, we adopt common words as base concept\nvocabulary and leverage auxiliary unlabeled images to construct a\nVision-to-Concept (V2C) tokenizer that can explicitly quantize images into\ntheir most relevant visual concepts, thus creating a vision-oriented concept\nbottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM\nwhich is training efficient and interpretable with high accuracy. Our V2C-CBM\nhas matched or outperformed LLM-supervised CBMs on various visual\nclassification benchmarks, validating the efficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Bottleneck Models (CBMs) offer inherent interpretability by initially\ntranslating images into human-comprehensible concepts, followed by a linear\ncombination of these concepts for classification. However, the annotation of\nconcepts for visual recognition tasks requires extensive expert knowledge and\nlabor, constraining the broad adoption of CBMs. Recent approaches have\nleveraged the knowledge of large language models to construct concept\nbottlenecks, with multimodal models like CLIP subsequently mapping image\nfeatures into the concept feature space for classification. Despite this, the\nconcepts produced by language models can be verbose and may introduce\nnon-visual attributes, which hurts accuracy and interpretability. In this\nstudy, we investigate to avoid these issues by constructing CBMs directly from\nmultimodal models. To this end, we adopt common words as base concept\nvocabulary and leverage auxiliary unlabeled images to construct a\nVision-to-Concept (V2C) tokenizer that can explicitly quantize images into\ntheir most relevant visual concepts, thus creating a vision-oriented concept\nbottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM\nwhich is training efficient and interpretable with high accuracy. Our V2C-CBM\nhas matched or outperformed LLM-supervised CBMs on various visual\nclassification benchmarks, validating the efficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Hangzhou He"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Xinliang Zhang"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Yanye Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanye Lu"
                },
                "author": "Yanye Lu",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04970v1",
                "updated": "2025-01-09T04:59:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    59,
                    15,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T04:59:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    59,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "Battling the Non-stationarity in Time Series Forecasting via Test-time\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Battling the Non-stationarity in Time Series Forecasting via Test-time\n  Adaptation"
                },
                "summary": "Deep Neural Networks have spearheaded remarkable advancements in time series\nforecasting (TSF), one of the major tasks in time series modeling. Nonetheless,\nthe non-stationarity of time series undermines the reliability of pre-trained\nsource time series forecasters in mission-critical deployment settings. In this\nstudy, we introduce a pioneering test-time adaptation framework tailored for\nTSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source\nforecasters to continuously shifting test distributions while preserving the\ncore semantic information learned during pre-training. The novel utilization of\npartially-observed ground truth and gated calibration module enables proactive,\nrobust, and model-agnostic adaptation of source forecasters. Experiments on\ndiverse benchmark datasets and cutting-edge architectures demonstrate the\nefficacy and generality of TAFAS, especially in long-term forecasting scenarios\nthat suffer from significant distribution shifts. The code is available at\nhttps://github.com/kimanki/TAFAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks have spearheaded remarkable advancements in time series\nforecasting (TSF), one of the major tasks in time series modeling. Nonetheless,\nthe non-stationarity of time series undermines the reliability of pre-trained\nsource time series forecasters in mission-critical deployment settings. In this\nstudy, we introduce a pioneering test-time adaptation framework tailored for\nTSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source\nforecasters to continuously shifting test distributions while preserving the\ncore semantic information learned during pre-training. The novel utilization of\npartially-observed ground truth and gated calibration module enables proactive,\nrobust, and model-agnostic adaptation of source forecasters. Experiments on\ndiverse benchmark datasets and cutting-edge architectures demonstrate the\nefficacy and generality of TAFAS, especially in long-term forecasting scenarios\nthat suffer from significant distribution shifts. The code is available at\nhttps://github.com/kimanki/TAFAS."
                },
                "authors": [
                    {
                        "name": "HyunGi Kim"
                    },
                    {
                        "name": "Siwon Kim"
                    },
                    {
                        "name": "Jisoo Mok"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02795v2",
                "updated": "2025-01-09T04:50:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    50,
                    16,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-06T06:29:55Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    29,
                    55,
                    0,
                    6,
                    0
                ],
                "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across\nvarious reasoning tasks, yet building a single model that consistently excels\nacross all domains remains challenging. This paper addresses this problem by\nexploring strategies to integrate multiple domain-specialized models into an\nefficient pivot model.We propose two fusion strategies to combine the strengths\nof multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially\ndistills each source model into the pivot model, followed by a weight merging\nstep to integrate the distilled models into the final model. This method\nachieves strong performance but requires substantial training effort; and (2) a\nunified fusion approach that aggregates all source models' outputs\nsimultaneously.To improve the fusion process, we introduce a novel\nRate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K\nratios during parameter merging for enhanced flexibility and\nstability.Furthermore, we propose an uncertainty-based weighting method for the\nunified approach, which dynamically balances the contributions of source models\nand outperforms other logits/distribution ensemble methods.We achieved accuracy\nimprovements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval\ntasks, respectively."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Yan"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuhao Fu"
                    },
                    {
                        "name": "Baoyi He"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Yining Di"
                    },
                    {
                        "name": "Chunlin Ji"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04967v1",
                "updated": "2025-01-09T04:41:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    41,
                    50,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T04:41:50Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    41,
                    50,
                    3,
                    9,
                    0
                ],
                "title": "Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time\n  Series Filtration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time\n  Series Filtration"
                },
                "summary": "Current machine learning (ML)-based algorithms for filtering\nelectroencephalography (EEG) time series data face challenges related to\ncumbersome training times, regularization, and accurate reconstruction. To\naddress these shortcomings, we present an ML filtration algorithm driven by a\nlogistic covariance-targeted adversarial denoising autoencoder (TADA). We\nhypothesize that the expressivity of a targeted, correlation-driven\nconvolutional autoencoder will enable effective time series filtration while\nminimizing compute requirements (e.g., runtime, model size). Furthermore, we\nexpect that adversarial training with covariance rescaling will minimize signal\ndegradation. To test this hypothesis, a TADA system prototype was trained and\nevaluated on the task of removing electromyographic (EMG) noise from EEG data\nin the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects.\nThe TADA filter surpasses conventional signal filtration algorithms across\nquantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE),\nand performs competitively against other deep learning architectures at a\nreduced model size of less than 400,000 trainable parameters. Further\nexperimentation will be necessary to assess the viability of TADA on a wider\nrange of deployment cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current machine learning (ML)-based algorithms for filtering\nelectroencephalography (EEG) time series data face challenges related to\ncumbersome training times, regularization, and accurate reconstruction. To\naddress these shortcomings, we present an ML filtration algorithm driven by a\nlogistic covariance-targeted adversarial denoising autoencoder (TADA). We\nhypothesize that the expressivity of a targeted, correlation-driven\nconvolutional autoencoder will enable effective time series filtration while\nminimizing compute requirements (e.g., runtime, model size). Furthermore, we\nexpect that adversarial training with covariance rescaling will minimize signal\ndegradation. To test this hypothesis, a TADA system prototype was trained and\nevaluated on the task of removing electromyographic (EMG) noise from EEG data\nin the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects.\nThe TADA filter surpasses conventional signal filtration algorithms across\nquantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE),\nand performs competitively against other deep learning architectures at a\nreduced model size of less than 400,000 trainable parameters. Further\nexperimentation will be necessary to assess the viability of TADA on a wider\nrange of deployment cases."
                },
                "authors": [
                    {
                        "name": "Benjamin J. Choi"
                    },
                    {
                        "name": "Griffin Milsap"
                    },
                    {
                        "name": "Clara A. Scholl"
                    },
                    {
                        "name": "Francesco Tenore"
                    },
                    {
                        "name": "Mattson Ogg"
                    }
                ],
                "author_detail": {
                    "name": "Mattson Ogg"
                },
                "arxiv_affiliation": "Johns Hopkins University Applied Physics Laboratory, Laurel, MD, United States of America",
                "author": "Mattson Ogg",
                "arxiv_comment": "[Accepted] Artificial Intelligence for Time Series Analysis (AI4TS):\n  Theory, Algorithms, and Applications @ AAAI 2025, Philadelphia, PA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04961v1",
                "updated": "2025-01-09T04:26:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    26,
                    15,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T04:26:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    26,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Domain-adaptive Post-training for Financial LLMs"
                },
                "summary": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain-adaptive post-training of LLMs for the finance\ndomain. Our approach begins by identifying the core capabilities required for\nthe target domain and designing a comprehensive evaluation suite aligned with\nthese needs. We then analyze the effectiveness of key post-training stages,\nincluding continual pretraining, instruction tuning, and preference alignment.\nBuilding on these insights, we propose an effective training recipe centered on\na novel preference data distillation method, which leverages process signals\nfrom a generative reward model. The resulting model, Llama-Fin, achieves\nstate-of-the-art performance across a wide range of financial tasks. Our\nanalysis also highlights how each post-training stage contributes to distinct\ncapabilities, uncovering specific challenges and effective solutions, providing\nvaluable insights for domain adaptation of LLMs. Project page:\nhttps://github.com/SalesforceAIResearch/FinDap",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain-adaptive post-training of LLMs for the finance\ndomain. Our approach begins by identifying the core capabilities required for\nthe target domain and designing a comprehensive evaluation suite aligned with\nthese needs. We then analyze the effectiveness of key post-training stages,\nincluding continual pretraining, instruction tuning, and preference alignment.\nBuilding on these insights, we propose an effective training recipe centered on\na novel preference data distillation method, which leverages process signals\nfrom a generative reward model. The resulting model, Llama-Fin, achieves\nstate-of-the-art performance across a wide range of financial tasks. Our\nanalysis also highlights how each post-training stage contributes to distinct\ncapabilities, uncovering specific challenges and effective solutions, providing\nvaluable insights for domain adaptation of LLMs. Project page:\nhttps://github.com/SalesforceAIResearch/FinDap"
                },
                "authors": [
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07074v2",
                "updated": "2025-01-09T04:25:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    25,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-09T17:19:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning"
                },
                "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Yichuan Li"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Kyumin Lee"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04956v1",
                "updated": "2025-01-09T04:16:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    16,
                    55,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T04:16:55Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    16,
                    55,
                    3,
                    9,
                    0
                ],
                "title": "Topology-aware Microservice Architecture in Edge Networks: Deployment\n  Optimization and Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology-aware Microservice Architecture in Edge Networks: Deployment\n  Optimization and Implementation"
                },
                "summary": "As a ubiquitous deployment paradigm, integrating microservice architecture\n(MSA) into edge networks promises to enhance the flexibility and scalability of\nservices. However, it also presents significant challenges stemming from\ndispersed node locations and intricate network topologies. In this paper, we\nhave proposed a topology-aware MSA characterized by a three-tier network\ntraffic model encompassing the service, microservices, and edge node layers.\nThis model meticulously characterizes the complex dependencies between edge\nnetwork topologies and microservices, mapping microservice deployment onto link\ntraffic to accurately estimate communication delay. Building upon this model,\nwe have formulated a weighted sum communication delay optimization problem\nconsidering different types of services. Then, a novel topology-aware and\nindividual-adaptive microservices deployment (TAIA-MD) scheme is proposed to\nsolve the problem efficiently, which accurately senses the network topology and\nincorporates an individual-adaptive mechanism in a genetic algorithm to\naccelerate the convergence and avoid local optima. Extensive simulations show\nthat, compared to the existing deployment schemes, TAIA-MD improves the\ncommunication delay performance by approximately 30% to 60% and effectively\nenhances the overall network performance. Furthermore, we implement the TAIA-MD\nscheme on a practical microservice physical platform. The experimental results\ndemonstrate that TAIA-MD achieves superior robustness in withstanding link\nfailures and network fluctuations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a ubiquitous deployment paradigm, integrating microservice architecture\n(MSA) into edge networks promises to enhance the flexibility and scalability of\nservices. However, it also presents significant challenges stemming from\ndispersed node locations and intricate network topologies. In this paper, we\nhave proposed a topology-aware MSA characterized by a three-tier network\ntraffic model encompassing the service, microservices, and edge node layers.\nThis model meticulously characterizes the complex dependencies between edge\nnetwork topologies and microservices, mapping microservice deployment onto link\ntraffic to accurately estimate communication delay. Building upon this model,\nwe have formulated a weighted sum communication delay optimization problem\nconsidering different types of services. Then, a novel topology-aware and\nindividual-adaptive microservices deployment (TAIA-MD) scheme is proposed to\nsolve the problem efficiently, which accurately senses the network topology and\nincorporates an individual-adaptive mechanism in a genetic algorithm to\naccelerate the convergence and avoid local optima. Extensive simulations show\nthat, compared to the existing deployment schemes, TAIA-MD improves the\ncommunication delay performance by approximately 30% to 60% and effectively\nenhances the overall network performance. Furthermore, we implement the TAIA-MD\nscheme on a practical microservice physical platform. The experimental results\ndemonstrate that TAIA-MD achieves superior robustness in withstanding link\nfailures and network fluctuations."
                },
                "authors": [
                    {
                        "name": "Yuang Chen"
                    },
                    {
                        "name": "Chang Wu"
                    },
                    {
                        "name": "Fangyu Zhang"
                    },
                    {
                        "name": "Chengdi Lu"
                    },
                    {
                        "name": "Yongsheng Huang"
                    },
                    {
                        "name": "Hancheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hancheng Lu"
                },
                "author": "Hancheng Lu",
                "arxiv_comment": "15 pages, 17 figures, submitted to IEEE Transactions for potential\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.10168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.10168v3",
                "updated": "2025-01-09T04:13:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    13,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2023-07-19T17:54:43Z",
                "published_parsed": [
                    2023,
                    7,
                    19,
                    17,
                    54,
                    43,
                    2,
                    200,
                    0
                ],
                "title": "LLMs as Workers in Human-Computational Algorithms? Replicating\n  Crowdsourcing Pipelines with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Workers in Human-Computational Algorithms? Replicating\n  Crowdsourcing Pipelines with LLMs"
                },
                "summary": "LLMs have shown promise in replicating human-like behavior in crowdsourcing\ntasks that were previously thought to be exclusive to human abilities. However,\ncurrent efforts focus mainly on simple atomic tasks. We explore whether LLMs\ncan replicate more complex crowdsourcing pipelines. We find that modern LLMs\ncan simulate some of crowdworkers' abilities in these ``human computation\nalgorithms,'' but the level of success is variable and influenced by\nrequesters' understanding of LLM capabilities, the specific skills required for\nsub-tasks, and the optimal interaction modality for performing these sub-tasks.\nWe reflect on human and LLMs' different sensitivities to instructions, stress\nthe importance of enabling human-facing safeguards for LLMs, and discuss the\npotential of training humans and LLMs with complementary skill sets. Crucially,\nwe show that replicating crowdsourcing pipelines offers a valuable platform to\ninvestigate 1) the relative LLM strengths on different tasks (by\ncross-comparing their performances on sub-tasks) and 2) LLMs' potential in\ncomplex tasks, where they can complete part of the tasks while leaving others\nto humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown promise in replicating human-like behavior in crowdsourcing\ntasks that were previously thought to be exclusive to human abilities. However,\ncurrent efforts focus mainly on simple atomic tasks. We explore whether LLMs\ncan replicate more complex crowdsourcing pipelines. We find that modern LLMs\ncan simulate some of crowdworkers' abilities in these ``human computation\nalgorithms,'' but the level of success is variable and influenced by\nrequesters' understanding of LLM capabilities, the specific skills required for\nsub-tasks, and the optimal interaction modality for performing these sub-tasks.\nWe reflect on human and LLMs' different sensitivities to instructions, stress\nthe importance of enabling human-facing safeguards for LLMs, and discuss the\npotential of training humans and LLMs with complementary skill sets. Crucially,\nwe show that replicating crowdsourcing pipelines offers a valuable platform to\ninvestigate 1) the relative LLM strengths on different tasks (by\ncross-comparing their performances on sub-tasks) and 2) LLMs' potential in\ncomplex tasks, where they can complete part of the tasks while leaving others\nto humans."
                },
                "authors": [
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Haiyi Zhu"
                    },
                    {
                        "name": "Maya Albayrak"
                    },
                    {
                        "name": "Alexis Axon"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Wenxing Deng"
                    },
                    {
                        "name": "Ziqi Ding"
                    },
                    {
                        "name": "Bill Guo"
                    },
                    {
                        "name": "Sireesh Gururaja"
                    },
                    {
                        "name": "Tzu-Sheng Kuo"
                    },
                    {
                        "name": "Jenny T. Liang"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Ihita Mandal"
                    },
                    {
                        "name": "Jeremiah Milbauer"
                    },
                    {
                        "name": "Xiaolin Ni"
                    },
                    {
                        "name": "Namrata Padmanabhan"
                    },
                    {
                        "name": "Subhashini Ramkumar"
                    },
                    {
                        "name": "Alexis Sudjianto"
                    },
                    {
                        "name": "Jordan Taylor"
                    },
                    {
                        "name": "Ying-Jui Tseng"
                    },
                    {
                        "name": "Patricia Vaidos"
                    },
                    {
                        "name": "Zhijin Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chenyang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyang Yang"
                },
                "author": "Chenyang Yang",
                "arxiv_doi": "10.1145/3706599.3706690",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3706690",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.10168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.10168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "CHI 2025 Case Study Track",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04947v1",
                "updated": "2025-01-09T03:50:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    50,
                    0,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T03:50:00Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    50,
                    0,
                    3,
                    9,
                    0
                ],
                "title": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene\n  Recognition in Built Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene\n  Recognition in Built Environments"
                },
                "summary": "In assistive robotics serving people with disabilities (PWD), accurate place\nrecognition in built environments is crucial to ensure that robots navigate and\ninteract safely within diverse indoor spaces. Language interfaces, particularly\nthose powered by Large Language Models (LLM) and Vision Language Models (VLM),\nhold significant promise in this context, as they can interpret visual scenes\nand correlate them with semantic information. However, such interfaces are also\nknown for their hallucinated predictions. In addition, language instructions\nprovided by humans can also be ambiguous and lack precise details about\nspecific locations, objects, or actions, exacerbating the hallucination issue.\nIn this work, we introduce Seeing with Partial Certainty (SwPC) - a framework\ndesigned to measure and align uncertainty in VLM-based place recognition,\nenabling the model to recognize when it lacks confidence and seek assistance\nwhen necessary. This framework is built on the theory of conformal prediction\nto provide statistical guarantees on place recognition while minimizing\nrequests for human help in complex indoor environment settings. Through\nexperiments on the widely used richly-annotated scene dataset Matterport3D, we\nshow that SwPC significantly increases the success rate and decreases the\namount of human intervention required relative to the prior art. SwPC can be\nutilized with any VLMs directly without requiring model fine-tuning, offering a\npromising, lightweight approach to uncertainty modeling that complements and\nscales alongside the expanding capabilities of foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In assistive robotics serving people with disabilities (PWD), accurate place\nrecognition in built environments is crucial to ensure that robots navigate and\ninteract safely within diverse indoor spaces. Language interfaces, particularly\nthose powered by Large Language Models (LLM) and Vision Language Models (VLM),\nhold significant promise in this context, as they can interpret visual scenes\nand correlate them with semantic information. However, such interfaces are also\nknown for their hallucinated predictions. In addition, language instructions\nprovided by humans can also be ambiguous and lack precise details about\nspecific locations, objects, or actions, exacerbating the hallucination issue.\nIn this work, we introduce Seeing with Partial Certainty (SwPC) - a framework\ndesigned to measure and align uncertainty in VLM-based place recognition,\nenabling the model to recognize when it lacks confidence and seek assistance\nwhen necessary. This framework is built on the theory of conformal prediction\nto provide statistical guarantees on place recognition while minimizing\nrequests for human help in complex indoor environment settings. Through\nexperiments on the widely used richly-annotated scene dataset Matterport3D, we\nshow that SwPC significantly increases the success rate and decreases the\namount of human intervention required relative to the prior art. SwPC can be\nutilized with any VLMs directly without requiring model fine-tuning, offering a\npromising, lightweight approach to uncertainty modeling that complements and\nscales alongside the expanding capabilities of foundational models."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Vineet Kamat"
                    },
                    {
                        "name": "Carol Menassa"
                    }
                ],
                "author_detail": {
                    "name": "Carol Menassa"
                },
                "author": "Carol Menassa",
                "arxiv_comment": "10 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04945v1",
                "updated": "2025-01-09T03:34:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    34,
                    7,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T03:34:07Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    34,
                    7,
                    3,
                    9,
                    0
                ],
                "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models"
                },
                "summary": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints."
                },
                "authors": [
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00358v2",
                "updated": "2025-01-09T03:25:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    25,
                    24,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-31T09:22:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    22,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and\n  Embodied Sensors Enables Dynamic Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and\n  Embodied Sensors Enables Dynamic Scene Understanding"
                },
                "summary": "This paper investigates the problem of understanding dynamic 3D scenes from\negocentric observations, a key challenge in robotics and embodied AI. Unlike\nprior studies that explored this as long-form video understanding and utilized\negocentric video only, we instead propose an LLM-based agent, Embodied\nVideoAgent, which constructs scene memory from both egocentric video and\nembodied sensory inputs (e.g. depth and pose sensing). We further introduce a\nVLM-based approach to automatically update the memory when actions or\nactivities over objects are perceived. Embodied VideoAgent attains significant\nadvantages over counterparts in challenging reasoning and planning tasks in 3D\nscenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on\nEnvQA. We have also demonstrated its potential in various embodied AI tasks\nincluding generating embodied interactions and perception for robot\nmanipulation. The code and demo will be made public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the problem of understanding dynamic 3D scenes from\negocentric observations, a key challenge in robotics and embodied AI. Unlike\nprior studies that explored this as long-form video understanding and utilized\negocentric video only, we instead propose an LLM-based agent, Embodied\nVideoAgent, which constructs scene memory from both egocentric video and\nembodied sensory inputs (e.g. depth and pose sensing). We further introduce a\nVLM-based approach to automatically update the memory when actions or\nactivities over objects are perceived. Embodied VideoAgent attains significant\nadvantages over counterparts in challenging reasoning and planning tasks in 3D\nscenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on\nEnvQA. We have also demonstrated its potential in various embodied AI tasks\nincluding generating embodied interactions and perception for robot\nmanipulation. The code and demo will be made public."
                },
                "authors": [
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Rongpeng Su"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Rujie Wu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "project page: https://embodied-videoagent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15594v3",
                "updated": "2025-01-09T03:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    8,
                    17,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-23T16:03:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    16,
                    3,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "A Survey on LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-as-a-Judge"
                },
                "summary": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Jiawei Gu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Xuehao Zhai"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "arxiv_comment": "Corrected typos & more discussion on reasoning models 33 pages, 9\n  figures. arXiv admin note: text overlap with arXiv:2310.05470 by other\n  authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17052v2",
                "updated": "2025-01-09T02:33:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    33,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-22T15:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    5,
                    30,
                    6,
                    357,
                    0
                ],
                "title": "ViLBias: A Comprehensive Framework for Bias Detection through Linguistic\n  and Visual Cues , presenting Annotation Strategies, Evaluation, and Key\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLBias: A Comprehensive Framework for Bias Detection through Linguistic\n  and Visual Cues , presenting Annotation Strategies, Evaluation, and Key\n  Challenges"
                },
                "summary": "The integration of Large Language Models (LLMs) and Vision-Language Models\n(VLMs) opens new avenues for addressing complex challenges in multimodal\ncontent analysis, particularly in biased news detection. This study introduces\nVLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect\nlinguistic and visual biases in news content. We present a multimodal dataset\ncomprising textual content and corresponding images from diverse news sources.\nWe propose a hybrid annotation framework that combines LLM-based annotations\nwith human review to ensure high-quality labeling while reducing costs and\nenhancing scalability. Our evaluation compares the performance of\nstate-of-the-art SLMs and LLMs for both modalities (text and images) and the\nresults reveal that while SLMs are computationally efficient, LLMs demonstrate\nsuperior accuracy in identifying subtle framing and text-visual\ninconsistencies. Furthermore, empirical analysis shows that incorporating\nvisual cues alongside textual data improves bias detection accuracy by 3 to 5%.\nThis study provides a comprehensive exploration of LLMs, SLMs, and VLMs as\ntools for detecting multimodal biases in news content and highlights their\nrespective strengths, limitations, and potential for future applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) and Vision-Language Models\n(VLMs) opens new avenues for addressing complex challenges in multimodal\ncontent analysis, particularly in biased news detection. This study introduces\nVLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect\nlinguistic and visual biases in news content. We present a multimodal dataset\ncomprising textual content and corresponding images from diverse news sources.\nWe propose a hybrid annotation framework that combines LLM-based annotations\nwith human review to ensure high-quality labeling while reducing costs and\nenhancing scalability. Our evaluation compares the performance of\nstate-of-the-art SLMs and LLMs for both modalities (text and images) and the\nresults reveal that while SLMs are computationally efficient, LLMs demonstrate\nsuperior accuracy in identifying subtle framing and text-visual\ninconsistencies. Furthermore, empirical analysis shows that incorporating\nvisual cues alongside textual data improves bias detection accuracy by 3 to 5%.\nThis study provides a comprehensive exploration of LLMs, SLMs, and VLMs as\ntools for detecting multimodal biases in news content and highlights their\nrespective strengths, limitations, and potential for future applications"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Caesar Saleh"
                    },
                    {
                        "name": "Emrul Hasan"
                    },
                    {
                        "name": "Franklin Ogidi"
                    },
                    {
                        "name": "Maximus Powers"
                    },
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Roya Javadi"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Reza Khazaie"
                },
                "author": "Vahid Reza Khazaie",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04323v2",
                "updated": "2025-01-09T02:33:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    33,
                    4,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T07:47:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models"
                },
                "summary": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance."
                },
                "authors": [
                    {
                        "name": "Haonan Shi"
                    },
                    {
                        "name": "Tu Ouyang"
                    },
                    {
                        "name": "An Wang"
                    }
                ],
                "author_detail": {
                    "name": "An Wang"
                },
                "author": "An Wang",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04927v1",
                "updated": "2025-01-09T02:32:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    32,
                    40,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T02:32:40Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    32,
                    40,
                    3,
                    9,
                    0
                ],
                "title": "Investigating Numerical Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Numerical Translation with Large Language Models"
                },
                "summary": "The inaccurate translation of numbers can lead to significant security\nissues, ranging from financial setbacks to medical inaccuracies. While large\nlanguage models (LLMs) have made significant advancements in machine\ntranslation, their capacity for translating numbers has not been thoroughly\nexplored. This study focuses on evaluating the reliability of LLM-based machine\ntranslation systems when handling numerical data. In order to systematically\ntest the numerical translation capabilities of currently open source LLMs, we\nhave constructed a numerical translation dataset between Chinese and English\nbased on real business data, encompassing ten types of numerical translation.\nExperiments on the dataset indicate that errors in numerical translation are a\ncommon issue, with most open-source LLMs faltering when faced with our test\nscenarios. Especially when it comes to numerical types involving large units\nlike ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can\nhave error rates as high as 20%. Finally, we introduce three potential\nstrategies to mitigate the numerical mistranslations for large units.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inaccurate translation of numbers can lead to significant security\nissues, ranging from financial setbacks to medical inaccuracies. While large\nlanguage models (LLMs) have made significant advancements in machine\ntranslation, their capacity for translating numbers has not been thoroughly\nexplored. This study focuses on evaluating the reliability of LLM-based machine\ntranslation systems when handling numerical data. In order to systematically\ntest the numerical translation capabilities of currently open source LLMs, we\nhave constructed a numerical translation dataset between Chinese and English\nbased on real business data, encompassing ten types of numerical translation.\nExperiments on the dataset indicate that errors in numerical translation are a\ncommon issue, with most open-source LLMs faltering when faced with our test\nscenarios. Especially when it comes to numerical types involving large units\nlike ``million\", ``billion\", and \"yi\", even the latest llama3.1 8b model can\nhave error rates as high as 20%. Finally, we introduce three potential\nstrategies to mitigate the numerical mistranslations for large units."
                },
                "authors": [
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Yanqing Zhao"
                    },
                    {
                        "name": "Weidong Zhang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08275v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08275v4",
                "updated": "2025-01-09T02:31:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    31,
                    37,
                    3,
                    9,
                    0
                ],
                "published": "2023-10-12T12:24:52Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    12,
                    24,
                    52,
                    3,
                    285,
                    0
                ],
                "title": "Harnessing the Power of LLM to Support Binary Taint Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Power of LLM to Support Binary Taint Analysis"
                },
                "summary": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs."
                },
                "authors": [
                    {
                        "name": "Puzhuo Liu"
                    },
                    {
                        "name": "Chengnian Sun"
                    },
                    {
                        "name": "Yaowen Zheng"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Yuncheng Wang"
                    },
                    {
                        "name": "Zhenyang Xu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "arxiv_doi": "10.1145/3711816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08275v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08275v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "36 pages,16 figures",
                "arxiv_journal_ref": "TOSEM 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04070v2",
                "updated": "2025-01-09T02:20:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    20,
                    13,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-07T14:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    57,
                    8,
                    1,
                    7,
                    0
                ],
                "title": "More is not always better? Enhancing Many-Shot In-Context Learning with\n  Differentiated and Reweighting Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More is not always better? Enhancing Many-Shot In-Context Learning with\n  Differentiated and Reweighting Objectives"
                },
                "summary": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as the number of ICL\ndemonstrations increases from a few to many, performance tends to plateau and\neventually decline. We identify two primary causes for this trend: the\nsuboptimal negative log-likelihood (NLL) optimization objective and the\nincremental data noise. To address these issues, we introduce DrICL, a novel\noptimization method that enhances model performance through Differentiated\nLearning and advantage-based Reweighting objectives. Globally, DrICL utilizes\ndifferentiated learning to optimize the NLL objective, ensuring that many-shot\nperformance surpasses zero-shot levels. Locally, it dynamically adjusts the\nweighting of many-shot demonstrations by leveraging cumulative advantages\ninspired by reinforcement learning, thereby improving generalization. This\napproach allows the model to handle varying numbers of shots effectively,\nmitigating the impact of noisy data. Recognizing the lack of multi-task\ndatasets with diverse many-shot distributions, we develop the Many-Shot ICL\nBenchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers\nfrom 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.\nICL-50 facilitates the evaluation of many-shot ICL strategies across seven\nprominent NLP tasks and 50 distinct datasets. Experimental results demonstrate\nthat LLMs enhanced with DrICL achieve significant improvements in many-shot\nsetups across various tasks, including both in-domain and out-of-domain\nscenarios. We release the code and benchmark dataset hoping to facilitate\nfurther research in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as the number of ICL\ndemonstrations increases from a few to many, performance tends to plateau and\neventually decline. We identify two primary causes for this trend: the\nsuboptimal negative log-likelihood (NLL) optimization objective and the\nincremental data noise. To address these issues, we introduce DrICL, a novel\noptimization method that enhances model performance through Differentiated\nLearning and advantage-based Reweighting objectives. Globally, DrICL utilizes\ndifferentiated learning to optimize the NLL objective, ensuring that many-shot\nperformance surpasses zero-shot levels. Locally, it dynamically adjusts the\nweighting of many-shot demonstrations by leveraging cumulative advantages\ninspired by reinforcement learning, thereby improving generalization. This\napproach allows the model to handle varying numbers of shots effectively,\nmitigating the impact of noisy data. Recognizing the lack of multi-task\ndatasets with diverse many-shot distributions, we develop the Many-Shot ICL\nBenchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers\nfrom 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.\nICL-50 facilitates the evaluation of many-shot ICL strategies across seven\nprominent NLP tasks and 50 distinct datasets. Experimental results demonstrate\nthat LLMs enhanced with DrICL achieve significant improvements in many-shot\nsetups across various tasks, including both in-domain and out-of-domain\nscenarios. We release the code and benchmark dataset hoping to facilitate\nfurther research in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Shuo Shang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "13 pages, 8 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04908v1",
                "updated": "2025-01-09T01:47:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    47,
                    41,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T01:47:41Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    47,
                    41,
                    3,
                    9,
                    0
                ],
                "title": "HaVen: Hallucination-Mitigated LLM for Verilog Code Generation Aligned\n  with HDL Engineers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HaVen: Hallucination-Mitigated LLM for Verilog Code Generation Aligned\n  with HDL Engineers"
                },
                "summary": "Recently, the use of large language models (LLMs) for Verilog code generation\nhas attracted great research interest to enable hardware design automation.\nHowever, previous works have shown a gap between the ability of LLMs and the\npractical demands of hardware description language (HDL) engineering. This gap\nincludes differences in how engineers phrase questions and hallucinations in\nthe code generated. To address these challenges, we introduce HaVen, a novel\nLLM framework designed to mitigate hallucinations and align Verilog code\ngeneration with the practices of HDL engineers. HaVen tackles hallucination\nissues by proposing a comprehensive taxonomy and employing a chain-of-thought\n(CoT) mechanism to translate symbolic modalities (e.g. truth tables, state\ndiagrams, etc.) into accurate natural language descriptions. Furthermore, HaVen\nbridges this gap by using a data augmentation strategy. It synthesizes\nhigh-quality instruction-code pairs that match real HDL engineering practices.\nOur experiments demonstrate that HaVen significantly improves the correctness\nof Verilog code generation, outperforming state-of-the-art LLM-based Verilog\ngeneration methods on VerilogEval and RTLLM benchmark. HaVen is publicly\navailable at https://github.com/Intelligent-Computing-Research-Group/HaVen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the use of large language models (LLMs) for Verilog code generation\nhas attracted great research interest to enable hardware design automation.\nHowever, previous works have shown a gap between the ability of LLMs and the\npractical demands of hardware description language (HDL) engineering. This gap\nincludes differences in how engineers phrase questions and hallucinations in\nthe code generated. To address these challenges, we introduce HaVen, a novel\nLLM framework designed to mitigate hallucinations and align Verilog code\ngeneration with the practices of HDL engineers. HaVen tackles hallucination\nissues by proposing a comprehensive taxonomy and employing a chain-of-thought\n(CoT) mechanism to translate symbolic modalities (e.g. truth tables, state\ndiagrams, etc.) into accurate natural language descriptions. Furthermore, HaVen\nbridges this gap by using a data augmentation strategy. It synthesizes\nhigh-quality instruction-code pairs that match real HDL engineering practices.\nOur experiments demonstrate that HaVen significantly improves the correctness\nof Verilog code generation, outperforming state-of-the-art LLM-based Verilog\ngeneration methods on VerilogEval and RTLLM benchmark. HaVen is publicly\navailable at https://github.com/Intelligent-Computing-Research-Group/HaVen."
                },
                "authors": [
                    {
                        "name": "Yiyao Yang"
                    },
                    {
                        "name": "Fu Teng"
                    },
                    {
                        "name": "Pengju Liu"
                    },
                    {
                        "name": "Mengnan Qi"
                    },
                    {
                        "name": "Chenyang Lv"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04904v1",
                "updated": "2025-01-09T01:32:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    32,
                    44,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T01:32:44Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    32,
                    44,
                    3,
                    9,
                    0
                ],
                "title": "JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for\n  Conversational Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for\n  Conversational Speech Synthesis"
                },
                "summary": "Recently, there has been a growing demand for conversational speech synthesis\n(CSS) that generates more natural speech by considering the conversational\ncontext. To address this, we introduce JELLY, a novel CSS framework that\nintegrates emotion recognition and context reasoning for generating appropriate\nspeech in conversation by fine-tuning a large language model (LLM) with\nmultiple partial LoRA modules. We propose an Emotion-aware Q-former encoder,\nwhich enables the LLM to perceive emotions in speech. The encoder is trained to\nalign speech emotions with text, utilizing datasets of emotional speech. The\nentire model is then fine-tuned with conversational speech data to infer\nemotional context for generating emotionally appropriate speech in\nconversation. Our experimental results demonstrate that JELLY excels in\nemotional context modeling, synthesizing speech that naturally aligns with\nconversation, while mitigating the scarcity of emotional conversational speech\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing demand for conversational speech synthesis\n(CSS) that generates more natural speech by considering the conversational\ncontext. To address this, we introduce JELLY, a novel CSS framework that\nintegrates emotion recognition and context reasoning for generating appropriate\nspeech in conversation by fine-tuning a large language model (LLM) with\nmultiple partial LoRA modules. We propose an Emotion-aware Q-former encoder,\nwhich enables the LLM to perceive emotions in speech. The encoder is trained to\nalign speech emotions with text, utilizing datasets of emotional speech. The\nentire model is then fine-tuned with conversational speech data to infer\nemotional context for generating emotionally appropriate speech in\nconversation. Our experimental results demonstrate that JELLY excels in\nemotional context modeling, synthesizing speech that naturally aligns with\nconversation, while mitigating the scarcity of emotional conversational speech\ndatasets."
                },
                "authors": [
                    {
                        "name": "Jun-Hyeok Cha"
                    },
                    {
                        "name": "Seung-Bin Kim"
                    },
                    {
                        "name": "Hyung-Seok Oh"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04901v1",
                "updated": "2025-01-09T01:26:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    26,
                    59,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T01:26:59Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    26,
                    59,
                    3,
                    9,
                    0
                ],
                "title": "ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and generating natural language content, attracting widespread\npopularity in both industry and academia in recent years. An increasing number\nof services have sprung up which offer LLMs for various tasks via APIs.\nDifferent LLMs demonstrate expertise in different domains of queries (e.g.,\ntext classification queries). Meanwhile, LLMs of different scales, complexity,\nand performance are priced diversely. Driven by this observation, a growing\nnumber of researchers are investigating the LLM ensemble strategy with a focus\non cost-effectiveness, aiming to decrease overall usage costs while enhancing\nperformance. However, to the best of our knowledge, none of the existing works\naddresses the problem, i.e., how to find an LLM ensemble subject to a cost\nbudget, which maximizes the ensemble performance.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the ensemble selection problem as\nthat of selecting a set of LLMs subject to a cost budget such that the overall\nprediction accuracy is maximized. We theoretically establish the non-decreasing\nand non-submodular properties of the prediction accuracy function and provide\nevidence that the Optimal Ensemble Selection problem is likely to be NP-hard.\nSubsequently, we apply dynamic programming and propose an algorithm called\nThriftLLM. We prove that ThriftLLM achieves a near-optimal approximation\nguarantee. In addition, it achieves state-of-the-art query performance on\nmultiple real-world datasets against 3 competitors in our extensive\nexperimental evaluation, strongly supporting the effectiveness and superiority\nof our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and generating natural language content, attracting widespread\npopularity in both industry and academia in recent years. An increasing number\nof services have sprung up which offer LLMs for various tasks via APIs.\nDifferent LLMs demonstrate expertise in different domains of queries (e.g.,\ntext classification queries). Meanwhile, LLMs of different scales, complexity,\nand performance are priced diversely. Driven by this observation, a growing\nnumber of researchers are investigating the LLM ensemble strategy with a focus\non cost-effectiveness, aiming to decrease overall usage costs while enhancing\nperformance. However, to the best of our knowledge, none of the existing works\naddresses the problem, i.e., how to find an LLM ensemble subject to a cost\nbudget, which maximizes the ensemble performance.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the ensemble selection problem as\nthat of selecting a set of LLMs subject to a cost budget such that the overall\nprediction accuracy is maximized. We theoretically establish the non-decreasing\nand non-submodular properties of the prediction accuracy function and provide\nevidence that the Optimal Ensemble Selection problem is likely to be NP-hard.\nSubsequently, we apply dynamic programming and propose an algorithm called\nThriftLLM. We prove that ThriftLLM achieves a near-optimal approximation\nguarantee. In addition, it achieves state-of-the-art query performance on\nmultiple real-world datasets against 3 competitors in our extensive\nexperimental evaluation, strongly supporting the effectiveness and superiority\nof our method."
                },
                "authors": [
                    {
                        "name": "Keke Huang"
                    },
                    {
                        "name": "Yimin Shi"
                    },
                    {
                        "name": "Dujian Ding"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Yang Fei"
                    },
                    {
                        "name": "Laks Lakshmanan"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04899v1",
                "updated": "2025-01-09T01:24:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    24,
                    59,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T01:24:59Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    24,
                    59,
                    3,
                    9,
                    0
                ],
                "title": "SUGAR: Leveraging Contextual Confidence for Smarter Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUGAR: Leveraging Contextual Confidence for Smarter Retrieval"
                },
                "summary": "Bearing in mind the limited parametric knowledge of Large Language Models\n(LLMs), retrieval-augmented generation (RAG) which supplies them with the\nrelevant external knowledge has served as an approach to mitigate the issue of\nhallucinations to a certain extent. However, uniformly retrieving supporting\ncontext makes response generation source-inefficient, as triggering the\nretriever is not always necessary, or even inaccurate, when a model gets\ndistracted by noisy retrieved content and produces an unhelpful answer.\nMotivated by these issues, we introduce Semantic Uncertainty Guided Adaptive\nRetrieval (SUGAR), where we leverage context-based entropy to actively decide\nwhether to retrieve and to further determine between single-step and multi-step\nretrieval. Our empirical results show that selective retrieval guided by\nsemantic uncertainty estimation improves the performance across diverse\nquestion answering tasks, as well as achieves a more efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bearing in mind the limited parametric knowledge of Large Language Models\n(LLMs), retrieval-augmented generation (RAG) which supplies them with the\nrelevant external knowledge has served as an approach to mitigate the issue of\nhallucinations to a certain extent. However, uniformly retrieving supporting\ncontext makes response generation source-inefficient, as triggering the\nretriever is not always necessary, or even inaccurate, when a model gets\ndistracted by noisy retrieved content and produces an unhelpful answer.\nMotivated by these issues, we introduce Semantic Uncertainty Guided Adaptive\nRetrieval (SUGAR), where we leverage context-based entropy to actively decide\nwhether to retrieve and to further determine between single-step and multi-step\nretrieval. Our empirical results show that selective retrieval guided by\nsemantic uncertainty estimation improves the performance across diverse\nquestion answering tasks, as well as achieves a more efficient inference."
                },
                "authors": [
                    {
                        "name": "Hanna Zubkova"
                    },
                    {
                        "name": "Ji-Hoon Park"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05315v2",
                "updated": "2025-01-09T00:11:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    0,
                    11,
                    59,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-05T03:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    37,
                    7,
                    5,
                    279,
                    0
                ],
                "title": "PalmBench: A Comprehensive Benchmark of Compressed Large Language Models\n  on Mobile Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PalmBench: A Comprehensive Benchmark of Compressed Large Language Models\n  on Mobile Platforms"
                },
                "summary": "Deploying large language models (LLMs) locally on mobile devices is\nadvantageous in scenarios where transmitting data to remote cloud servers is\neither undesirable due to privacy concerns or impractical due to network\nconnection. Recent advancements (MLC, 2023a; Gerganov, 2023) have facilitated\nthe local deployment of LLMs. However, local deployment also presents\nchallenges, particularly in balancing quality (generative performance),\nlatency, and throughput within the hardware constraints of mobile devices. In\nthis paper, we introduce our lightweight, all-in-one automated benchmarking\nframework that allows users to evaluate LLMs on mobile devices. We provide a\ncomprehensive benchmark of various popular LLMs with different quantization\nconfigurations (both weights and activations) across multiple mobile platforms\nwith varying hardware capabilities. Unlike traditional benchmarks that assess\nfull-scale models on high-end GPU clusters, we focus on evaluating resource\nefficiency (memory and power consumption) and harmful output for compressed\nmodels on mobile devices. Our key observations include i) differences in energy\nefficiency and throughput across mobile platforms; ii) the impact of\nquantization on memory usage, GPU execution time, and power consumption; and\niii) accuracy and performance degradation of quantized models compared to their\nnon-quantized counterparts; and iv) the frequency of hallucinations and toxic\ncontent generated by compressed LLMs on mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) locally on mobile devices is\nadvantageous in scenarios where transmitting data to remote cloud servers is\neither undesirable due to privacy concerns or impractical due to network\nconnection. Recent advancements (MLC, 2023a; Gerganov, 2023) have facilitated\nthe local deployment of LLMs. However, local deployment also presents\nchallenges, particularly in balancing quality (generative performance),\nlatency, and throughput within the hardware constraints of mobile devices. In\nthis paper, we introduce our lightweight, all-in-one automated benchmarking\nframework that allows users to evaluate LLMs on mobile devices. We provide a\ncomprehensive benchmark of various popular LLMs with different quantization\nconfigurations (both weights and activations) across multiple mobile platforms\nwith varying hardware capabilities. Unlike traditional benchmarks that assess\nfull-scale models on high-end GPU clusters, we focus on evaluating resource\nefficiency (memory and power consumption) and harmful output for compressed\nmodels on mobile devices. Our key observations include i) differences in energy\nefficiency and throughput across mobile platforms; ii) the impact of\nquantization on memory usage, GPU execution time, and power consumption; and\niii) accuracy and performance degradation of quantized models compared to their\nnon-quantized counterparts; and iv) the frequency of hallucinations and toxic\ncontent generated by compressed LLMs on mobile devices."
                },
                "authors": [
                    {
                        "name": "Yilong Li"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "M Badri Narayanan"
                    },
                    {
                        "name": "Utkarsh Sharma"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Pan Hu"
                    },
                    {
                        "name": "Yijing Zeng"
                    },
                    {
                        "name": "Jayaram Raghuram"
                    },
                    {
                        "name": "Suman Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Suman Banerjee"
                },
                "author": "Suman Banerjee",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04880v1",
                "updated": "2025-01-08T23:28:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    23,
                    28,
                    28,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T23:28:28Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    23,
                    28,
                    28,
                    2,
                    8,
                    0
                ],
                "title": "Leveraging Log Probabilities in Language Models to Forecast Future\n  Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Log Probabilities in Language Models to Forecast Future\n  Events"
                },
                "summary": "In the constantly changing field of data-driven decision making, accurately\npredicting future events is crucial for strategic planning in various sectors.\nThe emergence of Large Language Models (LLMs) marks a significant advancement\nin this area, offering advanced tools that utilise extensive text data for\nprediction. In this industry paper, we introduce a novel method for AI-driven\nforesight using LLMs. Building on top of previous research, we employ data on\ncurrent trends and their trajectories for generating forecasts on 15 different\ntopics. Subsequently, we estimate their probabilities via a multi-step approach\nbased on log probabilities. We show we achieve a Brier score of 0.186, meaning\na +26% improvement over random chance and a +19% improvement over\nwidely-available AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the constantly changing field of data-driven decision making, accurately\npredicting future events is crucial for strategic planning in various sectors.\nThe emergence of Large Language Models (LLMs) marks a significant advancement\nin this area, offering advanced tools that utilise extensive text data for\nprediction. In this industry paper, we introduce a novel method for AI-driven\nforesight using LLMs. Building on top of previous research, we employ data on\ncurrent trends and their trajectories for generating forecasts on 15 different\ntopics. Subsequently, we estimate their probabilities via a multi-step approach\nbased on log probabilities. We show we achieve a Brier score of 0.186, meaning\na +26% improvement over random chance and a +19% improvement over\nwidely-available AI systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Soru"
                    },
                    {
                        "name": "Jim Marshall"
                    }
                ],
                "author_detail": {
                    "name": "Jim Marshall"
                },
                "author": "Jim Marshall",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04877v1",
                "updated": "2025-01-08T23:21:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    23,
                    21,
                    43,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T23:21:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    23,
                    21,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "Real-Time Textless Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Textless Dialogue Generation"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nprogress in text-based dialogue systems. These systems can now generate\nhigh-quality responses that are accurate and coherent across a wide range of\ntopics and tasks. However, spoken dialogue systems still lag behind in terms of\nnaturalness. They tend to produce robotic interactions, with issues such as\nslow response times, overly generic or cautious replies, and a lack of natural\nrhythm and fluid turn-taking. This shortcoming is largely due to the\nover-reliance on the traditional cascaded design, which involve separate,\nsequential components, as well as the use of text as an intermediate\nrepresentation. This paper propose a real-time, textless spoken dialogue\ngeneration model (RTTL-DG) that aims to overcome these challenges. Our system\nenables fluid turn-taking and generates responses with minimal delay by\nprocessing streaming spoken conversation directly. Additionally, our model\nincorporates backchannels, filters, laughter, and other paralinguistic signals,\nwhich are often absent in cascaded dialogue systems, to create more natural and\nhuman-like interactions. The implementations and generated samples are\navailable in our repository: https://github.com/mailong25/rts2s-dg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nprogress in text-based dialogue systems. These systems can now generate\nhigh-quality responses that are accurate and coherent across a wide range of\ntopics and tasks. However, spoken dialogue systems still lag behind in terms of\nnaturalness. They tend to produce robotic interactions, with issues such as\nslow response times, overly generic or cautious replies, and a lack of natural\nrhythm and fluid turn-taking. This shortcoming is largely due to the\nover-reliance on the traditional cascaded design, which involve separate,\nsequential components, as well as the use of text as an intermediate\nrepresentation. This paper propose a real-time, textless spoken dialogue\ngeneration model (RTTL-DG) that aims to overcome these challenges. Our system\nenables fluid turn-taking and generates responses with minimal delay by\nprocessing streaming spoken conversation directly. Additionally, our model\nincorporates backchannels, filters, laughter, and other paralinguistic signals,\nwhich are often absent in cascaded dialogue systems, to create more natural and\nhuman-like interactions. The implementations and generated samples are\navailable in our repository: https://github.com/mailong25/rts2s-dg"
                },
                "authors": [
                    {
                        "name": "Long Mai"
                    },
                    {
                        "name": "Julie Carson-Berndsen"
                    }
                ],
                "author_detail": {
                    "name": "Julie Carson-Berndsen"
                },
                "author": "Julie Carson-Berndsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06154v3",
                "updated": "2025-01-08T23:08:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    23,
                    8,
                    3,
                    2,
                    8,
                    0
                ],
                "published": "2024-10-08T15:55:40Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    55,
                    40,
                    1,
                    282,
                    0
                ],
                "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models"
                },
                "summary": "In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels."
                },
                "authors": [
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Zhuoyuan Mao"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Paul Gavrikov"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Shiqi Yang"
                    },
                    {
                        "name": "Saurav Jha"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Horst Possegger"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Code: https://github.com/jmiemirza/GLOV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03489v2",
                "updated": "2025-01-08T22:22:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    22,
                    22,
                    43,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-07T03:17:47Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    17,
                    47,
                    1,
                    7,
                    0
                ],
                "title": "Entropy-Guided Attention for Private LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Guided Attention for Private LLMs"
                },
                "summary": "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps://github.com/Nandan91/entropy-guided-attention-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps://github.com/Nandan91/entropy-guided-attention-llm"
                },
                "authors": [
                    {
                        "name": "Nandan Kumar Jha"
                    },
                    {
                        "name": "Brandon Reagen"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Reagen"
                },
                "author": "Brandon Reagen",
                "arxiv_comment": "Accepted to the 6th AAAI Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI), 2025. arXiv admin note: substantial text overlap with\n  arXiv:2410.13060",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04860v1",
                "updated": "2025-01-08T22:22:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    22,
                    22,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T22:22:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    22,
                    22,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "Exploring the Use of Robots for Diary Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Use of Robots for Diary Studies"
                },
                "summary": "As interest in studying in-the-wild human-robot interaction grows, there is a\nneed for methods to collect data over time and in naturalistic or potentially\nprivate environments. HRI researchers have increasingly used the diary method\nfor these studies, asking study participants to self-administer a structured\ndata collection instrument, i.e., a diary, over a period of time. Although the\ndiary method offers a unique window into settings that researchers may not have\naccess to, they also lack the interactivity and probing that interview-based\nmethods offer. In this paper, we explore a novel data collection method in\nwhich a robot plays the role of an interactive diary. We developed the Diary\nRobot system and performed in-home deployments for a week to evaluate the\nfeasibility and effectiveness of this approach. Using traditional text-based\nand audio-based diaries as benchmarks, we found that robots are able to\neffectively elicit the intended information. We reflect on our findings, and\ndescribe scenarios where the utilization of robots in diary studies as a data\ncollection instrument may be especially applicable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As interest in studying in-the-wild human-robot interaction grows, there is a\nneed for methods to collect data over time and in naturalistic or potentially\nprivate environments. HRI researchers have increasingly used the diary method\nfor these studies, asking study participants to self-administer a structured\ndata collection instrument, i.e., a diary, over a period of time. Although the\ndiary method offers a unique window into settings that researchers may not have\naccess to, they also lack the interactivity and probing that interview-based\nmethods offer. In this paper, we explore a novel data collection method in\nwhich a robot plays the role of an interactive diary. We developed the Diary\nRobot system and performed in-home deployments for a week to evaluate the\nfeasibility and effectiveness of this approach. Using traditional text-based\nand audio-based diaries as benchmarks, we found that robots are able to\neffectively elicit the intended information. We reflect on our findings, and\ndescribe scenarios where the utilization of robots in diary studies as a data\ncollection instrument may be especially applicable."
                },
                "authors": [
                    {
                        "name": "Michael F. Xu"
                    },
                    {
                        "name": "Bilge Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Mutlu"
                },
                "author": "Bilge Mutlu",
                "arxiv_comment": "Proceedings of the 29th ACM/IEEE International Conference on Human\n  Robot Interaction (HRI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04848v1",
                "updated": "2025-01-08T21:22:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    21,
                    22,
                    45,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T21:22:45Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    21,
                    22,
                    45,
                    2,
                    8,
                    0
                ],
                "title": "Exploring Large Language Models for Semantic Analysis and Categorization\n  of Android Malware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Semantic Analysis and Categorization\n  of Android Malware"
                },
                "summary": "Malware analysis is a complex process of examining and evaluating malicious\nsoftware's functionality, origin, and potential impact. This arduous process\ntypically involves dissecting the software to understand its components,\ninfection vector, propagation mechanism, and payload. Over the years, deep\nreverse engineering of malware has become increasingly tedious, mainly due to\nmodern malicious codebases' fast evolution and sophistication. Essentially,\nanalysts are tasked with identifying the elusive needle in the haystack within\nthe complexities of zero-day malware, all while under tight time constraints.\nThus, in this paper, we explore leveraging Large Language Models (LLMs) for\nsemantic malware analysis to expedite the analysis of known and novel samples.\nBuilt on GPT-4o-mini model, \\msp is designed to augment malware analysis for\nAndroid through a hierarchical-tiered summarization chain and strategic prompt\nengineering. Additionally, \\msp performs malware categorization, distinguishing\npotential malware from benign applications, thereby saving time during the\nmalware reverse engineering process. Despite not being fine-tuned for Android\nmalware analysis, we demonstrate that through optimized and advanced prompt\nengineering \\msp can achieve up to 77% classification accuracy while providing\nhighly robust summaries at functional, class, and package levels. In addition,\nleveraging the backward tracing of the summaries from package to function\nlevels allowed us to pinpoint the precise code snippets responsible for\nmalicious behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware analysis is a complex process of examining and evaluating malicious\nsoftware's functionality, origin, and potential impact. This arduous process\ntypically involves dissecting the software to understand its components,\ninfection vector, propagation mechanism, and payload. Over the years, deep\nreverse engineering of malware has become increasingly tedious, mainly due to\nmodern malicious codebases' fast evolution and sophistication. Essentially,\nanalysts are tasked with identifying the elusive needle in the haystack within\nthe complexities of zero-day malware, all while under tight time constraints.\nThus, in this paper, we explore leveraging Large Language Models (LLMs) for\nsemantic malware analysis to expedite the analysis of known and novel samples.\nBuilt on GPT-4o-mini model, \\msp is designed to augment malware analysis for\nAndroid through a hierarchical-tiered summarization chain and strategic prompt\nengineering. Additionally, \\msp performs malware categorization, distinguishing\npotential malware from benign applications, thereby saving time during the\nmalware reverse engineering process. Despite not being fine-tuned for Android\nmalware analysis, we demonstrate that through optimized and advanced prompt\nengineering \\msp can achieve up to 77% classification accuracy while providing\nhighly robust summaries at functional, class, and package levels. In addition,\nleveraging the backward tracing of the summaries from package to function\nlevels allowed us to pinpoint the precise code snippets responsible for\nmalicious behavior."
                },
                "authors": [
                    {
                        "name": "Brandon J Walton"
                    },
                    {
                        "name": "Mst Eshita Khatun"
                    },
                    {
                        "name": "James M Ghawaly"
                    },
                    {
                        "name": "Aisha Ali-Gombe"
                    }
                ],
                "author_detail": {
                    "name": "Aisha Ali-Gombe"
                },
                "author": "Aisha Ali-Gombe",
                "arxiv_journal_ref": "Workshop on AI for Cyber Threat Intelligence (WAITI) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04835v1",
                "updated": "2025-01-08T20:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    20,
                    39,
                    45,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T20:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    20,
                    39,
                    45,
                    2,
                    8,
                    0
                ],
                "title": "Do Code LLMs Understand Design Patterns?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Code LLMs Understand Design Patterns?"
                },
                "summary": "Code Large Language Models (LLMs) demonstrate great versatility in adapting\nto various downstream tasks, including code generation and completion, as well\nas bug detection and fixing. However, Code LLMs often fail to capture existing\ncoding standards, leading to the generation of code that conflicts with the\nrequired design patterns for a given project. As a result, developers must\npost-process to adapt the generated code to the project's design norms. In this\nwork, we empirically investigate the biases of Code LLMs in software\ndevelopment. Through carefully designed experiments, we assess the models'\nunderstanding of design patterns across recognition, comprehension, and\ngeneration. Our findings reveal that biases in Code LLMs significantly affect\nthe reliability of downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (LLMs) demonstrate great versatility in adapting\nto various downstream tasks, including code generation and completion, as well\nas bug detection and fixing. However, Code LLMs often fail to capture existing\ncoding standards, leading to the generation of code that conflicts with the\nrequired design patterns for a given project. As a result, developers must\npost-process to adapt the generated code to the project's design norms. In this\nwork, we empirically investigate the biases of Code LLMs in software\ndevelopment. Through carefully designed experiments, we assess the models'\nunderstanding of design patterns across recognition, comprehension, and\ngeneration. Our findings reveal that biases in Code LLMs significantly affect\nthe reliability of downstream tasks."
                },
                "authors": [
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Xuefeng Song"
                    },
                    {
                        "name": "Yunkun Wang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "accpeted by llm4code workshop in ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15237v3",
                "updated": "2025-01-08T20:34:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    20,
                    34,
                    2,
                    2,
                    8,
                    0
                ],
                "published": "2024-08-27T17:56:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
                },
                "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "NeurIPS 2024. v3 updates: fix format errors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04810v1",
                "updated": "2025-01-08T19:54:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    54,
                    31,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T19:54:31Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    54,
                    31,
                    2,
                    8,
                    0
                ],
                "title": "On the Impact of Requirements Smells in Prompts: The Case of Automated\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Requirements Smells in Prompts: The Case of Automated\n  Traceability"
                },
                "summary": "Large language models (LLMs) are increasingly used to generate software\nartifacts, such as source code, tests, and trace links. Requirements play a\ncentral role in shaping the input prompts that guide LLMs, as they are often\nused as part of the prompts to synthesize the artifacts. However, the impact of\nrequirements formulation on LLM performance remains unclear. In this paper, we\ninvestigate the role of requirements smells-indicators of potential issues like\nambiguity and inconsistency-when used in prompts for LLMs. We conducted\nexperiments using two LLMs focusing on automated trace link generation between\nrequirements and code. Our results show mixed outcomes: while requirements\nsmells had a small but significant effect when predicting whether a requirement\nwas implemented in a piece of code (i.e., a trace link exists), no significant\neffect was observed when tracing the requirements with the associated lines of\ncode. These findings suggest that requirements smells can affect LLM\nperformance in certain SE tasks but may not uniformly impact all tasks. We\nhighlight the need for further research to understand these nuances and propose\nfuture work toward developing guidelines for mitigating the negative effects of\nrequirements smells in AI-driven SE processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to generate software\nartifacts, such as source code, tests, and trace links. Requirements play a\ncentral role in shaping the input prompts that guide LLMs, as they are often\nused as part of the prompts to synthesize the artifacts. However, the impact of\nrequirements formulation on LLM performance remains unclear. In this paper, we\ninvestigate the role of requirements smells-indicators of potential issues like\nambiguity and inconsistency-when used in prompts for LLMs. We conducted\nexperiments using two LLMs focusing on automated trace link generation between\nrequirements and code. Our results show mixed outcomes: while requirements\nsmells had a small but significant effect when predicting whether a requirement\nwas implemented in a piece of code (i.e., a trace link exists), no significant\neffect was observed when tracing the requirements with the associated lines of\ncode. These findings suggest that requirements smells can affect LLM\nperformance in certain SE tasks but may not uniformly impact all tasks. We\nhighlight the need for further research to understand these nuances and propose\nfuture work toward developing guidelines for mitigating the negative effects of\nrequirements smells in AI-driven SE processes."
                },
                "authors": [
                    {
                        "name": "Andreas Vogelsang"
                    },
                    {
                        "name": "Alexander Korn"
                    },
                    {
                        "name": "Giovanna Broccia"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Jannik Fischbach"
                    },
                    {
                        "name": "Chetan Arora"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Arora"
                },
                "author": "Chetan Arora",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM 47th International Conference on Software\n  Engineering: New Ideas and Emerging Results (ICSE-NIER)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04776v1",
                "updated": "2025-01-08T19:00:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    0,
                    2,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T19:00:02Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    0,
                    2,
                    2,
                    8,
                    0
                ],
                "title": "IQPopt: Fast optimization of instantaneous quantum polynomial circuits\n  in JAX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IQPopt: Fast optimization of instantaneous quantum polynomial circuits\n  in JAX"
                },
                "summary": "IQPopt is a software package designed to optimize large-scale instantaneous\nquantum polynomial circuits on classical hardware. By exploiting an efficient\nclassical simulation algorithm for expectation value estimation, circuits with\nthousands of qubits and millions of gates can be optimized, provided the\nrelevant objective function has an efficient description in terms of Pauli-Z\ntype observables. Since sampling from instantaneous quantum polynomial circuits\nis widely believed to be hard for classical computers, this provides a method\nto identify powerful circuit instances before deployment and sampling on\nquantum hardware, where computational advantages may exist. The package\nleverages automatic differentiation in JAX, can be accelerated with access to\nhardware accelerators such as graphics processing units, and contains a\ndedicated module that can be used to train and evaluate quantum generative\nmodels via the maximum mean discrepancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IQPopt is a software package designed to optimize large-scale instantaneous\nquantum polynomial circuits on classical hardware. By exploiting an efficient\nclassical simulation algorithm for expectation value estimation, circuits with\nthousands of qubits and millions of gates can be optimized, provided the\nrelevant objective function has an efficient description in terms of Pauli-Z\ntype observables. Since sampling from instantaneous quantum polynomial circuits\nis widely believed to be hard for classical computers, this provides a method\nto identify powerful circuit instances before deployment and sampling on\nquantum hardware, where computational advantages may exist. The package\nleverages automatic differentiation in JAX, can be accelerated with access to\nhardware accelerators such as graphics processing units, and contains a\ndedicated module that can be used to train and evaluate quantum generative\nmodels via the maximum mean discrepancy."
                },
                "authors": [
                    {
                        "name": "Erik Recio-Armengol"
                    },
                    {
                        "name": "Joseph Bowles"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bowles"
                },
                "author": "Joseph Bowles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17309v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17309v3",
                "updated": "2025-01-08T19:00:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    0,
                    0,
                    2,
                    8,
                    0
                ],
                "published": "2024-10-22T18:00:00Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    18,
                    0,
                    0,
                    1,
                    296,
                    0
                ],
                "title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation"
                },
                "summary": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Yangqiaoyu Zhou"
                    },
                    {
                        "name": "Mingxuan Li"
                    },
                    {
                        "name": "Chenfei Yuan"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "37 pages, 9 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17309v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17309v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04695v1",
                "updated": "2025-01-08T18:58:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    22,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:58:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Re-ranking the Context for Multimodal Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-ranking the Context for Multimodal Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response."
                },
                "authors": [
                    {
                        "name": "Matin Mortaheb"
                    },
                    {
                        "name": "Mohammad A. Amir Khojastepour"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04694v1",
                "updated": "2025-01-08T18:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCoder: Encompassing Diversity and Complexity in Code Generation"
                },
                "summary": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Wenxiang Hu"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Jinsong Su"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Scarlett Li"
                    }
                ],
                "author_detail": {
                    "name": "Scarlett Li"
                },
                "author": "Scarlett Li",
                "arxiv_comment": "40 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v1",
                "updated": "2025-01-08T18:49:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "27 pages, 10 tables, 17 figures. The training data has been released.\n  The code and model are currently undergoing internal review. They will be\n  made available soon. Project url: https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04682v1",
                "updated": "2025-01-08T18:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    42,
                    48,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    42,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought"
                },
                "summary": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence."
                },
                "authors": [
                    {
                        "name": "Violet Xiang"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Chase Blagden"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Dakota Mahan"
                    },
                    {
                        "name": "Louis Castricato"
                    },
                    {
                        "name": "Jan-Philipp Franken"
                    },
                    {
                        "name": "Nick Haber"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04675v1",
                "updated": "2025-01-08T18:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    33,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    33,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations"
                },
                "summary": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries."
                },
                "authors": [
                    {
                        "name": "Archita Srivastava"
                    },
                    {
                        "name": "Abhas Kumar"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Prabhakar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Prabhakar Srinivasan"
                },
                "author": "Prabhakar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04670v1",
                "updated": "2025-01-08T18:30:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs"
                },
                "summary": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA."
                },
                "authors": [
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Lu Qi"
                    }
                ],
                "author_detail": {
                    "name": "Lu Qi"
                },
                "author": "Lu Qi",
                "arxiv_comment": "project page: https://zhouyiks.github.io/projects/CoLVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04661v1",
                "updated": "2025-01-08T18:15:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:15:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar"
                },
                "summary": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available."
                },
                "authors": [
                    {
                        "name": "Wesley Scivetti"
                    },
                    {
                        "name": "Melissa Torgbi"
                    },
                    {
                        "name": "Austin Blodgett"
                    },
                    {
                        "name": "Mollie Shichman"
                    },
                    {
                        "name": "Taylor Hudson"
                    },
                    {
                        "name": "Claire Bonial"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    }
                ],
                "author_detail": {
                    "name": "Harish Tayyar Madabushi"
                },
                "author": "Harish Tayyar Madabushi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]